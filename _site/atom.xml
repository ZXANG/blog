<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-18T19:33:29+08:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">Blog</title><author><name>ZX</name></author><entry><title type="html">第一篇blog</title><link href="http://localhost:4000/something/2020/03/11/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87/" rel="alternate" type="text/html" title="第一篇blog" /><published>2020-03-11T00:00:00+08:00</published><updated>2020-03-11T00:00:00+08:00</updated><id>http://localhost:4000/something/2020/03/11/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87</id><content type="html" xml:base="http://localhost:4000/something/2020/03/11/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87/">&lt;blockquote&gt;
  &lt;p&gt;如何搭建该网站！.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;准备工作&quot;&gt;准备工作&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;github pages&lt;/li&gt;
  &lt;li&gt;jekyll&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;具体过程&quot;&gt;具体过程&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;github pages 创建
  首先新建仓库， 需要以&lt;code class=&quot;highlighter-rouge&quot;&gt;username.github.io&lt;/code&gt;作为仓库名。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;jekyll
  选择模版，可以在&lt;a href=&quot;http://jekyllthemes.org/&quot;&gt;Jekyll Themes&lt;/a&gt;选择自己喜欢的模版。我选择的是 &lt;a href=&quot;http://jekyllthemes.org/themes/jekyll-theme-next/&quot;&gt;Next&lt;/a&gt;
模版，之所以选择该模版，是因为可以按照&lt;a href=&quot;http://theme-next.simpleyyt.com/&quot;&gt;Next 使用文档&lt;/a&gt;一步一步进行操作，对于小白
来说比较友好。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;遇到问题&quot;&gt;遇到问题&lt;/h2&gt;
&lt;h3 id=&quot;jekyll安装-psmac-os&quot;&gt;jekyll安装， ps：mac os&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ruby版本过低
  解决方案，安装rvm更新ruby，Rvm是一个命令行工具，可以管理多个版本的Ruby。&lt;a href=&quot;https://www.jianshu.com/p/f56addf0c870&quot;&gt;Mac使用RVM更新Ruby&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;权限问题
  使用gem遇到 write permissions for the /usr/bin directory；
    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;gem install &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; /usr/local/bin jekyll
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&lt;/p&gt;</content><author><name>ZX</name></author><category term="something" /><summary type="html">如何搭建该网站！. 准备工作 github pages jekyll 具体过程 github pages 创建 首先新建仓库， 需要以username.github.io作为仓库名。 jekyll 选择模版，可以在Jekyll Themes选择自己喜欢的模版。我选择的是 Next 模版，之所以选择该模版，是因为可以按照Next 使用文档一步一步进行操作，对于小白 来说比较友好。 遇到问题 jekyll安装， ps：mac os ruby版本过低 解决方案，安装rvm更新ruby，Rvm是一个命令行工具，可以管理多个版本的Ruby。Mac使用RVM更新Ruby 权限问题 使用gem遇到 write permissions for the /usr/bin directory； sudo gem install -n /usr/local/bin jekyll I</summary></entry><entry><title type="html">第十六章 灰度部署 (金丝雀部署)</title><link href="http://localhost:4000/sre/2020/01/16/%E7%81%B0%E5%BA%A6%E9%83%A8%E7%BD%B2/" rel="alternate" type="text/html" title="第十六章 灰度部署 (金丝雀部署)" /><published>2020-01-16T00:00:00+08:00</published><updated>2020-01-16T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/16/%E7%81%B0%E5%BA%A6%E9%83%A8%E7%BD%B2</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/16/%E7%81%B0%E5%BA%A6%E9%83%A8%E7%BD%B2/">&lt;blockquote&gt;
  &lt;p&gt;发布工程是一个术语，用来描述从存储库中获取代码发布到生产环境中，之间相关的全部过程和所有组件。自动化发布可以帮助避免许多发布工程的传统缺陷: 重复性和手工任务的辛苦、手动流程的不一致性、无法了解上线的确切状态以及回滚困难。发布工程的自动化已经在其他文献中得到了很好的介绍——例如，关于持续集成和持续交付的书籍(CI/CD)。&lt;br /&gt;
  我们将灰度发布定义为：对服务进行部分且有时间限制的变更部署，并同时进行评估。该评估将帮助我们决定是否继续上线。变更的服务部分是&lt;code class=&quot;highlighter-rouge&quot;&gt;the canary&lt;/code&gt;，服务的其余部分是&lt;code class=&quot;highlighter-rouge&quot;&gt;the control&lt;/code&gt;。支持这种方法的逻辑是，灰度发布通常在线上进行小流量发布，或者影响比&lt;code class=&quot;highlighter-rouge&quot;&gt;the control&lt;/code&gt;部分少得多的用户上。灰度发布是一个有效的A/B测试过程。&lt;br /&gt;
  我们将首先介绍发布工程的基础知识，以及通过自动化发布来建立共享词汇的益处。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- more --&gt;
&lt;h2 id=&quot;发布工程原理&quot;&gt;发布工程原理&lt;/h2&gt;
&lt;p&gt;发布工程的基本原理如下:&lt;/p&gt;
&lt;h5 id=&quot;可再生构建&quot;&gt;可再生构建&lt;/h5&gt;
&lt;p&gt;构建系统应该能够接受构建输入(源代码、资产等)并生成相同结果。与上周相同的输入(构建代码)应该在本周产生相同的输出。&lt;/p&gt;
&lt;h5 id=&quot;自动化构建&quot;&gt;自动化构建&lt;/h5&gt;
&lt;p&gt;一旦代码上传之后，能够自动化生成构建组件并将其上传到存储系统。&lt;/p&gt;
&lt;h5 id=&quot;自动化测试&quot;&gt;自动化测试&lt;/h5&gt;
&lt;p&gt;一旦自动构建系统构建了组件，某种类型的测试套件应该确保它们正常工作。&lt;/p&gt;
&lt;h5 id=&quot;自动化部署&quot;&gt;自动化部署&lt;/h5&gt;
&lt;p&gt;部署应该由计算机执行，而不是人。&lt;/p&gt;
&lt;h5 id=&quot;小型部署&quot;&gt;小型部署&lt;/h5&gt;
&lt;p&gt;构建系统应该支持小的、自包含的更改。&lt;/p&gt;

&lt;p&gt;这些原则为运维人员带来直接收益:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;通过消除手工和重复的任务来减轻工程师的操作负担。&lt;/li&gt;
  &lt;li&gt;强制同行评审和版本控制，因为自动化通常是基于代码的。&lt;/li&gt;
  &lt;li&gt;建立一致的、可重复的、自动化的流程，从而减少错误。&lt;/li&gt;
  &lt;li&gt;添加对发布管道的监控，通过解决以下问题进行测量和持续改进:
    &lt;ul&gt;
      &lt;li&gt;–发布版本需要多长时间生产环境才生效?&lt;/li&gt;
      &lt;li&gt;–发布成功的频率是多少?一个成功的版本是一个没有严重缺陷或SLO违规的、客户可用的版本。&lt;/li&gt;
      &lt;li&gt;–可以做哪些更改来尽早的捕获管道中的缺陷?&lt;/li&gt;
      &lt;li&gt;–哪些步骤可以并行化或进一步优化?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CI/CD与发布自动化相结合可以持续改进开发周期，如图16-1所示。当发布自动化时，你可以更频繁地发布。对于变更率很高的软件来说，更频繁地发布意味着在任何给定的发布工件中捆绑更少的更改。而更小的、自包含的发布工件使得在出现bug时回滚任何给定的发布工件变得成本更低、更容易。更快的发布节奏意味着可以更快地修复bug。
&lt;img src=&quot;/blog/something/images/SRE/16-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;平衡发布速度和可靠性&quot;&gt;平衡发布速度和可靠性&lt;/h2&gt;

&lt;p&gt;快速发布(以下称为&lt;code class=&quot;highlighter-rouge&quot;&gt;发布&lt;/code&gt;)和可靠性常常被视为相反的目标。企业希望以100%的可靠性尽快发布新特性和产品改进!然而这个目标是不可能实现的(因为100%从来不是可靠性的正确目标;参见第2章)，但可以在满足特定产品的特定可靠性目标的同时，尽可能快地进行交付。&lt;/p&gt;

&lt;p&gt;实现这个目标的第一步是了解发布对软件可靠性的影响。在谷歌的经验,大多数事件都是由二进制或配置推送导致的(见附录C)。许多类型的软件更改都可能导致系统故障 - 例如，底层组件的行为更改，依赖关系（例如API）的更改，或DNS等配置更改。&lt;/p&gt;

&lt;p&gt;尽管对软件进行变更存在固有的风险，但是这些变更(bug修复、安全补丁和新特性)对业务的成功是必需的。你可以使用SLOs和错误预算的概念来衡量发布新版本对可靠性的影响，而不是提倡反对变更。你的目标应该是在满足用户期望的可靠性目标的同时尽快发布软件。下一节将讨论如何使用canary流程来实现这些目标。&lt;/p&gt;

&lt;h3 id=&quot;分离变更频率不同的组件&quot;&gt;分离变更频率不同的组件&lt;/h3&gt;

&lt;p&gt;服务由具有不同变更频率的多个组件组成:二进制文件或代码、基础环境(如JVM、内核/OS)、库、服务配置或标志、特性/测试配置和用户配置。如果只有一种发布变更的方法，那么这些组件单独变更会比较困难。&lt;/p&gt;

&lt;p&gt;特性标志或测试框架(如Gertrude、Feature和PlanOut)允许你将特性启动从二进制版本中分离出来。如果二进制版本包含多个特性，你可以通过更改测试配置一次启用一个特性。这样，就没有必要将这些小的变更集合为一个大的变更，或者为每个特性执行单独的版本。更重要的是，如果只有一些新特性的行为不像预期的那样，你可以选择性地禁用这些特性，直到下一个构建/发布周期可以部署新的二进制文件为止。&lt;/p&gt;

&lt;p&gt;你可以将特性标志/试验原则应用于服务的任何类型的更改，而不仅仅是软件版本。&lt;/p&gt;

&lt;h2 id=&quot;canarying是什么&quot;&gt;Canarying是什么？&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Canarying&lt;/code&gt;一词是指将金丝雀带入煤矿以确定该矿是否对人类安全的做法。 由于鸟类比人类更小，呼吸更快，因此它们被危险气体毒害的速度比人类更快。&lt;/p&gt;

&lt;p&gt;即使你的发布管道是完全自动化的，在真正的流量到达服务之前，你依然无法检测到所有与发布相关的缺陷。当一个发布版本准备好部署到生产环境中时，你的测试策略应该充分保证该版本是安全的，并且按预期工作。然而，测试环境与生产环境并不是100%相同的，并且测试不可能会涵盖100％的场景。依然会存在一些会影响生产缺陷。如果一个版本立即部署到系统的全部地方，那么可能存在的缺陷亦将到达系统的全部地方。&lt;/p&gt;

&lt;p&gt;如果你能够快速地检测和解决缺陷，则可以接受此方案。但是，更安全的选择是:首先使用灰度发布向新版本导入一些生产流量。灰度发布允许部署管道在尽可能少地影响你的服务的前提下，更快地检测出问题。&lt;/p&gt;

&lt;h2 id=&quot;发布工程和灰度发布&quot;&gt;发布工程和灰度发布&lt;/h2&gt;

&lt;p&gt;在部署系统的新版本或其关键组件(如配置或数据)时，我们将变更(通常未公开给真实输入的更改，如面向用户的流量、或用户提供的批处理数据)打包。变更会带来新的特性和功能，但也存在部署之后出现问题的风险。我们的目标是通过测试一小部分流量来降低风险，以确保没有任何不良影响。我们将在本章后面讨论评估过程。&lt;/p&gt;

&lt;p&gt;灰度过程还让我们对变更充满信心，因为我们将其暴露给越来越大的流量。为变更引入实际生产流量还使我们能够识别在单元测试或负载测试等测试框架中可能不可见的问题，这些问题通常更为人为。&lt;/p&gt;

&lt;p&gt;我们将使用一个实际的示例来检查灰度过程及其评估，同时避免深入研究统计数据。相反，我们关注点是整个过程和典型的实际考虑。我们使用App Engine上的一个简单应用程序来说明发布的各个方面。&lt;/p&gt;

&lt;h3 id=&quot;灰度发布流程的需求&quot;&gt;灰度发布流程的需求&lt;/h3&gt;

&lt;p&gt;针对特定服务的灰度发布需要特定功能：
将变更通过灰度发布部署到服务全部子集的方法。      &lt;br /&gt;
一个评估过程，用来评估变更是&lt;code class=&quot;highlighter-rouge&quot;&gt;好&lt;/code&gt;还是&lt;code class=&quot;highlighter-rouge&quot;&gt;坏&lt;/code&gt;。
将评估集成到发布过程中。&lt;/p&gt;

&lt;p&gt;最后，当灰度检测到有问题的发布版本，并在没有误报的情况下识别出好的发布版本时，灰度发布展示了它的价值。&lt;/p&gt;

&lt;h3 id=&quot;我们的示例环境&quot;&gt;我们的示例环境&lt;/h3&gt;

&lt;p&gt;我们将使用一个简单的前端web服务应用程序来演示一些灰度发布的概念。该应用程序提供了一个基于http的API，消费者可以使用它来操作各种数据(如产品价格等简单信息)。示例应用程序有一些可调参数，我们可以使用这些参数来模拟各种生产环境，由灰度发布流程进行评估。例如，可以让应用程序为20%的请求返回错误，或者规定5%的请求至少需要两秒钟。&lt;/p&gt;

&lt;p&gt;我们使用部署在谷歌应用程序引擎上的应用程序来演示灰度发布流程，这些原则同样适用于其他环境。虽然示例应用程序是经过设计的，但是在实际场景中，类似的应用程序与我们的示例可以共享灰度发展中使用的指标。&lt;/p&gt;

&lt;p&gt;我们的示例服务有两个可能的版本:当前版本和候选版本。当前版本是当前部署在生产环境中的版本，而候选版本是新构建的版本。使用这两个版本来说明发布概念，以及如何实现灰度发布以使发布过程更安全。&lt;/p&gt;

&lt;h2 id=&quot;回滚部署与简单的canary部署比较&quot;&gt;回滚部署与简单的Canary部署比较&lt;/h2&gt;

&lt;p&gt;我们将在发生中断时根据错误预算节省和一般影响，来对没有灰度发布的部署流程和灰度发布流程进行比较。我们的部署过程以开发环境为基础。一旦我们感觉代码在开发环境中正常工作，我们就将该版本部署到生产环境中。&lt;/p&gt;

&lt;p&gt;在部署之后不久，监视开始报高错误率(参见图16-2，在图16-2中，为了模拟示例服务中的缺陷，对示例应用程序进行配置以使20％的请求失败)。对于示例，假设部署流程不支持回滚到以前已知的配置正常的版本时。修复这些错误的最佳选择就只有在生产版本中查找缺陷，对其进行补救，并在停机期间重新部署一个新版本。这种做法肯定会延长错误对用户的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/16-2.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图 16-2 部署之后错误率增加 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;为了改进这个初始部署过程，我们可以在使用灰度发布来减少推送错误代码所造成的影响。 我们需要一种方法来在小部分生产环境中运行候选版本，而不是一次性部署到生产环境。 然后将一小部分流量发送到该生产环境（the canary金丝雀）并将其与其他部分（the control 主控）进行比较。 使用此方法，我们可以在所有生产受到影响之前发现候选版本中的缺陷。&lt;/p&gt;

&lt;p&gt;我们在示例应用程序中的进行简单灰度发布，在应用程序的特定版本之间分配流量。 您可以使用App Engine或其他任何方法来分割流量（例如负载均衡器上的后端权重，代理配置或循环DNS记录）。&lt;/p&gt;

&lt;p&gt;图16-3显示了当我们使用灰度发布，变更的影响会大大降低;事实上，这些错误几乎不可见!这提出了一个有趣的问题:与总体流量趋势相比，灰度发布的流量趋势很难看到和跟踪。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/16-3.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图 16-3 部署之后错误率增canary部署错误率； 因为进行canary部署的只是系统的一小部分，因此总体错误率降低 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;为了更清楚地了解需要在合理范围内跟踪的错误，我们可以通过App Engine应用程序版本查看关键指标(HTTP响应代码)，如图16-4所示。当我们查看每个版本的分解趋势图时，我们可以清楚地看到新版本引入的错误。我们还可以从图16-4中观察到当前版本提供的错误非常少。&lt;/p&gt;

&lt;p&gt;现在，我们可以根据应用程序版本的HTTP错误率对部署进行调优。如果灰度发布的错误率大于全部系统的错误率，这表明canary部署是&lt;code class=&quot;highlighter-rouge&quot;&gt;糟糕的&lt;/code&gt;。我们应该暂停并回滚部署，或者联系他人来帮助解决问题。如果错误率相似，我们可以正常地进行部署。在图16-4中，我们的canary部署显然很糟糕，我们应该回滚它。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/16-4.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图 16-4 应用程序HTTP响应码； 新版本产生多数错误、当前版本产生小数错误（图中显示10%的log） &lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&quot;canary实施&quot;&gt;Canary实施&lt;/h2&gt;

&lt;p&gt;现在我们已经看到了一个相当简单的canary部署实现，接下来让我们更深入地了解成功的canary流程所需的参数。&lt;/p&gt;

&lt;h3 id=&quot;最小化slos和错误预算的风险&quot;&gt;最小化SLOs和错误预算的风险&lt;/h3&gt;

&lt;p&gt;第2章讨论了SLOs如何反映设计服务可用性的业务需求。这些需求也可以通过canary实现。canary进程的风险仅仅是我们错误预算的一小部分，它受到时间和canary规模大小的限制。&lt;/p&gt;

&lt;p&gt;全局部署会很快将SLO置于危险之中。如果实例中为系统全面部署候选版本，我们将面临20%的请求失败的风险。如果我们使用5%的canary规模，我们将为5%的流量提供20%错误，导致1%的总体错误率(如图16-3所示)。这个策略允许我们保留我们的错误预算—预算的影响与暴露于缺陷的流量的数量成正比。我们可以假设，对于全局部署和灰度部署，检测和回滚花费的时间差不多，但是当我们将灰度发布集成到部署过程中时，我们会以更低的成本获得有关新版本的有价值信息。&lt;/p&gt;

&lt;p&gt;这是一个假设负载均匀的极简模型。它还假设我们可以将整个错误预算用于灰度发布。这里我们只考虑新版本引入的不可用性，而不是实际可用性。我们的模型还假设新版本具有100%的失败率，这是最坏的情况。而进行灰度的部分不会导致线上系统100%不可用。我们还允许在灰度部署期间，整个系统的可用性低于SLO。&lt;/p&gt;

&lt;p&gt;这个模型有明显的缺陷，但它是一个可靠的起点，你可以根据业务需求进行调整。我们建议使用最简单的模型来满足你的技术和业务目标。根据我们的经验，专注于使模型在技术上尽可能正确，常常会导致在建模上的过度投资。对于具有高复杂性的服务，过于复杂的模型可能导致持续的模型调优，而没有真正的好处。&lt;/p&gt;

&lt;h3 id=&quot;选择灰度规模和持续时间&quot;&gt;选择灰度规模和持续时间&lt;/h3&gt;

&lt;p&gt;选择合适的灰度持续时间，需要考虑发布频率。 如果需要每天发布，那么在一次只运行一个灰度的情况下，无法使灰度保持一周,如果每周部署一次，就可以执行较长的灰度发布。 如果持续部署（例如，一天20次），灰度的持续时间必须明显缩短。 在一些说明里，虽然可以同时运行多个灰度，但这样做会增加大量精力来跟踪系统状态。 在任何情况下，需要快速推断系统状态时，同时运行多个灰度会成为问题。如果灰度重叠，同时运行多个灰度也会增加信号污染的风险。我们强烈建议一次只运行一个灰度。&lt;/p&gt;

&lt;p&gt;对于基本的评估，不需要大规模的灰度来检测关键条件。然而，一个有代表性的灰度发布流程需要跨多个维度进行决策:&lt;/p&gt;

&lt;h2 id=&quot;规模和持续时间&quot;&gt;规模和持续时间&lt;/h2&gt;

&lt;p&gt;它的规模应够大，持续时间应够长，足以代表整个部署。仅在接收到少量查询后终止canary部署，对于以具有不同功能的不同查询为特征的系统来说，这无法提供有用的信号。处理率越高，获取代表性样本所需的时间就越少，以确保所观察到的行为实际上是由变更引起的，而不仅仅是随机因素。&lt;/p&gt;

&lt;p&gt;流量&lt;/p&gt;

&lt;p&gt;我们需要在系统上接收足够的流量，以确保它是一个具有代表性的示例，并且系统有机会对输入做出负面反应。通常，请求越均匀，所需要的流量就越少。&lt;/p&gt;

&lt;p&gt;时间点&lt;/p&gt;

&lt;p&gt;性能缺陷通常只在高负载下出现，因此在非高峰时间部署可能不会触发性能相关的缺陷。&lt;/p&gt;

&lt;p&gt;度量指标&lt;/p&gt;

&lt;p&gt;灰度的代表性与我们选择评估的指标密切相关(我们将在本章后面讨论)。我们可以快速评估诸如查询成功之类的琐碎指标，但是其他指标(如队列深度)可能需要更多的时间或较大规模的灰度来提供清晰的信号。&lt;/p&gt;

&lt;p&gt;但问题是，这些要求可能相互冲突。Canarying是一种平衡行为，它通过对最坏情况的冷静分析和系统过去的实际记录来实现。一旦您从过去的灰度中收集了指标，您就可以根据典型的canary评估失败率而不是假想的最坏情况来选择canary参数。&lt;/p&gt;

&lt;h3 id=&quot;选择和评估度量标准&quot;&gt;选择和评估度量标准&lt;/h3&gt;

&lt;p&gt;到目前为止，我们一直在研究成功率，这是评估灰度发布的一个非常清晰和明显的指标。但是直觉上，我们知道这个单一的指标对于有意义的canary流程来说是不够的。如果我们以10倍的延迟为所有请求提供服务，或者在这样做时使用10倍的内存，那么我们可能也会遇到问题。并不是所有的指标都适合评估灰度发布。哪些指标最适合评估灰度发布版本是好是坏?&lt;/p&gt;

&lt;h3 id=&quot;度量标准应指出问题&quot;&gt;度量标准应指出问题&lt;/h3&gt;

&lt;p&gt;首先，指标需要能够指出服务中的问题。这很棘手，因为构成&lt;code class=&quot;highlighter-rouge&quot;&gt;问题&lt;/code&gt;的并不总是客观的。我们可能会认为用户请求失败是有问题的。但是如果一个请求的响应时间增加了10%，或者系统内存增加了10%?，这该如何判断？我们通常建议使用sla作为开始考虑canary指标的地方。良好的服务质量指数往往与服务健康状况密切相关。如果已经使用SLIs来度量SLO是否符合，那么我们可以重用这些工作。&lt;/p&gt;

&lt;p&gt;几乎任何指标在极端情况下都可能出现问题，但是向灰度流程中添加太多的指标也会产生成本。我们需要为每个指标正确定义可接受行为。如果可接受行为定义过于严格，我们会得到大量的误报;也就是说，我们会认为灰度很糟糕，即使实际不是这样。相反，如果对可接受行为的定义过于宽松，我们更有可能忽略掉有问题的灰度部署。正确选择什么是可接受的行为可能会成本较大——既耗时又需要分析。然而，如果做得不好，错误的结果会完全误导你。此外，随着服务、其特性集和行为的发展，您需要定期重新评估期望。&lt;/p&gt;

&lt;p&gt;我们应该根据这些指标多大程度上能够表明系统中实际用户的体验来进行排名，选择排名靠前的几个指标(可能不超过12个)。太多的度量标准会带来递减的回报，并且在某种程度上，收益会被维护它们的成本所抵消，或者在发布过程中如果不维护它们，会对发布结果无法保证100%的信任。&lt;/p&gt;

&lt;p&gt;为了使这个指导原则更加具体，让我们回头再来看示例。它有许多我们可以评估的指标:CPU使用量、内存占用、HTTP返回码(2xx、3xx等等)、响应延迟、正确性等等。在这种情况下，我们最好的度量标准可能是HTTP返回码和响应延迟，因为它们的降级最接近于实际用户影响。在这个场景中，CPU使用率并没有那么有用:资源使用的增加不一定会影响服务，并且可能导致不稳定或嘈杂的canary进程。这会导致操作人员禁用或忽略canary进程，这会首先破坏使用canary进程的目的。对于前端服务，我们直观地知道，响应较慢或响应失败通常会真实反映服务中存在的问题。&lt;/p&gt;

&lt;p&gt;HTTP返回码包含一些有趣的复杂情况，例如状态码404，它告诉我们没有找到资源。这可能是因为用户获得了错误的URL(想象一下在一个流行的论坛上分享了一个错误的URL)，或者因为服务器错误地停止了对资源的服务。通常，我们可以通过排除canary评估中的400级状态码，并添加黑盒监控来测试特定URL的存在，从而解决此类问题。然后，我们可以将黑盒数据作为canary分析的一部分，以帮助将canary流程与奇怪的用户行为隔离开来。&lt;/p&gt;

&lt;p&gt;度量标准应该具有代表性和可归属性&lt;/p&gt;

&lt;p&gt;观察到的指标变化其来源，应该清楚地归因于正在进行的变更，并且不应该受到外部因素的影响。&lt;/p&gt;

&lt;p&gt;在一个大的系统中(例如，许多服务器或许多容器)，我们可能会有外部性——超过连接的机器、运行具有不同性能特征的不同内核的机器，或者网络中过载的机器。此时金丝雀部分和主系统部分之间的差异，既是我们所部署的两个基础设施之间的差异，也会是我们变更导致的差异。&lt;/p&gt;

&lt;p&gt;管理金丝雀是多种力量之间的平衡。增加金丝雀的规模是减少这个问题影响的方法(如前所述)。当我们的系统达到我们认为的合理的金丝雀规模时，我们需要考虑我们选择的指标是否会显示出很大的差异。&lt;/p&gt;

&lt;p&gt;我们还应该知道canary和control环境之间共享的失败域;坏金丝雀会对控制产生负面影响，而系统中的坏行为可能会导致我们错误地评估金丝雀。同样，确保您的度量标准是良好隔离的。考虑一个同时运行我们的应用程序和其他进程的系统。整个系统的CPU使用量的急剧增加会导致糟糕的度量，因为系统中的其他进程(数据库负载、日志轮转等)可能会导致这种增加。更好的度量标准是在处理请求时所花费的CPU时间。更好的度量标准是在服务进程实际计划在CPU上的时间窗口上为处理请求服务所花费的CPU时间。虽然与我们的进程相关的严重超额的机器显然是一个问题(监控应该捕捉到它!)，但它不是由我们正在进行的更改引起的，因此不应该将其标记为金丝雀部署失败。&lt;/p&gt;

&lt;p&gt;金丝雀也需要是可归属的;也就是说，您还应该能够将canary度量与SLIs联系起来。如果一个度量可以在不影响服务的情况下发生巨大变化，那么它不适合用来评估灰度发布。&lt;/p&gt;

&lt;h3 id=&quot;评估前评估后依然是有风险的&quot;&gt;评估前/评估后依然是有风险的&lt;/h3&gt;

&lt;p&gt;canary过程的前后是归因问题的延伸。在这个过程中，旧系统被新系统完全替代，你的canary评估将在一段时间内比较变更之前和之后的系统行为。你可以将此过程称为&lt;code class=&quot;highlighter-rouge&quot;&gt;时空中的canary部署&lt;/code&gt;，在此过程中，您通过分割时间来选择A/B组，而不是通过机器、cookie或其他方法来分割总体。由于时间是观察到的指标变化的最大来源之一，因此很难在评估之前/之后来判断性能是否下降。&lt;/p&gt;

&lt;p&gt;虽然canary部署可能导致降级，但原有系统本身也可能会降级。如果需要长时间运行canary部署，就会变得更加复杂。例如，如果在周一进行发布，可能会将工作日的行为与周末的行为进行比较，从而引入大量噪音。在该示例中，用户可能在周末以不同的方式访问该服务。从而在canary进程中引入噪音。&lt;/p&gt;

&lt;p&gt;评估前/后过程本身引入了一个问题，即大而短的错误率(由前/后评估引入)是否优于小而长的错误率(由一个小金丝雀引入)。如果新版本完全被破坏，我们能多快地检测和恢复? 大规模的金丝雀之前/之后可以更快地检测到问题，但恢复的总体时间可能仍然相当长，与较小的金丝雀类似。在此期间，用户会一直受到影响。&lt;/p&gt;

&lt;h3 id=&quot;使用渐进的灰度会更好&quot;&gt;使用渐进的灰度会更好&lt;/h3&gt;

&lt;p&gt;选择的度量标准即使不符合我们理想中的属性，但仍然很有价值。我们可以通过使用更细微的灰度过程来介绍这些指标。&lt;/p&gt;

&lt;p&gt;我们可以使用包含多个阶段的canary来反映我们对度量的推理能力，而不是简单地评估单个canary阶段。在第一阶段，我们对这个版本没有信心或不了解。因此，我们希望使用一个小的阶段，以尽量减少负面影响。在小型灰度中，我们更喜欢能够最清晰地显示问题的指标——应用程序崩溃、请求失败等等。一旦这一阶段成功地过去，下一阶段将增加灰度规模，从而增强我们分析变化影响的信心。&lt;/p&gt;

&lt;h2 id=&quot;依赖和隔离&quot;&gt;依赖和隔离&lt;/h2&gt;

&lt;p&gt;正在测试的系统不会在完全真空中运行。出于实际原因，灰度和主系统可以共享后端、前端、网络、数据存储和其他基础设施。甚至可能与客户端有非常不明显的交互。例如，假设一个客户端发送了两个连续的请求。第一个请求可以由灰度部分来处理。其响应可能会改变第二个请求的内容，第二个请求可能会落在主系统部分，从而改变主系统的行为。&lt;/p&gt;

&lt;p&gt;不完美的隔离会带来几个后果。最重要的是，我们需要知道，如果灰度过程的结果表明我们应该停止生产变更并调查情况，那么灰度并不一定是错误的。这一事实对于一般的canarying来说是正确的，但是在实践中，它经常由于隔离问题而导致被强制执行。&lt;/p&gt;

&lt;p&gt;此外，不完美的隔离意味着灰度部署的错误行为也会对原始系统产生负面影响。Canarying是A/B比较，A和B有可能同时改变;这可能会导致评估灰度变得混乱。还必须使用绝对度量，例如定义的SLOs，以确保系统正确运行。&lt;/p&gt;

&lt;h2 id=&quot;在非交互系统中进行canarying&quot;&gt;在非交互系统中进行Canarying&lt;/h2&gt;

&lt;p&gt;本章重点讨论了交互式请求/响应系统，它在许多方面是最简单和最常讨论的系统设计。其他系统，如异步处理管道，也同样重要，但有不同的canarying注意事项，我们将简要列举。有关数据处理管道的canarying的更多信息，请参见第13章。&lt;/p&gt;

&lt;p&gt;首先，canary的持续时间和部署本质上依赖于工作单元处理的持续时间。当涉及到交互系统时，我们忽略了这个因素，假设工作单元处理的时间不会超过几秒钟，这比canary的持续时间要短。非交互式系统中的工作单元处理(如呈现管道或视频编码)可能需要更长的时间。因此，确保canary持续时间至少跨越单个工作单元的持续时间。&lt;/p&gt;

&lt;p&gt;对于非交互式系统，隔离可能变得更加复杂。许多管道系统只有一个工作分配程序和一组使用应用程序代码的工作人员。在多阶段管道中，工作单元由工作人员处理，然后返回到池中，由同一工作人员或另一个工作人员执行下一阶段的处理。金丝雀分析有助于确保处理特定工作单元的工人总是从相同的工人池中提取——要么是金丝雀池，要么是控制池。否则，信号就会变得越来越混杂(有关理清信号的需要的更多信息，请参见349页的&lt;code class=&quot;highlighter-rouge&quot;&gt;监视数据的要求&lt;/code&gt;)。&lt;/p&gt;

&lt;p&gt;最后，度量标准的选择可能更加复杂。我们可能感兴趣的是端到端处理工作单元的时间(类似于交互系统中的延迟)，以及处理本身的质量(当然，这是完全特定于应用程序的)。&lt;/p&gt;

&lt;p&gt;考虑到这些警告，canarying的一般概念仍然是可行的，并且适用相同的高级原则。&lt;/p&gt;

&lt;h2 id=&quot;监控要求&quot;&gt;监控要求&lt;/h2&gt;

&lt;p&gt;在评估灰度部署时，您必须能够将部署了灰度的系统与未部署灰度的系统进行比较。通常，这需要在构造监视系统时多加注意—有效的比较非常简单，并且能够产生有意义的结果。&lt;/p&gt;

&lt;p&gt;考虑之前的例子，在5%的规模中进行灰度，错误率为20%。因为监视很可能将系统作为一个整体来观察，所以它只能检测到1%的总体错误率。根据系统的不同，这个信号可能与其他错误源无法区分(参见图16-3)。&lt;/p&gt;

&lt;p&gt;如果我们通过按照服务请求的对象来（金丝雀与主系统）分解指标，(参见图16-4)我们可以清楚地看到主系统与canary之间的错误率，这清楚地说明了全局部署将带来什么。在这里，我们看到，对整个服务的监控不足以分析灰度是否ok。在收集监视数据时，能够执行细粒度的分解非常重要，这些分解使得能够区分金丝雀和主系统的指标。&lt;/p&gt;

&lt;p&gt;收集指标的另一个难点是金丝雀的部署受到设计的时间限制。当度量指标在特定时期内进行聚合时，这可能会导致问题。考虑每小时的度量误差。我们可以通过对过去一小时的请求求和来计算这个度量。如果我们使用这个度量来评估我们的canary，我们可能会遇到问题，如下面的时间表所述:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;某些事件会导致一些错误发生。&lt;/li&gt;
  &lt;li&gt;一只金丝雀被部署在5%的人口中;金丝雀的持续时间是30分钟。&lt;/li&gt;
  &lt;li&gt;canary系统开始监视每小时的错误度量，以确定部署是好是坏。&lt;/li&gt;
  &lt;li&gt;部署被检测为错误，因为每小时的错误度量与控制总体的每小时的错误显著不同。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;此场景是使用每小时计算一次的度量来评估仅30分钟长的部署的结果。因此，canary进程提供了一个非常模糊的信号。当使用度量来评估canary的成功时，确保度量的间隔与canary的持续时间相同或小于持续时间。&lt;/p&gt;

&lt;p&gt;相关概念&lt;/p&gt;

&lt;p&gt;通常，与客户的对话涉及到在生产中使用蓝/绿部署、人工负载生成和/或流量测试。这些概念类似于canarying，因此虽然它们不是严格意义上的金丝雀流程，但亦可使用。&lt;/p&gt;

&lt;h3 id=&quot;蓝绿部署&quot;&gt;蓝/绿部署&lt;/h3&gt;

&lt;p&gt;蓝/绿部署维护系统的两个实例：一个提供流量（绿色），另一个准备提供流量（蓝色）。 在蓝色环境中部署新版本后，将流量切换到其中。切换过程不需要停机，并且回滚只是简单逆转路由器而已。 一个缺点是该设置使用的资源是&lt;code class=&quot;highlighter-rouge&quot;&gt;传统&lt;/code&gt;部署的两倍。在该设置中，您正在有效地执行前/后金丝雀（前面已讨论过）。&lt;/p&gt;

&lt;p&gt;通过同时(而不是分开地)使用蓝/绿部署，您可以或多或少地将蓝色/绿色部署用作常规的金丝雀。在此策略中，您可以将canary部署到blue(备用)实例，并在绿色和蓝色环境之间缓慢地分配流量。您的评估和比较蓝色环境和绿色环境的指标都应该与流量控制相关。这种设置类似于A/B金丝雀，此时绿色环境是主系统，蓝色环境是金丝雀部署，金丝雀数量由发送到每个金丝雀的流量控制。&lt;/p&gt;

&lt;h3 id=&quot;人工负载生成&quot;&gt;人工负载生成&lt;/h3&gt;

&lt;p&gt;与其将实时用户流量暴露给canary部署，还不如在安全性方面犯点错误，使用人工负载。通常，您可以在多个部署阶段(QA、预生产，甚至在生产中)运行负载测试。虽然根据我们的定义，这些操作不符合canarying，但是它们仍然是找到缺陷的可行方法，但需要注意一些事项。&lt;/p&gt;

&lt;p&gt;使用人工负载进行测试可以很好地最大化代码覆盖率，但不能提供良好的状态覆盖率。在可变系统(具有缓存、cookie、请求关联等的系统)中人工模拟负载尤其困难。人工负载也可能无法准确地模拟真实系统中流量变化。有些问题可能只在无人工负载的情况下出现，从而导致覆盖率有所差距。&lt;/p&gt;

&lt;p&gt;人工负载在可变系统中也很难工作。例如，试图在计费系统上生成人工负载可能非常危险:系统可能开始向信用卡供应商发送呼叫，然后信用卡供应商将开始主动向客户收费。虽然我们可以避免测试危险的代码逻辑，但是在这些逻辑上缺乏测试会降低我们的测试覆盖率。&lt;/p&gt;

&lt;h3 id=&quot;流量测试&quot;&gt;流量测试&lt;/h3&gt;

&lt;p&gt;如果人工流量不具有代表性，我们可以复制流量并将其发送到生产系统和测试环境。这种技术被称为流量镜像。生产系统服务于实际流量并响应请求，canary部署服务于副本流量并丢弃响应。您甚至可以将canary响应与实际响应进行比较，并运行进一步的分析。&lt;/p&gt;

&lt;p&gt;这种策略可以提供有代表性的流量，但通常比更直接的canary流程更复杂。在有状态系统中，流量测试也不能充分识别风险;流量副本可能会在看似独立的部署之间引入意外的影响。例如，如果canary部署和生产系统共享一个缓存，人为导致的缓存命中率增加将使canary指标的性能度量无效。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;p&gt;您可以使用许多工具和方法来自动化版本发布，并将canarying引入到发布管道中。没有一种测试方法是万能的，测试策略应该由系统的需求和行为决定。Canarying可以作为一种简单、健壮且易于集成的方法来补充测试。当您及早发现系统缺陷时，用户受到的影响最小。Canarying还可以为频繁发布提供信心，并提高开发速度。正如测试方法必须随着系统需求和设计而发展一样，canarying也必须如此。&lt;/p&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">发布工程是一个术语，用来描述从存储库中获取代码发布到生产环境中，之间相关的全部过程和所有组件。自动化发布可以帮助避免许多发布工程的传统缺陷: 重复性和手工任务的辛苦、手动流程的不一致性、无法了解上线的确切状态以及回滚困难。发布工程的自动化已经在其他文献中得到了很好的介绍——例如，关于持续集成和持续交付的书籍(CI/CD)。 我们将灰度发布定义为：对服务进行部分且有时间限制的变更部署，并同时进行评估。该评估将帮助我们决定是否继续上线。变更的服务部分是the canary，服务的其余部分是the control。支持这种方法的逻辑是，灰度发布通常在线上进行小流量发布，或者影响比the control部分少得多的用户上。灰度发布是一个有效的A/B测试过程。 我们将首先介绍发布工程的基础知识，以及通过自动化发布来建立共享词汇的益处。 发布工程原理 发布工程的基本原理如下: 可再生构建 构建系统应该能够接受构建输入(源代码、资产等)并生成相同结果。与上周相同的输入(构建代码)应该在本周产生相同的输出。 自动化构建 一旦代码上传之后，能够自动化生成构建组件并将其上传到存储系统。 自动化测试 一旦自动构建系统构建了组件，某种类型的测试套件应该确保它们正常工作。 自动化部署 部署应该由计算机执行，而不是人。 小型部署 构建系统应该支持小的、自包含的更改。 这些原则为运维人员带来直接收益: 通过消除手工和重复的任务来减轻工程师的操作负担。 强制同行评审和版本控制，因为自动化通常是基于代码的。 建立一致的、可重复的、自动化的流程，从而减少错误。 添加对发布管道的监控，通过解决以下问题进行测量和持续改进: –发布版本需要多长时间生产环境才生效? –发布成功的频率是多少?一个成功的版本是一个没有严重缺陷或SLO违规的、客户可用的版本。 –可以做哪些更改来尽早的捕获管道中的缺陷? –哪些步骤可以并行化或进一步优化? CI/CD与发布自动化相结合可以持续改进开发周期，如图16-1所示。当发布自动化时，你可以更频繁地发布。对于变更率很高的软件来说，更频繁地发布意味着在任何给定的发布工件中捆绑更少的更改。而更小的、自包含的发布工件使得在出现bug时回滚任何给定的发布工件变得成本更低、更容易。更快的发布节奏意味着可以更快地修复bug。 平衡发布速度和可靠性 快速发布(以下称为发布)和可靠性常常被视为相反的目标。企业希望以100%的可靠性尽快发布新特性和产品改进!然而这个目标是不可能实现的(因为100%从来不是可靠性的正确目标;参见第2章)，但可以在满足特定产品的特定可靠性目标的同时，尽可能快地进行交付。 实现这个目标的第一步是了解发布对软件可靠性的影响。在谷歌的经验,大多数事件都是由二进制或配置推送导致的(见附录C)。许多类型的软件更改都可能导致系统故障 - 例如，底层组件的行为更改，依赖关系（例如API）的更改，或DNS等配置更改。 尽管对软件进行变更存在固有的风险，但是这些变更(bug修复、安全补丁和新特性)对业务的成功是必需的。你可以使用SLOs和错误预算的概念来衡量发布新版本对可靠性的影响，而不是提倡反对变更。你的目标应该是在满足用户期望的可靠性目标的同时尽快发布软件。下一节将讨论如何使用canary流程来实现这些目标。 分离变更频率不同的组件 服务由具有不同变更频率的多个组件组成:二进制文件或代码、基础环境(如JVM、内核/OS)、库、服务配置或标志、特性/测试配置和用户配置。如果只有一种发布变更的方法，那么这些组件单独变更会比较困难。 特性标志或测试框架(如Gertrude、Feature和PlanOut)允许你将特性启动从二进制版本中分离出来。如果二进制版本包含多个特性，你可以通过更改测试配置一次启用一个特性。这样，就没有必要将这些小的变更集合为一个大的变更，或者为每个特性执行单独的版本。更重要的是，如果只有一些新特性的行为不像预期的那样，你可以选择性地禁用这些特性，直到下一个构建/发布周期可以部署新的二进制文件为止。 你可以将特性标志/试验原则应用于服务的任何类型的更改，而不仅仅是软件版本。 Canarying是什么？ Canarying一词是指将金丝雀带入煤矿以确定该矿是否对人类安全的做法。 由于鸟类比人类更小，呼吸更快，因此它们被危险气体毒害的速度比人类更快。 即使你的发布管道是完全自动化的，在真正的流量到达服务之前，你依然无法检测到所有与发布相关的缺陷。当一个发布版本准备好部署到生产环境中时，你的测试策略应该充分保证该版本是安全的，并且按预期工作。然而，测试环境与生产环境并不是100%相同的，并且测试不可能会涵盖100％的场景。依然会存在一些会影响生产缺陷。如果一个版本立即部署到系统的全部地方，那么可能存在的缺陷亦将到达系统的全部地方。 如果你能够快速地检测和解决缺陷，则可以接受此方案。但是，更安全的选择是:首先使用灰度发布向新版本导入一些生产流量。灰度发布允许部署管道在尽可能少地影响你的服务的前提下，更快地检测出问题。 发布工程和灰度发布 在部署系统的新版本或其关键组件(如配置或数据)时，我们将变更(通常未公开给真实输入的更改，如面向用户的流量、或用户提供的批处理数据)打包。变更会带来新的特性和功能，但也存在部署之后出现问题的风险。我们的目标是通过测试一小部分流量来降低风险，以确保没有任何不良影响。我们将在本章后面讨论评估过程。 灰度过程还让我们对变更充满信心，因为我们将其暴露给越来越大的流量。为变更引入实际生产流量还使我们能够识别在单元测试或负载测试等测试框架中可能不可见的问题，这些问题通常更为人为。 我们将使用一个实际的示例来检查灰度过程及其评估，同时避免深入研究统计数据。相反，我们关注点是整个过程和典型的实际考虑。我们使用App Engine上的一个简单应用程序来说明发布的各个方面。 灰度发布流程的需求 针对特定服务的灰度发布需要特定功能： 将变更通过灰度发布部署到服务全部子集的方法。 一个评估过程，用来评估变更是好还是坏。 将评估集成到发布过程中。 最后，当灰度检测到有问题的发布版本，并在没有误报的情况下识别出好的发布版本时，灰度发布展示了它的价值。 我们的示例环境 我们将使用一个简单的前端web服务应用程序来演示一些灰度发布的概念。该应用程序提供了一个基于http的API，消费者可以使用它来操作各种数据(如产品价格等简单信息)。示例应用程序有一些可调参数，我们可以使用这些参数来模拟各种生产环境，由灰度发布流程进行评估。例如，可以让应用程序为20%的请求返回错误，或者规定5%的请求至少需要两秒钟。 我们使用部署在谷歌应用程序引擎上的应用程序来演示灰度发布流程，这些原则同样适用于其他环境。虽然示例应用程序是经过设计的，但是在实际场景中，类似的应用程序与我们的示例可以共享灰度发展中使用的指标。 我们的示例服务有两个可能的版本:当前版本和候选版本。当前版本是当前部署在生产环境中的版本，而候选版本是新构建的版本。使用这两个版本来说明发布概念，以及如何实现灰度发布以使发布过程更安全。 回滚部署与简单的Canary部署比较 我们将在发生中断时根据错误预算节省和一般影响，来对没有灰度发布的部署流程和灰度发布流程进行比较。我们的部署过程以开发环境为基础。一旦我们感觉代码在开发环境中正常工作，我们就将该版本部署到生产环境中。 在部署之后不久，监视开始报高错误率(参见图16-2，在图16-2中，为了模拟示例服务中的缺陷，对示例应用程序进行配置以使20％的请求失败)。对于示例，假设部署流程不支持回滚到以前已知的配置正常的版本时。修复这些错误的最佳选择就只有在生产版本中查找缺陷，对其进行补救，并在停机期间重新部署一个新版本。这种做法肯定会延长错误对用户的影响。 图 16-2 部署之后错误率增加 为了改进这个初始部署过程，我们可以在使用灰度发布来减少推送错误代码所造成的影响。 我们需要一种方法来在小部分生产环境中运行候选版本，而不是一次性部署到生产环境。 然后将一小部分流量发送到该生产环境（the canary金丝雀）并将其与其他部分（the control 主控）进行比较。 使用此方法，我们可以在所有生产受到影响之前发现候选版本中的缺陷。 我们在示例应用程序中的进行简单灰度发布，在应用程序的特定版本之间分配流量。 您可以使用App Engine或其他任何方法来分割流量（例如负载均衡器上的后端权重，代理配置或循环DNS记录）。 图16-3显示了当我们使用灰度发布，变更的影响会大大降低;事实上，这些错误几乎不可见!这提出了一个有趣的问题:与总体流量趋势相比，灰度发布的流量趋势很难看到和跟踪。 图 16-3 部署之后错误率增canary部署错误率； 因为进行canary部署的只是系统的一小部分，因此总体错误率降低 为了更清楚地了解需要在合理范围内跟踪的错误，我们可以通过App Engine应用程序版本查看关键指标(HTTP响应代码)，如图16-4所示。当我们查看每个版本的分解趋势图时，我们可以清楚地看到新版本引入的错误。我们还可以从图16-4中观察到当前版本提供的错误非常少。 现在，我们可以根据应用程序版本的HTTP错误率对部署进行调优。如果灰度发布的错误率大于全部系统的错误率，这表明canary部署是糟糕的。我们应该暂停并回滚部署，或者联系他人来帮助解决问题。如果错误率相似，我们可以正常地进行部署。在图16-4中，我们的canary部署显然很糟糕，我们应该回滚它。 图 16-4 应用程序HTTP响应码； 新版本产生多数错误、当前版本产生小数错误（图中显示10%的log） Canary实施 现在我们已经看到了一个相当简单的canary部署实现，接下来让我们更深入地了解成功的canary流程所需的参数。 最小化SLOs和错误预算的风险 第2章讨论了SLOs如何反映设计服务可用性的业务需求。这些需求也可以通过canary实现。canary进程的风险仅仅是我们错误预算的一小部分，它受到时间和canary规模大小的限制。 全局部署会很快将SLO置于危险之中。如果实例中为系统全面部署候选版本，我们将面临20%的请求失败的风险。如果我们使用5%的canary规模，我们将为5%的流量提供20%错误，导致1%的总体错误率(如图16-3所示)。这个策略允许我们保留我们的错误预算—预算的影响与暴露于缺陷的流量的数量成正比。我们可以假设，对于全局部署和灰度部署，检测和回滚花费的时间差不多，但是当我们将灰度发布集成到部署过程中时，我们会以更低的成本获得有关新版本的有价值信息。 这是一个假设负载均匀的极简模型。它还假设我们可以将整个错误预算用于灰度发布。这里我们只考虑新版本引入的不可用性，而不是实际可用性。我们的模型还假设新版本具有100%的失败率，这是最坏的情况。而进行灰度的部分不会导致线上系统100%不可用。我们还允许在灰度部署期间，整个系统的可用性低于SLO。 这个模型有明显的缺陷，但它是一个可靠的起点，你可以根据业务需求进行调整。我们建议使用最简单的模型来满足你的技术和业务目标。根据我们的经验，专注于使模型在技术上尽可能正确，常常会导致在建模上的过度投资。对于具有高复杂性的服务，过于复杂的模型可能导致持续的模型调优，而没有真正的好处。 选择灰度规模和持续时间 选择合适的灰度持续时间，需要考虑发布频率。 如果需要每天发布，那么在一次只运行一个灰度的情况下，无法使灰度保持一周,如果每周部署一次，就可以执行较长的灰度发布。 如果持续部署（例如，一天20次），灰度的持续时间必须明显缩短。 在一些说明里，虽然可以同时运行多个灰度，但这样做会增加大量精力来跟踪系统状态。 在任何情况下，需要快速推断系统状态时，同时运行多个灰度会成为问题。如果灰度重叠，同时运行多个灰度也会增加信号污染的风险。我们强烈建议一次只运行一个灰度。 对于基本的评估，不需要大规模的灰度来检测关键条件。然而，一个有代表性的灰度发布流程需要跨多个维度进行决策: 规模和持续时间 它的规模应够大，持续时间应够长，足以代表整个部署。仅在接收到少量查询后终止canary部署，对于以具有不同功能的不同查询为特征的系统来说，这无法提供有用的信号。处理率越高，获取代表性样本所需的时间就越少，以确保所观察到的行为实际上是由变更引起的，而不仅仅是随机因素。 流量 我们需要在系统上接收足够的流量，以确保它是一个具有代表性的示例，并且系统有机会对输入做出负面反应。通常，请求越均匀，所需要的流量就越少。 时间点 性能缺陷通常只在高负载下出现，因此在非高峰时间部署可能不会触发性能相关的缺陷。 度量指标 灰度的代表性与我们选择评估的指标密切相关(我们将在本章后面讨论)。我们可以快速评估诸如查询成功之类的琐碎指标，但是其他指标(如队列深度)可能需要更多的时间或较大规模的灰度来提供清晰的信号。 但问题是，这些要求可能相互冲突。Canarying是一种平衡行为，它通过对最坏情况的冷静分析和系统过去的实际记录来实现。一旦您从过去的灰度中收集了指标，您就可以根据典型的canary评估失败率而不是假想的最坏情况来选择canary参数。 选择和评估度量标准 到目前为止，我们一直在研究成功率，这是评估灰度发布的一个非常清晰和明显的指标。但是直觉上，我们知道这个单一的指标对于有意义的canary流程来说是不够的。如果我们以10倍的延迟为所有请求提供服务，或者在这样做时使用10倍的内存，那么我们可能也会遇到问题。并不是所有的指标都适合评估灰度发布。哪些指标最适合评估灰度发布版本是好是坏? 度量标准应指出问题 首先，指标需要能够指出服务中的问题。这很棘手，因为构成问题的并不总是客观的。我们可能会认为用户请求失败是有问题的。但是如果一个请求的响应时间增加了10%，或者系统内存增加了10%?，这该如何判断？我们通常建议使用sla作为开始考虑canary指标的地方。良好的服务质量指数往往与服务健康状况密切相关。如果已经使用SLIs来度量SLO是否符合，那么我们可以重用这些工作。 几乎任何指标在极端情况下都可能出现问题，但是向灰度流程中添加太多的指标也会产生成本。我们需要为每个指标正确定义可接受行为。如果可接受行为定义过于严格，我们会得到大量的误报;也就是说，我们会认为灰度很糟糕，即使实际不是这样。相反，如果对可接受行为的定义过于宽松，我们更有可能忽略掉有问题的灰度部署。正确选择什么是可接受的行为可能会成本较大——既耗时又需要分析。然而，如果做得不好，错误的结果会完全误导你。此外，随着服务、其特性集和行为的发展，您需要定期重新评估期望。 我们应该根据这些指标多大程度上能够表明系统中实际用户的体验来进行排名，选择排名靠前的几个指标(可能不超过12个)。太多的度量标准会带来递减的回报，并且在某种程度上，收益会被维护它们的成本所抵消，或者在发布过程中如果不维护它们，会对发布结果无法保证100%的信任。 为了使这个指导原则更加具体，让我们回头再来看示例。它有许多我们可以评估的指标:CPU使用量、内存占用、HTTP返回码(2xx、3xx等等)、响应延迟、正确性等等。在这种情况下，我们最好的度量标准可能是HTTP返回码和响应延迟，因为它们的降级最接近于实际用户影响。在这个场景中，CPU使用率并没有那么有用:资源使用的增加不一定会影响服务，并且可能导致不稳定或嘈杂的canary进程。这会导致操作人员禁用或忽略canary进程，这会首先破坏使用canary进程的目的。对于前端服务，我们直观地知道，响应较慢或响应失败通常会真实反映服务中存在的问题。 HTTP返回码包含一些有趣的复杂情况，例如状态码404，它告诉我们没有找到资源。这可能是因为用户获得了错误的URL(想象一下在一个流行的论坛上分享了一个错误的URL)，或者因为服务器错误地停止了对资源的服务。通常，我们可以通过排除canary评估中的400级状态码，并添加黑盒监控来测试特定URL的存在，从而解决此类问题。然后，我们可以将黑盒数据作为canary分析的一部分，以帮助将canary流程与奇怪的用户行为隔离开来。 度量标准应该具有代表性和可归属性 观察到的指标变化其来源，应该清楚地归因于正在进行的变更，并且不应该受到外部因素的影响。 在一个大的系统中(例如，许多服务器或许多容器)，我们可能会有外部性——超过连接的机器、运行具有不同性能特征的不同内核的机器，或者网络中过载的机器。此时金丝雀部分和主系统部分之间的差异，既是我们所部署的两个基础设施之间的差异，也会是我们变更导致的差异。 管理金丝雀是多种力量之间的平衡。增加金丝雀的规模是减少这个问题影响的方法(如前所述)。当我们的系统达到我们认为的合理的金丝雀规模时，我们需要考虑我们选择的指标是否会显示出很大的差异。 我们还应该知道canary和control环境之间共享的失败域;坏金丝雀会对控制产生负面影响，而系统中的坏行为可能会导致我们错误地评估金丝雀。同样，确保您的度量标准是良好隔离的。考虑一个同时运行我们的应用程序和其他进程的系统。整个系统的CPU使用量的急剧增加会导致糟糕的度量，因为系统中的其他进程(数据库负载、日志轮转等)可能会导致这种增加。更好的度量标准是在处理请求时所花费的CPU时间。更好的度量标准是在服务进程实际计划在CPU上的时间窗口上为处理请求服务所花费的CPU时间。虽然与我们的进程相关的严重超额的机器显然是一个问题(监控应该捕捉到它!)，但它不是由我们正在进行的更改引起的，因此不应该将其标记为金丝雀部署失败。 金丝雀也需要是可归属的;也就是说，您还应该能够将canary度量与SLIs联系起来。如果一个度量可以在不影响服务的情况下发生巨大变化，那么它不适合用来评估灰度发布。 评估前/评估后依然是有风险的 canary过程的前后是归因问题的延伸。在这个过程中，旧系统被新系统完全替代，你的canary评估将在一段时间内比较变更之前和之后的系统行为。你可以将此过程称为时空中的canary部署，在此过程中，您通过分割时间来选择A/B组，而不是通过机器、cookie或其他方法来分割总体。由于时间是观察到的指标变化的最大来源之一，因此很难在评估之前/之后来判断性能是否下降。 虽然canary部署可能导致降级，但原有系统本身也可能会降级。如果需要长时间运行canary部署，就会变得更加复杂。例如，如果在周一进行发布，可能会将工作日的行为与周末的行为进行比较，从而引入大量噪音。在该示例中，用户可能在周末以不同的方式访问该服务。从而在canary进程中引入噪音。 评估前/后过程本身引入了一个问题，即大而短的错误率(由前/后评估引入)是否优于小而长的错误率(由一个小金丝雀引入)。如果新版本完全被破坏，我们能多快地检测和恢复? 大规模的金丝雀之前/之后可以更快地检测到问题，但恢复的总体时间可能仍然相当长，与较小的金丝雀类似。在此期间，用户会一直受到影响。 使用渐进的灰度会更好 选择的度量标准即使不符合我们理想中的属性，但仍然很有价值。我们可以通过使用更细微的灰度过程来介绍这些指标。 我们可以使用包含多个阶段的canary来反映我们对度量的推理能力，而不是简单地评估单个canary阶段。在第一阶段，我们对这个版本没有信心或不了解。因此，我们希望使用一个小的阶段，以尽量减少负面影响。在小型灰度中，我们更喜欢能够最清晰地显示问题的指标——应用程序崩溃、请求失败等等。一旦这一阶段成功地过去，下一阶段将增加灰度规模，从而增强我们分析变化影响的信心。 依赖和隔离 正在测试的系统不会在完全真空中运行。出于实际原因，灰度和主系统可以共享后端、前端、网络、数据存储和其他基础设施。甚至可能与客户端有非常不明显的交互。例如，假设一个客户端发送了两个连续的请求。第一个请求可以由灰度部分来处理。其响应可能会改变第二个请求的内容，第二个请求可能会落在主系统部分，从而改变主系统的行为。 不完美的隔离会带来几个后果。最重要的是，我们需要知道，如果灰度过程的结果表明我们应该停止生产变更并调查情况，那么灰度并不一定是错误的。这一事实对于一般的canarying来说是正确的，但是在实践中，它经常由于隔离问题而导致被强制执行。 此外，不完美的隔离意味着灰度部署的错误行为也会对原始系统产生负面影响。Canarying是A/B比较，A和B有可能同时改变;这可能会导致评估灰度变得混乱。还必须使用绝对度量，例如定义的SLOs，以确保系统正确运行。 在非交互系统中进行Canarying 本章重点讨论了交互式请求/响应系统，它在许多方面是最简单和最常讨论的系统设计。其他系统，如异步处理管道，也同样重要，但有不同的canarying注意事项，我们将简要列举。有关数据处理管道的canarying的更多信息，请参见第13章。 首先，canary的持续时间和部署本质上依赖于工作单元处理的持续时间。当涉及到交互系统时，我们忽略了这个因素，假设工作单元处理的时间不会超过几秒钟，这比canary的持续时间要短。非交互式系统中的工作单元处理(如呈现管道或视频编码)可能需要更长的时间。因此，确保canary持续时间至少跨越单个工作单元的持续时间。 对于非交互式系统，隔离可能变得更加复杂。许多管道系统只有一个工作分配程序和一组使用应用程序代码的工作人员。在多阶段管道中，工作单元由工作人员处理，然后返回到池中，由同一工作人员或另一个工作人员执行下一阶段的处理。金丝雀分析有助于确保处理特定工作单元的工人总是从相同的工人池中提取——要么是金丝雀池，要么是控制池。否则，信号就会变得越来越混杂(有关理清信号的需要的更多信息，请参见349页的监视数据的要求)。 最后，度量标准的选择可能更加复杂。我们可能感兴趣的是端到端处理工作单元的时间(类似于交互系统中的延迟)，以及处理本身的质量(当然，这是完全特定于应用程序的)。 考虑到这些警告，canarying的一般概念仍然是可行的，并且适用相同的高级原则。 监控要求 在评估灰度部署时，您必须能够将部署了灰度的系统与未部署灰度的系统进行比较。通常，这需要在构造监视系统时多加注意—有效的比较非常简单，并且能够产生有意义的结果。 考虑之前的例子，在5%的规模中进行灰度，错误率为20%。因为监视很可能将系统作为一个整体来观察，所以它只能检测到1%的总体错误率。根据系统的不同，这个信号可能与其他错误源无法区分(参见图16-3)。 如果我们通过按照服务请求的对象来（金丝雀与主系统）分解指标，(参见图16-4)我们可以清楚地看到主系统与canary之间的错误率，这清楚地说明了全局部署将带来什么。在这里，我们看到，对整个服务的监控不足以分析灰度是否ok。在收集监视数据时，能够执行细粒度的分解非常重要，这些分解使得能够区分金丝雀和主系统的指标。 收集指标的另一个难点是金丝雀的部署受到设计的时间限制。当度量指标在特定时期内进行聚合时，这可能会导致问题。考虑每小时的度量误差。我们可以通过对过去一小时的请求求和来计算这个度量。如果我们使用这个度量来评估我们的canary，我们可能会遇到问题，如下面的时间表所述: 某些事件会导致一些错误发生。 一只金丝雀被部署在5%的人口中;金丝雀的持续时间是30分钟。 canary系统开始监视每小时的错误度量，以确定部署是好是坏。 部署被检测为错误，因为每小时的错误度量与控制总体的每小时的错误显著不同。 此场景是使用每小时计算一次的度量来评估仅30分钟长的部署的结果。因此，canary进程提供了一个非常模糊的信号。当使用度量来评估canary的成功时，确保度量的间隔与canary的持续时间相同或小于持续时间。 相关概念 通常，与客户的对话涉及到在生产中使用蓝/绿部署、人工负载生成和/或流量测试。这些概念类似于canarying，因此虽然它们不是严格意义上的金丝雀流程，但亦可使用。 蓝/绿部署 蓝/绿部署维护系统的两个实例：一个提供流量（绿色），另一个准备提供流量（蓝色）。 在蓝色环境中部署新版本后，将流量切换到其中。切换过程不需要停机，并且回滚只是简单逆转路由器而已。 一个缺点是该设置使用的资源是传统部署的两倍。在该设置中，您正在有效地执行前/后金丝雀（前面已讨论过）。 通过同时(而不是分开地)使用蓝/绿部署，您可以或多或少地将蓝色/绿色部署用作常规的金丝雀。在此策略中，您可以将canary部署到blue(备用)实例，并在绿色和蓝色环境之间缓慢地分配流量。您的评估和比较蓝色环境和绿色环境的指标都应该与流量控制相关。这种设置类似于A/B金丝雀，此时绿色环境是主系统，蓝色环境是金丝雀部署，金丝雀数量由发送到每个金丝雀的流量控制。 人工负载生成 与其将实时用户流量暴露给canary部署，还不如在安全性方面犯点错误，使用人工负载。通常，您可以在多个部署阶段(QA、预生产，甚至在生产中)运行负载测试。虽然根据我们的定义，这些操作不符合canarying，但是它们仍然是找到缺陷的可行方法，但需要注意一些事项。 使用人工负载进行测试可以很好地最大化代码覆盖率，但不能提供良好的状态覆盖率。在可变系统(具有缓存、cookie、请求关联等的系统)中人工模拟负载尤其困难。人工负载也可能无法准确地模拟真实系统中流量变化。有些问题可能只在无人工负载的情况下出现，从而导致覆盖率有所差距。 人工负载在可变系统中也很难工作。例如，试图在计费系统上生成人工负载可能非常危险:系统可能开始向信用卡供应商发送呼叫，然后信用卡供应商将开始主动向客户收费。虽然我们可以避免测试危险的代码逻辑，但是在这些逻辑上缺乏测试会降低我们的测试覆盖率。 流量测试 如果人工流量不具有代表性，我们可以复制流量并将其发送到生产系统和测试环境。这种技术被称为流量镜像。生产系统服务于实际流量并响应请求，canary部署服务于副本流量并丢弃响应。您甚至可以将canary响应与实际响应进行比较，并运行进一步的分析。 这种策略可以提供有代表性的流量，但通常比更直接的canary流程更复杂。在有状态系统中，流量测试也不能充分识别风险;流量副本可能会在看似独立的部署之间引入意外的影响。例如，如果canary部署和生产系统共享一个缓存，人为导致的缓存命中率增加将使canary指标的性能度量无效。 结论 您可以使用许多工具和方法来自动化版本发布，并将canarying引入到发布管道中。没有一种测试方法是万能的，测试策略应该由系统的需求和行为决定。Canarying可以作为一种简单、健壮且易于集成的方法来补充测试。当您及早发现系统缺陷时，用户受到的影响最小。Canarying还可以为频繁发布提供信心，并提高开发速度。正如测试方法必须随着系统需求和设计而发展一样，canarying也必须如此。 前言</summary></entry><entry><title type="html">第十章 事后总结：从失败中学习</title><link href="http://localhost:4000/sre/2020/01/10/%E4%BA%8B%E5%90%8E%E6%80%BB%E7%BB%93-%E4%BB%8E%E5%A4%B1%E8%B4%A5%E4%B8%AD%E5%AD%A6%E4%B9%A0/" rel="alternate" type="text/html" title="第十章 事后总结：从失败中学习" /><published>2020-01-10T00:00:00+08:00</published><updated>2020-01-10T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/10/%E4%BA%8B%E5%90%8E%E6%80%BB%E7%BB%93:%E4%BB%8E%E5%A4%B1%E8%B4%A5%E4%B8%AD%E5%AD%A6%E4%B9%A0</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/10/%E4%BA%8B%E5%90%8E%E6%80%BB%E7%BB%93-%E4%BB%8E%E5%A4%B1%E8%B4%A5%E4%B8%AD%E5%AD%A6%E4%B9%A0/">&lt;!-- more --&gt;

&lt;p&gt;经验表明，真正对事不对人的事后总结文化可以带来更可靠的系统——这也是我们认为这种做法对于创建和维护一个成功的SRE组织十分重要的原因。&lt;/p&gt;

&lt;p&gt;将事后总结引入组织既是一种文化变革，也是一种技术变革。这样的转变似乎是令人生畏的，本章的关键点是，做出这种改变是可能的，不是一个难以克服的挑战。不要指望你的系统能够自行解决问题。你可以从引入一个非常基础的事后调查程序开始，反映和调整你的流程来适应你的组织——跟很多事情一样，没有放之四海而皆准的方法。&lt;/p&gt;

&lt;p&gt;如果总结的很好，可以采取行动并广泛分享，事后总结可以成为积极推动组织变革和防止重复故障的利器。为了可以说明优秀的事后总结的写作原则，本章介绍了一个曾经发生在Google的故障案例进行研究。一个糟糕的事后总结的案例可以突显出为什么“糟糕”的事后总结对于努力创建健康的事后分析文化的组织是有害的。将糟糕的事后总结与实际事后总结进行比较后，可以突出高质量事后总结的原则和最佳实践。&lt;/p&gt;

&lt;p&gt;本章的第二部分分享了我们在实现强大的事后总结文化激励机制以及识别（和补救）文化破裂的预兆方面所学到的知识。&lt;/p&gt;

&lt;p&gt;最后，我们提供了用于引导事后总结文化的工具和模板。&lt;/p&gt;

&lt;p&gt;有关对事不对人的事后总结哲学的全部讨论，请参阅我们的第一本书“站点可靠性工程”中的第15章。&lt;/p&gt;

&lt;h2 id=&quot;案例分析&quot;&gt;案例分析&lt;/h2&gt;

&lt;p&gt;本案例研究的是例行机架退役导致用户服务延迟增加的case。我们的自动化维护系统的一个bug，加上限速不足，导致数千台承载生产流量的服务器同时宕机。&lt;/p&gt;

&lt;p&gt;虽然Google的大多数服务器都位于我们的专有数据中心，但我们在托管设施（或“colos”）中也有机架代理/缓存机器。colos中包含我们代理机器的机架被称为卫星，由于卫星经常进行例行维护和升级，所以在任何时间点都会安装或退役许多卫星机架。在Google，这些维护流程基本是自动化的。&lt;/p&gt;

&lt;p&gt;退役过程使用我们称为diskerase的过程覆盖机架中所有驱动器的全部内容。一旦机器被发送到diskerase，它曾存储的数据将不再可检索。典型的机架退役步骤如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;获取卫星上所有活跃的机器&lt;br /&gt;
  machines = GetMachines(satellite)&lt;br /&gt;
将所有通过“filter”匹配到的候选机器发送到decom&lt;br /&gt;
  SendToDecom(candidates = GetAllSatelliteMachines(), filter=machines)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们的案例研究始于一个标记为退役的卫星架。退役过程的diskerase步骤已成功完成，但负责剩余机器退役的自动化失败了。为了调试失败，我们重新尝试了停用过程。第二次退役如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;获取卫星上所有活跃的机器  &lt;br /&gt;
    machines  = GetMachines(satellite)&lt;br /&gt;
 因为decom流已经在运行了，因此”machines”是一个空的列表&lt;br /&gt;
 API bug：anmpty list被视为“无过滤，即为在全部机器上运行”，而不是“不在任何机器上运行”&lt;br /&gt;
 将所有通过“filter”匹配到的候选机器发送到decom&lt;br /&gt;
    SendToDecom(candidates=GetAllSatelliteMachines(), filter=machines)&lt;br /&gt;
 将所有候选机器发送到diskerase。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;几分钟内，全球所有卫星机的磁盘数据都被删除。这些机器处于惰性状态，无法接受用户的连接，因此后续用户的连接直接路由到我们的数据中心。结果导致用户延迟增加。得益于良好的容量规划，很少有用户注意到这两天我们在受影响的colo机架中重新安装机器。事件发生后，我们花了几周的时间进行审核，并为自动化添加了更多的健全性检查，使我们的退役工作流程具有幂等性（其任意多次执行所产生的影响均与一次执行的影响相同）。&lt;/p&gt;

&lt;p&gt;故障发生的三年后，我们遇到了类似的事件：由于许多卫星被耗尽导致用户延迟增加。根据原始事后总结，本次事件后实施的行动大大减小了二次事故的影响范围和速度。&lt;/p&gt;

&lt;p&gt;假设你是负责为此案例研究撰写事后总结总结的人。你想了解什么，以及你会采取什么行动来防止此类故障再次发生？&lt;/p&gt;

&lt;p&gt;让我们从这个事件的一个糟糕的事后总结示例开始。&lt;/p&gt;

&lt;h2 id=&quot;糟糕的事后总结示例&quot;&gt;糟糕的事后总结示例：&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;-事后总结所有卫星机器进入diskerase流程-&quot;&gt;&lt;center&gt; 事后总结：所有卫星机器进入diskerase流程 &lt;/center&gt;&lt;/h3&gt;
  &lt;p&gt;&lt;strong&gt;2014-8-11&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;负责人&lt;/strong&gt;：maxone@，logantwo@， sydneythree@，dylanfour@&lt;br /&gt;
&lt;strong&gt;与以下人员共享&lt;/strong&gt;：satellite-infra-team@&lt;br /&gt;
&lt;strong&gt;状态&lt;/strong&gt;：不可更改&lt;br /&gt;
&lt;strong&gt;事件日期&lt;/strong&gt;：2014年8月11日&lt;br /&gt;
&lt;strong&gt;发布时间&lt;/strong&gt;：2014年12月30日&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;执行摘要&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;：所有卫星机器都被发送到diskerase，实际导致Google Edge不可用。  &lt;br /&gt;
&lt;strong&gt;根本原因&lt;/strong&gt;：dylanfour@忽略了自动化设置并手动运行了集群启动逻辑，从而触发了现有bug。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;故障摘要&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;故障持续时间&lt;/strong&gt;：40分钟&lt;br /&gt;
&lt;strong&gt;受影响的产品&lt;/strong&gt;：satellite-infra-team   &lt;br /&gt;
&lt;strong&gt;受影响的产品百分比&lt;/strong&gt;：所有卫星集群。 &lt;br /&gt;
&lt;strong&gt;用户影响&lt;/strong&gt;：进入卫星的所有查询都是由Core提供的，导致延迟增加。 &lt;br /&gt;
&lt;strong&gt;收入影响&lt;/strong&gt;：由于查询异常，部分广告未投放。目前确切的收入影响未知。 &lt;br /&gt;
&lt;strong&gt;监控&lt;/strong&gt;：监控报警。 &lt;br /&gt;
&lt;strong&gt;解决方案&lt;/strong&gt;：将流量转移到核心，然后手动修复边缘集群。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;背景（可选）&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;em&gt;用户影响：&lt;/em&gt; &lt;br /&gt;
  进入卫星的所有查询都是由Core提供的，导致延迟增加。&lt;/li&gt;
    &lt;li&gt;&lt;em&gt;收入影响：&lt;/em&gt;  &lt;br /&gt;
  由于查询异常，部分广告未投放。&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;根本原因和触发点&lt;/strong&gt; &lt;br /&gt;
集群开启和关闭的自动化并不意味着是幂等的，该工具有安全措施确保某些步骤不能多次运行。但无法阻止某个人手动多次运行代码，也没有文件提及这个问题。因此，大多数团队成员认为工具失效时，可以多次运行该过程。&lt;/p&gt;

  &lt;p&gt;正好在机架的常规退役期间发生这种情况。机架正在被一个新的基于lota的卫星取代。dylanfour@忽略了已经执行了一次启动并在第一次执行时出现问题。由于粗心以及无知，他们引发了一个错误的交互，即将所有卫星机器分配给了弃用团队。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;为恢复生产投入的努力&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;经验教训&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;事情进展顺利：
      &lt;ul&gt;
        &lt;li&gt;报警及时发现问题。&lt;/li&gt;
        &lt;li&gt;事故处理进展顺利。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;事情进展受挫：
      &lt;ul&gt;
        &lt;li&gt;团队（特别是maxone@，logantwo@）从未编写任何文档告知SRE不要多次运行自动化。&lt;/li&gt;
        &lt;li&gt;on-call处理的不够及时，没有阻止大多数卫星机器被删除。这已经不是第一次出现处理故障不及时的情况了。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;幸运的
      &lt;ul&gt;
        &lt;li&gt;Core能够为原本进入边缘集群的所有流量提供服务。简直无法相信我们这次能幸免于难！&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;行动列表&lt;/strong&gt;&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;完善自动化工具。&lt;/td&gt;
        &lt;td&gt;缓解&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;logantwo@&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;报警改善&lt;/td&gt;
        &lt;td&gt;检测&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;sydneythree@ 需要学习正确的跨站点轮值文档，避免发生重复的问题&lt;/td&gt;
        &lt;td&gt;缓解&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;BUG6789&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;训练大家不要运行不安全的命令&lt;/td&gt;
        &lt;td&gt;避免&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;&lt;strong&gt;名词解释&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;为什么这份总结十分糟糕&quot;&gt;为什么这份总结十分糟糕？&lt;/h3&gt;

&lt;p&gt;示例包含一些我们试图避免的常见故障模式。以下说明如何改进这份事后总结。&lt;/p&gt;

&lt;h4 id=&quot;缺少上下文&quot;&gt;缺少上下文&lt;/h4&gt;

&lt;p&gt;从一开始，作为示例的事后总结引入了特定于流量服务的术语（例如，卫星）和Google机器管理自动化的较低层（例如，“diskerase”）。如果你需要提供其他上下文作为总结的一部分，请在背景或名词解释部分进行添加（可以链接到更长的文档）。上述总结在这两部分都是空白的。&lt;/p&gt;

&lt;p&gt;如果在编写事后总结时没有将内容置于正确的上下文中，文档可能会被误解或被忽略。谨记，你的受众并不只仅仅是有着直接关系的团队。&lt;/p&gt;

&lt;h4 id=&quot;省略了关键细节&quot;&gt;省略了关键细节&lt;/h4&gt;

&lt;p&gt;多个部分包含摘要但缺少重要细节。例如：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题总结&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;针对多个服务的中断，应该提供数字来表示影响范围。示例中唯一的数据是故障的持续时间。没有足够的细节来评估故障的规模或影响。即使没有一个具体值，一个估计的值也比没有数据好。毕竟，你如果不知道如何衡量它，那么你也不知道它是否被修复了！&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;根本原因和触发点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;确定根本原因和触发点是编写事后总结的重要原因之一。示例包含一个描述根本原因和触发点的小段落，但没有探索故障的底层细节。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为恢复生产投入的努力&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对读者来说，事后总结是事件记录。一份好的事后总结能让读者知道发生了什么，该如何减轻问题，以及用户受到了什么影响。很多问题的答案通常在投入恢复的努力中可以找到，但示例中，这部分是空的。&lt;/p&gt;

&lt;p&gt;如果故障值得写入事后总结中，那么你还应该花时间准确获取并记录必要的详细信息。读者可以全面了解停机情况，更重要的是可以了解新事物。&lt;/p&gt;

&lt;h4 id=&quot;缺失了关键行动列表&quot;&gt;缺失了关键行动列表&lt;/h4&gt;

&lt;p&gt;示例的“行动列表”（AIs）部分缺少防止此类故障再次发生的可执行的行动计划。例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;行动列表主要是缓解措施。为了最大程度的降低重复发生中断的几率，还应该包括一些预防性和修复性的操作。一个“预防性的”行动列表说明我们“让人不会轻易出错”。一般来说，试图改变人类行为比改变自动化系统和流程更不靠谱。（正如Dan.Milstein曾打趣道：“让我们为未来做好计划，因为未来的我们跟今天一样愚蠢。”）&lt;/li&gt;
  &lt;li&gt;所有操作项都标记了相同的优先级。没法确认首先要执行的行动。&lt;/li&gt;
  &lt;li&gt;列表前两个操作项用词模糊不清，如“改善”和“完善”。用词不当会很难衡量和理解成功的标准。&lt;/li&gt;
  &lt;li&gt;只有一个操作项标明跟踪bug。如果没有正式的跟踪过程，事后总结的行动列表往往会被遗忘，导致故障再次发生。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;用Google全天候运维副总裁Ben TreynorSloss的话来说：“对我们的用户而言，没有后续行动的事后总结和没有事后总结并无差别。因此，所有影响到用户的事后总结都必须至少有一个与它们相关的P[01]错误。例外情况由我亲自审查，但几乎没有例外。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;适得其反的指责&quot;&gt;适得其反的指责&lt;/h4&gt;
&lt;p&gt;每一次事后总结都可能会陷入相互指责中。来看一些例子：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;事情变得糟糕&lt;/strong&gt;&lt;br /&gt;
整个团队都被指责要对故障负责，尤其是这两名成员（maxone@和logantwo@）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;行动列表&lt;/strong&gt;&lt;br /&gt;
列表的最后一项指向了sydneythree@，负责跨站点轮岗。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;根本原因和触发点&lt;/strong&gt;&lt;br /&gt;
dylanfour@为本次故障全权负责。&lt;/p&gt;

&lt;p&gt;在事后总结中强调个体似乎是个好主意，但实际上，这种做法会导致团队成员不愿承担风险，因为害怕被当众羞辱。他们可能会掩盖那些防止再次发生故障的事实。&lt;/p&gt;

&lt;h4 id=&quot;animated语言&quot;&gt;Animated语言&lt;/h4&gt;
&lt;p&gt;事后总结基于事实，不应该受个人判断和主观语言的影响，应该考虑多种观点并尊重他人。示例中包含了多个反面例子：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;根本原因和触发点&lt;/strong&gt;&lt;br /&gt;
多余的语言（例如，“粗心无知”）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;事情变得糟糕&lt;/strong&gt;&lt;br /&gt;
Animated文字（例如，“这太荒谬了”）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;幸运的&lt;/strong&gt;&lt;br /&gt;
一种夸张的感叹（例如，“无法相信我们能够幸免于难！”）&lt;/p&gt;

&lt;p&gt;动画语言和对事件的戏剧性描述稀释了关键信息，降低了警惕性。应该提供可验证的数据来证明事件的严重性。&lt;/p&gt;

&lt;h4 id=&quot;缺少负责人&quot;&gt;缺少负责人&lt;/h4&gt;
&lt;p&gt;宣布官方所有权会产生问责制，从而促进采取行动。示例中有几个缺少所有权的例子：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;总结列出了四个负责人。理想情况下，负责人是单点联系人，负责事后总结，后续以及完善工作。&lt;/li&gt;
  &lt;li&gt;“行动列表”部分很少甚至没有提及各条目的负责人。没有明确负责人的行动项目不大可能会被解决。
最好的是拥有一个负责人和多个协作者。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;受众有限&quot;&gt;受众有限&lt;/h4&gt;
&lt;p&gt;示例事后总结仅在团队成员之间共享。默认情况下，公司的每个人都应该可以访问该文档。建议尽可能广泛的分享你的事后总结，甚至和客户分享。总结的价值和所创造的经验成正比。能够从过去事件中吸取教训的人越多，重复发生的可能性就越低。周密和诚实的事后总结也是恢复信任的关键。&lt;/p&gt;

&lt;p&gt;随着体验和舒适度的提高，可以将“受众”扩展到非人类。成熟的事后总结文化通常会添加机器可读标签（和其他元数据）以启用下游分析。&lt;/p&gt;

&lt;h4 id=&quot;延迟发布&quot;&gt;延迟发布&lt;/h4&gt;
&lt;p&gt;示例是在事件发生四个月后公布的。在此期间，如果事故再次发生（实际上确实发生过），团队成员可能会忘记总结中提到的关键细节。&lt;/p&gt;

&lt;h2 id=&quot;优秀的事后总结示例&quot;&gt;优秀的事后总结示例&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;这是一份真实的事后总结。个人和团队的名字均为虚构，为了保护敏感容量信息，我们使用了占位符替换实际值。在你为内部分享的事后总结中，应该包含具体的数字。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;-事后总结所有发送到diskerase的卫星机器-&quot;&gt;&lt;center&gt; 事后总结：所有发送到diskerase的卫星机器 &lt;/center&gt;&lt;/h3&gt;
  &lt;p&gt;&lt;strong&gt;2014-8-11&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;负责人&lt;/strong&gt;：&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;事后总结：maxone@，logantwo@，&lt;/li&gt;
    &lt;li&gt;数据中心自动化：sydneythree@，&lt;/li&gt;
    &lt;li&gt;网络：dylanfour@&lt;/li&gt;
    &lt;li&gt;服务器管理：finfive@&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;与以下人员共享&lt;/strong&gt;：all_engineering_employees@google.com@  &lt;br /&gt;
&lt;strong&gt;状态&lt;/strong&gt;：不可更改&lt;br /&gt;
&lt;strong&gt;事件日期&lt;/strong&gt;：2014年8月11日, 星期一，PST8PDT 17:10至17:50   &lt;br /&gt;
&lt;strong&gt;发布时间&lt;/strong&gt;：2014年8月15日, 星期五&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;执行摘要&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;：前端查询丢失。&lt;br /&gt;
部分广告未投放&lt;br /&gt;
近两天由卫星提供的服务延迟增加&lt;br /&gt;
&lt;strong&gt;根本原因&lt;/strong&gt;：自动调节系统中的某个错误导致所有机架的所有卫星机器被发送到diskerase。导致所有卫星机器进入decom工作流程，磁盘数据被擦除。致使全球卫星前端中断。。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;故障摘要&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;故障持续时间&lt;/strong&gt;：故障：周一，8月11日，PST8PDT 17:10至17:50。
                     周三，8月13日，07:46进行重建工作，之后故障解除。 &lt;br /&gt;
&lt;strong&gt;受影响的产品&lt;/strong&gt;：前端基础设施，尤其是卫星所在地。  &lt;br /&gt;
&lt;strong&gt;受影响的产品百分比&lt;/strong&gt;：全球——所有由卫星提供服务的流量（占全球查询的60%）。 &lt;br /&gt;
&lt;strong&gt;用户影响&lt;/strong&gt;：[ ]前端查询在40分钟内有所下降（[ ]QPS在此期间处于平均值，占全球流量的百分比[ ]）。所有由卫星提供的服务延迟增加。 &lt;br /&gt;
&lt;strong&gt;收入影响&lt;/strong&gt;：目前未知确切的收入影响。 &lt;br /&gt;
&lt;strong&gt;监控&lt;/strong&gt;：黑盒报警：对于每个卫星，流量团队收到“卫星a12bcd34有过多失败的HTTP请求”报警。 &lt;br /&gt;
&lt;strong&gt;解决方案&lt;/strong&gt;：通过将所有Google的前端流量转移到核心集群，以用户请求的额外延迟为代价，迅速缓解故障。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;背景（可选）&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;em&gt;用户影响：&lt;/em&gt;
      &lt;ul&gt;
        &lt;li&gt;[_]个前端查询在40分钟的时间段内丢失，QPS在此期间持平，占全球流量的[ ]%。我们的监控表明这是个大故障；然而数据并不可靠，监控将自身停止监控的但仍在服务的卫星视为请求被拒绝。附录描述了如何估算上述数字。。&lt;/li&gt;
        &lt;li&gt;最近两天所有由卫星提供的服务延迟增加。
          &lt;ul&gt;
            &lt;li&gt;–核心集群附近的RTT峰值[ ]ms&lt;/li&gt;
            &lt;li&gt;–对于更依赖卫星的地点（例如澳大利亚，新西兰，印度），延迟+[ ]ms&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;em&gt;收入影响：&lt;/em&gt;
      &lt;ul&gt;
        &lt;li&gt;由于请求丢失，部分广告未投放。目前尚不清楚确切的收入影响&lt;/li&gt;
        &lt;li&gt;显示和视频：由于日常波动，数据的误差较大，但我们估计故障当天有[ ]%到[ ]%的收入损失。&lt;/li&gt;
        &lt;li&gt;搜索：在17:00到18:00之间，在使用同样的误差时，有[ ]%到[ ]%的损失。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;em&gt;团队影响：&lt;/em&gt;
      &lt;ul&gt;
        &lt;li&gt;流量团队花了大约48小时全力投入重建卫星。&lt;/li&gt;
        &lt;li&gt;因为需要对过载的对等链路进行流量工程设计，NST的故障/报警负载高于正常值。&lt;/li&gt;
        &lt;li&gt;由于GFE的缓存命中率降低，某些服务可能会在前端提供更多响应。
          &lt;ul&gt;
            &lt;li&gt;—例如，请参阅线程[链接]关于[缓存依赖服务]。&lt;/li&gt;
            &lt;li&gt;—[缓存相关服务]在恢复之前，GFE的缓存命中率从[ ]%下降到[ ]%。&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;故障文件&lt;/strong&gt;  &lt;br /&gt;
  [我们的事后总结文档的链接。]&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;根本原因和触发点&lt;/strong&gt; &lt;br /&gt;
流量管理服务器中一个长期存在的输入验证bug，由于手动重新执行a12bcd34卫星的工作流程而被触发的。该bug删除了执行下线操作的机器的约束，发送并停用了所有卫星机器。 
因此，数据中心自动化执行了下线工作流程，擦除了大多数卫星机器的硬盘驱动器，在此之前无法停止这项操作。
Traffic Admin服务器提供ReleaseSatelliteMachines RPC。此处理程序使用三个MDB API调用卫星停用：&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;查找与边缘节点关联的机架名称（例如，a12bcd34 -&amp;gt;）。&lt;/li&gt;
    &lt;li&gt;查找与机架关联的机器名称（-&amp;gt;等）。&lt;/li&gt;
    &lt;li&gt;将这些计算机重新分配给diskerase，间接触发下线工作流程。&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;由于MDB API的行为以及安全检查不是幂等的。如果卫星节点先前已成功发送到decom，则上面步骤2返回一个空列表，步骤3中将其解释为机器主机名上没有约束。
这种危险行为已存在一段时间，但被调用不安全操作的工作流程隐藏：调用RPC的工作流程步骤标记为“运行一次”，意味着工作流引擎一旦成功就不会重新执行RPC。
但是，“运行一次”的语义不适用于工作流的多个实例。当集群启停团队手动启动a12bcd34的另一个工作流时，会触发admin_server bug。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;为恢复生产投入的努力&lt;/strong&gt;&lt;br /&gt;
    [我们的时间线日志的链接已被省略。在真正的事后总结中，这些信息始终包含在内。]&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;经验教训&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;事情进展顺利：
      &lt;ul&gt;
        &lt;li&gt;疏散边缘。核心中的GFE明确的进行容量规划能允许这种情况发生，生产主干也是如此（除了对等链路之外；请参阅下一节中的故障列表）。这种边缘疏散使得流量团队能迅速减缓故障。&lt;/li&gt;
        &lt;li&gt;自动减轻卫星故障。覆盖的线路自动将来自故障卫星的流量拉回到核心集群，并且当检测到异常抖动时会自行排出。&lt;/li&gt;
        &lt;li&gt;尽管可能会造成混乱，但卫星decom/diskerase工作十分高效和迅速。&lt;/li&gt;
        &lt;li&gt;故障通过OMG触发了快速的IMAG响应，并且该工具适用于持续的事件跟踪。跨团队的反应非常棒，OMG帮助大家保持交流。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;事情进展受挫：
      &lt;ul&gt;
        &lt;li&gt;故障
          &lt;ul&gt;
            &lt;li&gt;Traffic Admin服务器没有对其发送到MDB的命令进行适当的健全性检查。所有命令都应该是幂等的，或者至少在重复调用时是自动防故障的。&lt;/li&gt;
            &lt;li&gt;MDB不拒绝缺少主机名约束的所有权更改请求。&lt;/li&gt;
            &lt;li&gt;decom工作流程不与其他数据源（例如，计划的机架decom）交叉检查decom请求。因此，对已清除（地理上不同）的机器的请求没有异议。&lt;/li&gt;
            &lt;li&gt;decom工作流不受速率限制。一旦机器进入decom，磁盘擦除和其他decom步骤以最大速度进行。&lt;/li&gt;
            &lt;li&gt;当卫星停止服务时，由于出口流量转移到不同位置，Google和各国之间的一些对等链路过载，而请求是从核心集群提供服务。导致了在卫星恢复且匹配NST缓解工作之前，选择对等链路的阻塞短暂爆发。&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
        &lt;li&gt;恢复
          &lt;ul&gt;
            &lt;li&gt;重新安装卫星机器的速度很慢且不可靠。在高延迟链路末端传输到卫星时，重新安装使用TFTP传输数据效果不佳。&lt;/li&gt;
            &lt;li&gt;Autoreplacer基础架构无法在故障时设置GFE的[ ]。需要多个SRE并行手动执行设置来匹配自动化设置的速度。以下因素导致自动化的缓慢：
              &lt;ul&gt;
                &lt;li&gt;—SSH超时阻止了Autoreplacer在远程卫星上的操作。&lt;/li&gt;
                &lt;li&gt;—无论机器是否已具有正确的版本，都执行了慢速内核升级过程。&lt;/li&gt;
                &lt;li&gt;—Autoreplacer中的并发回归阻止了每个工作机器运行两个以上的机器设置任务。&lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
            &lt;li&gt;当23%的目标被移除时，没有触发监控配置的安全检查参数（25%变化），当读取相同内容（剩余的29%）时触发了。导致重新启用卫星监控延迟30分钟。&lt;/li&gt;
            &lt;li&gt;“安装人员”有限，因此，变更过程困难又缓慢。&lt;/li&gt;
            &lt;li&gt;使用超级用户权限将机器从diskerase拉回来时留下了很多僵尸进程，导致后续清理困难。&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;幸运的
      &lt;ul&gt;
        &lt;li&gt;核心集群的GFE与卫星GFE的管理方式不同。他们没有受到decom的影响。&lt;/li&gt;
        &lt;li&gt;同样，YouTube的CDN作为独立的基础设施运行，因此没有受到影响。否则故障将更加严重和持久。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;行动列表&lt;/strong&gt;  &lt;br /&gt;
由于此事件的广泛性，我们将行动列表分为五个主题：&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;预防/风险教训&lt;/li&gt;
    &lt;li&gt;紧急响应&lt;/li&gt;
    &lt;li&gt;监控/报警&lt;/li&gt;
    &lt;li&gt;卫星/边缘&lt;/li&gt;
    &lt;li&gt;清理/其他&lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;表10-1.预防/风险教训&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;审核所有能够将实时服务器转换为宕机状态的系统（即，不仅仅是维修和diskerase工作流）&lt;/td&gt;
        &lt;td&gt;调查&lt;/td&gt;
        &lt;td&gt;P1&lt;/td&gt;
        &lt;td&gt;sydneythree@&lt;/td&gt;
        &lt;td&gt;BUG1234&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;提交bug，跟踪BUG1234识别出的所有系统的拒绝错误输入实施的情况。&lt;/td&gt;
        &lt;td&gt;预防&lt;/td&gt;
        &lt;td&gt;P1&lt;/td&gt;
        &lt;td&gt;sydneythree@&lt;/td&gt;
        &lt;td&gt;BUG1235&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;禁止任何单个会影响跨命名空间/类边界的服务器操作。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P1&lt;/td&gt;
        &lt;td&gt;maxone@&lt;/td&gt;
        &lt;td&gt;BUG1236&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;流量管理服务器需要进行安全检查才能在超过[ ]节点数的情况下运行。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P1&lt;/td&gt;
        &lt;td&gt;dylanfour@&lt;/td&gt;
        &lt;td&gt;BUG1237&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;流量管理服务器应该依据&lt;安全检查服务&gt;批准破坏性工作。&lt;/安全检查服务&gt;&lt;/td&gt;
        &lt;td&gt;预防&lt;/td&gt;
        &lt;td&gt;P0&lt;/td&gt;
        &lt;td&gt;logantwo@&lt;/td&gt;
        &lt;td&gt;BUG1238&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;MDB应拒绝对非预期的当前约束提供值的操作。&lt;/td&gt;
        &lt;td&gt;预防&lt;/td&gt;
        &lt;td&gt;P0&lt;/td&gt;
        &lt;td&gt;louseven@&lt;/td&gt;
        &lt;td&gt;BUG1239&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;表10-2.紧急响应&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;确保由核心提供的服务不会使出口网络链路过载。&lt;/td&gt;
        &lt;td&gt;修复&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;rileyslx@&lt;/td&gt;
        &lt;td&gt;BUG1240&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;确保在[我们的紧急停止文档]和[我们的升级联系页面]下注明了decom工作流问题。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;logantwo@&lt;/td&gt;
        &lt;td&gt;BUG1241&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;为decom工作流添加一个大红色禁用按钮a&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P0&lt;/td&gt;
        &lt;td&gt;maxone@&lt;/td&gt;
        &lt;td&gt;BUG1242&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;a : 由于灾难性环境中避免进一步损坏的关闭开关（例如，紧急电源关闭按钮）的常见术语。
表10-3.监控/报警&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;监控目标的安全检查，不允许发布无法回滚的变更。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;dylanfour@&lt;/td&gt;
        &lt;td&gt;BUG1243&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;当超过[ ]%的机器下线需要添加报警。16:38机器从卫星上取下，17:10开始世界范围的报警。&lt;/td&gt;
        &lt;td&gt;监测&lt;/td&gt;
        &lt;td&gt;P1&lt;/td&gt;
        &lt;td&gt;rileyslx@&lt;/td&gt;
        &lt;td&gt;BUG1244&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;表10-4.卫星/边缘&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;利用IPXE配合HTTPS可以使重新安装更快更可靠。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;dylanfour@&lt;/td&gt;
        &lt;td&gt;BUG1245&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;表10-5.清理/其他&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;在我们的工具中查看与MDB相关的代码，并将管理服务器备份放至unwedge调节中。&lt;/td&gt;
        &lt;td&gt;修复&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;rileyslx@&lt;/td&gt;
        &lt;td&gt;BUG1246&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;安排DiRT测试：在diskerase后带回卫星；对YouTube CDN执行相同操作。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;louseven@&lt;/td&gt;
        &lt;td&gt;BUG1247&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;&lt;strong&gt;名词解释&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;管理服务器&lt;/code&gt;
RPC服务器，支持自动化为前端服务基础结构执行特权操作。自动化服务器常参与PCR和集群启停操作。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Autoreplacer&lt;/code&gt;
将非Borgified服务器从一台机器移动到另一台机器的系统。在机器故障时保持服务运行，并且支持colo重新配置。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Borg&lt;/code&gt;
集群管理系统，旨在管理大规模任务和机器资源。Borg拥有Borg单元中所有机器，并将任务分配给具有可用资源的机器。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Decom&lt;/code&gt;&lt;br /&gt;
退役的缩写。设备的decom是一个与许多运维团队相关的过程。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Diskerase&lt;/code&gt;&lt;br /&gt;
在生产硬盘驱动器离开Google数据中心前安全擦除生产硬盘的过程（以及相关的硬件/软件系统）。diskerase是decom工作流的一个步骤。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GFE（Google前端）&lt;/code&gt;&lt;br /&gt;
外部连接(几乎)所有谷歌服务的服务器。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;*IMAG（Google事件管理）&lt;/code&gt;&lt;br /&gt;
一个程序，一种标准，以一致的方式来处理从系统中断到自然灾害的所有类型的事件——并组织有效的响应。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MDB（机器数据库）&lt;/code&gt;&lt;br /&gt;
事件管理仪表盘/工具，用于跟踪和管理Google所有正在进行的事件的中心位置。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;卫星&lt;/code&gt;&lt;br /&gt;
小巧便宜的机器机架，仅提供提供来自Google网络边缘的非视频、前端流量。几乎没有传统的生产集群基础设施可用于卫星。卫星不同于CDN，它提供来自Google边缘网络的YouTube视频内容，以及来自互联网中更广泛的其他地方的视频内容。YouTube CDN未受此事件的影响。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;附录&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;为什么释放卫星机器不是幂等的？&lt;/code&gt; &lt;br /&gt;
[该问题的回复已被删除]&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;管理服务器将所有卫星分配给diskerase团队后发生了什么？&lt;/code&gt; &lt;br /&gt;
[该问题的回复已被删除]&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;故障期间真正的QPS损失是多少？&lt;/code&gt; &lt;br /&gt;
[该问题的回复已被删除]&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;IRC日志&lt;/code&gt;&lt;br /&gt;
[IRC日志已被删除]&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;图表&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;更快的延迟统计——卫星曾为我们做过什么？&lt;/code&gt;&lt;br /&gt;
从这次故障经验得到，卫星会在核心集群附近的许多位置产生[ ]ms延迟，离主干更远的位置甚至会达到[ ]ms：&lt;br /&gt;
[图表的解释已被删除]&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;核心与边缘服务负载&lt;/code&gt;&lt;br /&gt;
为重建服务所付出的努力是个很好的例证。边缘服务恢复50%的流量需要大约36小时，恢复到正常的流量水平需要额外的12小时（见图10-1和图10-2）。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;来自流量转换的对等压力&lt;/code&gt; 
 [图表省略]&lt;br /&gt;
 该图显示了由网络区域聚合的数据包丢失情况。在活动期间有一些短的尖峰，但大部分损失发生在卫星覆盖少的各个地区的高峰时刻。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;人与机器，GFE&lt;/code&gt;&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p&gt;[省略了人机与自动机器设置速率的图表说明。]
&lt;img src=&quot;/blog/something/images/SRE/10-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图10-1.故障期间核心与边缘QPS分布 &lt;/center&gt;&lt;/p&gt;
  &lt;/blockquote&gt;

  &lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/10-2.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图10-2.故障期间核心与边缘QPS分布（替代表示） &lt;/center&gt;&lt;/p&gt;

  &lt;h3 id=&quot;为什么这份事后总结示例更好&quot;&gt;为什么这份事后总结示例更好？&lt;/h3&gt;
  &lt;p&gt;这个事后总结符合了好几条写作要求。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;明晰&quot;&gt;明晰&lt;/h3&gt;
&lt;p&gt;事后总结组织的很好，详细解释了关键术语。例如：&lt;/p&gt;

&lt;h4 id=&quot;名词解释&quot;&gt;名词解释&lt;/h4&gt;
&lt;p&gt;一个精心编写的名词解释让事后总结更容易被大众接受和理解。&lt;/p&gt;

&lt;h4 id=&quot;行动列表&quot;&gt;行动列表&lt;/h4&gt;
&lt;p&gt;这是一个涉及许多小的行动列表的大事件。按照主题对操作项进行分组可以更加轻松的分配负责人和优先级。&lt;/p&gt;

&lt;h4 id=&quot;量化指标&quot;&gt;量化指标&lt;/h4&gt;
&lt;p&gt;事后总结提供了相关事件的有用数据，例如缓存命中率、流量级别和影响持续时间。数据的相关部分将显示原始来源的链接。这种数据透明性消除了歧义并为读者提供了上下文参考。&lt;/p&gt;

&lt;h3 id=&quot;具体行动列表&quot;&gt;具体行动列表&lt;/h3&gt;
&lt;p&gt;没有行动列表的事后总结是无效的。这些行动列表有一些显著特点：&lt;/p&gt;

&lt;h4 id=&quot;负责人&quot;&gt;负责人&lt;/h4&gt;
&lt;p&gt;所以操作项都有负责人和bug跟踪号。&lt;/p&gt;

&lt;h4 id=&quot;优先级&quot;&gt;优先级&lt;/h4&gt;
&lt;p&gt;为所有操作项分配优先级。&lt;/p&gt;

&lt;h4 id=&quot;可测性&quot;&gt;可测性&lt;/h4&gt;
&lt;p&gt;操作项具有可验证的最终状态（例如，“当我们的机器中超过X%的机器下线时添加报警”）。&lt;/p&gt;

&lt;h4 id=&quot;预防措施&quot;&gt;预防措施&lt;/h4&gt;
&lt;p&gt;每个操作项“主题”都有预防/缓解操作项，这些操作项有助于避免故障重复发生（例如，“禁止任何单个会影响跨命名空间/类边界的服务器操作。”）&lt;/p&gt;

&lt;h3 id=&quot;不指责&quot;&gt;不指责&lt;/h3&gt;
&lt;p&gt;作者关注的是系统设计中的差距，正是这些差距导致了非预期的故障，例如：&lt;/p&gt;

&lt;h4 id=&quot;事情进展受挫&quot;&gt;事情进展受挫&lt;/h4&gt;
&lt;p&gt;没有任何人或团队因此事件受到指责。&lt;/p&gt;

&lt;h4 id=&quot;根本原因和触发点&quot;&gt;根本原因和触发点&lt;/h4&gt;
&lt;p&gt;关注“什么”出了问题，而不是“谁”造成了这一问题。&lt;/p&gt;

&lt;h4 id=&quot;行动列表-1&quot;&gt;行动列表&lt;/h4&gt;
&lt;p&gt;旨在改善系统而不是改善人。&lt;/p&gt;

&lt;h3 id=&quot;深度&quot;&gt;深度&lt;/h3&gt;
&lt;p&gt;事后调查不仅仅是调查系统故障的近似区域，也研究了多个团队的影响和系统缺陷。尤其：&lt;/p&gt;

&lt;h4 id=&quot;影响&quot;&gt;影响&lt;/h4&gt;
&lt;p&gt;本节包含来自不同视角的大量细节，尽可能的做到平衡，客观。&lt;/p&gt;

&lt;h4 id=&quot;根本原因和触发点-1&quot;&gt;根本原因和触发点&lt;/h4&gt;
&lt;p&gt;本节对事件进行深入研究，找到根本原因和触发点。&lt;/p&gt;

&lt;h4 id=&quot;由数据推及结论&quot;&gt;由数据推及结论&lt;/h4&gt;
&lt;p&gt;提出的所有结论均基于事实和数据。用于得出结论的数据都和文档相关联。&lt;/p&gt;

&lt;h4 id=&quot;其他资源&quot;&gt;其他资源&lt;/h4&gt;
&lt;p&gt;以图表的形式进一步呈现有用的信息。向不熟悉系统的读者解释图表帮助其理解上下文。&lt;/p&gt;

&lt;h3 id=&quot;及时&quot;&gt;及时&lt;/h3&gt;
&lt;p&gt;事件结束后不到一星期就写完并传播了事后总结。快速的事后总结往往更加准确，因为此时任何参与者心理都记着这件事。而受故障影响的人正在等待一个解释，证明你们已经控制了故障。等待的时间越长，他们就越能散发想象，那样对你十分不利。&lt;/p&gt;

&lt;h3 id=&quot;简明&quot;&gt;简明&lt;/h3&gt;
&lt;p&gt;该事件是全球范围的事件，影响多个系统。事后总结记录了且随后分析了大量数据。冗长的数据源（例如聊天记录和系统日志）被抽象化，未经编辑的版本可从主文档中链接到。总体而言，这份总结在冗长和可读性之间取得了平衡。&lt;/p&gt;

&lt;h2 id=&quot;组织激励&quot;&gt;组织激励&lt;/h2&gt;
&lt;p&gt;理想情况下，高级领导应该支持和鼓励有效的事后总结。本节描述了一个组织如何激励健康的事后总结文化。我们着重描述了总结文化失败的征兆，并给出了一些解决方案。同时还提供了工具和模板来简化和自动化事后处理流程。&lt;/p&gt;

&lt;h3 id=&quot;模型以及对事不对人&quot;&gt;模型以及对事不对人&lt;/h3&gt;
&lt;p&gt;为了正确的支持事后总结文化，领导者应始终如一的坚持对事不对人的原则，并在事后讨论中鼓励对事不对人。可以使用一些具体策略来强制组织执行对事不对人这一准则。&lt;/p&gt;

&lt;h4 id=&quot;使用对事不对人的语言&quot;&gt;使用对事不对人的语言&lt;/h4&gt;
&lt;p&gt;指责性的语言会影响团队协作。请考虑以下情况：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sandy 错过了服务Foo培训，且不确定如何运行特定的更新命令。因而导致故障时间的延长。
SRE Jesse [对Sandy的leader说]:“你是经理，为什么不确保每个人都完成培训？”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个交流凸显了一个主要问题，即让收件人处于劣势。更平衡的回应是：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;SRE Jesse [对Sandy的leader]：“看完事后总结，能够注意到on-call错过了一次重要的培训，导致没有更快的解决故障。因此是否应该要求团队成员加入on-call轮转之前都完成此培训？或者是否可以提醒on-call，如果操作卡住可以尽快升级事件。毕竟，升级不是错误——尤其它有助于降低客户的负担！从长远来看，因为一些细节很容易被忘记，因此我们不能完全依赖培训。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;事后总结的创作要包含所有事件参与者&quot;&gt;事后总结的创作要包含所有事件参与者&lt;/h4&gt;
&lt;p&gt;当事后总结是单人或由单个团队编写时，很容易忽略导致故障的关键因素。&lt;/p&gt;

&lt;h4 id=&quot;收集反馈&quot;&gt;收集反馈&lt;/h4&gt;
&lt;p&gt;明确的审查过程和事后计划可以帮助防止指责的语言和观点在组织内传播。有关的结构化审核流程请参阅第221页的“事后检查清单”部分。&lt;/p&gt;

&lt;h3 id=&quot;奖励事后总结&quot;&gt;奖励事后总结&lt;/h3&gt;
&lt;p&gt;事后总结培训是积极推动组织变革和防止重复故障的有效工具。如果写的够好，采取行动并广泛分享，可以考虑以下策略来激励事后总结文化。&lt;/p&gt;

&lt;h4 id=&quot;对完成行动列表进行奖励&quot;&gt;对完成行动列表进行奖励&lt;/h4&gt;
&lt;p&gt;如果你奖励工程师编写事后总结而不是完成了相关的行动列表，那么可能会出现总结中的行动项目未完成的事情。需要在编写事后总结和成功完成行动计划间平衡奖励措施。&lt;/p&gt;

&lt;h4 id=&quot;对积极的组织变革进行奖励&quot;&gt;对积极的组织变革进行奖励&lt;/h4&gt;
&lt;p&gt;你可以将事后总结的重要性作为提高组织影响的依据，通过对标奖金、积极的绩效评估、晋升等作为奖励。来激励并广泛实施事后总结教训。&lt;/p&gt;

&lt;h4 id=&quot;突出提高可靠性&quot;&gt;突出提高可靠性&lt;/h4&gt;
&lt;p&gt;随着时间的推移，有效的事后总结可以减少故障，让系统更加可靠。因此，团队可以专注于特性功能的开发速度，而不是基础架构的修补上。在总结、演示文稿和绩效评估中强调这些改进在本质上是会提供动力的。&lt;/p&gt;

&lt;h4 id=&quot;把事后总结的负责人当做领导者&quot;&gt;把事后总结的负责人当做领导者&lt;/h4&gt;
&lt;p&gt;通过电子邮件或会议完成事后总结，或者通过给作者向受众提供经验教训的机会，能够吸引那些喜欢公共赞誉的人。对于寻求同行认可的工程师而言，可以将负责人设置为某种类型的故障的“专家”。例如，你可能会听到有人说：“和Sara说，她现在是专家了。她参与了事后总结的撰写，并且想出了解决问题的方法！”&lt;/p&gt;

&lt;h4 id=&quot;游戏化&quot;&gt;游戏化&lt;/h4&gt;
&lt;p&gt;一些人会被成就感和更远大的目标所激励，例如修复系统薄弱点和提高可靠性。对于这些人而言，事后总结行动列表的记录或完成所获得的成就已经是奖励了。在Google，我们每年举办两次“FixIt”周。完成最重要的行动列表项目的SRE会收到小额赞赏和吹牛的权利。图10-3显示了一个事后总结排行榜的示例。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/10-3.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图10-3.事后总结排行榜 &lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&quot;公开分享事后总结&quot;&gt;公开分享事后总结&lt;/h3&gt;
&lt;p&gt;为了在组织内保持健康的事后总结文化，要尽可能广泛的分享事后总结。以下策略可以提供一些帮助。&lt;/p&gt;

&lt;h4 id=&quot;在整个组织内分享总结&quot;&gt;在整个组织内分享总结&lt;/h4&gt;
&lt;p&gt;在内部沟通渠道、电子邮件、Slack等中宣传事后总结的可用性。如果你负责一个公司，可以分享一个最近的有趣的事后总结。&lt;/p&gt;

&lt;h4 id=&quot;进行跨团队审核&quot;&gt;进行跨团队审核&lt;/h4&gt;
&lt;p&gt;对事后总结进行跨团队审查。过程中，一个团队过一遍故障，其他团队提出问题并间接学习。在Google，几个办公室都设有非正式的事后总结俱乐部，向所有员工开放。&lt;/p&gt;

&lt;p&gt;此外，由开发人员、SRE和组织领导组成的跨职能小组审核整个事后总结流程。他们每个月都会审查事后总结过程和模板的有效性。&lt;/p&gt;

&lt;h4 id=&quot;进行培训练习&quot;&gt;进行培训练习&lt;/h4&gt;
&lt;p&gt;使用“命运之轮”训练新入职工程师：一群工程师重新扮演事后总结的角色，当时的事故总控负责人也参与其中，确保这次演习尽可能的“真实”。&lt;/p&gt;

&lt;h4 id=&quot;每周总结事件和故障&quot;&gt;每周总结事件和故障&lt;/h4&gt;
&lt;p&gt;每周对过去七天内发生的事件和故障的进行总结，并尽可能广泛的进行分享。&lt;/p&gt;

&lt;h3 id=&quot;响应事后总结文化的失败&quot;&gt;响应事后总结文化的失败&lt;/h3&gt;
&lt;p&gt;事后总结文化的崩溃可能并不明显，以下介绍了常见的故障模式和推荐的解决方案。&lt;/p&gt;

&lt;h4 id=&quot;逃避&quot;&gt;逃避&lt;/h4&gt;
&lt;p&gt;逃避事后总结过程可能是一个组织的事后总结文化失败的征兆，例如，假设SRE 主管 Parker无意中听到以下对话：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;SWE Sam：哇，你听说了这次的大故障了吗？&lt;br /&gt;
SWE Riley：听说了，太可怕了。他们现在得写一个事后总结了。&lt;br /&gt;
SWE Sam：不是吧，还好我没有参与进去。&lt;br /&gt;
SWE Riley：对啊，我是真的不想参与那个讨论会议。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;确保对产生这些抱怨的事件进行高可见度的事后总结可以避免这种逃避。此外，分享高质量的案例并讨论参与的人如何获得奖励将有助于重新团结每个人。。&lt;/p&gt;

&lt;h4 id=&quot;没有强调文化&quot;&gt;没有强调文化&lt;/h4&gt;
&lt;p&gt;当高级管理人员使用责备的语言做出回应可能会使事情更糟糕。假设一个高级领导在会议上对故障做出以下声明：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;VP Ash：我知道应该对事不对人，但有人事先知道这个操作可能会产生问题，你为什么不听那个人的？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以通过更有建设性的话术来减少损害，例如：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;SRE Dana：我确信每个人的出发点都是好的，所以为了保持对事不对人的准则，我们一般会这样问：是否有任何应该注意到的告警以及我们为何会忽略它们。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于组织而言，立足正确的出发点，并根据现有的最佳信息做出决策，调查误导性信息的来源比分配责任更有帮助。（如果你知道敏捷原则，那么你应该对这个更加清楚。）&lt;/p&gt;

&lt;h4 id=&quot;没有时间写事后总结&quot;&gt;没有时间写事后总结&lt;/h4&gt;
&lt;p&gt;优质的事后总结撰写是需要时间的。当一个团队负担其他任务时，总结的质量会受到影响。低质量的没有完整的行动列表的事后总结会更容易导致故障复发。事后总结是你写给团队未来成员的信件：以免你不小心教给未来的队友一个错误的教训，保持一致的质量标准十分重要。应该优先考虑事后检查工作，跟踪事后完成情况和审查，并让团队有足够的时间来实施相关的行动计划。我们在第220页的“工具和模板”一节中讨论的工具可以帮助完成此项活动。&lt;/p&gt;

&lt;h4 id=&quot;重复故障&quot;&gt;重复故障&lt;/h4&gt;
&lt;p&gt;如果团队在遇到类似故障时采用以前的经验失败了，那么就该深入挖掘了。要考虑以下问题：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;行动列表项目是否需要很长时间才能完成？&lt;/li&gt;
  &lt;li&gt;故障速度是否超过了可靠性的修复速度？&lt;/li&gt;
  &lt;li&gt;最先获得的是正确的行动项目吗？&lt;/li&gt;
  &lt;li&gt;重构的故障服务是否过期？&lt;/li&gt;
  &lt;li&gt;是否把Band-Aids定位成更严重的问题上了？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果你发现了系统性流程或技术问题，应该退一步考虑整体服务运行状况。将每个类似事件的事后总结作者聚集到一起，讨论防止故障重复发生的最佳行动方案。&lt;/p&gt;

&lt;h2 id=&quot;工具和模板&quot;&gt;工具和模板&lt;/h2&gt;
&lt;p&gt;一组工具和模板可以让编写事后总结和管理相关数据变得更轻松，从而引导事后总结文化。在这一领域，你可以利用Google和其他公司提供的大量资源。&lt;/p&gt;

&lt;h3 id=&quot;事后总结模板&quot;&gt;事后总结模板&lt;/h3&gt;
&lt;p&gt;模板可以使编写和分享完整的事后总结更加轻松。使用标准格式可以使非专业的读者更容易理解事后处理过程。你可以自定义模板。例如，获取特定团队的元数据（如数据中心团队的硬件品牌/型号）或受移动团队影响的Android版本可能更有用。随着团队在这方面的成熟，还可以自定义模板。&lt;/p&gt;

&lt;h4 id=&quot;google模板&quot;&gt;Google模板&lt;/h4&gt;
&lt;p&gt;Google已经通过&lt;a href=&quot;http://g.co/SiteReliabilityWorkbookMaterials&quot;&gt;http://g.co/SiteReliabilityWorkbookMaterials&lt;/a&gt;以Google文档格式分享了我们的事后模板。我们在内部主要使用Docs来编写事后总结，可以通过共享编辑权限和注释促进合作。我们的一些内部工具可以使用元数据预填充此模板让事后总结更容易编写。我们利用Google Apps脚本自动化部分创作，并将大量数据捕获到特定的部分和表格中，以便我们的事后总结存储库更容易解析数据。&lt;/p&gt;

&lt;h4 id=&quot;其他行业模板&quot;&gt;其他行业模板&lt;/h4&gt;
&lt;p&gt;其他几家公司和个人分享了他们的事后总结模板：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;报警职责&lt;/li&gt;
  &lt;li&gt;改编原始的Google可靠性站点工程书籍模型&lt;/li&gt;
  &lt;li&gt;GitHub上托管的四个模板列表&lt;/li&gt;
  &lt;li&gt;GitHub用户Julian Dunn&lt;/li&gt;
  &lt;li&gt;服务器故障&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;事后总结工具&quot;&gt;事后总结工具&lt;/h3&gt;
&lt;p&gt;在撰写本文时，Google的事后总结管理工具无法供外部使用（请查看我们的博客获取最新更新）。但是，我们可以解释我们的工具是如何促进事后总结文化的。&lt;/p&gt;

&lt;h4 id=&quot;事件管理工具&quot;&gt;事件管理工具&lt;/h4&gt;
&lt;p&gt;我们的事件管理工具收集并存储大量关于故障的有用数据，并将该数据自动推动到事后总结模板中。我们推送的数据类型包括：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;故障指挥人和其他角色&lt;/li&gt;
  &lt;li&gt;详细的事件时间表和IRC日志&lt;/li&gt;
  &lt;li&gt;受影响的服务和导致根本原因的服务&lt;/li&gt;
  &lt;li&gt;事件严重性&lt;/li&gt;
  &lt;li&gt;事件检测机制&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;事后总结清单&quot;&gt;事后总结清单&lt;/h4&gt;
&lt;p&gt;为了帮助作者确保正确完成事后检查，我们提供了一个事后检查清单，通过关键步骤引导负责人。以下是列表中的一些示例检查：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;对事件影响进行全面评估。&lt;/li&gt;
  &lt;li&gt;进行足够详细的根本原因分析，推动行动列表的规划。&lt;/li&gt;
  &lt;li&gt;确保行动列表项目通过服务技术主管的审查和批准。&lt;/li&gt;
  &lt;li&gt;和更多的组织分享事后总结。
完整的清单可在&lt;a href=&quot;http://g.co/SiteReliabilityWorkbookMaterials&quot;&gt;http://g.co/SiteReliabilityWorkbookMaterials&lt;/a&gt;找到。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;归档事后总结&quot;&gt;归档事后总结&lt;/h4&gt;
&lt;p&gt;我们将事后总结归档在一个名为Requiem的工具中，这样任何Google员工都可以轻松找到它们。我们的事件管理工具会自动将所有事后总结推送到Requiem，组织中的任何人都可以发布他们的事后总结给所有人查看。我们有成千上万的总结存档，可以追溯到2009年。Requiem会解析个人事后总结的元数据，使其可以用于搜索、分析和总结。&lt;/p&gt;

&lt;h4 id=&quot;跟进事后总结&quot;&gt;跟进事后总结&lt;/h4&gt;
&lt;p&gt;我们的事后总结归档在Requiem的数据库中。任何生成的操作项都会在我们的集中式bug跟踪系统中归档为bug。因此，我们可以监控每个事后总结的行动项目的结束与否。通过这种级别的跟踪，可以确保行动项目不会有漏洞以致服务越来越不稳定。图10-4显示了由我们的工具启用的事后总结操作项监控的模型。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/10-4.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图10-4.事后总结行动项目监控 &lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&quot;事后总结分析&quot;&gt;事后总结分析&lt;/h4&gt;
&lt;p&gt;我们的事后总结管理工具将信息存储在数据库中以供分析。团队可以使用这些数据编写有关其事后趋势的总结，并确定易受攻击的系统。这有助于我们发现潜在的不稳定因素或可能被忽略的故障管理障碍。例如，图10-5显示了使用我们的分析工具构建的图表。这些图表显示了我们每个组织每月有多少次事后追踪、事件平均持续时间、检测时间、解决时间和故障半径的趋势。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/10-5.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图10-5.事后总结分析 &lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&quot;其他行业工具&quot;&gt;其他行业工具&lt;/h4&gt;
&lt;p&gt;以下是一些可以帮助你创建、组织和分析事后总结的第三方工具：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;报警职责事后调查&lt;/li&gt;
  &lt;li&gt;Etsy的档案室&lt;/li&gt;
  &lt;li&gt;VictorOps
尽管完全实现自动化编写事后总结是不可能的，但我们发现事后总结模板和工具会使整个流程更加顺畅。这些工具可以节省时间，让作者能够专注于事后的关键部分，例如根本原因的分析和行动项目计划。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;
&lt;p&gt;在培养事后总结文化的持续投资中，能够减少故障，为用户提供更好的体验，以及让依赖你的人对你更加信任。这些实践的应用可以使系统设计更完善、宕机时间更短、工程师工作效率更高且工作更快乐。如果最坏的情况确实发生并且故障再次发生，那么你受到的损失会更小并且恢复的更快，而且有更多的数据帮助健壮生产环境。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">经验表明，真正对事不对人的事后总结文化可以带来更可靠的系统——这也是我们认为这种做法对于创建和维护一个成功的SRE组织十分重要的原因。 将事后总结引入组织既是一种文化变革，也是一种技术变革。这样的转变似乎是令人生畏的，本章的关键点是，做出这种改变是可能的，不是一个难以克服的挑战。不要指望你的系统能够自行解决问题。你可以从引入一个非常基础的事后调查程序开始，反映和调整你的流程来适应你的组织——跟很多事情一样，没有放之四海而皆准的方法。 如果总结的很好，可以采取行动并广泛分享，事后总结可以成为积极推动组织变革和防止重复故障的利器。为了可以说明优秀的事后总结的写作原则，本章介绍了一个曾经发生在Google的故障案例进行研究。一个糟糕的事后总结的案例可以突显出为什么“糟糕”的事后总结对于努力创建健康的事后分析文化的组织是有害的。将糟糕的事后总结与实际事后总结进行比较后，可以突出高质量事后总结的原则和最佳实践。 本章的第二部分分享了我们在实现强大的事后总结文化激励机制以及识别（和补救）文化破裂的预兆方面所学到的知识。 最后，我们提供了用于引导事后总结文化的工具和模板。 有关对事不对人的事后总结哲学的全部讨论，请参阅我们的第一本书“站点可靠性工程”中的第15章。 案例分析 本案例研究的是例行机架退役导致用户服务延迟增加的case。我们的自动化维护系统的一个bug，加上限速不足，导致数千台承载生产流量的服务器同时宕机。 虽然Google的大多数服务器都位于我们的专有数据中心，但我们在托管设施（或“colos”）中也有机架代理/缓存机器。colos中包含我们代理机器的机架被称为卫星，由于卫星经常进行例行维护和升级，所以在任何时间点都会安装或退役许多卫星机架。在Google，这些维护流程基本是自动化的。 退役过程使用我们称为diskerase的过程覆盖机架中所有驱动器的全部内容。一旦机器被发送到diskerase，它曾存储的数据将不再可检索。典型的机架退役步骤如下： 获取卫星上所有活跃的机器 machines = GetMachines(satellite) 将所有通过“filter”匹配到的候选机器发送到decom SendToDecom(candidates = GetAllSatelliteMachines(), filter=machines) 我们的案例研究始于一个标记为退役的卫星架。退役过程的diskerase步骤已成功完成，但负责剩余机器退役的自动化失败了。为了调试失败，我们重新尝试了停用过程。第二次退役如下： 获取卫星上所有活跃的机器 machines = GetMachines(satellite) 因为decom流已经在运行了，因此”machines”是一个空的列表 API bug：anmpty list被视为“无过滤，即为在全部机器上运行”，而不是“不在任何机器上运行” 将所有通过“filter”匹配到的候选机器发送到decom SendToDecom(candidates=GetAllSatelliteMachines(), filter=machines) 将所有候选机器发送到diskerase。 几分钟内，全球所有卫星机的磁盘数据都被删除。这些机器处于惰性状态，无法接受用户的连接，因此后续用户的连接直接路由到我们的数据中心。结果导致用户延迟增加。得益于良好的容量规划，很少有用户注意到这两天我们在受影响的colo机架中重新安装机器。事件发生后，我们花了几周的时间进行审核，并为自动化添加了更多的健全性检查，使我们的退役工作流程具有幂等性（其任意多次执行所产生的影响均与一次执行的影响相同）。 故障发生的三年后，我们遇到了类似的事件：由于许多卫星被耗尽导致用户延迟增加。根据原始事后总结，本次事件后实施的行动大大减小了二次事故的影响范围和速度。 假设你是负责为此案例研究撰写事后总结总结的人。你想了解什么，以及你会采取什么行动来防止此类故障再次发生？ 让我们从这个事件的一个糟糕的事后总结示例开始。 糟糕的事后总结示例： 事后总结：所有卫星机器进入diskerase流程 2014-8-11: 负责人：maxone@，logantwo@， sydneythree@，dylanfour@ 与以下人员共享：satellite-infra-team@ 状态：不可更改 事件日期：2014年8月11日 发布时间：2014年12月30日 执行摘要 影响：所有卫星机器都被发送到diskerase，实际导致Google Edge不可用。 根本原因：dylanfour@忽略了自动化设置并手动运行了集群启动逻辑，从而触发了现有bug。 故障摘要 故障持续时间：40分钟 受影响的产品：satellite-infra-team 受影响的产品百分比：所有卫星集群。 用户影响：进入卫星的所有查询都是由Core提供的，导致延迟增加。 收入影响：由于查询异常，部分广告未投放。目前确切的收入影响未知。 监控：监控报警。 解决方案：将流量转移到核心，然后手动修复边缘集群。 背景（可选） 影响 用户影响： 进入卫星的所有查询都是由Core提供的，导致延迟增加。 收入影响： 由于查询异常，部分广告未投放。 根本原因和触发点 集群开启和关闭的自动化并不意味着是幂等的，该工具有安全措施确保某些步骤不能多次运行。但无法阻止某个人手动多次运行代码，也没有文件提及这个问题。因此，大多数团队成员认为工具失效时，可以多次运行该过程。 正好在机架的常规退役期间发生这种情况。机架正在被一个新的基于lota的卫星取代。dylanfour@忽略了已经执行了一次启动并在第一次执行时出现问题。由于粗心以及无知，他们引发了一个错误的交互，即将所有卫星机器分配给了弃用团队。 为恢复生产投入的努力 经验教训 事情进展顺利： 报警及时发现问题。 事故处理进展顺利。 事情进展受挫： 团队（特别是maxone@，logantwo@）从未编写任何文档告知SRE不要多次运行自动化。 on-call处理的不够及时，没有阻止大多数卫星机器被删除。这已经不是第一次出现处理故障不及时的情况了。 幸运的 Core能够为原本进入边缘集群的所有流量提供服务。简直无法相信我们这次能幸免于难！ 行动列表 行动列表 类型 优先级 负责人 bug跟踪 完善自动化工具。 缓解 P2 logantwo@   报警改善 检测 P2     sydneythree@ 需要学习正确的跨站点轮值文档，避免发生重复的问题 缓解 P2   BUG6789 训练大家不要运行不安全的命令 避免 P2     名词解释 为什么这份总结十分糟糕？ 示例包含一些我们试图避免的常见故障模式。以下说明如何改进这份事后总结。 缺少上下文 从一开始，作为示例的事后总结引入了特定于流量服务的术语（例如，卫星）和Google机器管理自动化的较低层（例如，“diskerase”）。如果你需要提供其他上下文作为总结的一部分，请在背景或名词解释部分进行添加（可以链接到更长的文档）。上述总结在这两部分都是空白的。 如果在编写事后总结时没有将内容置于正确的上下文中，文档可能会被误解或被忽略。谨记，你的受众并不只仅仅是有着直接关系的团队。 省略了关键细节 多个部分包含摘要但缺少重要细节。例如： 问题总结 针对多个服务的中断，应该提供数字来表示影响范围。示例中唯一的数据是故障的持续时间。没有足够的细节来评估故障的规模或影响。即使没有一个具体值，一个估计的值也比没有数据好。毕竟，你如果不知道如何衡量它，那么你也不知道它是否被修复了！ 根本原因和触发点 确定根本原因和触发点是编写事后总结的重要原因之一。示例包含一个描述根本原因和触发点的小段落，但没有探索故障的底层细节。 为恢复生产投入的努力 对读者来说，事后总结是事件记录。一份好的事后总结能让读者知道发生了什么，该如何减轻问题，以及用户受到了什么影响。很多问题的答案通常在投入恢复的努力中可以找到，但示例中，这部分是空的。 如果故障值得写入事后总结中，那么你还应该花时间准确获取并记录必要的详细信息。读者可以全面了解停机情况，更重要的是可以了解新事物。 缺失了关键行动列表 示例的“行动列表”（AIs）部分缺少防止此类故障再次发生的可执行的行动计划。例如： 行动列表主要是缓解措施。为了最大程度的降低重复发生中断的几率，还应该包括一些预防性和修复性的操作。一个“预防性的”行动列表说明我们“让人不会轻易出错”。一般来说，试图改变人类行为比改变自动化系统和流程更不靠谱。（正如Dan.Milstein曾打趣道：“让我们为未来做好计划，因为未来的我们跟今天一样愚蠢。”） 所有操作项都标记了相同的优先级。没法确认首先要执行的行动。 列表前两个操作项用词模糊不清，如“改善”和“完善”。用词不当会很难衡量和理解成功的标准。 只有一个操作项标明跟踪bug。如果没有正式的跟踪过程，事后总结的行动列表往往会被遗忘，导致故障再次发生。 用Google全天候运维副总裁Ben TreynorSloss的话来说：“对我们的用户而言，没有后续行动的事后总结和没有事后总结并无差别。因此，所有影响到用户的事后总结都必须至少有一个与它们相关的P[01]错误。例外情况由我亲自审查，但几乎没有例外。” 适得其反的指责 每一次事后总结都可能会陷入相互指责中。来看一些例子： 事情变得糟糕 整个团队都被指责要对故障负责，尤其是这两名成员（maxone@和logantwo@）。 行动列表 列表的最后一项指向了sydneythree@，负责跨站点轮岗。 根本原因和触发点 dylanfour@为本次故障全权负责。 在事后总结中强调个体似乎是个好主意，但实际上，这种做法会导致团队成员不愿承担风险，因为害怕被当众羞辱。他们可能会掩盖那些防止再次发生故障的事实。 Animated语言 事后总结基于事实，不应该受个人判断和主观语言的影响，应该考虑多种观点并尊重他人。示例中包含了多个反面例子： 根本原因和触发点 多余的语言（例如，“粗心无知”） 事情变得糟糕 Animated文字（例如，“这太荒谬了”） 幸运的 一种夸张的感叹（例如，“无法相信我们能够幸免于难！”） 动画语言和对事件的戏剧性描述稀释了关键信息，降低了警惕性。应该提供可验证的数据来证明事件的严重性。 缺少负责人 宣布官方所有权会产生问责制，从而促进采取行动。示例中有几个缺少所有权的例子： 总结列出了四个负责人。理想情况下，负责人是单点联系人，负责事后总结，后续以及完善工作。 “行动列表”部分很少甚至没有提及各条目的负责人。没有明确负责人的行动项目不大可能会被解决。 最好的是拥有一个负责人和多个协作者。 受众有限 示例事后总结仅在团队成员之间共享。默认情况下，公司的每个人都应该可以访问该文档。建议尽可能广泛的分享你的事后总结，甚至和客户分享。总结的价值和所创造的经验成正比。能够从过去事件中吸取教训的人越多，重复发生的可能性就越低。周密和诚实的事后总结也是恢复信任的关键。 随着体验和舒适度的提高，可以将“受众”扩展到非人类。成熟的事后总结文化通常会添加机器可读标签（和其他元数据）以启用下游分析。 延迟发布 示例是在事件发生四个月后公布的。在此期间，如果事故再次发生（实际上确实发生过），团队成员可能会忘记总结中提到的关键细节。 优秀的事后总结示例 这是一份真实的事后总结。个人和团队的名字均为虚构，为了保护敏感容量信息，我们使用了占位符替换实际值。在你为内部分享的事后总结中，应该包含具体的数字。 事后总结：所有发送到diskerase的卫星机器 2014-8-11: 负责人： 事后总结：maxone@，logantwo@， 数据中心自动化：sydneythree@， 网络：dylanfour@ 服务器管理：finfive@ 与以下人员共享：all_engineering_employees@google.com@ 状态：不可更改 事件日期：2014年8月11日, 星期一，PST8PDT 17:10至17:50 发布时间：2014年8月15日, 星期五 执行摘要 影响：前端查询丢失。 部分广告未投放 近两天由卫星提供的服务延迟增加 根本原因：自动调节系统中的某个错误导致所有机架的所有卫星机器被发送到diskerase。导致所有卫星机器进入decom工作流程，磁盘数据被擦除。致使全球卫星前端中断。。 故障摘要 故障持续时间：故障：周一，8月11日，PST8PDT 17:10至17:50。 周三，8月13日，07:46进行重建工作，之后故障解除。 受影响的产品：前端基础设施，尤其是卫星所在地。 受影响的产品百分比：全球——所有由卫星提供服务的流量（占全球查询的60%）。 用户影响：[ ]前端查询在40分钟内有所下降（[ ]QPS在此期间处于平均值，占全球流量的百分比[ ]）。所有由卫星提供的服务延迟增加。 收入影响：目前未知确切的收入影响。 监控：黑盒报警：对于每个卫星，流量团队收到“卫星a12bcd34有过多失败的HTTP请求”报警。 解决方案：通过将所有Google的前端流量转移到核心集群，以用户请求的额外延迟为代价，迅速缓解故障。 背景（可选） 影响 用户影响： [_]个前端查询在40分钟的时间段内丢失，QPS在此期间持平，占全球流量的[ ]%。我们的监控表明这是个大故障；然而数据并不可靠，监控将自身停止监控的但仍在服务的卫星视为请求被拒绝。附录描述了如何估算上述数字。。 最近两天所有由卫星提供的服务延迟增加。 –核心集群附近的RTT峰值[ ]ms –对于更依赖卫星的地点（例如澳大利亚，新西兰，印度），延迟+[ ]ms 收入影响： 由于请求丢失，部分广告未投放。目前尚不清楚确切的收入影响 显示和视频：由于日常波动，数据的误差较大，但我们估计故障当天有[ ]%到[ ]%的收入损失。 搜索：在17:00到18:00之间，在使用同样的误差时，有[ ]%到[ ]%的损失。 团队影响： 流量团队花了大约48小时全力投入重建卫星。 因为需要对过载的对等链路进行流量工程设计，NST的故障/报警负载高于正常值。 由于GFE的缓存命中率降低，某些服务可能会在前端提供更多响应。 —例如，请参阅线程[链接]关于[缓存依赖服务]。 —[缓存相关服务]在恢复之前，GFE的缓存命中率从[ ]%下降到[ ]%。 故障文件 [我们的事后总结文档的链接。] 根本原因和触发点 流量管理服务器中一个长期存在的输入验证bug，由于手动重新执行a12bcd34卫星的工作流程而被触发的。该bug删除了执行下线操作的机器的约束，发送并停用了所有卫星机器。 因此，数据中心自动化执行了下线工作流程，擦除了大多数卫星机器的硬盘驱动器，在此之前无法停止这项操作。 Traffic Admin服务器提供ReleaseSatelliteMachines RPC。此处理程序使用三个MDB API调用卫星停用： 查找与边缘节点关联的机架名称（例如，a12bcd34 -&amp;gt;）。 查找与机架关联的机器名称（-&amp;gt;等）。 将这些计算机重新分配给diskerase，间接触发下线工作流程。 由于MDB API的行为以及安全检查不是幂等的。如果卫星节点先前已成功发送到decom，则上面步骤2返回一个空列表，步骤3中将其解释为机器主机名上没有约束。 这种危险行为已存在一段时间，但被调用不安全操作的工作流程隐藏：调用RPC的工作流程步骤标记为“运行一次”，意味着工作流引擎一旦成功就不会重新执行RPC。 但是，“运行一次”的语义不适用于工作流的多个实例。当集群启停团队手动启动a12bcd34的另一个工作流时，会触发admin_server bug。 为恢复生产投入的努力 [我们的时间线日志的链接已被省略。在真正的事后总结中，这些信息始终包含在内。] 经验教训 事情进展顺利： 疏散边缘。核心中的GFE明确的进行容量规划能允许这种情况发生，生产主干也是如此（除了对等链路之外；请参阅下一节中的故障列表）。这种边缘疏散使得流量团队能迅速减缓故障。 自动减轻卫星故障。覆盖的线路自动将来自故障卫星的流量拉回到核心集群，并且当检测到异常抖动时会自行排出。 尽管可能会造成混乱，但卫星decom/diskerase工作十分高效和迅速。 故障通过OMG触发了快速的IMAG响应，并且该工具适用于持续的事件跟踪。跨团队的反应非常棒，OMG帮助大家保持交流。 事情进展受挫： 故障 Traffic Admin服务器没有对其发送到MDB的命令进行适当的健全性检查。所有命令都应该是幂等的，或者至少在重复调用时是自动防故障的。 MDB不拒绝缺少主机名约束的所有权更改请求。 decom工作流程不与其他数据源（例如，计划的机架decom）交叉检查decom请求。因此，对已清除（地理上不同）的机器的请求没有异议。 decom工作流不受速率限制。一旦机器进入decom，磁盘擦除和其他decom步骤以最大速度进行。 当卫星停止服务时，由于出口流量转移到不同位置，Google和各国之间的一些对等链路过载，而请求是从核心集群提供服务。导致了在卫星恢复且匹配NST缓解工作之前，选择对等链路的阻塞短暂爆发。 恢复 重新安装卫星机器的速度很慢且不可靠。在高延迟链路末端传输到卫星时，重新安装使用TFTP传输数据效果不佳。 Autoreplacer基础架构无法在故障时设置GFE的[ ]。需要多个SRE并行手动执行设置来匹配自动化设置的速度。以下因素导致自动化的缓慢： —SSH超时阻止了Autoreplacer在远程卫星上的操作。 —无论机器是否已具有正确的版本，都执行了慢速内核升级过程。 —Autoreplacer中的并发回归阻止了每个工作机器运行两个以上的机器设置任务。 当23%的目标被移除时，没有触发监控配置的安全检查参数（25%变化），当读取相同内容（剩余的29%）时触发了。导致重新启用卫星监控延迟30分钟。 “安装人员”有限，因此，变更过程困难又缓慢。 使用超级用户权限将机器从diskerase拉回来时留下了很多僵尸进程，导致后续清理困难。 幸运的 核心集群的GFE与卫星GFE的管理方式不同。他们没有受到decom的影响。 同样，YouTube的CDN作为独立的基础设施运行，因此没有受到影响。否则故障将更加严重和持久。 行动列表 由于此事件的广泛性，我们将行动列表分为五个主题： 预防/风险教训 紧急响应 监控/报警 卫星/边缘 清理/其他 表10-1.预防/风险教训 行动列表 类型 优先级 负责人 bug跟踪 审核所有能够将实时服务器转换为宕机状态的系统（即，不仅仅是维修和diskerase工作流） 调查 P1 sydneythree@ BUG1234 提交bug，跟踪BUG1234识别出的所有系统的拒绝错误输入实施的情况。 预防 P1 sydneythree@ BUG1235 禁止任何单个会影响跨命名空间/类边界的服务器操作。 减缓 P1 maxone@ BUG1236 流量管理服务器需要进行安全检查才能在超过[ ]节点数的情况下运行。 减缓 P1 dylanfour@ BUG1237 流量管理服务器应该依据批准破坏性工作。 预防 P0 logantwo@ BUG1238 MDB应拒绝对非预期的当前约束提供值的操作。 预防 P0 louseven@ BUG1239 表10-2.紧急响应 行动列表 类型 优先级 负责人 bug跟踪 确保由核心提供的服务不会使出口网络链路过载。 修复 P2 rileyslx@ BUG1240 确保在[我们的紧急停止文档]和[我们的升级联系页面]下注明了decom工作流问题。 减缓 P2 logantwo@ BUG1241 为decom工作流添加一个大红色禁用按钮a 减缓 P0 maxone@ BUG1242 a : 由于灾难性环境中避免进一步损坏的关闭开关（例如，紧急电源关闭按钮）的常见术语。 表10-3.监控/报警 行动列表 类型 优先级 负责人 bug跟踪 监控目标的安全检查，不允许发布无法回滚的变更。 减缓 P2 dylanfour@ BUG1243 当超过[ ]%的机器下线需要添加报警。16:38机器从卫星上取下，17:10开始世界范围的报警。 监测 P1 rileyslx@ BUG1244 表10-4.卫星/边缘 行动列表 类型 优先级 负责人 bug跟踪 利用IPXE配合HTTPS可以使重新安装更快更可靠。 减缓 P2 dylanfour@ BUG1245 表10-5.清理/其他 行动列表 类型 优先级 负责人 bug跟踪 在我们的工具中查看与MDB相关的代码，并将管理服务器备份放至unwedge调节中。 修复 P2 rileyslx@ BUG1246 安排DiRT测试：在diskerase后带回卫星；对YouTube CDN执行相同操作。 减缓 P2 louseven@ BUG1247 名词解释 管理服务器 RPC服务器，支持自动化为前端服务基础结构执行特权操作。自动化服务器常参与PCR和集群启停操作。 Autoreplacer 将非Borgified服务器从一台机器移动到另一台机器的系统。在机器故障时保持服务运行，并且支持colo重新配置。 Borg 集群管理系统，旨在管理大规模任务和机器资源。Borg拥有Borg单元中所有机器，并将任务分配给具有可用资源的机器。 Decom 退役的缩写。设备的decom是一个与许多运维团队相关的过程。 Diskerase 在生产硬盘驱动器离开Google数据中心前安全擦除生产硬盘的过程（以及相关的硬件/软件系统）。diskerase是decom工作流的一个步骤。 GFE（Google前端） 外部连接(几乎)所有谷歌服务的服务器。 *IMAG（Google事件管理） 一个程序，一种标准，以一致的方式来处理从系统中断到自然灾害的所有类型的事件——并组织有效的响应。 MDB（机器数据库） 事件管理仪表盘/工具，用于跟踪和管理Google所有正在进行的事件的中心位置。 卫星 小巧便宜的机器机架，仅提供提供来自Google网络边缘的非视频、前端流量。几乎没有传统的生产集群基础设施可用于卫星。卫星不同于CDN，它提供来自Google边缘网络的YouTube视频内容，以及来自互联网中更广泛的其他地方的视频内容。YouTube CDN未受此事件的影响。 附录 为什么释放卫星机器不是幂等的？ [该问题的回复已被删除] 管理服务器将所有卫星分配给diskerase团队后发生了什么？ [该问题的回复已被删除] 故障期间真正的QPS损失是多少？ [该问题的回复已被删除] IRC日志 [IRC日志已被删除] 图表 更快的延迟统计——卫星曾为我们做过什么？ 从这次故障经验得到，卫星会在核心集群附近的许多位置产生[ ]ms延迟，离主干更远的位置甚至会达到[ ]ms： [图表的解释已被删除] 核心与边缘服务负载 为重建服务所付出的努力是个很好的例证。边缘服务恢复50%的流量需要大约36小时，恢复到正常的流量水平需要额外的12小时（见图10-1和图10-2）。 来自流量转换的对等压力 [图表省略] 该图显示了由网络区域聚合的数据包丢失情况。在活动期间有一些短的尖峰，但大部分损失发生在卫星覆盖少的各个地区的高峰时刻。 人与机器，GFE [省略了人机与自动机器设置速率的图表说明。] 图10-1.故障期间核心与边缘QPS分布 图10-2.故障期间核心与边缘QPS分布（替代表示） 为什么这份事后总结示例更好？ 这个事后总结符合了好几条写作要求。 明晰 事后总结组织的很好，详细解释了关键术语。例如： 名词解释 一个精心编写的名词解释让事后总结更容易被大众接受和理解。 行动列表 这是一个涉及许多小的行动列表的大事件。按照主题对操作项进行分组可以更加轻松的分配负责人和优先级。 量化指标 事后总结提供了相关事件的有用数据，例如缓存命中率、流量级别和影响持续时间。数据的相关部分将显示原始来源的链接。这种数据透明性消除了歧义并为读者提供了上下文参考。 具体行动列表 没有行动列表的事后总结是无效的。这些行动列表有一些显著特点： 负责人 所以操作项都有负责人和bug跟踪号。 优先级 为所有操作项分配优先级。 可测性 操作项具有可验证的最终状态（例如，“当我们的机器中超过X%的机器下线时添加报警”）。 预防措施 每个操作项“主题”都有预防/缓解操作项，这些操作项有助于避免故障重复发生（例如，“禁止任何单个会影响跨命名空间/类边界的服务器操作。”） 不指责 作者关注的是系统设计中的差距，正是这些差距导致了非预期的故障，例如： 事情进展受挫 没有任何人或团队因此事件受到指责。 根本原因和触发点 关注“什么”出了问题，而不是“谁”造成了这一问题。 行动列表 旨在改善系统而不是改善人。 深度 事后调查不仅仅是调查系统故障的近似区域，也研究了多个团队的影响和系统缺陷。尤其： 影响 本节包含来自不同视角的大量细节，尽可能的做到平衡，客观。 根本原因和触发点 本节对事件进行深入研究，找到根本原因和触发点。 由数据推及结论 提出的所有结论均基于事实和数据。用于得出结论的数据都和文档相关联。 其他资源 以图表的形式进一步呈现有用的信息。向不熟悉系统的读者解释图表帮助其理解上下文。 及时 事件结束后不到一星期就写完并传播了事后总结。快速的事后总结往往更加准确，因为此时任何参与者心理都记着这件事。而受故障影响的人正在等待一个解释，证明你们已经控制了故障。等待的时间越长，他们就越能散发想象，那样对你十分不利。 简明 该事件是全球范围的事件，影响多个系统。事后总结记录了且随后分析了大量数据。冗长的数据源（例如聊天记录和系统日志）被抽象化，未经编辑的版本可从主文档中链接到。总体而言，这份总结在冗长和可读性之间取得了平衡。 组织激励 理想情况下，高级领导应该支持和鼓励有效的事后总结。本节描述了一个组织如何激励健康的事后总结文化。我们着重描述了总结文化失败的征兆，并给出了一些解决方案。同时还提供了工具和模板来简化和自动化事后处理流程。 模型以及对事不对人 为了正确的支持事后总结文化，领导者应始终如一的坚持对事不对人的原则，并在事后讨论中鼓励对事不对人。可以使用一些具体策略来强制组织执行对事不对人这一准则。 使用对事不对人的语言 指责性的语言会影响团队协作。请考虑以下情况： Sandy 错过了服务Foo培训，且不确定如何运行特定的更新命令。因而导致故障时间的延长。 SRE Jesse [对Sandy的leader说]:“你是经理，为什么不确保每个人都完成培训？” 这个交流凸显了一个主要问题，即让收件人处于劣势。更平衡的回应是： SRE Jesse [对Sandy的leader]：“看完事后总结，能够注意到on-call错过了一次重要的培训，导致没有更快的解决故障。因此是否应该要求团队成员加入on-call轮转之前都完成此培训？或者是否可以提醒on-call，如果操作卡住可以尽快升级事件。毕竟，升级不是错误——尤其它有助于降低客户的负担！从长远来看，因为一些细节很容易被忘记，因此我们不能完全依赖培训。” 事后总结的创作要包含所有事件参与者 当事后总结是单人或由单个团队编写时，很容易忽略导致故障的关键因素。 收集反馈 明确的审查过程和事后计划可以帮助防止指责的语言和观点在组织内传播。有关的结构化审核流程请参阅第221页的“事后检查清单”部分。 奖励事后总结 事后总结培训是积极推动组织变革和防止重复故障的有效工具。如果写的够好，采取行动并广泛分享，可以考虑以下策略来激励事后总结文化。 对完成行动列表进行奖励 如果你奖励工程师编写事后总结而不是完成了相关的行动列表，那么可能会出现总结中的行动项目未完成的事情。需要在编写事后总结和成功完成行动计划间平衡奖励措施。 对积极的组织变革进行奖励 你可以将事后总结的重要性作为提高组织影响的依据，通过对标奖金、积极的绩效评估、晋升等作为奖励。来激励并广泛实施事后总结教训。 突出提高可靠性 随着时间的推移，有效的事后总结可以减少故障，让系统更加可靠。因此，团队可以专注于特性功能的开发速度，而不是基础架构的修补上。在总结、演示文稿和绩效评估中强调这些改进在本质上是会提供动力的。 把事后总结的负责人当做领导者 通过电子邮件或会议完成事后总结，或者通过给作者向受众提供经验教训的机会，能够吸引那些喜欢公共赞誉的人。对于寻求同行认可的工程师而言，可以将负责人设置为某种类型的故障的“专家”。例如，你可能会听到有人说：“和Sara说，她现在是专家了。她参与了事后总结的撰写，并且想出了解决问题的方法！” 游戏化 一些人会被成就感和更远大的目标所激励，例如修复系统薄弱点和提高可靠性。对于这些人而言，事后总结行动列表的记录或完成所获得的成就已经是奖励了。在Google，我们每年举办两次“FixIt”周。完成最重要的行动列表项目的SRE会收到小额赞赏和吹牛的权利。图10-3显示了一个事后总结排行榜的示例。 图10-3.事后总结排行榜 公开分享事后总结 为了在组织内保持健康的事后总结文化，要尽可能广泛的分享事后总结。以下策略可以提供一些帮助。 在整个组织内分享总结 在内部沟通渠道、电子邮件、Slack等中宣传事后总结的可用性。如果你负责一个公司，可以分享一个最近的有趣的事后总结。 进行跨团队审核 对事后总结进行跨团队审查。过程中，一个团队过一遍故障，其他团队提出问题并间接学习。在Google，几个办公室都设有非正式的事后总结俱乐部，向所有员工开放。 此外，由开发人员、SRE和组织领导组成的跨职能小组审核整个事后总结流程。他们每个月都会审查事后总结过程和模板的有效性。 进行培训练习 使用“命运之轮”训练新入职工程师：一群工程师重新扮演事后总结的角色，当时的事故总控负责人也参与其中，确保这次演习尽可能的“真实”。 每周总结事件和故障 每周对过去七天内发生的事件和故障的进行总结，并尽可能广泛的进行分享。 响应事后总结文化的失败 事后总结文化的崩溃可能并不明显，以下介绍了常见的故障模式和推荐的解决方案。 逃避 逃避事后总结过程可能是一个组织的事后总结文化失败的征兆，例如，假设SRE 主管 Parker无意中听到以下对话： SWE Sam：哇，你听说了这次的大故障了吗？ SWE Riley：听说了，太可怕了。他们现在得写一个事后总结了。 SWE Sam：不是吧，还好我没有参与进去。 SWE Riley：对啊，我是真的不想参与那个讨论会议。 确保对产生这些抱怨的事件进行高可见度的事后总结可以避免这种逃避。此外，分享高质量的案例并讨论参与的人如何获得奖励将有助于重新团结每个人。。 没有强调文化 当高级管理人员使用责备的语言做出回应可能会使事情更糟糕。假设一个高级领导在会议上对故障做出以下声明： VP Ash：我知道应该对事不对人，但有人事先知道这个操作可能会产生问题，你为什么不听那个人的？ 可以通过更有建设性的话术来减少损害，例如： SRE Dana：我确信每个人的出发点都是好的，所以为了保持对事不对人的准则，我们一般会这样问：是否有任何应该注意到的告警以及我们为何会忽略它们。 对于组织而言，立足正确的出发点，并根据现有的最佳信息做出决策，调查误导性信息的来源比分配责任更有帮助。（如果你知道敏捷原则，那么你应该对这个更加清楚。） 没有时间写事后总结 优质的事后总结撰写是需要时间的。当一个团队负担其他任务时，总结的质量会受到影响。低质量的没有完整的行动列表的事后总结会更容易导致故障复发。事后总结是你写给团队未来成员的信件：以免你不小心教给未来的队友一个错误的教训，保持一致的质量标准十分重要。应该优先考虑事后检查工作，跟踪事后完成情况和审查，并让团队有足够的时间来实施相关的行动计划。我们在第220页的“工具和模板”一节中讨论的工具可以帮助完成此项活动。 重复故障 如果团队在遇到类似故障时采用以前的经验失败了，那么就该深入挖掘了。要考虑以下问题： 行动列表项目是否需要很长时间才能完成？ 故障速度是否超过了可靠性的修复速度？ 最先获得的是正确的行动项目吗？ 重构的故障服务是否过期？ 是否把Band-Aids定位成更严重的问题上了？ 如果你发现了系统性流程或技术问题，应该退一步考虑整体服务运行状况。将每个类似事件的事后总结作者聚集到一起，讨论防止故障重复发生的最佳行动方案。 工具和模板 一组工具和模板可以让编写事后总结和管理相关数据变得更轻松，从而引导事后总结文化。在这一领域，你可以利用Google和其他公司提供的大量资源。 事后总结模板 模板可以使编写和分享完整的事后总结更加轻松。使用标准格式可以使非专业的读者更容易理解事后处理过程。你可以自定义模板。例如，获取特定团队的元数据（如数据中心团队的硬件品牌/型号）或受移动团队影响的Android版本可能更有用。随着团队在这方面的成熟，还可以自定义模板。 Google模板 Google已经通过http://g.co/SiteReliabilityWorkbookMaterials以Google文档格式分享了我们的事后模板。我们在内部主要使用Docs来编写事后总结，可以通过共享编辑权限和注释促进合作。我们的一些内部工具可以使用元数据预填充此模板让事后总结更容易编写。我们利用Google Apps脚本自动化部分创作，并将大量数据捕获到特定的部分和表格中，以便我们的事后总结存储库更容易解析数据。 其他行业模板 其他几家公司和个人分享了他们的事后总结模板： 报警职责 改编原始的Google可靠性站点工程书籍模型 GitHub上托管的四个模板列表 GitHub用户Julian Dunn 服务器故障 事后总结工具 在撰写本文时，Google的事后总结管理工具无法供外部使用（请查看我们的博客获取最新更新）。但是，我们可以解释我们的工具是如何促进事后总结文化的。 事件管理工具 我们的事件管理工具收集并存储大量关于故障的有用数据，并将该数据自动推动到事后总结模板中。我们推送的数据类型包括： 故障指挥人和其他角色 详细的事件时间表和IRC日志 受影响的服务和导致根本原因的服务 事件严重性 事件检测机制 事后总结清单 为了帮助作者确保正确完成事后检查，我们提供了一个事后检查清单，通过关键步骤引导负责人。以下是列表中的一些示例检查： 对事件影响进行全面评估。 进行足够详细的根本原因分析，推动行动列表的规划。 确保行动列表项目通过服务技术主管的审查和批准。 和更多的组织分享事后总结。 完整的清单可在http://g.co/SiteReliabilityWorkbookMaterials找到。 归档事后总结 我们将事后总结归档在一个名为Requiem的工具中，这样任何Google员工都可以轻松找到它们。我们的事件管理工具会自动将所有事后总结推送到Requiem，组织中的任何人都可以发布他们的事后总结给所有人查看。我们有成千上万的总结存档，可以追溯到2009年。Requiem会解析个人事后总结的元数据，使其可以用于搜索、分析和总结。 跟进事后总结 我们的事后总结归档在Requiem的数据库中。任何生成的操作项都会在我们的集中式bug跟踪系统中归档为bug。因此，我们可以监控每个事后总结的行动项目的结束与否。通过这种级别的跟踪，可以确保行动项目不会有漏洞以致服务越来越不稳定。图10-4显示了由我们的工具启用的事后总结操作项监控的模型。 图10-4.事后总结行动项目监控 事后总结分析 我们的事后总结管理工具将信息存储在数据库中以供分析。团队可以使用这些数据编写有关其事后趋势的总结，并确定易受攻击的系统。这有助于我们发现潜在的不稳定因素或可能被忽略的故障管理障碍。例如，图10-5显示了使用我们的分析工具构建的图表。这些图表显示了我们每个组织每月有多少次事后追踪、事件平均持续时间、检测时间、解决时间和故障半径的趋势。 图10-5.事后总结分析 其他行业工具 以下是一些可以帮助你创建、组织和分析事后总结的第三方工具： 报警职责事后调查 Etsy的档案室 VictorOps 尽管完全实现自动化编写事后总结是不可能的，但我们发现事后总结模板和工具会使整个流程更加顺畅。这些工具可以节省时间，让作者能够专注于事后的关键部分，例如根本原因的分析和行动项目计划。 结论 在培养事后总结文化的持续投资中，能够减少故障，为用户提供更好的体验，以及让依赖你的人对你更加信任。这些实践的应用可以使系统设计更完善、宕机时间更短、工程师工作效率更高且工作更快乐。如果最坏的情况确实发生并且故障再次发生，那么你受到的损失会更小并且恢复的更快，而且有更多的数据帮助健壮生产环境。</summary></entry><entry><title type="html">第九章 故障响应</title><link href="http://localhost:4000/sre/2020/01/09/%E4%BA%8B%E4%BB%B6%E5%93%8D%E5%BA%94/" rel="alternate" type="text/html" title="第九章 故障响应" /><published>2020-01-09T00:00:00+08:00</published><updated>2020-01-09T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/09/%E4%BA%8B%E4%BB%B6%E5%93%8D%E5%BA%94</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/09/%E4%BA%8B%E4%BB%B6%E5%93%8D%E5%BA%94/">&lt;!-- more --&gt;
&lt;blockquote&gt;
  &lt;p&gt;写在开头:&lt;/p&gt;

  &lt;p&gt;故障总会发生！如何对故障快速进行有组织的响应？Google方式是一个很好的借鉴。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每个团队都希望服务没有故障发生，但这个世界是不完美的，像停电这类的事件还是会发生的。当一个紧急故障需要多人或团队协作解决时，响应故障并解决问题是我们要应对的。&lt;/p&gt;

&lt;p&gt;处理故障的目标是减轻影响或将服务恢复到先前的状态。而故障管理意味着以有效的方式协调团队的响应工作，并确保响应者和利益相关者之间的信息流通。包括谷歌在内的许多科技公司都有自己的“最佳实践”来管理应急响应，并且在不断的完善。&lt;/p&gt;

&lt;p&gt;故障管理的基本前提是以结构化的方式响应故障。大规模故障是单人无法解决的；结构化的应急响应可以减少混乱。在灾难发生之前制定沟通和协调规范，使你的团队可以集中精力解决故障，而不用担心沟通和协调问题。&lt;/p&gt;

&lt;p&gt;故障响应的过程并不是困难的事情。大量实践经验总结可以为我们提供一些指导，比如在第一本SRE书中的《故障管理》章节。故障响应的基本原则包括以下内容：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;保持清晰的指挥线&lt;/li&gt;
  &lt;li&gt;明确角色&lt;/li&gt;
  &lt;li&gt;随时掌握调试工作记录&lt;/li&gt;
  &lt;li&gt;响应进度的实时更新&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本章介绍了如何在Google和PagerDuty中进行故障管理。通过一些Case说明我们在其中达到的目标以及没有达到的目标。 “将最佳实践付诸实践”（第191页）中的清单可以帮助你开始创建自己的故障响应实践。&lt;/p&gt;

&lt;h2 id=&quot;google故障管理&quot;&gt;Google故障管理&lt;/h2&gt;

&lt;p&gt;故障响应是用于响应和管理故障的系统。由一个框架和一组定义的程序组成。Google的故障响应系统是基于故障命令系统（ICS）实现的。&lt;/p&gt;

&lt;h3 id=&quot;故障指挥系统&quot;&gt;故障指挥系统&lt;/h3&gt;

&lt;p&gt;ICS成立于1968年，是消防员管理野火的一种方式。该框架提供了在故障响应期间进行通信和明确角色的标准化方法。基于该模型，公司后来采用ICS来应对计算机和系统故障。本章探讨了两个这样的框架：PagerDuty的故障响应流程和Google故障管理（IMAG）。&lt;/p&gt;

&lt;p&gt;故障响应框架有三个共同目标，也称为事件管理的“三个C”（3C）：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;协调响应工作&lt;/li&gt;
  &lt;li&gt;在故障响应者、组织内部和外部之间进行通信&lt;/li&gt;
  &lt;li&gt;保持对故障响应的控制&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当故障响应出现问题时，罪魁祸首可能出现在其中一个方面。掌握3C对于有效的事件响应至关重要。&lt;/p&gt;

&lt;h3 id=&quot;故障响应中的主要角色&quot;&gt;故障响应中的主要角色&lt;/h3&gt;

&lt;p&gt;故障响应中的主要角色是故障指挥官（IC），通信主管（CL）和操作或行动主管（OL）。 IMAG将这些角色组织成一个层次结构：IC引导故障响应，CL和OL报告给IC。&lt;/p&gt;

&lt;p&gt;灾难发生时，声明故障的人通常会进入IC角色并指挥故障的高级别状态。 IC专注于3C，并执行以下操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;命令和协调故障响应，根据需要委派角色。默认情况下，IC会假定尚未委派的所有角色。&lt;/li&gt;
  &lt;li&gt;有效沟通。&lt;/li&gt;
  &lt;li&gt;保持对故障响应的控制。&lt;/li&gt;
  &lt;li&gt;与其他相应人员合作解决此事件。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;IC可以将其角色交给其他人并承担OL角色，或将OL角色分配给其他人。 OL通过应用工具来缓解或解决故障，从而对故障做出响应。&lt;/p&gt;

&lt;p&gt;虽然IC和OL致力于减轻和解决故障，但CL是故障响应团队的消息通讯核心。 CL的主要职责包括定期向故障响应团队和利益相关者实时更新故障响应的进展。&lt;/p&gt;

&lt;p&gt;CL和OL都可以领导一个团队来帮助管理他们特定的故障响应区域。这些团队可以根据需要进行扩展或收缩。如果故障变得足够小，那么CL角色就可以被包含到IC角色中。&lt;/p&gt;

&lt;h2 id=&quot;案例研究&quot;&gt;案例研究&lt;/h2&gt;

&lt;p&gt;以下四个大型故障说明故障响应如何在实践中起作用。其中三个案例研究来自Google，最后一个是来自PagerDuty的案例研究，该研究提供了其他组织如何使用ICS派生框架的观点。Google的例子以一个没有有效管理的故障开始，并在有故障管理时对故障响应的改观。&lt;/p&gt;

&lt;h3 id=&quot;案例研究1软件错误灯还开着但没有人google-home&quot;&gt;案例研究1：软件错误—灯还开着，但没有人(Google HOME)&lt;/h3&gt;

&lt;p&gt;这个例子展示了故障发生初期不进行通报，在没有工具快速有效地响应故障时，团队是如何响应故障的。虽然这一故障在没有重大灾难的情况下得到了解决，但是越早组织，就会产生更好的效果。&lt;/p&gt;

&lt;h4 id=&quot;背景&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;Google Home是一款智能扬声器和家庭助手，可响应语音命令。语音命令与Google Home的软件（称为Google智能助理）进行交互。&lt;/p&gt;

&lt;p&gt;当用户说出一个触发Google智能助理的热门词组时，就会启动与Google Home的互动。多个用户可以通过培训助理来监听给定的热门词汇，从而使用相同的Google Home设备。识别扬声器的热词模型是在客户端进行的，但是数据（例如扬声器识别文件）存储在服务器上。服务器处理数据的双向流，为了在繁忙的时候处理超载，服务器对Google助手有一个配额政策。为了保护服务器免受过大请求值的影响，配额限制明显高于在给定设备上的Google助手的基准使用量。&lt;/p&gt;

&lt;p&gt;Google智能助理1.88版中的错误导致扬声器识别文件的获取频率超过预期的50倍，超出此配额。最初，美国中部的GoogleHome用户只有很少的流量损失。然而，随着所有GoogleHome设备的推出逐步增加，用户在2017年6月3日的周末期间丢失了一半的请求。&lt;/p&gt;

&lt;h4 id=&quot;故障&quot;&gt;故障&lt;/h4&gt;

&lt;p&gt;太平洋标准时间5月22日星期一上午11:48，谷歌主页on-call开发人员Jasper正在看着每秒查询（QPS）图表并注意到一些奇怪的事情：谷歌助理每隔30分钟就会收到培训数据，而不是像预期的那样每天一次。他停止了已经推到了25％用户的1.88版本发布。他提了一个漏洞——称之为bug 12345——用谷歌的漏洞追踪系统来探索为什么会发生这种情况。在这一问题上，他指出，谷歌助手每天会对数据进行48次的ping请求，导致其超过了QPS的容量。&lt;/p&gt;

&lt;p&gt;另一位开发人员Melinda将此问题与先前报告的漏洞相关联，我们将其称为错误67890：每当应用程序刷新设备身份验证和注册状态时，语音处理器都会重新启动。该版本将在版本1.88发布后修复，因此该团队要求临时增加模型的配额，以减轻额外查询的过载。&lt;/p&gt;

&lt;p&gt;版本1.88版本再次启动并继续推出，到5月31日星期三达到50％的用户。不幸的是，该团队后来了解到错误67890，虽然负责一些额外的流量，但并不是Jasper所注意到的更频繁的取回的真正根源。&lt;/p&gt;

&lt;p&gt;同一天早上，客户开始向Google支持小组报告问题：每当有人说“OK Google”（或任何其他热门词汇来激活Google Home）时，设备都会回复并显示错误消息，此问题阻止用户向Google智能助理发出命令。该团队开始调查可能导致用户报告的错误的原因。他们怀疑配额问题，所以他们要求增加配额，这似乎可以缓解这个问题。&lt;/p&gt;

&lt;p&gt;与此同时，该团队继续调查bug 12345，看看是什么触发了错误。尽管在调试过程的早期就建立了配额连接，但是客户端和服务器开发人员之间的误解导致了开发人员在故障排除过程中走错了方向，而完整的解决方案仍然无法实现。&lt;/p&gt;

&lt;p&gt;该团队还对为什么Google智能助理的流量不断达到配额限制感到困惑。客户端和服务器开发人员对客户端错误感到困惑，这些错误似乎没有被服务器端的任何问题触发。开发人员将日志记录添加到下一个版本，以帮助团队更好地理解错误，并希望在解决事件方面取得进展。&lt;/p&gt;

&lt;p&gt;截至6月1日星期四，用户报告此问题已得到解决。没有报道任何新问题，因此版本1.88版本继续推出。但是，原始问题的根本原因尚未确定。&lt;/p&gt;

&lt;p&gt;在6月3日的周六早上，版本1.88的发布超过了50%。这一发布是在一个周末进行的，当时开发团队并没有人值班。该团队并没有遵循在工作日期间执行部署的最佳实践，以确保开发人员在场。&lt;/p&gt;

&lt;p&gt;在6月3日星期六，当版本1.88发布的时候达到了100%，客户端又一次达到了Google助理流量的服务器限制。来自客户的错误报告开始出现。谷歌员工报告称，他们的Google HOME设备出现了错误。Google Home支持小组收到了大量关于此问题的客户来电，反馈也包括了推文和Reddit帖子。 Google Home的帮助论坛也出现了正在讨论这个问题的帖子。尽管有大量的用户报告和反馈，bug并没有升级到更高的优先级。&lt;/p&gt;

&lt;p&gt;6月4日星期日，随着客户报告数量的不断增加，支持团队最终将错误优先级提升到最高水平。该团队没有通报故障，而是继续通过“常规”方法解决问题—使用bug跟踪系统进行通信。on-call开发人员注意到一个数据中心集群中的错误率，SRE进行了ping操作，要求他们排除它。与此同时，该团队提出了另一项增加配额的请求。之后，开发团队的一名工程师注意到流量通道已将错误推入其他单元格，这为配额问题提供了额外的证据。下午3：33，开发人员团队经理再次增加了Google智能助理的配额，并停止了对用户的影响。故障结束。该团队此后不久确定了根本原因（参见前面的“环境”部分）。&lt;/p&gt;

&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;

&lt;p&gt;故障处理的某些方面进展顺利，而其他方面则有改进的余地。&lt;/p&gt;

&lt;p&gt;首先，工程师在周末处理故障并为解决问题提供了宝贵的意见，这既好又坏，虽然团队重视这些人在周末所付出的时间和精力，但成功的故障管理不应该依赖于个人的英勇行为。如果无法联系到开发人员怎么办？Google鼓励良好的工作与生活平衡——工程师不应在空闲时间内解决与工作相关的问题。相反，我们应该在工作时间进行部署，或者组织一次在工作时间之外提供付费保障的on-call轮换。&lt;/p&gt;

&lt;p&gt;接下来，该团队致力于缓解这一问题。Google首先要求是阻止故障的影响，然后找到根本原因（除非根本原因只是在早期发现）。一旦问题得到缓解，找到根本原因同样重要，以防止问题再次发生。在这个故障事件中，成功地在三个不同的场合控制了故障影响，但团队只有在发现根本原因后才能防止问题再次发生。在第一次故障得到控制之后，最好是等到根本原因完全确定后再开始实施，避免周末发生大混乱。最后，在问题首次出现时，团队没有通告故障。我们的经验表明，管理故障可以更快地解决。尽早通告故障可确保：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;防止客户端和服务器开发人员之间的沟通错误。&lt;/li&gt;
  &lt;li&gt;根源识别和故障解决发生得更快。&lt;/li&gt;
  &lt;li&gt;相关团队提前进入，使外部沟通更快捷顺畅。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集中式通信是IMAG协议的重要原则。例如，当灾难发生时，SRE通常聚集在一个“作战室”中。作战室可以是像会议室一样的真实环境，也可以是虚拟的：团队可能聚集在IRC频道。这里的关键是将所有的故障响应者集中在一个地方，并实时沟通以管理并最终解决故障。&lt;/p&gt;

&lt;h3 id=&quot;案例研究2服务故障如果可以请对我进行缓存&quot;&gt;案例研究2：服务故障—如果可以，请对我进行缓存&lt;/h3&gt;

&lt;p&gt;下面的故障说明了当一个“专家级”团队试图调试一个具有复杂交互系统时会发生什么——没有一个人能够掌握所有的细节。&lt;/p&gt;
&lt;h4 id=&quot;背景-1&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;Kubernetes是一个由许多公司和个人贡献者共同建立的开源容器管理系统。 Google Kubernetes Engine（或称GKE）是Google管理的系统，可为用户创建、托管和运行Kubernetes群集。用户以最适合他们的方式上传和管理工作负载。当用户首次创建新群集时，GKE将获取并初始化其群集所需的Docker镜像。理想情况下，这些组件是在内部获取和构建的，因此我们可以验证它们。但Kubernetes是一个开源系统，新的依赖关系有时会让系统出现问题。&lt;/p&gt;

&lt;h4 id=&quot;故障-1&quot;&gt;故障&lt;/h4&gt;

&lt;p&gt;在太平洋标准时间一个星期四上午6点41分，伦敦GKE的on-call人员Zara寻访了多个区域的CreateCluster探查器故障，新的集群没有被成功地创建。 Zara检查了prober仪表板，发现两个区域的故障率都在60％以上。他确认此问题影响了用户创建新群集的功能，但现有群集的流量未受影响。 Zara遵循GKE的文件化程序，于上午7:06对故障进行了通告。最初，有四人参与处理了这一故障：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Zara是第一个注意到这个问题的人，因此被指定为默认故障指挥官。&lt;/li&gt;
  &lt;li&gt;Zara的两名队友。&lt;/li&gt;
  &lt;li&gt;Rohit由故障程序分配的客户支持工程师。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于Rohit总部设在苏黎世，Zara（IC）开设了一个GKE Panic IRC频道，团队可以在那里一起调试。在另外两个SRE深入调查监控和错误信息时，Zara解释了故障及其对Rohit的影响。截至上午7点24分，Rohit向用户发布通知称，创建集群的功能在欧洲西部地区出现故障。这提高了故障的等级。早上7点到8点20分，Zara、Rohit和其他人一直致力于解决这个问题。他们检查了集群启动日志，其中显示了一个错误：无法运行Kubelet；无法创建证书签名——他们需要确定证书创建的哪个部分失败。SRE研究了网络、资源可用性和证书签名过程。所有这些似乎都运作得很好。上午8点22分，Zara向故障管理系统发布了故障调查摘要，并寻找可以帮助他的开发人员。值得庆幸的是，GKE有一位on-call开发人员可以在紧急情况下提供协助。开发人员Victoria加入了该频道。他要求跟踪错误，并要求团队将问题上报给基础架构的on-call团队。现在是上午8点45分，第一个西雅图SRE，II-Seong来到办公室，轻轻地泡了杯咖啡，为这一天做好了准备。 II-Seong是一名资深的SRE，在故障响应方面拥有多年的经验。当他得知正在发生的故障时，他立即进入故障响应频道。首先，II-Seong根据警报的时间检查当天的发布情况，并确定当天的发布不会导致事故障的发生。然后，他开始整理工作文件，收集笔记。他建议Zara将故障升级为基础架构，云网络和计算引擎团队，以尽可能多地消除这些根源。由于Zara升级，其他人加入了故障响应：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GKE节点的开发负责人&lt;/li&gt;
  &lt;li&gt;云网络值班人员&lt;/li&gt;
  &lt;li&gt;计算引擎值班人员&lt;/li&gt;
  &lt;li&gt;Herais，另一个西雅图SRE&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上午9点10分，故障频道有十几个参与者。故障发生2.5小时，没有找到根本原因，也没有控制住影响。沟通成为了一种挑战。通常情况下，从伦敦到西雅图的on-call交接时间是上午10点，但Zara决定在上午10点之前将故障指挥权移交给II-Seong，因为他对IMAG有更多的经验。作为故障指挥官，II-Seong建立了一个正式的响应组织结构来解决这一事件。然后，他指定Zara为Ops Lead，Herais为通信（Comms）负责人。 Rohit仍然是外部沟通负责人。 Herais立即发送了一封“全体人员齐上阵”的电子邮件给GKE，包括所有开发人员的负责人，并要求专家加入故障响应。到目前为止，故障响应者了解到了以下情况：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当节点试图向主机注册时，集群创建失败。&lt;/li&gt;
  &lt;li&gt;错误消息表明证书签名模块是罪魁祸首。&lt;/li&gt;
  &lt;li&gt;欧洲所有集群创建都失败了;其地区都正常。&lt;/li&gt;
  &lt;li&gt;欧洲没有其他GCP服务出现网络或配额问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;之后，GKE安全团队成员Puanani加入了这项工作。他注意到证书签发者没有开始。证书签发者试图从DockerHub中提取图像，图像似乎已损坏。Victoria（GKE on-call开发人员）在两个地理位置运行了Docker的图像拉取命令。它在欧洲的集群上运行失败，在美国的集群上成功。这表明欧洲集群是问题的所在。上午9点56分，该团队确定了一个看似合理的根本原因。&lt;/p&gt;

&lt;p&gt;因为DockerHub有一个外部依赖，故障控制和故障溯源将特别具有挑战性。对于Docker的工作人员来说，第一个控制故障的方法是快速修复。第二个选择是重新配置集群，从不同的位置获取图像，例如Google容器注册表（GCR），Goole的安全图像托管系统。所有其他依赖项，包括对图像的其他引用，都位于GCR中。 II-Seong指派负责人追查这两种可能。然后，他委派了一个小组去调查失败的集群。讨论对于IRC来说过于密集，因此详细的调试转移到共享文档，IRC成为决策的中心。&lt;/p&gt;

&lt;p&gt;对于第二个可能，推送新配置意味着重建二进制文件，这需要大约一个小时。上午10:59，当团队完成90％的重建工作时，他们发现另一个使用错误图像获取路径的位置。作为回应，他们不得不重新启动构建。当IRC的工程师们致力于这两个缓解方案时，SRE中的Tugay有了一个想法。如果他们拦截了Docker的pull请求并使用内部缓存图像替换Docker的响应，那么如何重建配置并将其推出（一个笨重且有风险的过程）呢？ GCR有一个镜像可以做到这一点。 Tugay联系了GCR的SRE团队，他们确认团队可以在Docker配置上设置&amp;lt;–registry-mirror =https://mirror.gcr.io&amp;gt;。Tugay开始设置此功能，并发现镜像已经到位！&lt;/p&gt;

&lt;p&gt;上午11点29分，Tugay向IRC报告说这些图像是从GCR镜像中拉出来的，而不是DockerHub。上午11点37分，故障指挥官在呼叫GCR值班人员。上午11点59分，GCR值班人员在欧洲存储层清除了腐败的图像。截止到下午12点11分，所有欧洲地区的误差都降至0％。&lt;/p&gt;

&lt;p&gt;故障结束了。剩下的只是清理工作，故障文档的整理。在修复之前，CreateCluster在欧洲失败了6小时40分钟。在IRC中，整个事件中出现了41个独立用户，IRC日志扩展到26,000个单词。这项工作在不同的时间分拆了七个IMAG工作组，并且在任何时候都有多达四个同时工作。六支on-call队伍的参与，后期包含28个行动项目。&lt;/p&gt;

&lt;h4 id=&quot;总结-1&quot;&gt;总结&lt;/h4&gt;

&lt;p&gt;无论从哪个角度，GKE CreateCluster中断是一件重大故障。让我们探讨一下哪些事情进行得很顺利，哪些事情本可以处理得更好。&lt;/p&gt;

&lt;p&gt;什么进展顺利？该团队有几个记录在案的升级路径，熟悉故障响应策略。GKE值班人员 Zara很快就证实了这种影响正在影响实际客户。然后，她使用了一个常用的故障管理系统来通知Rohit，Rohit将故障告知客户。&lt;/p&gt;

&lt;p&gt;什么可以更好地处理？该服务本身有一些值得关注的方面。复杂性和对专家的依赖是有问题的。记录对于故障定位来说是不够的，并且团队因为DockerHub上的变化而分散了注意力，而且这不是真正的问题。&lt;/p&gt;

&lt;p&gt;故障发生之初，故障指挥官没有制定一个正式的应急预案。虽然Zara承担了这一角色并将对话转移到IRC，但他可以更积极地协调信息和做出决策。结果，少数应急人员在没有协调的情况下进行自己的调查。II-Seong在第一个警报后两小时就建立了正式的故障响应组织。&lt;/p&gt;

&lt;p&gt;最后，该故障揭示了GKE灾难准备方面的一个不足：该服务没有任何可以减少用户影响的早期通用预案。通用预案是第一响应者采取的降低影响的操作流程，甚至在完全理解根本原因之前都可以使用。例如，当中断与发布周期相关时，响应者可以回滚最近的版本，或者重新配置负载平衡器以避免错误被本地化时出现区域。值得注意的是，通用预案也有弊端，可能会导致其他服务中断。然而，虽然它们可能具有比精确解决方案更广泛的影响，但是当团队发现并解决根本原因时，它们可以快速到位以停止进一步扩大影响。&lt;/p&gt;

&lt;p&gt;让我们再次查看此故障的时间表，看看通用预案可能在哪些方面有效：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;上午7点（评估影响）。 Zara确认用户受到中断的影响。&lt;/li&gt;
  &lt;li&gt;上午9:56（找到可能的原因）。 Puanani和Victoria确定了一个可能的根本原因。&lt;/li&gt;
  &lt;li&gt;上午10点59分（定制缓解措施）。几个团队成员致力于重建二进制文件以推送一个新配置，该配置将从不同位置获取图像。&lt;/li&gt;
  &lt;li&gt;上午11:59（找到根本原因并解决问题）。 Tugay和GCR值班人员取消了GCR缓存，并从其欧洲存储层中清除了损坏的图像。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在步骤2（找到可能的原因）之后的通用预案在这里将非常有用。如果响应者在发现问题后将所有发布回滚到已知的良好状态，则故障将在上午10点之前得到缓解。为了缓解故障，不必完全了解详细信息——你只需要知道根本原因的位置即可。能够在完全理解其中断之前缓解中断对于运行具有高可用性的强大服务至关重要。&lt;/p&gt;

&lt;p&gt;在这种情况下，响应者可以从某种回滚的工具中受益。通用预案工具确实需要时间来开发。创建通用缓解工具的正确时间是在故障发生之前，而不是在应对紧急情况时。浏览postmortems是一种发现缓解故障的好方法，这些缓解故障在回滚过程中非常有用，并将它们构建到服务中，以便在将来更好地管理故障。&lt;/p&gt;

&lt;p&gt;重要的是要记住，第一响应者必须优先考虑通用预案，否则解决问题的时间还会拉的很长。实施通用预案措施（例如回滚）可加快恢复速度并使客户更快乐。最终，客户并不关心你是否完全理解导致中断的原因。他们想要的是停止接收错误。将控制影响作为首要任务，应以下列方式处理积极故障:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;评估故障的影响。&lt;/li&gt;
  &lt;li&gt;减轻影响。&lt;/li&gt;
  &lt;li&gt;对故障进行根本原因分析。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;故障结束后，定位故障的原因并写下事后分析报告。之后，你可以运行故障响应练习演练来修复系统中的漏洞，并且工程师可以在项目中处理这些漏洞。&lt;/p&gt;

&lt;h3 id=&quot;案例研究3断电闪电从未发生过两次直到它发生&quot;&gt;案例研究3：断电—闪电从未发生过两次……直到它发生&lt;/h3&gt;

&lt;p&gt;前面的示例表明，如果没有良好的故障响应策略时，会出现什么问题。下一个示例演示了成功管理的事件。当你遵循一个定义良好且清晰的响应协议时，您甚至可以轻松地处理罕见或异常的事件。&lt;/p&gt;

&lt;h4 id=&quot;背景-2&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;电网事件（例如雷击）导致进入数据中心设施的电力变化很大。影响电网的雷击是罕见的，但并不出人意料。Google可以使用备用发电机和电池来防止突发的意外断电，这些设备经过了充分的测试，并且已知可以在这些情况下正常工作。&lt;/p&gt;

&lt;p&gt;Google的许多服务器都有大量的磁盘连接到它们，这些磁盘位于服务器上方或下方的一个单独的托盘上。这些托盘有自己的不间断电源(UPS)电池。当停电发生时，备用发电机会启动，但启动需要几分钟时间。在此期间，连接到服务器和磁盘托盘上的备用电池提供电力，直到备用发电机完全运行，从而防止电网事件影响数据中心的运行。&lt;/p&gt;

&lt;h4 id=&quot;故障-2&quot;&gt;故障&lt;/h4&gt;

&lt;p&gt;2015年年中，比利时谷歌数据中心附近的电网在两分钟内被闪电击中四次。数据中心的备用发电机被激活，为所有的机器供电。当备份发电机启动时，大多数服务器都使用备用电池运行了几分钟。&lt;/p&gt;

&lt;p&gt;磁盘托盘中的UPS电池并没有在第三次和第四次雷击时将电量用在备用电池上，因为雷击间隔太近了。结果，磁盘托盘失去了电力，直到备用发电机开始工作。这些服务器没有断电，但无法访问那些有电的磁盘。&lt;/p&gt;

&lt;p&gt;在持久磁盘存储中丢失大量磁盘托盘会导致许多在Google Compute Engine(GCE)上运行的虚拟机(VM)实例出现读写错误。持久磁盘SRE在调用时立即发送了这些错误。一旦持久磁盘SRE团队确定了影响，就会向所有受影响的各方通报故障。持久磁盘SRE on-call人员承担了故障指挥官的角色。&lt;/p&gt;

&lt;p&gt;经过利益相关者之间的初步调查和沟通，我们确定:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;由于临时断电而丢失磁盘托盘的每台机器都需要重新启动。&lt;/li&gt;
  &lt;li&gt;在等待重新启动时，一些客户VMs在读取和写入磁盘时出现问题。&lt;/li&gt;
  &lt;li&gt;任何同时具有磁盘托盘和客户vm的主机都不能在不丢失未受影响的客户vm的情况下“重新启动”。持久磁盘SRE请求GCE SRE将未受影响的vm迁移到其他主机。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;持久磁盘SRE的主要值班人员的保留了IC角色，因为该团队对客户影响的可视性最强。&lt;/p&gt;

&lt;p&gt;运维团队成员的任务如下:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;安全恢复电源，使用电网电源而不是备用发电机。&lt;/li&gt;
  &lt;li&gt;重新启动所有非vm主机。&lt;/li&gt;
  &lt;li&gt;协调持久磁盘SRE和GCE SRE，在重新启动之前安全地将vm从受影响的机器移开。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;前两个目标被清楚地定义、很好地理解和记录。数据中心运维值班人员立即开始安全恢复电源，定期向IC提供状态报告。持久磁盘SRE定义了重新启动所有机器而不是虚拟机的程序。一个团队成员开始重新启动这些机器。&lt;/p&gt;

&lt;p&gt;第三个目标更加模糊，不包括任何现有的程序。故障指挥员指派了一个专门的行动小组成员与GCE SRE和持久磁盘SRE进行协调。这些团队合作将VMs安全地从受影响的机器移开，以便重新启动受影响的机器。IC密切关注着他们的进展，并意识到这项工作需要快速编写新的工具。IC组织了更多的工程师向运维团队报告，以便他们能够创建必要的工具。&lt;/p&gt;

&lt;p&gt;沟通负责人观察并询问所有与事件相关的活动，并负责向多个受众报告准确的信息：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;公司领导需要关于问题严重程度的信息，并确保问题得到解决。&lt;/li&gt;
  &lt;li&gt;有存储问题的团队需要知道他们的存储何时可以再次完全可用。&lt;/li&gt;
  &lt;li&gt;需要主动告知外部客户他们的磁盘在这个云区域的问题。&lt;/li&gt;
  &lt;li&gt;提交支持票据的特定客户需要知道他们所看到的问题的更多信息，以及关于解决方案和时间安排的建议。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在我们减轻了最初的客户影响之后，我们需要做一些后续工作，例如:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;诊断为什么磁盘托盘使用的UPS失败，并确保它不会再次发生。&lt;/li&gt;
  &lt;li&gt;更换发生故障的数据中心的电池。&lt;/li&gt;
  &lt;li&gt;手动清除由于同时丢失这么多存储系统而导致的“卡住”操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;事后分析显示，只有一小部分未写入磁盘——在事故发生期间断电的机器上的等待写入操作。由于持久磁盘快照和所有云存储数据都存储在多个数据中心中以实现冗余，因此只有0.000001%的运行GCE计算机的数据丢失，并且只有运行实例的数据存在风险。&lt;/p&gt;

&lt;h4 id=&quot;总结-2&quot;&gt;总结&lt;/h4&gt;

&lt;p&gt;通过及早通报故障，并以明确的领导组织结构有效地处理了这个复杂的故障。&lt;/p&gt;

&lt;p&gt;故障指挥官将恢复电源和重启服务器的正常问题委托给了适当的运维负责人。工程师们致力于解决这个问题，并将他们的进展报告给运维主管。&lt;/p&gt;

&lt;p&gt;要同时满足GCE和持久磁盘的需求，更复杂的问题需要在多个团队之间进行协调决策和交互。事故指挥员确保从两个小组中指派适当的行动小组成员来处理事故，并直接与他们一起工作，朝着解决问题的方向努力。事故指挥官明智地将注意力集中在事故最重要的方面：尽快解决受影响客户的需求。&lt;/p&gt;

&lt;h3 id=&quot;案例研究4pagerduty事件响应&quot;&gt;案例研究4：PagerDuty事件响应&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;作者：PagerDuty的Arup Chakrabarti&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PagerDuty在几年的时间里开发并完善了内部故障响应实践。最初，配备了一名常设的全公司故障指挥官，并为每个服务配备专门的工程师参与故障响应。随着PagerDuty发展到超过400名员工和几十个工程团队，故障响应过程也发生了变化。每隔几个月都会仔细检查故障响应流程，并更新它们以反映业务需求。所有的经验总结都记录在https://response.pagerduty.com上。我们的故障响应过程并不是一成不变的；它们就像我们的业务一样不断变化和发展。&lt;/p&gt;

&lt;h4 id=&quot;pagerduty重大事件响应&quot;&gt;PagerDuty重大事件响应&lt;/h4&gt;

&lt;p&gt;通常，小的故障只需要一个值班工程师来响应。当涉及到更大的故障时，我们非常重视团队合作。在高压力和高影响的情况下，工程师不应该感到孤独。我们使用以下技巧来促进团队合作：&lt;/p&gt;

&lt;h4 id=&quot;参与模拟演习&quot;&gt;参与模拟演习&lt;/h4&gt;

&lt;p&gt;我们教授团队合作的一个方法是参加“失败星期五”。PagerDuty从Netflix公司的Simian Army(猿人部队)里汲取灵感，制作了这个节目。最初，Failure Friday是一个手动的故障注入练习，目的是了解更多关于我们的系统可能崩溃的方式。今天，我们还使用这个每周练习来重现生产和事件响应场景中的常见问题。&lt;/p&gt;

&lt;p&gt;在“失败星期五”开始之前，我们提名一个故障指挥官(通常是一个训练成为IC的人)。在进行故障注入练习时，他们应该表现得像真正的IC一样。在整个演练过程中，主题专家使用与实际事件相同的过程和术语。这种做法既使新值班工程师熟悉事故障响应语言和流程，又为经验丰富的值班工程师提供了一种复习。&lt;/p&gt;

&lt;h4 id=&quot;玩限时模拟游戏&quot;&gt;玩限时模拟游戏&lt;/h4&gt;

&lt;p&gt;虽然“失败星期五”练习对工程师在不同角色和过程中培训大有帮助，但它们不能完全复制实际重大事故的紧迫性。我们使用具有时限紧迫性的模拟游戏来捕捉事件响应的这一方面。&lt;/p&gt;

&lt;p&gt;“继续说下去，没有人会爆炸”是我们大量使用的一款游戏。它要求玩家在限定时间内共同拆除炸弹。游戏的压力和密集的交流性质迫使玩家有效地合作和有效地协同工作。&lt;/p&gt;

&lt;h4 id=&quot;从以往的故障中吸取教训&quot;&gt;从以往的故障中吸取教训&lt;/h4&gt;

&lt;p&gt;从过去的故障中学习可以帮助我们更好地应对未来的重大故障。为此，我们进行并定期审查故障分析报告。&lt;/p&gt;

&lt;p&gt;PagerDuty事后调查过程包括开放式会议和全面记录。通过使这些信息易于访问和发现，我们的目标是减少未来事件的解决时间，或防止未来故障一起发生。&lt;/p&gt;

&lt;p&gt;我们还会记录所有涉及重大故障的电话，这样我们就可以从实时通信feed中学习。&lt;/p&gt;

&lt;p&gt;让我们看看最近的一个故障，其中PagerDuty不得不利用我们的故障响应流程。故障发生在2017年10月6日，持续时间超过10个小时，但对客户的影响非常小。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;下午7:53 PagerDuty SRE团队的一名成员被告知PagerDuty内部NTP服务器正在显示时钟漂移。值班的SRE验证了所有自动恢复操作已经执行，并完成了相关运行手册中的缓解步骤。这个工作记录在SRE团队的专用Slack频道中。&lt;/li&gt;
  &lt;li&gt;晚上8点20分，PagerDuty软件团队A的一名成员收到了关于他们服务中时钟漂移错误的自动警报。软件团队A和SRE团队致力于解决这个问题。&lt;/li&gt;
  &lt;li&gt;晚上9点17分，PagerDuty软件团队B的一名成员收到了关于他们服务上时钟漂移错误的自动警报。B组的工程师加入了Slack频道，该频道已经对问题进行了测试和调试&lt;/li&gt;
  &lt;li&gt;晚上9点49分，值班SRE宣布发生重大故障，并通知值班故障指挥官。&lt;/li&gt;
  &lt;li&gt;晚上9点55分，IC组建了响应团队，其中包括依赖NTP服务的每个on-call工程师，以及PagerDuty的客户支持。IC让响应小组加入专门的电话会议和Slack频道。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在接下来的8个小时里，响应小组致力于解决和减轻这个问题。当我们运行手册中的程序没有解决问题时，响应团队开始有条理地尝试新的恢复选项。&lt;/p&gt;

&lt;p&gt;在这段时间里，我们每四个小时轮换一次on-call工程师和IC。这样做可以鼓励工程师们休息，并为响应团队带来新的想法。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;上午5：33，值班的SRE对NTP服务器进行了配置更改。&lt;/li&gt;
  &lt;li&gt;上午6点13分，IC与他们各自的值班工程师验证所有的服务都恢复了。验证完成后，IC关闭了电话会议和Slack频道，并宣布故障完成。鉴于NTP服务的广泛影响，有必要进行事后分析。在结束故障之前，IC将事后分析分配给值班的SRE小组。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;用于故障响应的工具&quot;&gt;用于故障响应的工具&lt;/h4&gt;

&lt;p&gt;我们的故障响应流程利用了三个主要工具：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PagerDuty。我们将所有值班的信息、服务所有权、事后分析、事件元数据等存储在PagerDuty中。这使我们能够在出现问题时迅速组建正确的团队。&lt;/li&gt;
  &lt;li&gt;Slack。我们保持一个专门的频道(#incident-war-room)，作为所有主题专家和故障指挥官的聚会场所。该频道主要用作记录员的信息分类，用于捕获操作、所有者和时间戳。&lt;/li&gt;
  &lt;li&gt;电话会议&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当被要求加入任何故障响应时，on-call的工程师需要拨打一个固定的会议电话号码。我们希望所有的协调决策都在电话会议中做出，而决策结果都记录在Slack中。我们发现这是做出决策的最快方法。我们还会记录每次通话，以确保我们可以重新创建任何时间轴，以防记录员遗漏了重要的细节。&lt;/p&gt;

&lt;p&gt;虽然Slack和电话会议是我们的沟通渠道，但你应该使用最适合贵公司及其工程师的沟通方式。&lt;/p&gt;

&lt;p&gt;在PagerDuty中，我们如何处理响应直接关系到公司的成功。我们不是毫无准备地面对这些故障，而是通过进行模拟练习，回顾以往的故障，选择合适的工具来帮助我们应对可能发生的任何重大事故障，从而有目的地为故障做准备。&lt;/p&gt;

&lt;h2 id=&quot;将最佳实践付诸实践&quot;&gt;将最佳实践付诸实践&lt;/h2&gt;

&lt;p&gt;我们见过一些处理得很好的故障的例子，有些则没有。当警报提醒你一个问题的时候，已经来不及考虑如何处理这个故障了。开始考虑故障管理过程的时间是在故障发生之前。那么，在灾难降临之前，你如何准备并将理论付诸实践呢？本节提供了一些建议。&lt;/p&gt;

&lt;h3 id=&quot;故障响应训练&quot;&gt;故障响应训练&lt;/h3&gt;

&lt;p&gt;我们强烈建议培训应急人员来组织故障，这样他们在真正的紧急情况下就有一个模式可以遵循。知道如何组织一个故障，在整个故障中使用共同的语言，并分享相同的期望，可以减少沟通失误的可能性。&lt;/p&gt;

&lt;p&gt;完整的故障指挥系统方法可能超出了你的需要，但是你可以通过选择故障管理过程中对你的组织非常重要的部分来开发处理故障的框架。例如:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;让接听电话的人知道，他们可以在故障发生时委派和升级。&lt;/li&gt;
  &lt;li&gt;鼓励采取缓解措施。&lt;/li&gt;
  &lt;li&gt;定义故障指挥官、通信主管和运维主管角色。
你可以调整和总结你的故障响应框架，并创建一个PPT展示给新的团队成员。我们了解到，当人们能够将故障反应理论与实际场景和具体行动联系起来时，他们更容易接受故障响应训练。因此，一定要包括亲身实践的练习，分享过去发生的故障，分析哪些进展顺利，哪些进展不太顺利。还可以考虑使用专门从事事件响应课程和培训的外部机构。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;事先做好准备&quot;&gt;事先做好准备&lt;/h3&gt;

&lt;p&gt;除了故障响应训练，它还有助于事先为故障做好准备。使用以下的技巧和策略来做好准备。&lt;/p&gt;

&lt;h3 id=&quot;确定沟通渠道&quot;&gt;确定沟通渠道&lt;/h3&gt;

&lt;p&gt;事先决定并同意一个通信渠道(Slack, a phone bridge, IRC, HipChat，等等)——没有事故指挥官想在事故发生时做出这样的决定。练习使用它，这样就不会有意外了。如果可能的话，选择一个团队已经熟悉的沟通渠道，这样团队中的每个人都能舒服地使用它。&lt;/p&gt;

&lt;h3 id=&quot;让利益相关方知情&quot;&gt;让利益相关方知情&lt;/h3&gt;

&lt;p&gt;除非你承认某个事件正在发生并且正在被积极处理，否则人们会自动地认为没有采取任何措施来解决这个问题。同样，如果你在问题减轻或解决后忘记取消响应，人们会认为事件正在发生。你可以通过定期更新状态，在事件发生的整个过程中不断通知受众，从而抢占这种动态。准备一份联系人列表(请参阅下一条提示)可以节省宝贵的时间，确保你不会错过任何人。&lt;/p&gt;

&lt;p&gt;提前考虑如何起草、审查、批准和发布公共博客文章或新闻稿。在Google，团队寻求公关团队的指导。另外，为共享信息准备两三个现成的模板，确保值班的人知道如何发送它们。没有人愿意在没有指导原则的极端压力下写这些公告。这些模板使得与公众共享信息变得简单，压力最小。&lt;/p&gt;

&lt;h3 id=&quot;准备一份联系人列表&quot;&gt;准备一份联系人列表&lt;/h3&gt;

&lt;p&gt;事先准备好要发邮件或浏览的人的名单可以节省大量的时间和精力。在第180页的“案例研究2：如果可以的话缓存我”中，通讯主管通过发送电子邮件给预先准备好的GKE列表，发出了一个“全体人员待命”的电话。&lt;/p&gt;

&lt;h3 id=&quot;建立故障标准&quot;&gt;建立故障标准&lt;/h3&gt;

&lt;p&gt;有时很明显，告警问题确实是一个故障。其他时候，情况就不那么清楚了。有一个确定的标准列表来确定一个问题是否确实是一个事件是很有帮助的。一个团队可以通过查看过去的停机情况，并考虑到已知的高风险区域，从而得出一个可靠的标准列表。&lt;/p&gt;

&lt;p&gt;综上所述，在应对事件时，建立协调和沟通的共同基础是很重要的。确定沟通事件的方式，你的受众是谁，以及在事件中谁负责。这些指南易于制定，对缩短事件的解决时间有很大的影响。&lt;/p&gt;

&lt;h3 id=&quot;演练&quot;&gt;演练&lt;/h3&gt;

&lt;p&gt;故障管理过程中的最后一步是实践你的故障管理技能。通过在不太危急的情况下进行练习，你的团队会在闪电袭击时养成良好的习惯和行为模式——无论是比喻意义上还是字面意义上。通过培训介绍了事件响应理论之后，实践可以确保事件响应技能保持新鲜。&lt;/p&gt;

&lt;p&gt;有几种方法来进行故障管理演练。Google在全公司范围内运行弹性测试(称为灾难恢复测试，或DiRT；请参阅Kripa Krishnan的文章《抵御意外》(Weathering theUnexpected)。在这篇文章中，我们创建了一个可控的紧急情况，实际上并不影响客户。团队对受控的紧急情况做出反应，就好像这是真正的紧急情况一样。随后，各小组回顾了应急反应程序，并讨论了发生了什么。接受失败作为一种学习的方式，在发现的差距中发现价值，让我们的领导参与进来是成功在Google建立DiRT计划的关键。在较小的范围内，我们使用诸如“不幸之轮”(参见“网络可靠性工程中的灾难角色扮演”)等练习来应对特定事件。&lt;/p&gt;

&lt;p&gt;你还可以通过有意地将次要问题视为需要大规模响应的主要问题来练习事件响应。这可以让你的团队在现实世界中使用低风险的过程和工具进行实践。&lt;/p&gt;

&lt;p&gt;演练是一个尝试新的故障响应技能的友好方式。团队中任何可能深入到故障响应的人——SREs、开发人员、甚至客户支持和营销合作伙伴——都应该对这些策略感到满意。&lt;/p&gt;

&lt;p&gt;要进行演练，可以制造中断并允许你的团队对事件进行响应。你还可以从事后分析中制造中断，其中包含大量事件管理演练的想法。尽可能使用真实的工具来管理事件。考虑破坏你的测试环境，以便团队能够使用现有工具执行真正的故障排除。&lt;/p&gt;

&lt;p&gt;如果这些演习是周期性的，那么它们就会有用得多。你可以通过对每次练习进行跟踪，详细列出哪些做得好，哪些做得不好，以及如何更好地处理事情，来让演练产生影响。进行演练最有价值的部分是检查它们的结果，这可以揭示事件管理中的任何漏洞。一旦你知道了它们是什么，你就可以努力关闭它们。&lt;/p&gt;

&lt;h2 id=&quot;总结-3&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;当灾难来临时做好准备。如果你的团队定期实践和更新故障响应过程，那么当不可避免的故障发生时，便不会感到恐慌。&lt;/p&gt;

&lt;p&gt;在故障发生前，你需要与同事合作的圈子会随着事件的规模而扩大。当你和你不认识的人一起工作的时候，流程会帮助你建立你需要的快速解决方案的结构。我们强烈建议在未报警前提前建立这些流程。定期回顾并重复你的事件管理计划和剧本。&lt;/p&gt;

&lt;p&gt;事故指挥系统是一个简单的概念，很容易理解。它会根据公司的规模和事件的大小进行放大或缩小。虽然理解起来很简单，但实现起来却并不容易，尤其是在突然发生恐慌时。在紧急情况下保持冷静并遵循反应结构需要练习，练习可以建立“肌肉记忆”。这使你对待真正的紧急情况有了信心。&lt;/p&gt;

&lt;p&gt;我们强烈建议在你的团队繁忙的日程中抽出一些时间，定期实践事件管理。确保得到领导的支持，让他们有专门的实践时间，并确保他们了解事件响应如何工作，以防你需要让他们参与到真正的事件中。备灾可以从响应时间中节省宝贵的时间或数小时，并使你具有竞争优势。没有任何一家公司总是能把事情做好——从错误中吸取教训，继续前进，下次做得更好。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">写在开头: 故障总会发生！如何对故障快速进行有组织的响应？Google方式是一个很好的借鉴。 每个团队都希望服务没有故障发生，但这个世界是不完美的，像停电这类的事件还是会发生的。当一个紧急故障需要多人或团队协作解决时，响应故障并解决问题是我们要应对的。 处理故障的目标是减轻影响或将服务恢复到先前的状态。而故障管理意味着以有效的方式协调团队的响应工作，并确保响应者和利益相关者之间的信息流通。包括谷歌在内的许多科技公司都有自己的“最佳实践”来管理应急响应，并且在不断的完善。 故障管理的基本前提是以结构化的方式响应故障。大规模故障是单人无法解决的；结构化的应急响应可以减少混乱。在灾难发生之前制定沟通和协调规范，使你的团队可以集中精力解决故障，而不用担心沟通和协调问题。 故障响应的过程并不是困难的事情。大量实践经验总结可以为我们提供一些指导，比如在第一本SRE书中的《故障管理》章节。故障响应的基本原则包括以下内容： 保持清晰的指挥线 明确角色 随时掌握调试工作记录 响应进度的实时更新 本章介绍了如何在Google和PagerDuty中进行故障管理。通过一些Case说明我们在其中达到的目标以及没有达到的目标。 “将最佳实践付诸实践”（第191页）中的清单可以帮助你开始创建自己的故障响应实践。 Google故障管理 故障响应是用于响应和管理故障的系统。由一个框架和一组定义的程序组成。Google的故障响应系统是基于故障命令系统（ICS）实现的。 故障指挥系统 ICS成立于1968年，是消防员管理野火的一种方式。该框架提供了在故障响应期间进行通信和明确角色的标准化方法。基于该模型，公司后来采用ICS来应对计算机和系统故障。本章探讨了两个这样的框架：PagerDuty的故障响应流程和Google故障管理（IMAG）。 故障响应框架有三个共同目标，也称为事件管理的“三个C”（3C）： 协调响应工作 在故障响应者、组织内部和外部之间进行通信 保持对故障响应的控制 当故障响应出现问题时，罪魁祸首可能出现在其中一个方面。掌握3C对于有效的事件响应至关重要。 故障响应中的主要角色 故障响应中的主要角色是故障指挥官（IC），通信主管（CL）和操作或行动主管（OL）。 IMAG将这些角色组织成一个层次结构：IC引导故障响应，CL和OL报告给IC。 灾难发生时，声明故障的人通常会进入IC角色并指挥故障的高级别状态。 IC专注于3C，并执行以下操作： 命令和协调故障响应，根据需要委派角色。默认情况下，IC会假定尚未委派的所有角色。 有效沟通。 保持对故障响应的控制。 与其他相应人员合作解决此事件。 IC可以将其角色交给其他人并承担OL角色，或将OL角色分配给其他人。 OL通过应用工具来缓解或解决故障，从而对故障做出响应。 虽然IC和OL致力于减轻和解决故障，但CL是故障响应团队的消息通讯核心。 CL的主要职责包括定期向故障响应团队和利益相关者实时更新故障响应的进展。 CL和OL都可以领导一个团队来帮助管理他们特定的故障响应区域。这些团队可以根据需要进行扩展或收缩。如果故障变得足够小，那么CL角色就可以被包含到IC角色中。 案例研究 以下四个大型故障说明故障响应如何在实践中起作用。其中三个案例研究来自Google，最后一个是来自PagerDuty的案例研究，该研究提供了其他组织如何使用ICS派生框架的观点。Google的例子以一个没有有效管理的故障开始，并在有故障管理时对故障响应的改观。 案例研究1：软件错误—灯还开着，但没有人(Google HOME) 这个例子展示了故障发生初期不进行通报，在没有工具快速有效地响应故障时，团队是如何响应故障的。虽然这一故障在没有重大灾难的情况下得到了解决，但是越早组织，就会产生更好的效果。 背景 Google Home是一款智能扬声器和家庭助手，可响应语音命令。语音命令与Google Home的软件（称为Google智能助理）进行交互。 当用户说出一个触发Google智能助理的热门词组时，就会启动与Google Home的互动。多个用户可以通过培训助理来监听给定的热门词汇，从而使用相同的Google Home设备。识别扬声器的热词模型是在客户端进行的，但是数据（例如扬声器识别文件）存储在服务器上。服务器处理数据的双向流，为了在繁忙的时候处理超载，服务器对Google助手有一个配额政策。为了保护服务器免受过大请求值的影响，配额限制明显高于在给定设备上的Google助手的基准使用量。 Google智能助理1.88版中的错误导致扬声器识别文件的获取频率超过预期的50倍，超出此配额。最初，美国中部的GoogleHome用户只有很少的流量损失。然而，随着所有GoogleHome设备的推出逐步增加，用户在2017年6月3日的周末期间丢失了一半的请求。 故障 太平洋标准时间5月22日星期一上午11:48，谷歌主页on-call开发人员Jasper正在看着每秒查询（QPS）图表并注意到一些奇怪的事情：谷歌助理每隔30分钟就会收到培训数据，而不是像预期的那样每天一次。他停止了已经推到了25％用户的1.88版本发布。他提了一个漏洞——称之为bug 12345——用谷歌的漏洞追踪系统来探索为什么会发生这种情况。在这一问题上，他指出，谷歌助手每天会对数据进行48次的ping请求，导致其超过了QPS的容量。 另一位开发人员Melinda将此问题与先前报告的漏洞相关联，我们将其称为错误67890：每当应用程序刷新设备身份验证和注册状态时，语音处理器都会重新启动。该版本将在版本1.88发布后修复，因此该团队要求临时增加模型的配额，以减轻额外查询的过载。 版本1.88版本再次启动并继续推出，到5月31日星期三达到50％的用户。不幸的是，该团队后来了解到错误67890，虽然负责一些额外的流量，但并不是Jasper所注意到的更频繁的取回的真正根源。 同一天早上，客户开始向Google支持小组报告问题：每当有人说“OK Google”（或任何其他热门词汇来激活Google Home）时，设备都会回复并显示错误消息，此问题阻止用户向Google智能助理发出命令。该团队开始调查可能导致用户报告的错误的原因。他们怀疑配额问题，所以他们要求增加配额，这似乎可以缓解这个问题。 与此同时，该团队继续调查bug 12345，看看是什么触发了错误。尽管在调试过程的早期就建立了配额连接，但是客户端和服务器开发人员之间的误解导致了开发人员在故障排除过程中走错了方向，而完整的解决方案仍然无法实现。 该团队还对为什么Google智能助理的流量不断达到配额限制感到困惑。客户端和服务器开发人员对客户端错误感到困惑，这些错误似乎没有被服务器端的任何问题触发。开发人员将日志记录添加到下一个版本，以帮助团队更好地理解错误，并希望在解决事件方面取得进展。 截至6月1日星期四，用户报告此问题已得到解决。没有报道任何新问题，因此版本1.88版本继续推出。但是，原始问题的根本原因尚未确定。 在6月3日的周六早上，版本1.88的发布超过了50%。这一发布是在一个周末进行的，当时开发团队并没有人值班。该团队并没有遵循在工作日期间执行部署的最佳实践，以确保开发人员在场。 在6月3日星期六，当版本1.88发布的时候达到了100%，客户端又一次达到了Google助理流量的服务器限制。来自客户的错误报告开始出现。谷歌员工报告称，他们的Google HOME设备出现了错误。Google Home支持小组收到了大量关于此问题的客户来电，反馈也包括了推文和Reddit帖子。 Google Home的帮助论坛也出现了正在讨论这个问题的帖子。尽管有大量的用户报告和反馈，bug并没有升级到更高的优先级。 6月4日星期日，随着客户报告数量的不断增加，支持团队最终将错误优先级提升到最高水平。该团队没有通报故障，而是继续通过“常规”方法解决问题—使用bug跟踪系统进行通信。on-call开发人员注意到一个数据中心集群中的错误率，SRE进行了ping操作，要求他们排除它。与此同时，该团队提出了另一项增加配额的请求。之后，开发团队的一名工程师注意到流量通道已将错误推入其他单元格，这为配额问题提供了额外的证据。下午3：33，开发人员团队经理再次增加了Google智能助理的配额，并停止了对用户的影响。故障结束。该团队此后不久确定了根本原因（参见前面的“环境”部分）。 总结 故障处理的某些方面进展顺利，而其他方面则有改进的余地。 首先，工程师在周末处理故障并为解决问题提供了宝贵的意见，这既好又坏，虽然团队重视这些人在周末所付出的时间和精力，但成功的故障管理不应该依赖于个人的英勇行为。如果无法联系到开发人员怎么办？Google鼓励良好的工作与生活平衡——工程师不应在空闲时间内解决与工作相关的问题。相反，我们应该在工作时间进行部署，或者组织一次在工作时间之外提供付费保障的on-call轮换。 接下来，该团队致力于缓解这一问题。Google首先要求是阻止故障的影响，然后找到根本原因（除非根本原因只是在早期发现）。一旦问题得到缓解，找到根本原因同样重要，以防止问题再次发生。在这个故障事件中，成功地在三个不同的场合控制了故障影响，但团队只有在发现根本原因后才能防止问题再次发生。在第一次故障得到控制之后，最好是等到根本原因完全确定后再开始实施，避免周末发生大混乱。最后，在问题首次出现时，团队没有通告故障。我们的经验表明，管理故障可以更快地解决。尽早通告故障可确保： 防止客户端和服务器开发人员之间的沟通错误。 根源识别和故障解决发生得更快。 相关团队提前进入，使外部沟通更快捷顺畅。 集中式通信是IMAG协议的重要原则。例如，当灾难发生时，SRE通常聚集在一个“作战室”中。作战室可以是像会议室一样的真实环境，也可以是虚拟的：团队可能聚集在IRC频道。这里的关键是将所有的故障响应者集中在一个地方，并实时沟通以管理并最终解决故障。 案例研究2：服务故障—如果可以，请对我进行缓存 下面的故障说明了当一个“专家级”团队试图调试一个具有复杂交互系统时会发生什么——没有一个人能够掌握所有的细节。 背景 Kubernetes是一个由许多公司和个人贡献者共同建立的开源容器管理系统。 Google Kubernetes Engine（或称GKE）是Google管理的系统，可为用户创建、托管和运行Kubernetes群集。用户以最适合他们的方式上传和管理工作负载。当用户首次创建新群集时，GKE将获取并初始化其群集所需的Docker镜像。理想情况下，这些组件是在内部获取和构建的，因此我们可以验证它们。但Kubernetes是一个开源系统，新的依赖关系有时会让系统出现问题。 故障 在太平洋标准时间一个星期四上午6点41分，伦敦GKE的on-call人员Zara寻访了多个区域的CreateCluster探查器故障，新的集群没有被成功地创建。 Zara检查了prober仪表板，发现两个区域的故障率都在60％以上。他确认此问题影响了用户创建新群集的功能，但现有群集的流量未受影响。 Zara遵循GKE的文件化程序，于上午7:06对故障进行了通告。最初，有四人参与处理了这一故障： Zara是第一个注意到这个问题的人，因此被指定为默认故障指挥官。 Zara的两名队友。 Rohit由故障程序分配的客户支持工程师。 由于Rohit总部设在苏黎世，Zara（IC）开设了一个GKE Panic IRC频道，团队可以在那里一起调试。在另外两个SRE深入调查监控和错误信息时，Zara解释了故障及其对Rohit的影响。截至上午7点24分，Rohit向用户发布通知称，创建集群的功能在欧洲西部地区出现故障。这提高了故障的等级。早上7点到8点20分，Zara、Rohit和其他人一直致力于解决这个问题。他们检查了集群启动日志，其中显示了一个错误：无法运行Kubelet；无法创建证书签名——他们需要确定证书创建的哪个部分失败。SRE研究了网络、资源可用性和证书签名过程。所有这些似乎都运作得很好。上午8点22分，Zara向故障管理系统发布了故障调查摘要，并寻找可以帮助他的开发人员。值得庆幸的是，GKE有一位on-call开发人员可以在紧急情况下提供协助。开发人员Victoria加入了该频道。他要求跟踪错误，并要求团队将问题上报给基础架构的on-call团队。现在是上午8点45分，第一个西雅图SRE，II-Seong来到办公室，轻轻地泡了杯咖啡，为这一天做好了准备。 II-Seong是一名资深的SRE，在故障响应方面拥有多年的经验。当他得知正在发生的故障时，他立即进入故障响应频道。首先，II-Seong根据警报的时间检查当天的发布情况，并确定当天的发布不会导致事故障的发生。然后，他开始整理工作文件，收集笔记。他建议Zara将故障升级为基础架构，云网络和计算引擎团队，以尽可能多地消除这些根源。由于Zara升级，其他人加入了故障响应： GKE节点的开发负责人 云网络值班人员 计算引擎值班人员 Herais，另一个西雅图SRE 上午9点10分，故障频道有十几个参与者。故障发生2.5小时，没有找到根本原因，也没有控制住影响。沟通成为了一种挑战。通常情况下，从伦敦到西雅图的on-call交接时间是上午10点，但Zara决定在上午10点之前将故障指挥权移交给II-Seong，因为他对IMAG有更多的经验。作为故障指挥官，II-Seong建立了一个正式的响应组织结构来解决这一事件。然后，他指定Zara为Ops Lead，Herais为通信（Comms）负责人。 Rohit仍然是外部沟通负责人。 Herais立即发送了一封“全体人员齐上阵”的电子邮件给GKE，包括所有开发人员的负责人，并要求专家加入故障响应。到目前为止，故障响应者了解到了以下情况： 当节点试图向主机注册时，集群创建失败。 错误消息表明证书签名模块是罪魁祸首。 欧洲所有集群创建都失败了;其地区都正常。 欧洲没有其他GCP服务出现网络或配额问题。 之后，GKE安全团队成员Puanani加入了这项工作。他注意到证书签发者没有开始。证书签发者试图从DockerHub中提取图像，图像似乎已损坏。Victoria（GKE on-call开发人员）在两个地理位置运行了Docker的图像拉取命令。它在欧洲的集群上运行失败，在美国的集群上成功。这表明欧洲集群是问题的所在。上午9点56分，该团队确定了一个看似合理的根本原因。 因为DockerHub有一个外部依赖，故障控制和故障溯源将特别具有挑战性。对于Docker的工作人员来说，第一个控制故障的方法是快速修复。第二个选择是重新配置集群，从不同的位置获取图像，例如Google容器注册表（GCR），Goole的安全图像托管系统。所有其他依赖项，包括对图像的其他引用，都位于GCR中。 II-Seong指派负责人追查这两种可能。然后，他委派了一个小组去调查失败的集群。讨论对于IRC来说过于密集，因此详细的调试转移到共享文档，IRC成为决策的中心。 对于第二个可能，推送新配置意味着重建二进制文件，这需要大约一个小时。上午10:59，当团队完成90％的重建工作时，他们发现另一个使用错误图像获取路径的位置。作为回应，他们不得不重新启动构建。当IRC的工程师们致力于这两个缓解方案时，SRE中的Tugay有了一个想法。如果他们拦截了Docker的pull请求并使用内部缓存图像替换Docker的响应，那么如何重建配置并将其推出（一个笨重且有风险的过程）呢？ GCR有一个镜像可以做到这一点。 Tugay联系了GCR的SRE团队，他们确认团队可以在Docker配置上设置&amp;lt;–registry-mirror =https://mirror.gcr.io&amp;gt;。Tugay开始设置此功能，并发现镜像已经到位！ 上午11点29分，Tugay向IRC报告说这些图像是从GCR镜像中拉出来的，而不是DockerHub。上午11点37分，故障指挥官在呼叫GCR值班人员。上午11点59分，GCR值班人员在欧洲存储层清除了腐败的图像。截止到下午12点11分，所有欧洲地区的误差都降至0％。 故障结束了。剩下的只是清理工作，故障文档的整理。在修复之前，CreateCluster在欧洲失败了6小时40分钟。在IRC中，整个事件中出现了41个独立用户，IRC日志扩展到26,000个单词。这项工作在不同的时间分拆了七个IMAG工作组，并且在任何时候都有多达四个同时工作。六支on-call队伍的参与，后期包含28个行动项目。 总结 无论从哪个角度，GKE CreateCluster中断是一件重大故障。让我们探讨一下哪些事情进行得很顺利，哪些事情本可以处理得更好。 什么进展顺利？该团队有几个记录在案的升级路径，熟悉故障响应策略。GKE值班人员 Zara很快就证实了这种影响正在影响实际客户。然后，她使用了一个常用的故障管理系统来通知Rohit，Rohit将故障告知客户。 什么可以更好地处理？该服务本身有一些值得关注的方面。复杂性和对专家的依赖是有问题的。记录对于故障定位来说是不够的，并且团队因为DockerHub上的变化而分散了注意力，而且这不是真正的问题。 故障发生之初，故障指挥官没有制定一个正式的应急预案。虽然Zara承担了这一角色并将对话转移到IRC，但他可以更积极地协调信息和做出决策。结果，少数应急人员在没有协调的情况下进行自己的调查。II-Seong在第一个警报后两小时就建立了正式的故障响应组织。 最后，该故障揭示了GKE灾难准备方面的一个不足：该服务没有任何可以减少用户影响的早期通用预案。通用预案是第一响应者采取的降低影响的操作流程，甚至在完全理解根本原因之前都可以使用。例如，当中断与发布周期相关时，响应者可以回滚最近的版本，或者重新配置负载平衡器以避免错误被本地化时出现区域。值得注意的是，通用预案也有弊端，可能会导致其他服务中断。然而，虽然它们可能具有比精确解决方案更广泛的影响，但是当团队发现并解决根本原因时，它们可以快速到位以停止进一步扩大影响。 让我们再次查看此故障的时间表，看看通用预案可能在哪些方面有效： 上午7点（评估影响）。 Zara确认用户受到中断的影响。 上午9:56（找到可能的原因）。 Puanani和Victoria确定了一个可能的根本原因。 上午10点59分（定制缓解措施）。几个团队成员致力于重建二进制文件以推送一个新配置，该配置将从不同位置获取图像。 上午11:59（找到根本原因并解决问题）。 Tugay和GCR值班人员取消了GCR缓存，并从其欧洲存储层中清除了损坏的图像。 在步骤2（找到可能的原因）之后的通用预案在这里将非常有用。如果响应者在发现问题后将所有发布回滚到已知的良好状态，则故障将在上午10点之前得到缓解。为了缓解故障，不必完全了解详细信息——你只需要知道根本原因的位置即可。能够在完全理解其中断之前缓解中断对于运行具有高可用性的强大服务至关重要。 在这种情况下，响应者可以从某种回滚的工具中受益。通用预案工具确实需要时间来开发。创建通用缓解工具的正确时间是在故障发生之前，而不是在应对紧急情况时。浏览postmortems是一种发现缓解故障的好方法，这些缓解故障在回滚过程中非常有用，并将它们构建到服务中，以便在将来更好地管理故障。 重要的是要记住，第一响应者必须优先考虑通用预案，否则解决问题的时间还会拉的很长。实施通用预案措施（例如回滚）可加快恢复速度并使客户更快乐。最终，客户并不关心你是否完全理解导致中断的原因。他们想要的是停止接收错误。将控制影响作为首要任务，应以下列方式处理积极故障: 评估故障的影响。 减轻影响。 对故障进行根本原因分析。 故障结束后，定位故障的原因并写下事后分析报告。之后，你可以运行故障响应练习演练来修复系统中的漏洞，并且工程师可以在项目中处理这些漏洞。 案例研究3：断电—闪电从未发生过两次……直到它发生 前面的示例表明，如果没有良好的故障响应策略时，会出现什么问题。下一个示例演示了成功管理的事件。当你遵循一个定义良好且清晰的响应协议时，您甚至可以轻松地处理罕见或异常的事件。 背景 电网事件（例如雷击）导致进入数据中心设施的电力变化很大。影响电网的雷击是罕见的，但并不出人意料。Google可以使用备用发电机和电池来防止突发的意外断电，这些设备经过了充分的测试，并且已知可以在这些情况下正常工作。 Google的许多服务器都有大量的磁盘连接到它们，这些磁盘位于服务器上方或下方的一个单独的托盘上。这些托盘有自己的不间断电源(UPS)电池。当停电发生时，备用发电机会启动，但启动需要几分钟时间。在此期间，连接到服务器和磁盘托盘上的备用电池提供电力，直到备用发电机完全运行，从而防止电网事件影响数据中心的运行。 故障 2015年年中，比利时谷歌数据中心附近的电网在两分钟内被闪电击中四次。数据中心的备用发电机被激活，为所有的机器供电。当备份发电机启动时，大多数服务器都使用备用电池运行了几分钟。 磁盘托盘中的UPS电池并没有在第三次和第四次雷击时将电量用在备用电池上，因为雷击间隔太近了。结果，磁盘托盘失去了电力，直到备用发电机开始工作。这些服务器没有断电，但无法访问那些有电的磁盘。 在持久磁盘存储中丢失大量磁盘托盘会导致许多在Google Compute Engine(GCE)上运行的虚拟机(VM)实例出现读写错误。持久磁盘SRE在调用时立即发送了这些错误。一旦持久磁盘SRE团队确定了影响，就会向所有受影响的各方通报故障。持久磁盘SRE on-call人员承担了故障指挥官的角色。 经过利益相关者之间的初步调查和沟通，我们确定: 由于临时断电而丢失磁盘托盘的每台机器都需要重新启动。 在等待重新启动时，一些客户VMs在读取和写入磁盘时出现问题。 任何同时具有磁盘托盘和客户vm的主机都不能在不丢失未受影响的客户vm的情况下“重新启动”。持久磁盘SRE请求GCE SRE将未受影响的vm迁移到其他主机。 持久磁盘SRE的主要值班人员的保留了IC角色，因为该团队对客户影响的可视性最强。 运维团队成员的任务如下: 安全恢复电源，使用电网电源而不是备用发电机。 重新启动所有非vm主机。 协调持久磁盘SRE和GCE SRE，在重新启动之前安全地将vm从受影响的机器移开。 前两个目标被清楚地定义、很好地理解和记录。数据中心运维值班人员立即开始安全恢复电源，定期向IC提供状态报告。持久磁盘SRE定义了重新启动所有机器而不是虚拟机的程序。一个团队成员开始重新启动这些机器。 第三个目标更加模糊，不包括任何现有的程序。故障指挥员指派了一个专门的行动小组成员与GCE SRE和持久磁盘SRE进行协调。这些团队合作将VMs安全地从受影响的机器移开，以便重新启动受影响的机器。IC密切关注着他们的进展，并意识到这项工作需要快速编写新的工具。IC组织了更多的工程师向运维团队报告，以便他们能够创建必要的工具。 沟通负责人观察并询问所有与事件相关的活动，并负责向多个受众报告准确的信息： 公司领导需要关于问题严重程度的信息，并确保问题得到解决。 有存储问题的团队需要知道他们的存储何时可以再次完全可用。 需要主动告知外部客户他们的磁盘在这个云区域的问题。 提交支持票据的特定客户需要知道他们所看到的问题的更多信息，以及关于解决方案和时间安排的建议。 在我们减轻了最初的客户影响之后，我们需要做一些后续工作，例如: 诊断为什么磁盘托盘使用的UPS失败，并确保它不会再次发生。 更换发生故障的数据中心的电池。 手动清除由于同时丢失这么多存储系统而导致的“卡住”操作。 事后分析显示，只有一小部分未写入磁盘——在事故发生期间断电的机器上的等待写入操作。由于持久磁盘快照和所有云存储数据都存储在多个数据中心中以实现冗余，因此只有0.000001%的运行GCE计算机的数据丢失，并且只有运行实例的数据存在风险。 总结 通过及早通报故障，并以明确的领导组织结构有效地处理了这个复杂的故障。 故障指挥官将恢复电源和重启服务器的正常问题委托给了适当的运维负责人。工程师们致力于解决这个问题，并将他们的进展报告给运维主管。 要同时满足GCE和持久磁盘的需求，更复杂的问题需要在多个团队之间进行协调决策和交互。事故指挥员确保从两个小组中指派适当的行动小组成员来处理事故，并直接与他们一起工作，朝着解决问题的方向努力。事故指挥官明智地将注意力集中在事故最重要的方面：尽快解决受影响客户的需求。 案例研究4：PagerDuty事件响应 作者：PagerDuty的Arup Chakrabarti PagerDuty在几年的时间里开发并完善了内部故障响应实践。最初，配备了一名常设的全公司故障指挥官，并为每个服务配备专门的工程师参与故障响应。随着PagerDuty发展到超过400名员工和几十个工程团队，故障响应过程也发生了变化。每隔几个月都会仔细检查故障响应流程，并更新它们以反映业务需求。所有的经验总结都记录在https://response.pagerduty.com上。我们的故障响应过程并不是一成不变的；它们就像我们的业务一样不断变化和发展。 PagerDuty重大事件响应 通常，小的故障只需要一个值班工程师来响应。当涉及到更大的故障时，我们非常重视团队合作。在高压力和高影响的情况下，工程师不应该感到孤独。我们使用以下技巧来促进团队合作： 参与模拟演习 我们教授团队合作的一个方法是参加“失败星期五”。PagerDuty从Netflix公司的Simian Army(猿人部队)里汲取灵感，制作了这个节目。最初，Failure Friday是一个手动的故障注入练习，目的是了解更多关于我们的系统可能崩溃的方式。今天，我们还使用这个每周练习来重现生产和事件响应场景中的常见问题。 在“失败星期五”开始之前，我们提名一个故障指挥官(通常是一个训练成为IC的人)。在进行故障注入练习时，他们应该表现得像真正的IC一样。在整个演练过程中，主题专家使用与实际事件相同的过程和术语。这种做法既使新值班工程师熟悉事故障响应语言和流程，又为经验丰富的值班工程师提供了一种复习。 玩限时模拟游戏 虽然“失败星期五”练习对工程师在不同角色和过程中培训大有帮助，但它们不能完全复制实际重大事故的紧迫性。我们使用具有时限紧迫性的模拟游戏来捕捉事件响应的这一方面。 “继续说下去，没有人会爆炸”是我们大量使用的一款游戏。它要求玩家在限定时间内共同拆除炸弹。游戏的压力和密集的交流性质迫使玩家有效地合作和有效地协同工作。 从以往的故障中吸取教训 从过去的故障中学习可以帮助我们更好地应对未来的重大故障。为此，我们进行并定期审查故障分析报告。 PagerDuty事后调查过程包括开放式会议和全面记录。通过使这些信息易于访问和发现，我们的目标是减少未来事件的解决时间，或防止未来故障一起发生。 我们还会记录所有涉及重大故障的电话，这样我们就可以从实时通信feed中学习。 让我们看看最近的一个故障，其中PagerDuty不得不利用我们的故障响应流程。故障发生在2017年10月6日，持续时间超过10个小时，但对客户的影响非常小。 下午7:53 PagerDuty SRE团队的一名成员被告知PagerDuty内部NTP服务器正在显示时钟漂移。值班的SRE验证了所有自动恢复操作已经执行，并完成了相关运行手册中的缓解步骤。这个工作记录在SRE团队的专用Slack频道中。 晚上8点20分，PagerDuty软件团队A的一名成员收到了关于他们服务中时钟漂移错误的自动警报。软件团队A和SRE团队致力于解决这个问题。 晚上9点17分，PagerDuty软件团队B的一名成员收到了关于他们服务上时钟漂移错误的自动警报。B组的工程师加入了Slack频道，该频道已经对问题进行了测试和调试 晚上9点49分，值班SRE宣布发生重大故障，并通知值班故障指挥官。 晚上9点55分，IC组建了响应团队，其中包括依赖NTP服务的每个on-call工程师，以及PagerDuty的客户支持。IC让响应小组加入专门的电话会议和Slack频道。 在接下来的8个小时里，响应小组致力于解决和减轻这个问题。当我们运行手册中的程序没有解决问题时，响应团队开始有条理地尝试新的恢复选项。 在这段时间里，我们每四个小时轮换一次on-call工程师和IC。这样做可以鼓励工程师们休息，并为响应团队带来新的想法。 上午5：33，值班的SRE对NTP服务器进行了配置更改。 上午6点13分，IC与他们各自的值班工程师验证所有的服务都恢复了。验证完成后，IC关闭了电话会议和Slack频道，并宣布故障完成。鉴于NTP服务的广泛影响，有必要进行事后分析。在结束故障之前，IC将事后分析分配给值班的SRE小组。 用于故障响应的工具 我们的故障响应流程利用了三个主要工具： PagerDuty。我们将所有值班的信息、服务所有权、事后分析、事件元数据等存储在PagerDuty中。这使我们能够在出现问题时迅速组建正确的团队。 Slack。我们保持一个专门的频道(#incident-war-room)，作为所有主题专家和故障指挥官的聚会场所。该频道主要用作记录员的信息分类，用于捕获操作、所有者和时间戳。 电话会议 当被要求加入任何故障响应时，on-call的工程师需要拨打一个固定的会议电话号码。我们希望所有的协调决策都在电话会议中做出，而决策结果都记录在Slack中。我们发现这是做出决策的最快方法。我们还会记录每次通话，以确保我们可以重新创建任何时间轴，以防记录员遗漏了重要的细节。 虽然Slack和电话会议是我们的沟通渠道，但你应该使用最适合贵公司及其工程师的沟通方式。 在PagerDuty中，我们如何处理响应直接关系到公司的成功。我们不是毫无准备地面对这些故障，而是通过进行模拟练习，回顾以往的故障，选择合适的工具来帮助我们应对可能发生的任何重大事故障，从而有目的地为故障做准备。 将最佳实践付诸实践 我们见过一些处理得很好的故障的例子，有些则没有。当警报提醒你一个问题的时候，已经来不及考虑如何处理这个故障了。开始考虑故障管理过程的时间是在故障发生之前。那么，在灾难降临之前，你如何准备并将理论付诸实践呢？本节提供了一些建议。 故障响应训练 我们强烈建议培训应急人员来组织故障，这样他们在真正的紧急情况下就有一个模式可以遵循。知道如何组织一个故障，在整个故障中使用共同的语言，并分享相同的期望，可以减少沟通失误的可能性。 完整的故障指挥系统方法可能超出了你的需要，但是你可以通过选择故障管理过程中对你的组织非常重要的部分来开发处理故障的框架。例如: 让接听电话的人知道，他们可以在故障发生时委派和升级。 鼓励采取缓解措施。 定义故障指挥官、通信主管和运维主管角色。 你可以调整和总结你的故障响应框架，并创建一个PPT展示给新的团队成员。我们了解到，当人们能够将故障反应理论与实际场景和具体行动联系起来时，他们更容易接受故障响应训练。因此，一定要包括亲身实践的练习，分享过去发生的故障，分析哪些进展顺利，哪些进展不太顺利。还可以考虑使用专门从事事件响应课程和培训的外部机构。 事先做好准备 除了故障响应训练，它还有助于事先为故障做好准备。使用以下的技巧和策略来做好准备。 确定沟通渠道 事先决定并同意一个通信渠道(Slack, a phone bridge, IRC, HipChat，等等)——没有事故指挥官想在事故发生时做出这样的决定。练习使用它，这样就不会有意外了。如果可能的话，选择一个团队已经熟悉的沟通渠道，这样团队中的每个人都能舒服地使用它。 让利益相关方知情 除非你承认某个事件正在发生并且正在被积极处理，否则人们会自动地认为没有采取任何措施来解决这个问题。同样，如果你在问题减轻或解决后忘记取消响应，人们会认为事件正在发生。你可以通过定期更新状态，在事件发生的整个过程中不断通知受众，从而抢占这种动态。准备一份联系人列表(请参阅下一条提示)可以节省宝贵的时间，确保你不会错过任何人。 提前考虑如何起草、审查、批准和发布公共博客文章或新闻稿。在Google，团队寻求公关团队的指导。另外，为共享信息准备两三个现成的模板，确保值班的人知道如何发送它们。没有人愿意在没有指导原则的极端压力下写这些公告。这些模板使得与公众共享信息变得简单，压力最小。 准备一份联系人列表 事先准备好要发邮件或浏览的人的名单可以节省大量的时间和精力。在第180页的“案例研究2：如果可以的话缓存我”中，通讯主管通过发送电子邮件给预先准备好的GKE列表，发出了一个“全体人员待命”的电话。 建立故障标准 有时很明显，告警问题确实是一个故障。其他时候，情况就不那么清楚了。有一个确定的标准列表来确定一个问题是否确实是一个事件是很有帮助的。一个团队可以通过查看过去的停机情况，并考虑到已知的高风险区域，从而得出一个可靠的标准列表。 综上所述，在应对事件时，建立协调和沟通的共同基础是很重要的。确定沟通事件的方式，你的受众是谁，以及在事件中谁负责。这些指南易于制定，对缩短事件的解决时间有很大的影响。 演练 故障管理过程中的最后一步是实践你的故障管理技能。通过在不太危急的情况下进行练习，你的团队会在闪电袭击时养成良好的习惯和行为模式——无论是比喻意义上还是字面意义上。通过培训介绍了事件响应理论之后，实践可以确保事件响应技能保持新鲜。 有几种方法来进行故障管理演练。Google在全公司范围内运行弹性测试(称为灾难恢复测试，或DiRT；请参阅Kripa Krishnan的文章《抵御意外》(Weathering theUnexpected)。在这篇文章中，我们创建了一个可控的紧急情况，实际上并不影响客户。团队对受控的紧急情况做出反应，就好像这是真正的紧急情况一样。随后，各小组回顾了应急反应程序，并讨论了发生了什么。接受失败作为一种学习的方式，在发现的差距中发现价值，让我们的领导参与进来是成功在Google建立DiRT计划的关键。在较小的范围内，我们使用诸如“不幸之轮”(参见“网络可靠性工程中的灾难角色扮演”)等练习来应对特定事件。 你还可以通过有意地将次要问题视为需要大规模响应的主要问题来练习事件响应。这可以让你的团队在现实世界中使用低风险的过程和工具进行实践。 演练是一个尝试新的故障响应技能的友好方式。团队中任何可能深入到故障响应的人——SREs、开发人员、甚至客户支持和营销合作伙伴——都应该对这些策略感到满意。 要进行演练，可以制造中断并允许你的团队对事件进行响应。你还可以从事后分析中制造中断，其中包含大量事件管理演练的想法。尽可能使用真实的工具来管理事件。考虑破坏你的测试环境，以便团队能够使用现有工具执行真正的故障排除。 如果这些演习是周期性的，那么它们就会有用得多。你可以通过对每次练习进行跟踪，详细列出哪些做得好，哪些做得不好，以及如何更好地处理事情，来让演练产生影响。进行演练最有价值的部分是检查它们的结果，这可以揭示事件管理中的任何漏洞。一旦你知道了它们是什么，你就可以努力关闭它们。 总结 当灾难来临时做好准备。如果你的团队定期实践和更新故障响应过程，那么当不可避免的故障发生时，便不会感到恐慌。 在故障发生前，你需要与同事合作的圈子会随着事件的规模而扩大。当你和你不认识的人一起工作的时候，流程会帮助你建立你需要的快速解决方案的结构。我们强烈建议在未报警前提前建立这些流程。定期回顾并重复你的事件管理计划和剧本。 事故指挥系统是一个简单的概念，很容易理解。它会根据公司的规模和事件的大小进行放大或缩小。虽然理解起来很简单，但实现起来却并不容易，尤其是在突然发生恐慌时。在紧急情况下保持冷静并遵循反应结构需要练习，练习可以建立“肌肉记忆”。这使你对待真正的紧急情况有了信心。 我们强烈建议在你的团队繁忙的日程中抽出一些时间，定期实践事件管理。确保得到领导的支持，让他们有专门的实践时间，并确保他们了解事件响应如何工作，以防你需要让他们参与到真正的事件中。备灾可以从响应时间中节省宝贵的时间或数小时，并使你具有竞争优势。没有任何一家公司总是能把事情做好——从错误中吸取教训，继续前进，下次做得更好。</summary></entry><entry><title type="html">第八章 On-Call</title><link href="http://localhost:4000/sre/2020/01/08/On-Call/" rel="alternate" type="text/html" title="第八章 On-Call" /><published>2020-01-08T00:00:00+08:00</published><updated>2020-01-08T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/08/On-Call</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/08/On-Call/">&lt;!-- more --&gt;
&lt;p&gt;On-call轮值意味着在某段时间内随叫随到，随时响应紧急问题。站点可靠性工程师（SRE）通常需要参与on-call轮值工作。在on-call轮值期间，SRE需根据需要判断、缓解、修复或升级事件。此外，SRE还需定期响应非紧急生产事件。&lt;/p&gt;

&lt;p&gt;在Google，on-call轮值是SRE的职责之一。SRE团队可以缓解故障，修复生产环境问题且自动执行运维任务。由于大多数SRE团队的运维任务还未完全实现自动化，升级需要人为联系on-call工程师。SRE团队的on-call工作是根据所支持系统的重要程度或系统所处的开发状态而定的。根据我们的经验，大多数SRE团队都需要参与on-call轮值工作。&lt;/p&gt;

&lt;p&gt;On-call是一个庞大而复杂的话题，限制因素很多，但是试错率却很少。我们的第一本书《Site Reliability Engineering》第11章“on-call轮值”已经探讨过这一主题。本章介绍一些我们收到的有关该章的反馈和问题。其中包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“我们不是Google，我们的规模要小得多。我们没有那么多人参与轮值，并且没有位于不同时区的站点。你在第一本书中描述的内容与我无关。”&lt;/li&gt;
  &lt;li&gt;“我们的开发人员和DevOps是混合在一起参与on-call轮值的。如何组织是最佳方案？如何分担责任？”&lt;/li&gt;
  &lt;li&gt;“我们的on-call工程师在24小时轮值中约要处理100个问题。很多问题都被忽略了，而真正重要的问题也淹没其中。我们应该从哪开始处理？”&lt;/li&gt;
  &lt;li&gt;“我们的on-call轮值周转率很高，如何解决团队内部的认知差距？”&lt;/li&gt;
  &lt;li&gt;“我们打算将我们的DevOps团队重组为SRE(注1)。”&lt;/li&gt;
  &lt;li&gt;SRE on-call、DevOps on-call和开发人员on-call的区别是什么？因为DevOps团队非常关注这点，所以请具体说明。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们将为这些问题提供实用的建议。Google是一家拥有成熟SRE组织的大型公司，多年来我们学到的很多东西都可以应用于任何规模和成熟度的公司或者组织中。Google在各种规模的服务上都有数百个on-call轮值人员，从简单到复杂的服务对应着不同的on-call设定。on-call并不专属于SRE：许多开发团队对于他们所负责的服务也需要on-call。每个on-call设定都应满足对应服务的需要。&lt;/p&gt;

&lt;p&gt;本章介绍Google内部以及其他公司的on-call设定。你的设定和情况可能与我们展示的具体示例不同，但我们所涵盖的基本概念是普遍适用的。&lt;/p&gt;

&lt;p&gt;在深入研究和分析手机报警负载的原因后，我们提出优化报警信息设置的策略并最大限度的减少负载。&lt;/p&gt;

&lt;p&gt;最后，我们分享了Google内部实践的两个例子：on-call的灵活性和on-call团队的动态变化。这些实践证明无论on-call设定的数学方法是什么，都不能完全依赖on-call人员的后续处理。需要适当考虑对on-call人员进行激励和人文关怀。&lt;/p&gt;

&lt;h2 id=&quot;回顾第一本sre书中on-call轮值&quot;&gt;回顾第一本SRE书中“On-Call轮值”&lt;/h2&gt;

&lt;p&gt;《站点可靠性工程师》在“On-Call轮值”一章中阐述了Google on-call轮值的基本原则。本节将讨论该章节的重点内容。&lt;/p&gt;

&lt;p&gt;在Google，on-call的目标是确保不牺牲on-call工程师的健康为前提下覆盖到重点服务，保障服务的可靠性。因此，SRE团队的工作应该是个健康的平衡的职责搭配：on-call和日常项目工作。我们要求SRE团队至少花50%的时间进行工程项目开发，以便战略性的解决生产环境中发现的各种问题。团队人员必须确保有时间参与项目开发工作。&lt;/p&gt;

&lt;p&gt;为确保有足够时间跟进，每个on-call轮值班次内最多跟进两个故障（注2）。如果报警信息负载过多，需要采取纠正措施。（我们将在本章后面探讨报警信息负载。）注2：不管同一个“问题”发出了多少报警，都被定义为一个“故障”。一个轮值班次是12个小时。&lt;/p&gt;

&lt;p&gt;安全感（注3）对于on-call的有效轮转至关重要。on-call期间的压力很大，为了减轻on-call人员的压力，调节他们的生活，应该提供一系列清晰的程序和问题升级路线的支持。注3：David Blank-Edelman（O’Reilly）在Seeking SRE一书中有更多关于此主题的内容。&lt;/p&gt;

&lt;p&gt;针对工作时间之外的on-call工作应给予合理的补贴。不同的公司可能有不同的方式进行补贴。Google提供年假或者现金补贴，同时按一定程度的工资比例作为上限。补贴措施激励SRE按需参与on-call工作，且避免因经济原因而过多的参与on-call工作。&lt;/p&gt;

&lt;h2 id=&quot;google内部和其他公司的on-call设定示例&quot;&gt;Google内部和其他公司的On-Call设定示例&lt;/h2&gt;

&lt;p&gt;本节介绍了Google和Evernote的on-call设置，这是一家致力于帮助个人和团队创建、汇总和共享信息的跨平台应用程序的加利福尼亚公司。我们探讨了每家公司on-call设定、理念以及实践背后的原因。&lt;/p&gt;

&lt;h3 id=&quot;google组建新团队&quot;&gt;Google：组建新团队&lt;/h3&gt;

&lt;h4 id=&quot;初始场景&quot;&gt;初始场景&lt;/h4&gt;

&lt;p&gt;几年前，Google Mountain View（地名：山景城）的SER Sara组建了一个新的SRE团队，该团队需要在三个月内胜任on-call工作。Google的大多数SRE团队默认新员工需要三到九个月时间来准备承担on-call工作。新组建的Mountain View SRE团队将支持三个Google Apps服务，这些服务之前是由位于华盛顿州Kirkland（地名：柯克兰）的SRE团队提供支持的（从Mountain View起飞需要两小时才能到达）。Kirkland团队在伦敦有一个姊妹团队，该团队会继续支持这些服务，以及新组建的Mountain View SRE团队和部分产品开发团队（注4）注4：Google的SRE团队通过跨时区协同工作来确保服务的连续性。&lt;/p&gt;

&lt;p&gt;新成立的MountainView SRE团队很快聚集了七个人：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sara，SRE技术主管&lt;/li&gt;
  &lt;li&gt;Mike，来自另一个SRE团队的具有丰富经验的SRE&lt;/li&gt;
  &lt;li&gt;从新产品开发团队转岗过来的SRE&lt;/li&gt;
  &lt;li&gt;四名Nooglers（Nooglers：新员工，特指近期才为Google工作的人）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;面对新服务的on-call工作，即使是个成熟的团队，也是充满挑战的，而新的Mountain View SRE团队是一个相对初级的团队。尽管如此，新团队做到了在不牺牲服务质量或项目速度的前提下提供服务。他们很快着手改进服务，包括将机器成本降低40%，通过灰度发布和其它安全检查完成自动化发布。新团队持续提供可靠性服务，目标为99.98%的可用性，或每季度约26分钟的停机时间。&lt;/p&gt;

&lt;p&gt;新的SRE团队是如何通过自我完善来实现该目标的？答案是通过入门项目，指导和培训。&lt;/p&gt;

&lt;h4 id=&quot;培训方案&quot;&gt;培训方案&lt;/h4&gt;

&lt;p&gt;虽然新的SRE团队对他们的服务对象知之甚少，但Sara和Mike对Google的生产环境和SRE工作非常熟悉，且四位Nooglers也通过了公司的招聘，Sara和Mike整理了一份包含二十多个重点领域的清单供组员在on-call之前进行练习，例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;管理生产作业&lt;/li&gt;
  &lt;li&gt;了解调试信息&lt;/li&gt;
  &lt;li&gt;集群流量切换&lt;/li&gt;
  &lt;li&gt;回滚有问题的版本&lt;/li&gt;
  &lt;li&gt;阻止或限制恶意请求&lt;/li&gt;
  &lt;li&gt;提供额外的服务能力&lt;/li&gt;
  &lt;li&gt;使用监控系统（报警和仪表盘）&lt;/li&gt;
  &lt;li&gt;了解服务的架构，各种组件以及依赖关系&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nooglers(新人)通过研究现有文档和代码（指导，实践编码教程）找到这类信息，并通过研究入门项目来理解相关主题。当团队成员学习到与Nooglers的初学者项目相关的主题时，该成员会召开简短的临时会议，将所学知识分享给其他成员。Sara和Mike会介绍剩余的主题。该团队还进行实践练习，通过执行常见的调试和降损操作帮助成员形成肌肉记忆，增加对自己能力的信心。&lt;/p&gt;

&lt;p&gt;除了练习清单外，这个新团队还进行了一系列“深度潜水”来深入了解他们的服务。团队浏览了监控控制台，确定正在运行的作业，并尝试修复最近的报警。Sara和Mike解释道，要想做到对每项服务都十分熟悉，工程师并非需要多年的专业知识。他们指导团队也是从这一原则出发，鼓励Nooglers熟悉这些服务。他们每个人理解的知识都是有限的，这会教导成员在何时向别人寻求帮助。&lt;/p&gt;

&lt;p&gt;在整个成长过程，新团队并不孤单。Sara和Mike前往其它SRE团队和产品开发团队，向他们取经。新团队通过视频会议、邮件和IRC与Kirkland和伦敦团队进行沟通。此外，该团队还参加每周的生产会议，查看on-call轮值表和事后报告，并浏览现有的服务文档。Kirkland团队派来一名SRE与新团队交流解答问题，伦敦的SRE整理了一套完整的灾难情景，并在Google灾难恢复培训期间进行了运行展示（请参阅“站点可靠性工程”第33章“灾难预案与演习”部分）。&lt;/p&gt;

&lt;p&gt;该小组还通过“幸运之轮”训练演习（见第28章“故障处理分角色演习”一节）如何on-call，扮演各类角色，练习解决生产问题。演习期间，鼓励所有SRE提供解决模拟生产环境故障的建议。在每个人的能力都得到增强之后，团队仍举办这类演习，每个成员轮流担任演习负责人，并且将演习过程记录下来以供未来参考。&lt;/p&gt;

&lt;p&gt;在进行on-call工作之前，团队查看了有关on-call工程师职责的指导原则。例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在每次轮岗开始时，要从上一个on-call工程师那获得轮岗转换邮件。&lt;/li&gt;
  &lt;li&gt;on-call工程师需要先止损，然后确保完全解决问题。&lt;/li&gt;
  &lt;li&gt;在轮岗结束时，on-call工程师向待命的on-call发送轮岗转换邮件通知。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;操作指南规定了问题何时升级到其他人以及如何为大型事件编写事后总结报告。&lt;/p&gt;

&lt;p&gt;最后，该团队阅读并更新了on-call的操作指南。操作指南中有针对报警的详细说明，解释了报警的严重级别和影响，还有针对完全解除报警的操作意见和需要采取的措施。对于SRE，每当产生报警时，都会创建对应的操作指南记录。这些记录可以减小on-call压力，平均恢复时间（MTTR）以及人为犯错误的风险。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;-维护操作指南-&quot;&gt;&lt;center&gt; 维护操作指南 &lt;/center&gt;&lt;/h4&gt;
  &lt;p&gt;操作指南中涉及的细节变化与生产环境的变化保持同步。针对日常发布，指南可能需要随着发布时间进行更新。就像任何一种方式的沟通一样，编写一份好的文档是很难的。因此，如何维护好你的操作指南呢？&lt;/p&gt;

  &lt;p&gt;Google的一些SRE主张操作指南条目要保持通用性，这样迭代的速度就会缓慢些。例如，所有的“RPC Errors High”报警放在一个条目下，经验丰富的on-call工程师可以结合当前报警服务的架构图进行阅读。为了减少人员变更因素影响以及降低MTTR，另有部分SRE主张逐步开放分享操作指南。如果你的团队对指南中的做法另有异议，那么操作指南可能会衍生出多个分支。&lt;/p&gt;

  &lt;p&gt;这个话题颇具争议。不管你想做成什么样的，但至少你和你的团队要明确操作指南的最小粒度和结构化的细节，并时刻关注操作指南的内容是否累计到超出了最初的设定。在这过程中，要学会将实践获取的知识自动化部署到到监控平台。如果你的操作指南是个明确的由命令组成的列表，当对应的报警触发时，on-call工程师会按照列表执行命令的话，我们建议将其转化为自动化执行。&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;两个月后，Sara，Mike和他们的SRE承担了即将卸任的Kirkland SRE团队的on-call备岗工作。在第三个月，他们成了on-call主岗，Kirkland SRE作为备岗。通过这样的方式，Sara和他的SRE团队随时可以替代Kirkland SRE团队。接下来，Nooglers作为更有经验的SRE成员的备岗加入了轮值工作。&lt;/p&gt;

&lt;p&gt;丰富的文档和前文所述的各种策略方法都有助于团队形成坚实的基础并迅速获得提升。虽然on-call意味着压力，但团队已经具备足够的信心让他们在采取行动之前不会怀疑自己。即使他们升级事件，他们的反应也是基于团队的集体知识以及自身心理素质，仍然是个称职的on-call工程师。&lt;/p&gt;

&lt;h4 id=&quot;后续&quot;&gt;后续&lt;/h4&gt;

&lt;p&gt;虽然MountainView SRE仍在成长，但他们了解到他们在伦敦的姊妹团队将转而负责新的项目，并且在苏黎世成立了一个新团队来负责伦敦团队之前的工作。对于第二次工作交接，Mountain View SRE使用了相同的方法，事实证明也是成功的。MountainView SRE之前准备的入职和培训资料帮助新成立的苏黎世SRE团队获得成功。&lt;/p&gt;

&lt;p&gt;当一群SRE组成一个新团队时，Mountain View SRE的方法是有效的，但当团队新加入一个成员时，只需要用更轻量级的方法即可。考虑到将来的轮转，SRE绘制了服务架构图，并将基础培训列表正式化为一系列的练习，这些练习无需导师全程参与，可由成员半自主完成。例如描述存储层，执行扩容以及了解HTTP请求过程。&lt;/p&gt;

&lt;h3 id=&quot;evernote在云中寻找我们的根&quot;&gt;Evernote：在云中寻找我们的根&lt;/h3&gt;

&lt;h4 id=&quot;将我们的本地基础架构迁移到云&quot;&gt;将我们的本地基础架构迁移到云&lt;/h4&gt;

&lt;p&gt;如生活中的大多数事情一样，需求是发明之母，因此我们并没有着手重新设计我们的on-call流程。在2016年12月之前，Evernote仅运行在本地数据中心，支持我们的单体式应用程序。我们的网络和服务器在设计时考虑了特定的架构和数据流，结合其他一些约束，意味着我们缺乏支持水平架构所需的灵活性。Google Cloud Platform（GCP）为我们提供了具体的解决方案。但是，仍有一个障碍需要克服：将我们的所有产品和基础设施迁移到GCP。历时70天，通过艰苦卓绝的努力和无数壮举（例如，移动数千台服务器和3.5PB的数据），我们住进了新家。至此，我们的工作仍未完成：我们如何监控，报警，最重要的——如何在新环境应对问题？&lt;/p&gt;

&lt;h4 id=&quot;调整on-call策略和流程&quot;&gt;调整on-call策略和流程&lt;/h4&gt;

&lt;p&gt;应用迁移到云环境激发了我们基础设施快速增长的潜力，减小了我们的基础设施快速增长的阻力，但我们的on-call策略和流程尚未随着这种增长而调整。迁移完成后，我们着手解决问题。在之前的物理数据中心中，我们几乎在每个组件中都创建了冗余。对我们而言，组件故障很常见，但基本上很少有单个组件对用户体验产生负面影响。要知道任何小的抖动都是源于系统的某处故障，而因为我们可以控制它，所以我们的基础设施非常稳定。我们的报警策略是基于以下思路构建的：一些丢弃的数据包，导致JDBC（Java数据库连接）连接异常，意味着VM（虚拟机）主机即将发生故障，或控制面板某一开关一直处于失常。甚至在我们第一天步入云端之前，我们就意识到这种类型的报警/响应系统在未来是不可行的。在实时迁移和网络延迟的世界中，我们需要采用更全面的监控方法。&lt;/p&gt;

&lt;p&gt;根据第一原则重新构建报警事件，并将这些原则写下来作为明确的SLO（服务级别目标），这有助于团队明确重要信息，从监控基础架构中减少冗余。我们专注更高级别的指标，例如API响应而非类似MySQL中的InnDB行锁等低级别的基础结构，这意味着将更多时间集中在用户在服务中断时遇到的痛点上。对团队而言，也意味着可以减少追踪瞬态问题的时间。相对应的，有了更多的睡眠时间，更高效，工作满意度也更高。&lt;/p&gt;

&lt;h4 id=&quot;重构监控和指标&quot;&gt;重构监控和指标&lt;/h4&gt;

&lt;p&gt;我们的on-call轮值人员是由一个小而充满斗志的工程师团队组成，他们负责生产基础设施和一些业务系统（例如，升级和构建管道基础设施）。每周进行一次轮岗，且在每日的晨会上对之前一天的事故进行复盘。我们的团队规模小，但责任范围大，因此需要努力减轻流程负担，专注于尽快响应报警/报警分类/处理报警/事后分析复盘。我们实现这一目标的方法之一是通过维护简单但有效的报警SLA（服务级别协议）来保持低信噪比。我们将指标或监控基础架构产生的所有故障分成三类：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;P1：立即处理&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;需要立即采取行动&lt;/li&gt;
  &lt;li&gt;呼叫on-call&lt;/li&gt;
  &lt;li&gt;导致事件分类&lt;/li&gt;
  &lt;li&gt;是否影响SLO&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;P2：下个工作日处理&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;通常不面向客户，或范围有限&lt;/li&gt;
  &lt;li&gt;向团队发送电子邮件并通知事件流向&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;P3：故障仅需知晓&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;信息收集在仪表盘，自动发送的邮件中等&lt;/li&gt;
  &lt;li&gt;与容量规划相关的信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所有P1和P2故障都附有故障单，用于描述事件分类，跟踪处理措施，SLO影响，发生的次数和事后报告链接。&lt;/p&gt;

&lt;p&gt;当出现P1级别的事件时，on-call人员需要评估该事件对用户的影响。从1到3对事件的严重性做分级。对于严重性等级为1（Sev 1）的事件，我们有一套标准流程以便响应人员尽可能快速的做出升级决策。故障升级后，我们会组建一个故障团队开始故障处理流程。由记录员和通讯负责人负责故障通报，沟通渠道是完全开放的。故障解决后，我们会主动进行复盘并在公司内部分享结果。对于级别为Sev 2或Sev 3的故障，on-call人员负责处理故障以及故障的事后报告。&lt;/p&gt;

&lt;p&gt;保持流程的轻量化有助于参与项目工作的同事胜任on-call工作，鼓励on-call在遇到故障时立即采取修复行动，并在完成故障复盘后发现工具或流程中的不足之处。通过这种方式，在每次on-call轮岗期间都有持续的改进和灵活的循环方式，能和环境的变化速度保持一致。我们的目标是每个on-call轮次都比上一次更好。&lt;/p&gt;

&lt;h4 id=&quot;追踪观察我们的表现&quot;&gt;追踪观察我们的表现&lt;/h4&gt;

&lt;p&gt;随着SLO的引入，我们希望按照时间维度跟踪性能，并将这些信息分享给公司内部的利益相关者。我们举办了月度级别的服务回顾会议，任何有兴趣的人都可以参加，会议主要回顾和讨论上个月份的服务情况。同时通过该会议判断on-call人员的负担，on-call的负担情况是团队健康的晴雨表，在压力过大时我们需要讨论缓解措施。会议的另一目的是在公司内部宣传SLO的重要性，督促技术团队对我们的服务健康和我们的健康负责。&lt;/p&gt;

&lt;h4 id=&quot;与cre合作&quot;&gt;与CRE合作&lt;/h4&gt;

&lt;p&gt;合理表达我们在SLO方面的目标是与Google客户可靠性工程（CRE）团队合作的奠定了基础。在与CRE讨论我们的SLO以明确它们是否真实可衡量后，两个团队决定针对会影响SLO的故障，都参与报警接收。隐藏在云抽象层背后的故障根本原因是很难被找到的，Google员工的参与在黑盒事件分类方面给予了帮助。更重要的是，这项举措进一步减小了用户最关心的MTTR。&lt;/p&gt;

&lt;h4 id=&quot;保持自我延续的循环&quot;&gt;保持自我延续的循环&lt;/h4&gt;

&lt;p&gt;我们现在有更多时间从团队角度思考如何推进业务发展，而不是将所有时间投入到分类/根因分析/事后分析的事情上。例如，改进我们的微服务平台，为我们的产品开发团队建立生产环境标准等项目。后者包括了我们重组on-call时遵循的很多原则，对于团队第一次上战场“接收报警”十分有帮助。因此，我们也延长改善了on-call的循环时间。&lt;/p&gt;

&lt;h2 id=&quot;实际实施细节&quot;&gt;实际实施细节&lt;/h2&gt;

&lt;p&gt;至此，我们已经讨论了Google和Google以外的on-call设定的细节。但是对于即将参与on-call有哪些具体考虑因素呢？以下部分更深入的讨论了这些实现细节：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;报警负载——它是什么，如何工作，如何管理它&lt;/li&gt;
  &lt;li&gt;如何让on-call时间表更具灵活性，为SRE创造更健康的工作/生活平衡环境&lt;/li&gt;
  &lt;li&gt;对于特定的SRE团队和合作团队要有动态的团队策略&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;手机报警负载剖析&quot;&gt;手机报警负载剖析&lt;/h3&gt;

&lt;p&gt;你的手机一直有报警提醒，多到团队都受到了影响。假定你已经阅读了《》Site Reliability Engineering》的第31章，并和你的团队以及所支持的开发团队定期召开生产会议。现在大家都知道你的on-call工程师因为报警负载而不开心。然后呢？&lt;/p&gt;

&lt;p&gt;手机报警负载的定义是on-call工程师在轮值期间（每天或每周）收到的报警数量。一个故障可能导致多个报警产生。我们将介绍各种影响手机报警负载的因素并提出最小化报警负载的技术。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;-合适的响应时间-&quot;&gt;&lt;center&gt; 合适的响应时间 &lt;/center&gt;&lt;/h3&gt;

  &lt;p&gt;除非有充足的理由，否则工程师无需在收到报警的几分钟内上机器处理问题。虽然面向客户的创收服务故障需要立即响应，但一些不太严重的问题（例如，备份失败），你完全可以在几小时内处理。&lt;/p&gt;

  &lt;p&gt;我们建议你检查当前的报警设置，判断是否应该为当前触发报警的所有事件提供报警服务。你可能试图采用自动修复来解决问题（相对于人为修复，计算机能更好的解决问题）或者采用工单（如果它不是高优先级）。表8-1显示了一些案例和相对的响应。&lt;/p&gt;

&lt;/blockquote&gt;

&lt;center&gt;  表8-1. 实际响应时间案例 &lt;/center&gt;

&lt;style&gt;
table th:nth-of-type(1) { width: 20%; }
table th:nth-of-type(2) { width: 12.5%; }
table th:nth-of-type(3) { width: 67.5%; }
&lt;/style&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;故障描述&lt;/th&gt;
      &lt;th&gt;响应时间&lt;/th&gt;
      &lt;th&gt;对SRE的影响&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;影响盈利的网络中断&lt;/td&gt;
      &lt;td&gt;5分钟&lt;/td&gt;
      &lt;td&gt;SRE需要保证手头的电脑有足够的电量且能联网；不能外出；必须始终与备岗协调保持联系。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;客户订单处理系统挂了&lt;/td&gt;
      &lt;td&gt;30分钟&lt;/td&gt;
      &lt;td&gt;SRE可以出门，短期在外；在此期间，备岗无需保持在线。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;用于预发的数据库备份失败&lt;/td&gt;
      &lt;td&gt;提工单（工作时间处理）&lt;/td&gt;
      &lt;td&gt;无。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;场景超负荷的团队&quot;&gt;场景：超负荷的团队&lt;/h4&gt;
&lt;p&gt;（假设）负责前端负载均衡和终端用户连接的Connection SRE团队发现自身的报警负载超负荷了。他们已经建立了每次轮值报警事件小于2次的目标，但在过去一年中，他们每次轮值平均接收5次报警事件。分析表明，有三分之一的轮值班次内报警数量超出预设值。团队成员已经及时的响应了报警，但仍然无法解决根因；没有足够的时间找到报警根因并妥善处理解决后续问题。一些工程师离开了团队，加入了运维负担较小的团队。由于on-call工程师的时间只够用来缓解眼前的故障，没法跟踪报警的根本原因。&lt;/p&gt;

&lt;p&gt;团队的视野是开阔的：拥有遵循SRE最佳实践的成熟的监控系统，遵循SRE最佳时间。报警阈值设置和SLO保持一致，且报警本质上是基于业务表现特征的，意味着仅在客户受到影响时才会触发。高层管理人员在获知这些信息后，认为该团队已经处于超负荷状态，为了让团队恢复健康状态，他们开始审查项目计划。&lt;/p&gt;

&lt;p&gt;不幸的是，随着时间的推移，Connection团队已经从10多个开发团队中获得了软件组件的所有权，并且对Google面向客户的边缘和骨干网络有着强依赖。群际关系很复杂，也慢慢变得难以管理。&lt;/p&gt;

&lt;p&gt;尽管团队遵循构建监控的最佳实践方法，但所面临的很多报警都超出了他们的直接控制范围。例如，黑盒探测可能会因网络拥塞而失败，导致数据丢包。团队可以采取的唯一措施就是将事件升级到直接负责该网络的团队。&lt;/p&gt;

&lt;p&gt;除了运维负担外，团队还需要为前端系统提供新功能，供所有Google服务使用。更糟糕的是，他们的基础架构正在从一个已有10年历史的遗留框架和集群管理系统中迁移到更高的支持替代品中。该团队的服务受到全所未有的变化速度的影响，这些变化本身也引起了大部分的on-call负担。&lt;/p&gt;

&lt;p&gt;该团队需要各种技术来平衡减少过多的报警负载，团队的技术项目经理和人事经理向高级管理层提交了一份项目建议书，高级管理层审核并通过该建议书。团队全力投入减小报警负载中，在此过程中也获得了宝贵的经验教训。&lt;/p&gt;

&lt;h4 id=&quot;报警负载来源&quot;&gt;报警负载来源&lt;/h4&gt;

&lt;p&gt;解决报警负载的第一步是明确负载出现的原因。报警负载受到三个主要因素的影响：生产环境中的bug （注5）、报警和人为因素。这些因素都有对应的来源，本节将详细讨论其中一部分来源。&lt;/p&gt;

&lt;p&gt;注5：文中的“bug”是由软件或配置错误导致的非预期的系统行为。代码中的逻辑错误，二进制文件的错误配置，错误的容量规划，错误配置的负载平衡或新发现的漏洞都是导致报警负载的“生产 bug”的原因。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;对于生产环境：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;生产环境中存在的bug数量&lt;/li&gt;
  &lt;li&gt;将新bug引入生产环境&lt;/li&gt;
  &lt;li&gt;识别到新引入的bug的速度&lt;/li&gt;
  &lt;li&gt;缓解bug并从生产环境中删除之的速度&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;对于报警：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;触发报警的阈值&lt;/li&gt;
  &lt;li&gt;引入新的报警规则&lt;/li&gt;
  &lt;li&gt;将服务的SLO与其所依赖的服务的SLO关联对齐&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;对于人为因素：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;严格的修复和追踪bug&lt;/li&gt;
  &lt;li&gt;收集报警的数据质量&lt;/li&gt;
  &lt;li&gt;注意报警负载变化&lt;/li&gt;
  &lt;li&gt;人为驱动的生产环境变化&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;已经存在的bug&lt;/strong&gt;。不存在完美的系统。无论是在你的代码里，还是在你依赖的软件和库中，或者是接口之间，产品总会存在bug。虽然这些bug可能并不会立即触发报警，但它们却是客观存在的。你可以利用一些技术来识别或防止尚未导致报警的bug：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;确保系统的复杂度和实际相符，并不是越复杂越好。（见第7章）。&lt;/li&gt;
  &lt;li&gt;利用修复bug的机会，定期更新系统所依赖的软件或库（请参阅下一节有关新bug的部分）。&lt;/li&gt;
  &lt;li&gt;定期执行破坏性测试或模糊测试（例如，使用Netflix的Chaos Monkey）。&lt;/li&gt;
  &lt;li&gt;除集成和单元测试外，还执行常规负载测试。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;新bug&lt;/strong&gt;。理想情况下，SRE团队及其合作的开发团队应该在新bug进入生产环境之前检测到。事实上自动化测试漏测了很多bug，这些bug最终进入了生产环境。&lt;/p&gt;

&lt;p&gt;软件测试是个覆盖面很广的主题（例如，Martin Fowler on Testing）。这项技术在减少进入生产环境的bug数量以及减少bug在生产环境停留的时间方面很有帮助：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;随着时间推移不断改进测试（方法、技术）。尤其是你在生产环境中每发现一个bug，都要自问“如何才能在预发环境检测到这个bug？”确保有必要的工程技术跟进解决此问题。（请参阅“严谨跟踪”，第164页）。&lt;/li&gt;
  &lt;li&gt;不要忽略负载测试，虽然负载测试的优先级常被视为低于功能测试的。但许多bug仅在特定的负载条件下或特定的请求组合中才会显露出来。&lt;/li&gt;
  &lt;li&gt;在生产环境中集成（使用类似生产环境但是是合成的流量进行测试）。我们将在本书的第5章简要讨论生成合成流量。&lt;/li&gt;
  &lt;li&gt;在生产环境执行canarying（第16章）。&lt;/li&gt;
  &lt;li&gt;对新bug保持较低的容忍度。遵循“检测，回滚，修复和发布”策略，而不是“检测，虽然找到bug，但继续发布，修复并再次发布”策略。（相关详细信息，请参阅第162页的“减少延迟”。）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这种回滚策略需要可预测且频繁发布，因此回滚任何版本的成本都很小。我们在《Site Reliability Engineering》一书“发布工程”章节中讨论了相关主题。&lt;/p&gt;

&lt;p&gt;一些bug可能仅仅是由于改变客户端行为导致的。例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;仅在特定负载水平下出现的bug——例如，9月返校流量，黑色星期五，网络星期一，或一年中夏令时即欧洲和北美时差一小时的那一周，意味着更多用户同时保持清醒和在线状态。&lt;/li&gt;
  &lt;li&gt;只有特定混合请求才显示的bug——例如，用于亚洲字符集的语言编码，更接近亚洲的服务器的流量消耗更大。&lt;/li&gt;
  &lt;li&gt;仅在用户以意想不到的方式运行系统时才会显示的bug——例如，（在）航空公司订票系统使用的日期（下运行系统）！因此，为了测试能够覆盖到不常发生的行为（导致的bug），扩展您的测试方案是十分必要的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当生产系统受到多个并发错误的影响时，判断报警是由于现有bug还是新bug引起的是很困难的。最大限度的减少生产环境中的bug不仅可以减少报警负载，还对新bug的识别和分类很有帮助。因此。尽快从系统中删除生产环境的bug至关重要，修复现有bug的优先级应该在开发新功能之上；如果过程中需要跨团队合作，请参阅第18章。&lt;/p&gt;

&lt;p&gt;架构或程序问题，例如自动健康检查，自我修复和减小负载，可能需要大量的工程工作来解决。为简单起见，我们将这些问题视为“bug”，即使它们的规模、复杂度或解决它们需要的工作量很大。&lt;/p&gt;

&lt;p&gt;《SiteReliability Engineering》中第3章描述了错误预算如何控制新bug发布到生产环境的方法。例如，当服务的SLO超过其总季度错误预算的某一部分时——事先在开发人员和SRE团队间达成一致意见——可以暂停新功能开发以及和功能相关的部署，以专注于系统稳定，减少报警的频率。&lt;/p&gt;

&lt;p&gt;示例中的Connection团队采用严格的策略，要求每次故障都需要追踪bug。该举措能让团队的技术项目经理知道产生新bug的根本原因在哪。数据显示，人为错误是生产环境中新bug产生的第二大常见原因。&lt;/p&gt;

&lt;p&gt;由于人类容易出错，如果对生产系统所做的所有变更都是通过（人为开发的）配置自动生成的，那么效果会更好。在对生产环境进行变更之前，自动化手段可以执行人类无法进行的测试。Connection团队是半手工的对生产环境进行复杂的变更的。毫无疑问，团队的手动变更有时会出错；该团队引入了触发报警的新bug。在新bug进入生产系统并触发报警前，将要做出类似变更的自动化系统就会判断出这种变更是不安全。技术项目经理将这些数据提供给团队，说服他们优先考虑进行自动化项目。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;识别延迟&lt;/strong&gt;。及时识别报警的原因十分重要，这个识别的时间越长，意味着报警再次产生的几率越大。例如，有一个仅在高负载情况下才会产生的报警，如果在下一个峰值之前未识别有问题的代码或配置，那么问题可能会再次发生。你可以用这些技术来减少报警识别时间：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;使用合理有效的报警和控制台&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;确保报警页面链接到相关的监控控制台，且该控制台突出显示系统运行超出规范的位置。在控制台中，将黑盒和白盒监控报警相关联，并对关联的图表执行相同的操作。确保操作指南是最新的，提供相应每种报警类型的行动建议。on-call工程师应在相应的报警触发时用最新信息更新操作指南。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;实践应急响应&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;进行“幸运之轮”练习（在《Site Reliability Engineering》中有描述），和同事共享常用的和针对特定服务的调试技术。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;执行小变更&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;如果您频繁执行局部（部分功能、部分模块）的变更而不是偶尔的整体（所有功能、所有模块）变更，那么能很容易的将bug与引入它们对应的变更相关联。第16章中描述的Canarying版本给出了一个判断，表明新bug是否是由于新版本引起的。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;日志变更&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;将变更信息聚合到可搜索的时间线中可以更简单（且更快）的将新bug与引入它们的变更相关联。Jenkins的Slack插件可能会有所帮助。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;寻求帮助&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;在《SiteReliability Engineering》“故障管理”中，我们讨论了共同管理大型故障的问题。on-call工程师从来不会只是一个人；要让你的团队在寻求帮助时有安全感。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;减少延误&lt;/strong&gt;。一旦找到bug，修复bug所需的时间越长，就越可能再次发生问题并产生报警。可以考虑这些减少延误的技术：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;回滚变更&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果bug是在一次最近的代码、配置变更中引入的，在安全和恰当的情况下（单独回滚代码、配置可能是必要的，但如果bug是因为数据损坏导致的，那只回滚代码、配置就不能解决问题了）我们可以通过立即回滚生产环境的变更消除bug。谨记，即使是“快速修复”也需要时间进行验证，构建和发布。验证是至关重要的，要确保“快速修复”确实可以修复bug，并且不会引入额外的bug或其他非预期的影响。通常，采取“回滚，修复和发布”要优于“发布，修复和再发布”操作。&lt;/li&gt;
  &lt;li&gt;如果你的目标是99.99%的可用性，那么每季度约有15分钟的错误预算时间。上线发布的构建步骤可能需要15分钟以上，因此回滚对用户的影响更小。（99.999%的可用性对应每季度80秒的错误预算，这样的系统可能需要自我修复的属性，超出了本章的讨论范围。）&lt;/li&gt;
  &lt;li&gt;如果可能，避免接入无法回滚的变更，例如API不兼容的变更和锁步版本。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;使用功能隔离&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;设计你的系统，以便在功能X出错时，可以通过一个功能标志禁用它，而不影响功能Y。该策略还能提高发布速率让禁用X功能变得简单——且不需知道产品经理是否习惯于禁用功能。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;切走请求流量&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;把请求流量从出现bug的系统组件中切走（即重定向客户请求）。例如，如果bug是代码或配置上线导致的，并且是逐步发布到生产环境中的，那么你还有机会通过把流量从已发生变更的基础架构的元素切走（达到快速止损的目的）。这样你可以在几秒钟内降低对客户的影响，但回滚可能需要几分钟或更长时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;报警&lt;/strong&gt;。Google SRE每次轮值时间即12小时最多发生两次不同的报警事件，因此我们对如何配置报警以及如何引入新的报警是经过深思熟虑的。网站可靠性工程“监控分布式系统”描述了Google定义报警阈值的方法。严格遵守这些准则有助于健康的on-call轮转。&lt;/p&gt;

&lt;p&gt;需要强调一下，这章讨论了一些关键元素：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;收到的所有报警都应该立即去操作。我们希望团队在收到系统无法自愈的报警后立刻采取行动。信噪比要高，确保较低的误报率；低信噪比会增加on-call工程师产生“狼来了”的感觉的几率。&lt;/li&gt;
  &lt;li&gt;如果团队的报警规则是基于SLO，或错误上限（请参阅站点可靠性工程中的“黑盒监控与白盒监控”部分），那么所有参与开发和维护站点可靠性的团队都需要认同SLO的重要性并明确他们的工作优先级。&lt;/li&gt;
  &lt;li&gt;如果团队完全基于SLO和现象制订报警策略，那么放宽报警阈值是对报警的合理调整。&lt;/li&gt;
  &lt;li&gt;就像新的代码，新的报警策略也应该经过彻底和周密的审查，每条报警都应该有对应的操作指南条目。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接收报警会对人产生负面的心理影响。为了最大限度的减少这类影响，最好只有在真正需要时才引入新的报警规则。团队中任何人都可以编写新的报警策略，但新的策略需要经过整个团队的审核建议以及提出替代方案。在将新策略发布到线上之前，要彻底测试生产中的新策略来审查是否有误报。例如，可以在报警触发时给作者发送电子邮件，而不是直接呼叫on-call工程师。&lt;/p&gt;

&lt;p&gt;新的报警信息会帮你发现之前并不知道的生产环境问题。在解决了这些bug之后，报警将仅被新bug触发，也起到了回归测试的作用。&lt;/p&gt;

&lt;p&gt;确保新报警在测试环境下运行的时间足够长，能适应典型的生产环境，例如常规软件部署，云提供商的维护需求，每周负载峰值等。通常一周的测试时间是足够的，但具体时间窗口仍取决于报警和系统。&lt;/p&gt;

&lt;p&gt;最后，利用测试期间报警的触发率预测新报警可能会产生的报警负担。对新报警配置的批准或禁止要以团队为单位。如果引入新报警会导致你的服务超出报警阈值，那么需要额外注意系统的稳定性。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;事后密切跟进&lt;/strong&gt;。目的是确定每个报警的根本原因。查找“根因”的范围要从机器层面延伸到团队流程层面。服务中断是由一个本可以通过单元测试发现的bug导致的吗？根因可能不是代码中的bug，而是代码审查中团队流程的bug。&lt;/p&gt;

&lt;p&gt;如果你知道根因，你就可以修复它防止再次困扰你或你的同事。如果你的团队无法确定根本原因，可以添加监控或日志记录，帮助在下次发生这种情况时找到报警的根本原因。如果没有足够的信息来识别bug，你可以做一些事情来帮助进一步调试bug。或者至少可以得出结论，即报警是由“未知原因”触发的。请牢记，身为on-call工程师，你永远不是孤军奋战，所以可以请同事帮忙检查你的发现，看看是否有遗漏的地方。通常，报警触发后有新的证据可用时，很快能找到报警的根本原因。&lt;/p&gt;

&lt;p&gt;将一个报警解释为“瞬态的”或由于系统“自行修复”或莫名其妙“消失”而不采取任何行动时，这个报警很可能会再次发生并导致另一个报警，会给下一下on-call工程师带来麻烦。&lt;/p&gt;

&lt;p&gt;简单修复眼前的bug（或进行一个“点”修复）错过了一个避免将来出现类似报警的黄金机会。把报警信息看作一个带来工程工作的机会，这种工程工作可以改进系统并且消除可能出现的一类bug。可以在你的团队的生产组件中归档项目bug来做到这一点，我们提倡通过收集这个项目会消除的bug以及报警数量，按轻重缓急进行bug修复。如果你的提案需要3个工作周或120个工作时来实施，并且报警平均需要4个工作时才能正确处理，那么30个报警产生后会有一个明确的盈亏平衡点。&lt;/p&gt;

&lt;p&gt;举个例子，假设有这样一种情况，在同一故障域上存在很多服务器，例如这些机器在数据中心中的同一个交换机下，会导致定期同时发生多个机器的故障。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;点修复&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;在众多故障域中重新平衡当前的覆盖区。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;系统修复&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;使用自动化手段确保此类服务器和所有其他类似服务器始终分布在足够的故障域中，并在必要时自动重新平衡。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;监控（或预防）修复&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;当故障域多样性低于预期水平但尚未影响服务时，预先发出警告。理想情况下，警报将是故障工单警报，而不是呼叫报警，因为不需要立即响应。尽管处于较低的冗余水平，该系统仍可以进行服务。&lt;/p&gt;

&lt;p&gt;为确保您对寻呼警报的后续工作有所了解，请考虑以下问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如何防止此特定bug再次发生？&lt;/li&gt;
  &lt;li&gt;对于此系统和我负责的其他系统，如何防止此类bug再次发生？&lt;/li&gt;
  &lt;li&gt;哪些测试可以防止此bug被发布到生产环境中？&lt;/li&gt;
  &lt;li&gt;哪些故障工单警报会触发操作以防止bug在被报警前变的严重？&lt;/li&gt;
  &lt;li&gt;在变得严重之前，哪些报警信息会出现在控制台上？&lt;/li&gt;
  &lt;li&gt;我是否最大化了修复bug带来的收益？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然，对于on-call工程师，仅仅提交值班期间发生的呼叫报警相关的bug是不够的。重要的是，SRE团队会迅速处理他们确定的bug以减少它们再次发生的可能性。要确保SRE和开发人员团队的资源规划考虑了响应bug所需的工作量。&lt;/p&gt;

&lt;p&gt;我们建议保留一小部分SRE和开发人员团队的时间来响应出现的生产bug。例如，Google on-call工程师通常不会在轮值期间处理项目工作。相反，他们处理可以改善系统健康状况的bug。确保你的团队常规下处理生产环境bug的优先级高于其他项目工作。SRE经理和技术主管应确保及时处理生产环境bug，必要时要升级到开发人员团队决策者。&lt;/p&gt;

&lt;p&gt;当电话报警严重到需要事后调查时，遵循此方法来安排和跟踪后续行动更为重要。（有关详细信息，请参阅第10章。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;数据质量&lt;/strong&gt;。一旦识别出系统中导致报警的bug，就会出现一些问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如何明确先修复那个bug？&lt;/li&gt;
  &lt;li&gt;如何得知系统中哪个组件导致大多数报警？&lt;/li&gt;
  &lt;li&gt;如何确定on-call工程师为解决这些报警而采取的重复性手动操作？&lt;/li&gt;
  &lt;li&gt;如何判断有多少报警仍有未识别的根本原因？&lt;/li&gt;
  &lt;li&gt;如何得知哪些bug是真实存在的、最严重的，而不是未确定的？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;答案很简单：收集数据！&lt;/p&gt;

&lt;p&gt;你可能会通过跟踪和收集on-call负载的方式来收集数据，但这种方法是有局限性的。更加可持续的做法是，为你的bug跟踪系统（例如，Jira，Issue-Tracker）中的每个电话报警提交一个bug，当on-call工程师意识到每个报警都是已存在的bug的表征时，需要在监控系统的相关报警和相关bug间建立链接。你将在一列中找到尚未解决的bug列表，以及每个相关联bug的页面列表。&lt;/p&gt;

&lt;p&gt;当你拥有有关报警原因的结构化数据，就可以着手分析数据生成报告，这些报告能够回答以下问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;哪些bug导致大多数的报警？理想情况下，我们会立即回滚并修复bug，但有时候，查找根本原因并部署修复程序需要很长时间，有时忽略关键报警并不是一个合理的选择。例如，上述Connection SRE团队可能会遇到持续的网络拥塞，这种拥塞无法立即解决，但仍需要跟踪问题。为团队收集导致了最多的报警和压力的生产环境问题的数据，支持进行数据驱动的有系统的、有优先级的对话。&lt;/li&gt;
  &lt;li&gt;系统的哪个组成部分是大多数报警的原因（支付网关，身份验证微服务等）？&lt;/li&gt;
  &lt;li&gt;与其他监控数据相关联时，特定报警是否与其他信号相对应（请求量高峰，并发客户会话数，注册次数，提款次数等）？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将bug数据和报警根本原因数据结构化还有其他好处：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;你可以自动填充现有bug列表（即已知bug），这对你所支持的团队可能有益处。&lt;/li&gt;
  &lt;li&gt;你可以根据每个bug导致的报警数确定bug的优先级。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;你所收集的数据质量将决定人工或机器可以做出的决策质量。为了确保高质量的数据，请考虑以下技术：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;定义并记录你的团队对报警数据收集的策略和预期。&lt;/li&gt;
  &lt;li&gt;设置来自监控系统的非呼叫报警，突出显示未处理报警的位置。经理和技术主管应确保达到预期。&lt;/li&gt;
  &lt;li&gt;当轮岗交接不符合预期时，队友应该相互帮助跟进。积极的评论有“也许跟bug123有关”，“我已经根据你的调查结果提交了bug报告，所以我们可以进一步跟进了”，或“这看起来像我上周三轮岗发生的事情：&amp;lt;报警，bug的链接&amp;gt;”强化预期行为，确保最大化的改进。没人愿意为上一轮岗就已发生的报警再接收一次报警。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;警觉&lt;/strong&gt;。很多时候，团队会因为多次减员而陷入运维过载中。为了避免温水煮青蛙，要注意on-call工程师的健康状况，确保SRE和开发团队始终优先考虑生产环境健康状况。&lt;/p&gt;

&lt;p&gt;以下技术可以帮助团队密切关注报警呼叫负载：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在生产会议上（参见“站点可靠性工程”中的“沟通：生产会议”一节，第31章），定期根据收集的结构化数据分析报警呼叫负载的趋势。追踪21天的平均值非常有用。&lt;/li&gt;
  &lt;li&gt;当呼叫报警负载超过你的团队事先明确的“告警”阈值时，设置针对技术主管或经理的故障单报警。&lt;/li&gt;
  &lt;li&gt;在SRE团队和开发团队之间定期召开会议，讨论当前的生产状况以及当前SRE为解决的生产环境bug。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;on-call灵活性&quot;&gt;on-call灵活性&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;-值班时长-&quot;&gt;&lt;center&gt; 值班时长 &lt;/center&gt;&lt;/h3&gt;

  &lt;p&gt;on-call值班期间每天需要处理一个或几个报警，因此值班安排必须是合理可持续的：我们建议将时长限制为12小时。较短的值班时长对on-call工程师的健康是有利的。当在岗时间太长时，团队成员大概率会觉得疲惫，随之而来的是他们在工作中可能会犯错误。如果一直进行on-call工作，大多数人无法保持高质量的产出。许多国家都有关于最长工作时间，休息时间和工作条件的法律。&lt;/p&gt;

  &lt;p&gt;虽然理想情况下是一直在白天值班，但12小时轮岗制也并不需要全球分布的团队。整夜12小时处于on-call中比on-call24小时或更长时间更好。即使工作在一个地方，你也可以进行12小时轮岗值班。例如，在为期一周的班次中，不是让一名工程师每天24小时on-call，而是两名工程师一人在白天on-call，一人在夜间on-call。&lt;/p&gt;

  &lt;p&gt;根据我们的经验，如果没有缓解机制，24小时on-call是不可持续的。虽然不理想，但偶尔on-call一整夜至少可以确保你的工程师的休息时间。另一个选择是缩短值班时间——比如3天值班，4天休息。&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h4 id=&quot;情景个人情况的变化&quot;&gt;情景：个人情况的变化&lt;/h4&gt;

&lt;p&gt;想象一下，你是一个大型服务的on-call团队成员，该服务具有跨越两个站点的24/7跟随太阳模型。为了在提高服务可靠性的同时保持运维负载的可控性，虽然你并不乐意在上午6点可能会接到报警，但你对你和团队正在进行的工作感到满意。&lt;/p&gt;

&lt;p&gt;一切都很好…直到某天你才意识到on-call的时间表和你个人生活的需求开始发生冲突。有许多潜在的原因——例如，成为父母，需要短期旅行，休假或生病。&lt;/p&gt;

&lt;p&gt;你需要on-call的职责和新的个人日程表能够共存。&lt;/p&gt;

&lt;p&gt;许多团队和组织在成熟时都面临这一挑战。随着时间推移，人们的需求发生变化，为了保持多元化团队成员的健康平衡，on-call轮值的需求变的多样化。保持健康，公平以及on-call工作和个人生活健康的平衡的关键在于灵活性。&lt;/p&gt;

&lt;p&gt;为满足团队成员的需求，确保覆盖到你的服务或产品，你可以通过多种方式灵活的进行on-call轮转。指定一套全面的，一刀切的指导方针是不可能的。我们鼓励将灵活性作为一项原则，而不是简单的采用此处列举的实例。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动化on-call时间安排&lt;/strong&gt;。随着团队的发展，时间表的安排受到以下约束——休假计划，on-call工作日与周末的分布，个人偏好，宗教要求等，时间表的安排变得越来越困难。你无法手动管理此任务，很难找到任何解决方案，更不用说公平的解决方案了。&lt;/p&gt;

&lt;p&gt;“公平”并不意味着跨团队成员的每个变化都是一致的。不同的人有不同的需求和不同的偏好。因此，团队应该分享这些偏好并尝试以智能的方式满足这些偏好。团队组成和首选项决定了你的团队是更喜欢统一分发，还是以自定义的方式来满足日程安排首选项。&lt;/p&gt;

&lt;p&gt;使用自动化工具来安排on-call班次会更容易。这个自动化工具应该有这些基本特征：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;它应该重新安排on-call班次以适应团队成员不断变化的需求。&lt;/li&gt;
  &lt;li&gt;它应该自动重新平衡报警负载以响应任何更改。&lt;/li&gt;
  &lt;li&gt;应该尽量通过考虑个人偏好来确保公平，例如“4月份周末不用上学”，以及历史信息，例如最近每位on-call的值班负载。&lt;/li&gt;
  &lt;li&gt;因此，on-call工程师可以依据on-call班次进行计划，但绝不能改变已经生成的时间表。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;时间表既可以是完全自动化的，也可以是团队人员安排。同时，一些团队更愿意让成员明确遵守时间表，而其他团队则对完全自动化的流程感到满意。如果你的需求很复杂，可以选择在内部开发自己的工具，此外也有许多商业和开源软件包可以帮助自动化生产on-call时间表。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;短期互换的计划&lt;/strong&gt;。on-call时间表通常会收到的短期变化请求。没人能在周一就承诺周四肯定不会感冒。或者你可能需要在on-call期间处理无法预料的紧急事务。&lt;/p&gt;

&lt;p&gt;你可能还希望因为非常规原因能够在on-call中换岗——例如，允许on-call人员参加运动训练课程。在这种情况下，团队成员可以交换一天的on-call日（例如，周日的一半）。非竞争性的互换通常是更好的选择。&lt;/p&gt;

&lt;p&gt;具有严格报警响应SLO的团队需要考虑通勤时间。如果你的报警响应SLO为5分组，而你的通勤时间为30，那么要确保其他人在你上班途中能处理紧急情况。&lt;/p&gt;

&lt;p&gt;为了在灵活性方面实现这些目标，我们建议给予团队成员权利更新on-call轮值表。此外，有一个记录下来的策略描述转换如何操作。权利下放的策略包括只有经理可以改变的完全集中的政策，到任何成员都可以改变的完全分散的政策。根据我们的经验，对变更进行同行评审可以在安全性和灵活性之间进行良好的权衡。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;长期休息的计划&lt;/strong&gt;。由于个人情况或职业倦怠的变化，有时团队成员需要停止on-call工作。团队的结构应该能够允许on-call人员暂时不参与值班。&lt;/p&gt;

&lt;p&gt;理想情况下，团队规模应该满足在（临时）员工减少时其他成员能够承受增加的运维负担。根据我们的经验，每个站点至少需要五个人进行多站点全天候的on-call，至少需要8个人进行单站点全天候的on-call。因此，假设每个站点需要一名额外的工程师来防止人员减少，每个站点（多站点）最多需要6名工程师，每个站点（单站点）为9名。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;兼职工作的时间表计划&lt;/strong&gt;。on-call工作的时间表看起来是不兼容的，但我们发现如果你采取某些预防措施，on-call工作和兼职工作是能够做到兼容的。以下讨论假设你的on-call成员是兼职工作，他们无法在兼职工作周之外完成值班工作。&lt;/p&gt;

&lt;p&gt;兼职工作主要有两种模式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;每周工作减少一天——例如，每周工作4天，而非5天&lt;/li&gt;
  &lt;li&gt;每天减少工作时间——例如，每天工作6小时，而非8小时
两种模式都可以兼容on-call工作，但需要对on-call时间进行调整。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果非工作日是一直不变的，那么第一个模式很容易和on-call工作兼容。对应的，你可以采用每周少于7天的on-call时间（例如，周一至周四，或周五至周日），并自动调整时间表以便在兼职工程师非工作时间不会参与on-call工作。&lt;/p&gt;

&lt;p&gt;第二种模式可以通过以下几种方式实现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;与另一名工程师分担on-call时间，这样当兼职工程师不在时，仍然有人值班。例如，如果on-call工程师需要从上午9点工作到下午4点，你可以将值班的前半部分（上午9点到下午3点）分配给他们，后半部分（下午3点到晚上9点）可以以相同的方式分配给其他on-call成员。&lt;/li&gt;
  &lt;li&gt;如果on-call频率不是太高，兼职工程师可以在on-call日工作整整几个小时也是可行的。
如站点可靠性工程的第11章所述，根据当地劳动法和法规，Google SRE会在正常工作时间之外补偿小时工资或休假时间。在确定on-call补偿时，要考虑兼职工程师的时间表。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了平衡项目时间和on-call时间，工作时间较少的工程师对应的工作内容应该少点。与小型团队相比，较大的团队更容易吸收额外的on-call负载。&lt;/p&gt;

&lt;h3 id=&quot;on-call团队动态&quot;&gt;on-call团队动态&lt;/h3&gt;

&lt;p&gt;我们的第一本书谈到了高报警负载和时间压力等压力因素是如何迫使on-call工程师采用基于直觉的未经详细考虑而非基于理性和数据的决策策略（参加该书第11章“安全感”一节）。基于团队心理学的讨论，你如何建立一个积极的动态团队？考虑一个on-call团队，其中包含以下一组假设问题。&lt;/p&gt;

&lt;h4 id=&quot;情景一周生存的文化&quot;&gt;情景：“一周生存”的文化&lt;/h4&gt;

&lt;p&gt;一家公司从几位创始人和少数员工开始，他们都是开发人员，每个人都相互了解，每个人都需要接收报警。&lt;/p&gt;

&lt;p&gt;公司规模开始变大。on-call的职责仅限于一小部分更有经验的功能开发人员，因为他们更了解系统。&lt;/p&gt;

&lt;p&gt;公司变得更大。他们增加了ops角色来解决可靠性问题。该团队负责生产环境监控，成员主要集中在运维，而非编码。功能开发人员和ops人员轮流进行on-call工作。功能开发人员在维护服务方面有最终决定权，而ops仅限于运维任务。到目前为止，有30名工程师参与on-call工作：25名功能开发人员和5名ops，都位于同一站点。&lt;/p&gt;

&lt;p&gt;团队被高报警量所困扰，尽管遵循了本章前面所述的建议，尽量减少报警负载，但团队的士气仍然很低落。由于功能开发人员优先考虑开发新功能，因此on-call的后续工作需要很长时间才能实现。&lt;/p&gt;

&lt;p&gt;更糟糕的是，由于功能开发人员关注的是自己子系统的健康状况，尽管团队中其他人提出了投诉，但有位功能开发人员坚持按错误率而非关键模块错误比率来进行报警。这些报警很嘈杂，会有很多误报或者不可执行的报警。&lt;/p&gt;

&lt;p&gt;高报警负载对on-call岗的其他成员的影响不会特别大，确实有许多报警，但大多数报警都没有花太多时间来解决。正如一名on-call工程师所说：“我快速浏览一下报警主题，知道它们是重复的。所以我要做的就是忽略它们。”&lt;/p&gt;

&lt;p&gt;听起来很熟悉？&lt;/p&gt;

&lt;p&gt;Google的一些团队在成熟的早期阶段遇到过类似问题。如果不小心处理，这些问题可能扰乱功能开发团队和运维团队，并阻碍on-call的操作。没有灵丹妙药能解决这些问题，但我们发现了一些特别有用的方法。虽然你的方法可能有所不同，但总体目标应该是相同的：建立积极的团队氛围，避免混乱。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;建议一：给你的ops工程师授权&lt;/strong&gt;。你可以根据本书和站点可靠性工程中列出的指南对运维组织进行重新构建，甚至可以更改名称（SRE或类似名称）来表示角色的更改。重新命名你的运维组织并非灵丹妙药，但它有助于体现别于旧的以操作为中心的模型的新的责任变化。向团队和整个公司明确说明SRE拥有站点操作权限，包括定义可靠性的共享路线图，推动问题的全面解决，维护监控策略等。功能开发人员是必要的协作者，但没有这些权限。&lt;/p&gt;

&lt;p&gt;回到我们之前假设的团队，本公告引入了以下运维变化：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作项仅分配给5个DevOps工程师——即SRE。SRE与项目专家合作——大多为开发人员——来完成这些任务。SRE就前面提到的：“错误率与错误比例”的报警策略与功能开发人员进行协商。&lt;/li&gt;
  &lt;li&gt;如果可能，鼓励SRE深入研究代码以自行进行更改。他们将代码审查发送给项目专家。这样有利于在SRE之间建立主人翁意识，并在未来的场合提升他们的技能和权威。
通过这种安排，功能开发人员是可靠性功能的明确协作者，且SRE有权利拥有站点以及改进站点的责任。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;建议二：改善团队关系&lt;/strong&gt;。另一种可能的解决方案是建立更强有力的团队关系。Google设置了一个“有趣的预算”，专门用于组织异地活动来加强团队合作。&lt;/p&gt;

&lt;p&gt;我们发现，强大的团队关系可以增强团队成员之间的理解和协作精神。因此，工程师修复bug，完成操作项目并且帮助同事的几率更高。例如，假设你关闭了夜间管道工作，但忘记关闭检查管道是否成功运行的监控。结果，同事在凌晨3点收到了报警。如果你和那位同事为处理报警花了点时间，你对这件事感到很抱歉，并在将来对此类操作更加小心。“我要保护我的同事”这一心态会转化成为更富有成效的工作氛围。&lt;/p&gt;

&lt;p&gt;我们还发现，无论职称和职能如何，让on-call的所有成员坐在一起，有助于改善团队关系。还可以鼓励团队一起吃午饭，不要低估这些相对简单的变化，它会直接影响团队动力。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;p&gt;SRE on-call与传统的ops角色不同。SRE不仅专注于日常运维，且拥有生产环境权限，并通过定义适当的可靠性阈值，开发自动化工具以及开展战略工程项目来获得更好的生产环境。on-call的站点操作十分重要，公司必须正确处理。&lt;/p&gt;

&lt;p&gt;on-call是个人和集体压力的根源。但如果你盯着怪物的眼睛看久了，就会发现智慧。本章阐述了一些关于on-call的案例；希望我们的经验可以帮助他人避免或解决类似的问题。&lt;/p&gt;

&lt;p&gt;如果你的on-call团队淹没在无休止的报警中，我们建议你退一步观察更顶层的情况，和其他SRE和合作伙伴团队对比讨论，一旦收集了必要的信息，就要系统的解决问题。对on-call工程师，on-call团队以及整个公司来说，构建合理的on-call机制是值得投入时间的。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">On-call轮值意味着在某段时间内随叫随到，随时响应紧急问题。站点可靠性工程师（SRE）通常需要参与on-call轮值工作。在on-call轮值期间，SRE需根据需要判断、缓解、修复或升级事件。此外，SRE还需定期响应非紧急生产事件。 在Google，on-call轮值是SRE的职责之一。SRE团队可以缓解故障，修复生产环境问题且自动执行运维任务。由于大多数SRE团队的运维任务还未完全实现自动化，升级需要人为联系on-call工程师。SRE团队的on-call工作是根据所支持系统的重要程度或系统所处的开发状态而定的。根据我们的经验，大多数SRE团队都需要参与on-call轮值工作。 On-call是一个庞大而复杂的话题，限制因素很多，但是试错率却很少。我们的第一本书《Site Reliability Engineering》第11章“on-call轮值”已经探讨过这一主题。本章介绍一些我们收到的有关该章的反馈和问题。其中包括： “我们不是Google，我们的规模要小得多。我们没有那么多人参与轮值，并且没有位于不同时区的站点。你在第一本书中描述的内容与我无关。” “我们的开发人员和DevOps是混合在一起参与on-call轮值的。如何组织是最佳方案？如何分担责任？” “我们的on-call工程师在24小时轮值中约要处理100个问题。很多问题都被忽略了，而真正重要的问题也淹没其中。我们应该从哪开始处理？” “我们的on-call轮值周转率很高，如何解决团队内部的认知差距？” “我们打算将我们的DevOps团队重组为SRE(注1)。” SRE on-call、DevOps on-call和开发人员on-call的区别是什么？因为DevOps团队非常关注这点，所以请具体说明。 我们将为这些问题提供实用的建议。Google是一家拥有成熟SRE组织的大型公司，多年来我们学到的很多东西都可以应用于任何规模和成熟度的公司或者组织中。Google在各种规模的服务上都有数百个on-call轮值人员，从简单到复杂的服务对应着不同的on-call设定。on-call并不专属于SRE：许多开发团队对于他们所负责的服务也需要on-call。每个on-call设定都应满足对应服务的需要。 本章介绍Google内部以及其他公司的on-call设定。你的设定和情况可能与我们展示的具体示例不同，但我们所涵盖的基本概念是普遍适用的。 在深入研究和分析手机报警负载的原因后，我们提出优化报警信息设置的策略并最大限度的减少负载。 最后，我们分享了Google内部实践的两个例子：on-call的灵活性和on-call团队的动态变化。这些实践证明无论on-call设定的数学方法是什么，都不能完全依赖on-call人员的后续处理。需要适当考虑对on-call人员进行激励和人文关怀。 回顾第一本SRE书中“On-Call轮值” 《站点可靠性工程师》在“On-Call轮值”一章中阐述了Google on-call轮值的基本原则。本节将讨论该章节的重点内容。 在Google，on-call的目标是确保不牺牲on-call工程师的健康为前提下覆盖到重点服务，保障服务的可靠性。因此，SRE团队的工作应该是个健康的平衡的职责搭配：on-call和日常项目工作。我们要求SRE团队至少花50%的时间进行工程项目开发，以便战略性的解决生产环境中发现的各种问题。团队人员必须确保有时间参与项目开发工作。 为确保有足够时间跟进，每个on-call轮值班次内最多跟进两个故障（注2）。如果报警信息负载过多，需要采取纠正措施。（我们将在本章后面探讨报警信息负载。）注2：不管同一个“问题”发出了多少报警，都被定义为一个“故障”。一个轮值班次是12个小时。 安全感（注3）对于on-call的有效轮转至关重要。on-call期间的压力很大，为了减轻on-call人员的压力，调节他们的生活，应该提供一系列清晰的程序和问题升级路线的支持。注3：David Blank-Edelman（O’Reilly）在Seeking SRE一书中有更多关于此主题的内容。 针对工作时间之外的on-call工作应给予合理的补贴。不同的公司可能有不同的方式进行补贴。Google提供年假或者现金补贴，同时按一定程度的工资比例作为上限。补贴措施激励SRE按需参与on-call工作，且避免因经济原因而过多的参与on-call工作。 Google内部和其他公司的On-Call设定示例 本节介绍了Google和Evernote的on-call设置，这是一家致力于帮助个人和团队创建、汇总和共享信息的跨平台应用程序的加利福尼亚公司。我们探讨了每家公司on-call设定、理念以及实践背后的原因。 Google：组建新团队 初始场景 几年前，Google Mountain View（地名：山景城）的SER Sara组建了一个新的SRE团队，该团队需要在三个月内胜任on-call工作。Google的大多数SRE团队默认新员工需要三到九个月时间来准备承担on-call工作。新组建的Mountain View SRE团队将支持三个Google Apps服务，这些服务之前是由位于华盛顿州Kirkland（地名：柯克兰）的SRE团队提供支持的（从Mountain View起飞需要两小时才能到达）。Kirkland团队在伦敦有一个姊妹团队，该团队会继续支持这些服务，以及新组建的Mountain View SRE团队和部分产品开发团队（注4）注4：Google的SRE团队通过跨时区协同工作来确保服务的连续性。 新成立的MountainView SRE团队很快聚集了七个人： Sara，SRE技术主管 Mike，来自另一个SRE团队的具有丰富经验的SRE 从新产品开发团队转岗过来的SRE 四名Nooglers（Nooglers：新员工，特指近期才为Google工作的人） 面对新服务的on-call工作，即使是个成熟的团队，也是充满挑战的，而新的Mountain View SRE团队是一个相对初级的团队。尽管如此，新团队做到了在不牺牲服务质量或项目速度的前提下提供服务。他们很快着手改进服务，包括将机器成本降低40%，通过灰度发布和其它安全检查完成自动化发布。新团队持续提供可靠性服务，目标为99.98%的可用性，或每季度约26分钟的停机时间。 新的SRE团队是如何通过自我完善来实现该目标的？答案是通过入门项目，指导和培训。 培训方案 虽然新的SRE团队对他们的服务对象知之甚少，但Sara和Mike对Google的生产环境和SRE工作非常熟悉，且四位Nooglers也通过了公司的招聘，Sara和Mike整理了一份包含二十多个重点领域的清单供组员在on-call之前进行练习，例如： 管理生产作业 了解调试信息 集群流量切换 回滚有问题的版本 阻止或限制恶意请求 提供额外的服务能力 使用监控系统（报警和仪表盘） 了解服务的架构，各种组件以及依赖关系 Nooglers(新人)通过研究现有文档和代码（指导，实践编码教程）找到这类信息，并通过研究入门项目来理解相关主题。当团队成员学习到与Nooglers的初学者项目相关的主题时，该成员会召开简短的临时会议，将所学知识分享给其他成员。Sara和Mike会介绍剩余的主题。该团队还进行实践练习，通过执行常见的调试和降损操作帮助成员形成肌肉记忆，增加对自己能力的信心。 除了练习清单外，这个新团队还进行了一系列“深度潜水”来深入了解他们的服务。团队浏览了监控控制台，确定正在运行的作业，并尝试修复最近的报警。Sara和Mike解释道，要想做到对每项服务都十分熟悉，工程师并非需要多年的专业知识。他们指导团队也是从这一原则出发，鼓励Nooglers熟悉这些服务。他们每个人理解的知识都是有限的，这会教导成员在何时向别人寻求帮助。 在整个成长过程，新团队并不孤单。Sara和Mike前往其它SRE团队和产品开发团队，向他们取经。新团队通过视频会议、邮件和IRC与Kirkland和伦敦团队进行沟通。此外，该团队还参加每周的生产会议，查看on-call轮值表和事后报告，并浏览现有的服务文档。Kirkland团队派来一名SRE与新团队交流解答问题，伦敦的SRE整理了一套完整的灾难情景，并在Google灾难恢复培训期间进行了运行展示（请参阅“站点可靠性工程”第33章“灾难预案与演习”部分）。 该小组还通过“幸运之轮”训练演习（见第28章“故障处理分角色演习”一节）如何on-call，扮演各类角色，练习解决生产问题。演习期间，鼓励所有SRE提供解决模拟生产环境故障的建议。在每个人的能力都得到增强之后，团队仍举办这类演习，每个成员轮流担任演习负责人，并且将演习过程记录下来以供未来参考。 在进行on-call工作之前，团队查看了有关on-call工程师职责的指导原则。例如： 在每次轮岗开始时，要从上一个on-call工程师那获得轮岗转换邮件。 on-call工程师需要先止损，然后确保完全解决问题。 在轮岗结束时，on-call工程师向待命的on-call发送轮岗转换邮件通知。 操作指南规定了问题何时升级到其他人以及如何为大型事件编写事后总结报告。 最后，该团队阅读并更新了on-call的操作指南。操作指南中有针对报警的详细说明，解释了报警的严重级别和影响，还有针对完全解除报警的操作意见和需要采取的措施。对于SRE，每当产生报警时，都会创建对应的操作指南记录。这些记录可以减小on-call压力，平均恢复时间（MTTR）以及人为犯错误的风险。 维护操作指南 操作指南中涉及的细节变化与生产环境的变化保持同步。针对日常发布，指南可能需要随着发布时间进行更新。就像任何一种方式的沟通一样，编写一份好的文档是很难的。因此，如何维护好你的操作指南呢？ Google的一些SRE主张操作指南条目要保持通用性，这样迭代的速度就会缓慢些。例如，所有的“RPC Errors High”报警放在一个条目下，经验丰富的on-call工程师可以结合当前报警服务的架构图进行阅读。为了减少人员变更因素影响以及降低MTTR，另有部分SRE主张逐步开放分享操作指南。如果你的团队对指南中的做法另有异议，那么操作指南可能会衍生出多个分支。 这个话题颇具争议。不管你想做成什么样的，但至少你和你的团队要明确操作指南的最小粒度和结构化的细节，并时刻关注操作指南的内容是否累计到超出了最初的设定。在这过程中，要学会将实践获取的知识自动化部署到到监控平台。如果你的操作指南是个明确的由命令组成的列表，当对应的报警触发时，on-call工程师会按照列表执行命令的话，我们建议将其转化为自动化执行。 两个月后，Sara，Mike和他们的SRE承担了即将卸任的Kirkland SRE团队的on-call备岗工作。在第三个月，他们成了on-call主岗，Kirkland SRE作为备岗。通过这样的方式，Sara和他的SRE团队随时可以替代Kirkland SRE团队。接下来，Nooglers作为更有经验的SRE成员的备岗加入了轮值工作。 丰富的文档和前文所述的各种策略方法都有助于团队形成坚实的基础并迅速获得提升。虽然on-call意味着压力，但团队已经具备足够的信心让他们在采取行动之前不会怀疑自己。即使他们升级事件，他们的反应也是基于团队的集体知识以及自身心理素质，仍然是个称职的on-call工程师。 后续 虽然MountainView SRE仍在成长，但他们了解到他们在伦敦的姊妹团队将转而负责新的项目，并且在苏黎世成立了一个新团队来负责伦敦团队之前的工作。对于第二次工作交接，Mountain View SRE使用了相同的方法，事实证明也是成功的。MountainView SRE之前准备的入职和培训资料帮助新成立的苏黎世SRE团队获得成功。 当一群SRE组成一个新团队时，Mountain View SRE的方法是有效的，但当团队新加入一个成员时，只需要用更轻量级的方法即可。考虑到将来的轮转，SRE绘制了服务架构图，并将基础培训列表正式化为一系列的练习，这些练习无需导师全程参与，可由成员半自主完成。例如描述存储层，执行扩容以及了解HTTP请求过程。 Evernote：在云中寻找我们的根 将我们的本地基础架构迁移到云 如生活中的大多数事情一样，需求是发明之母，因此我们并没有着手重新设计我们的on-call流程。在2016年12月之前，Evernote仅运行在本地数据中心，支持我们的单体式应用程序。我们的网络和服务器在设计时考虑了特定的架构和数据流，结合其他一些约束，意味着我们缺乏支持水平架构所需的灵活性。Google Cloud Platform（GCP）为我们提供了具体的解决方案。但是，仍有一个障碍需要克服：将我们的所有产品和基础设施迁移到GCP。历时70天，通过艰苦卓绝的努力和无数壮举（例如，移动数千台服务器和3.5PB的数据），我们住进了新家。至此，我们的工作仍未完成：我们如何监控，报警，最重要的——如何在新环境应对问题？ 调整on-call策略和流程 应用迁移到云环境激发了我们基础设施快速增长的潜力，减小了我们的基础设施快速增长的阻力，但我们的on-call策略和流程尚未随着这种增长而调整。迁移完成后，我们着手解决问题。在之前的物理数据中心中，我们几乎在每个组件中都创建了冗余。对我们而言，组件故障很常见，但基本上很少有单个组件对用户体验产生负面影响。要知道任何小的抖动都是源于系统的某处故障，而因为我们可以控制它，所以我们的基础设施非常稳定。我们的报警策略是基于以下思路构建的：一些丢弃的数据包，导致JDBC（Java数据库连接）连接异常，意味着VM（虚拟机）主机即将发生故障，或控制面板某一开关一直处于失常。甚至在我们第一天步入云端之前，我们就意识到这种类型的报警/响应系统在未来是不可行的。在实时迁移和网络延迟的世界中，我们需要采用更全面的监控方法。 根据第一原则重新构建报警事件，并将这些原则写下来作为明确的SLO（服务级别目标），这有助于团队明确重要信息，从监控基础架构中减少冗余。我们专注更高级别的指标，例如API响应而非类似MySQL中的InnDB行锁等低级别的基础结构，这意味着将更多时间集中在用户在服务中断时遇到的痛点上。对团队而言，也意味着可以减少追踪瞬态问题的时间。相对应的，有了更多的睡眠时间，更高效，工作满意度也更高。 重构监控和指标 我们的on-call轮值人员是由一个小而充满斗志的工程师团队组成，他们负责生产基础设施和一些业务系统（例如，升级和构建管道基础设施）。每周进行一次轮岗，且在每日的晨会上对之前一天的事故进行复盘。我们的团队规模小，但责任范围大，因此需要努力减轻流程负担，专注于尽快响应报警/报警分类/处理报警/事后分析复盘。我们实现这一目标的方法之一是通过维护简单但有效的报警SLA（服务级别协议）来保持低信噪比。我们将指标或监控基础架构产生的所有故障分成三类： P1：立即处理 需要立即采取行动 呼叫on-call 导致事件分类 是否影响SLO P2：下个工作日处理 通常不面向客户，或范围有限 向团队发送电子邮件并通知事件流向 P3：故障仅需知晓 信息收集在仪表盘，自动发送的邮件中等 与容量规划相关的信息 所有P1和P2故障都附有故障单，用于描述事件分类，跟踪处理措施，SLO影响，发生的次数和事后报告链接。 当出现P1级别的事件时，on-call人员需要评估该事件对用户的影响。从1到3对事件的严重性做分级。对于严重性等级为1（Sev 1）的事件，我们有一套标准流程以便响应人员尽可能快速的做出升级决策。故障升级后，我们会组建一个故障团队开始故障处理流程。由记录员和通讯负责人负责故障通报，沟通渠道是完全开放的。故障解决后，我们会主动进行复盘并在公司内部分享结果。对于级别为Sev 2或Sev 3的故障，on-call人员负责处理故障以及故障的事后报告。 保持流程的轻量化有助于参与项目工作的同事胜任on-call工作，鼓励on-call在遇到故障时立即采取修复行动，并在完成故障复盘后发现工具或流程中的不足之处。通过这种方式，在每次on-call轮岗期间都有持续的改进和灵活的循环方式，能和环境的变化速度保持一致。我们的目标是每个on-call轮次都比上一次更好。 追踪观察我们的表现 随着SLO的引入，我们希望按照时间维度跟踪性能，并将这些信息分享给公司内部的利益相关者。我们举办了月度级别的服务回顾会议，任何有兴趣的人都可以参加，会议主要回顾和讨论上个月份的服务情况。同时通过该会议判断on-call人员的负担，on-call的负担情况是团队健康的晴雨表，在压力过大时我们需要讨论缓解措施。会议的另一目的是在公司内部宣传SLO的重要性，督促技术团队对我们的服务健康和我们的健康负责。 与CRE合作 合理表达我们在SLO方面的目标是与Google客户可靠性工程（CRE）团队合作的奠定了基础。在与CRE讨论我们的SLO以明确它们是否真实可衡量后，两个团队决定针对会影响SLO的故障，都参与报警接收。隐藏在云抽象层背后的故障根本原因是很难被找到的，Google员工的参与在黑盒事件分类方面给予了帮助。更重要的是，这项举措进一步减小了用户最关心的MTTR。 保持自我延续的循环 我们现在有更多时间从团队角度思考如何推进业务发展，而不是将所有时间投入到分类/根因分析/事后分析的事情上。例如，改进我们的微服务平台，为我们的产品开发团队建立生产环境标准等项目。后者包括了我们重组on-call时遵循的很多原则，对于团队第一次上战场“接收报警”十分有帮助。因此，我们也延长改善了on-call的循环时间。 实际实施细节 至此，我们已经讨论了Google和Google以外的on-call设定的细节。但是对于即将参与on-call有哪些具体考虑因素呢？以下部分更深入的讨论了这些实现细节： 报警负载——它是什么，如何工作，如何管理它 如何让on-call时间表更具灵活性，为SRE创造更健康的工作/生活平衡环境 对于特定的SRE团队和合作团队要有动态的团队策略 手机报警负载剖析 你的手机一直有报警提醒，多到团队都受到了影响。假定你已经阅读了《》Site Reliability Engineering》的第31章，并和你的团队以及所支持的开发团队定期召开生产会议。现在大家都知道你的on-call工程师因为报警负载而不开心。然后呢？ 手机报警负载的定义是on-call工程师在轮值期间（每天或每周）收到的报警数量。一个故障可能导致多个报警产生。我们将介绍各种影响手机报警负载的因素并提出最小化报警负载的技术。 合适的响应时间 除非有充足的理由，否则工程师无需在收到报警的几分钟内上机器处理问题。虽然面向客户的创收服务故障需要立即响应，但一些不太严重的问题（例如，备份失败），你完全可以在几小时内处理。 我们建议你检查当前的报警设置，判断是否应该为当前触发报警的所有事件提供报警服务。你可能试图采用自动修复来解决问题（相对于人为修复，计算机能更好的解决问题）或者采用工单（如果它不是高优先级）。表8-1显示了一些案例和相对的响应。 表8-1. 实际响应时间案例 故障描述 响应时间 对SRE的影响 影响盈利的网络中断 5分钟 SRE需要保证手头的电脑有足够的电量且能联网；不能外出；必须始终与备岗协调保持联系。 客户订单处理系统挂了 30分钟 SRE可以出门，短期在外；在此期间，备岗无需保持在线。 用于预发的数据库备份失败 提工单（工作时间处理） 无。 场景：超负荷的团队 （假设）负责前端负载均衡和终端用户连接的Connection SRE团队发现自身的报警负载超负荷了。他们已经建立了每次轮值报警事件小于2次的目标，但在过去一年中，他们每次轮值平均接收5次报警事件。分析表明，有三分之一的轮值班次内报警数量超出预设值。团队成员已经及时的响应了报警，但仍然无法解决根因；没有足够的时间找到报警根因并妥善处理解决后续问题。一些工程师离开了团队，加入了运维负担较小的团队。由于on-call工程师的时间只够用来缓解眼前的故障，没法跟踪报警的根本原因。 团队的视野是开阔的：拥有遵循SRE最佳实践的成熟的监控系统，遵循SRE最佳时间。报警阈值设置和SLO保持一致，且报警本质上是基于业务表现特征的，意味着仅在客户受到影响时才会触发。高层管理人员在获知这些信息后，认为该团队已经处于超负荷状态，为了让团队恢复健康状态，他们开始审查项目计划。 不幸的是，随着时间的推移，Connection团队已经从10多个开发团队中获得了软件组件的所有权，并且对Google面向客户的边缘和骨干网络有着强依赖。群际关系很复杂，也慢慢变得难以管理。 尽管团队遵循构建监控的最佳实践方法，但所面临的很多报警都超出了他们的直接控制范围。例如，黑盒探测可能会因网络拥塞而失败，导致数据丢包。团队可以采取的唯一措施就是将事件升级到直接负责该网络的团队。 除了运维负担外，团队还需要为前端系统提供新功能，供所有Google服务使用。更糟糕的是，他们的基础架构正在从一个已有10年历史的遗留框架和集群管理系统中迁移到更高的支持替代品中。该团队的服务受到全所未有的变化速度的影响，这些变化本身也引起了大部分的on-call负担。 该团队需要各种技术来平衡减少过多的报警负载，团队的技术项目经理和人事经理向高级管理层提交了一份项目建议书，高级管理层审核并通过该建议书。团队全力投入减小报警负载中，在此过程中也获得了宝贵的经验教训。 报警负载来源 解决报警负载的第一步是明确负载出现的原因。报警负载受到三个主要因素的影响：生产环境中的bug （注5）、报警和人为因素。这些因素都有对应的来源，本节将详细讨论其中一部分来源。 注5：文中的“bug”是由软件或配置错误导致的非预期的系统行为。代码中的逻辑错误，二进制文件的错误配置，错误的容量规划，错误配置的负载平衡或新发现的漏洞都是导致报警负载的“生产 bug”的原因。 对于生产环境： 生产环境中存在的bug数量 将新bug引入生产环境 识别到新引入的bug的速度 缓解bug并从生产环境中删除之的速度 对于报警： 触发报警的阈值 引入新的报警规则 将服务的SLO与其所依赖的服务的SLO关联对齐 对于人为因素： 严格的修复和追踪bug 收集报警的数据质量 注意报警负载变化 人为驱动的生产环境变化 已经存在的bug。不存在完美的系统。无论是在你的代码里，还是在你依赖的软件和库中，或者是接口之间，产品总会存在bug。虽然这些bug可能并不会立即触发报警，但它们却是客观存在的。你可以利用一些技术来识别或防止尚未导致报警的bug： 确保系统的复杂度和实际相符，并不是越复杂越好。（见第7章）。 利用修复bug的机会，定期更新系统所依赖的软件或库（请参阅下一节有关新bug的部分）。 定期执行破坏性测试或模糊测试（例如，使用Netflix的Chaos Monkey）。 除集成和单元测试外，还执行常规负载测试。 新bug。理想情况下，SRE团队及其合作的开发团队应该在新bug进入生产环境之前检测到。事实上自动化测试漏测了很多bug，这些bug最终进入了生产环境。 软件测试是个覆盖面很广的主题（例如，Martin Fowler on Testing）。这项技术在减少进入生产环境的bug数量以及减少bug在生产环境停留的时间方面很有帮助： 随着时间推移不断改进测试（方法、技术）。尤其是你在生产环境中每发现一个bug，都要自问“如何才能在预发环境检测到这个bug？”确保有必要的工程技术跟进解决此问题。（请参阅“严谨跟踪”，第164页）。 不要忽略负载测试，虽然负载测试的优先级常被视为低于功能测试的。但许多bug仅在特定的负载条件下或特定的请求组合中才会显露出来。 在生产环境中集成（使用类似生产环境但是是合成的流量进行测试）。我们将在本书的第5章简要讨论生成合成流量。 在生产环境执行canarying（第16章）。 对新bug保持较低的容忍度。遵循“检测，回滚，修复和发布”策略，而不是“检测，虽然找到bug，但继续发布，修复并再次发布”策略。（相关详细信息，请参阅第162页的“减少延迟”。） 这种回滚策略需要可预测且频繁发布，因此回滚任何版本的成本都很小。我们在《Site Reliability Engineering》一书“发布工程”章节中讨论了相关主题。 一些bug可能仅仅是由于改变客户端行为导致的。例如： 仅在特定负载水平下出现的bug——例如，9月返校流量，黑色星期五，网络星期一，或一年中夏令时即欧洲和北美时差一小时的那一周，意味着更多用户同时保持清醒和在线状态。 只有特定混合请求才显示的bug——例如，用于亚洲字符集的语言编码，更接近亚洲的服务器的流量消耗更大。 仅在用户以意想不到的方式运行系统时才会显示的bug——例如，（在）航空公司订票系统使用的日期（下运行系统）！因此，为了测试能够覆盖到不常发生的行为（导致的bug），扩展您的测试方案是十分必要的。 当生产系统受到多个并发错误的影响时，判断报警是由于现有bug还是新bug引起的是很困难的。最大限度的减少生产环境中的bug不仅可以减少报警负载，还对新bug的识别和分类很有帮助。因此。尽快从系统中删除生产环境的bug至关重要，修复现有bug的优先级应该在开发新功能之上；如果过程中需要跨团队合作，请参阅第18章。 架构或程序问题，例如自动健康检查，自我修复和减小负载，可能需要大量的工程工作来解决。为简单起见，我们将这些问题视为“bug”，即使它们的规模、复杂度或解决它们需要的工作量很大。 《SiteReliability Engineering》中第3章描述了错误预算如何控制新bug发布到生产环境的方法。例如，当服务的SLO超过其总季度错误预算的某一部分时——事先在开发人员和SRE团队间达成一致意见——可以暂停新功能开发以及和功能相关的部署，以专注于系统稳定，减少报警的频率。 示例中的Connection团队采用严格的策略，要求每次故障都需要追踪bug。该举措能让团队的技术项目经理知道产生新bug的根本原因在哪。数据显示，人为错误是生产环境中新bug产生的第二大常见原因。 由于人类容易出错，如果对生产系统所做的所有变更都是通过（人为开发的）配置自动生成的，那么效果会更好。在对生产环境进行变更之前，自动化手段可以执行人类无法进行的测试。Connection团队是半手工的对生产环境进行复杂的变更的。毫无疑问，团队的手动变更有时会出错；该团队引入了触发报警的新bug。在新bug进入生产系统并触发报警前，将要做出类似变更的自动化系统就会判断出这种变更是不安全。技术项目经理将这些数据提供给团队，说服他们优先考虑进行自动化项目。 识别延迟。及时识别报警的原因十分重要，这个识别的时间越长，意味着报警再次产生的几率越大。例如，有一个仅在高负载情况下才会产生的报警，如果在下一个峰值之前未识别有问题的代码或配置，那么问题可能会再次发生。你可以用这些技术来减少报警识别时间： 使用合理有效的报警和控制台 确保报警页面链接到相关的监控控制台，且该控制台突出显示系统运行超出规范的位置。在控制台中，将黑盒和白盒监控报警相关联，并对关联的图表执行相同的操作。确保操作指南是最新的，提供相应每种报警类型的行动建议。on-call工程师应在相应的报警触发时用最新信息更新操作指南。 实践应急响应 进行“幸运之轮”练习（在《Site Reliability Engineering》中有描述），和同事共享常用的和针对特定服务的调试技术。 执行小变更 如果您频繁执行局部（部分功能、部分模块）的变更而不是偶尔的整体（所有功能、所有模块）变更，那么能很容易的将bug与引入它们对应的变更相关联。第16章中描述的Canarying版本给出了一个判断，表明新bug是否是由于新版本引起的。 日志变更 将变更信息聚合到可搜索的时间线中可以更简单（且更快）的将新bug与引入它们的变更相关联。Jenkins的Slack插件可能会有所帮助。 寻求帮助 在《SiteReliability Engineering》“故障管理”中，我们讨论了共同管理大型故障的问题。on-call工程师从来不会只是一个人；要让你的团队在寻求帮助时有安全感。 减少延误。一旦找到bug，修复bug所需的时间越长，就越可能再次发生问题并产生报警。可以考虑这些减少延误的技术： 回滚变更 如果bug是在一次最近的代码、配置变更中引入的，在安全和恰当的情况下（单独回滚代码、配置可能是必要的，但如果bug是因为数据损坏导致的，那只回滚代码、配置就不能解决问题了）我们可以通过立即回滚生产环境的变更消除bug。谨记，即使是“快速修复”也需要时间进行验证，构建和发布。验证是至关重要的，要确保“快速修复”确实可以修复bug，并且不会引入额外的bug或其他非预期的影响。通常，采取“回滚，修复和发布”要优于“发布，修复和再发布”操作。 如果你的目标是99.99%的可用性，那么每季度约有15分钟的错误预算时间。上线发布的构建步骤可能需要15分钟以上，因此回滚对用户的影响更小。（99.999%的可用性对应每季度80秒的错误预算，这样的系统可能需要自我修复的属性，超出了本章的讨论范围。） 如果可能，避免接入无法回滚的变更，例如API不兼容的变更和锁步版本。 使用功能隔离 设计你的系统，以便在功能X出错时，可以通过一个功能标志禁用它，而不影响功能Y。该策略还能提高发布速率让禁用X功能变得简单——且不需知道产品经理是否习惯于禁用功能。 切走请求流量 把请求流量从出现bug的系统组件中切走（即重定向客户请求）。例如，如果bug是代码或配置上线导致的，并且是逐步发布到生产环境中的，那么你还有机会通过把流量从已发生变更的基础架构的元素切走（达到快速止损的目的）。这样你可以在几秒钟内降低对客户的影响，但回滚可能需要几分钟或更长时间。 报警。Google SRE每次轮值时间即12小时最多发生两次不同的报警事件，因此我们对如何配置报警以及如何引入新的报警是经过深思熟虑的。网站可靠性工程“监控分布式系统”描述了Google定义报警阈值的方法。严格遵守这些准则有助于健康的on-call轮转。 需要强调一下，这章讨论了一些关键元素： 收到的所有报警都应该立即去操作。我们希望团队在收到系统无法自愈的报警后立刻采取行动。信噪比要高，确保较低的误报率；低信噪比会增加on-call工程师产生“狼来了”的感觉的几率。 如果团队的报警规则是基于SLO，或错误上限（请参阅站点可靠性工程中的“黑盒监控与白盒监控”部分），那么所有参与开发和维护站点可靠性的团队都需要认同SLO的重要性并明确他们的工作优先级。 如果团队完全基于SLO和现象制订报警策略，那么放宽报警阈值是对报警的合理调整。 就像新的代码，新的报警策略也应该经过彻底和周密的审查，每条报警都应该有对应的操作指南条目。 接收报警会对人产生负面的心理影响。为了最大限度的减少这类影响，最好只有在真正需要时才引入新的报警规则。团队中任何人都可以编写新的报警策略，但新的策略需要经过整个团队的审核建议以及提出替代方案。在将新策略发布到线上之前，要彻底测试生产中的新策略来审查是否有误报。例如，可以在报警触发时给作者发送电子邮件，而不是直接呼叫on-call工程师。 新的报警信息会帮你发现之前并不知道的生产环境问题。在解决了这些bug之后，报警将仅被新bug触发，也起到了回归测试的作用。 确保新报警在测试环境下运行的时间足够长，能适应典型的生产环境，例如常规软件部署，云提供商的维护需求，每周负载峰值等。通常一周的测试时间是足够的，但具体时间窗口仍取决于报警和系统。 最后，利用测试期间报警的触发率预测新报警可能会产生的报警负担。对新报警配置的批准或禁止要以团队为单位。如果引入新报警会导致你的服务超出报警阈值，那么需要额外注意系统的稳定性。 事后密切跟进。目的是确定每个报警的根本原因。查找“根因”的范围要从机器层面延伸到团队流程层面。服务中断是由一个本可以通过单元测试发现的bug导致的吗？根因可能不是代码中的bug，而是代码审查中团队流程的bug。 如果你知道根因，你就可以修复它防止再次困扰你或你的同事。如果你的团队无法确定根本原因，可以添加监控或日志记录，帮助在下次发生这种情况时找到报警的根本原因。如果没有足够的信息来识别bug，你可以做一些事情来帮助进一步调试bug。或者至少可以得出结论，即报警是由“未知原因”触发的。请牢记，身为on-call工程师，你永远不是孤军奋战，所以可以请同事帮忙检查你的发现，看看是否有遗漏的地方。通常，报警触发后有新的证据可用时，很快能找到报警的根本原因。 将一个报警解释为“瞬态的”或由于系统“自行修复”或莫名其妙“消失”而不采取任何行动时，这个报警很可能会再次发生并导致另一个报警，会给下一下on-call工程师带来麻烦。 简单修复眼前的bug（或进行一个“点”修复）错过了一个避免将来出现类似报警的黄金机会。把报警信息看作一个带来工程工作的机会，这种工程工作可以改进系统并且消除可能出现的一类bug。可以在你的团队的生产组件中归档项目bug来做到这一点，我们提倡通过收集这个项目会消除的bug以及报警数量，按轻重缓急进行bug修复。如果你的提案需要3个工作周或120个工作时来实施，并且报警平均需要4个工作时才能正确处理，那么30个报警产生后会有一个明确的盈亏平衡点。 举个例子，假设有这样一种情况，在同一故障域上存在很多服务器，例如这些机器在数据中心中的同一个交换机下，会导致定期同时发生多个机器的故障。 点修复 在众多故障域中重新平衡当前的覆盖区。 系统修复 使用自动化手段确保此类服务器和所有其他类似服务器始终分布在足够的故障域中，并在必要时自动重新平衡。 监控（或预防）修复 当故障域多样性低于预期水平但尚未影响服务时，预先发出警告。理想情况下，警报将是故障工单警报，而不是呼叫报警，因为不需要立即响应。尽管处于较低的冗余水平，该系统仍可以进行服务。 为确保您对寻呼警报的后续工作有所了解，请考虑以下问题： 如何防止此特定bug再次发生？ 对于此系统和我负责的其他系统，如何防止此类bug再次发生？ 哪些测试可以防止此bug被发布到生产环境中？ 哪些故障工单警报会触发操作以防止bug在被报警前变的严重？ 在变得严重之前，哪些报警信息会出现在控制台上？ 我是否最大化了修复bug带来的收益？ 当然，对于on-call工程师，仅仅提交值班期间发生的呼叫报警相关的bug是不够的。重要的是，SRE团队会迅速处理他们确定的bug以减少它们再次发生的可能性。要确保SRE和开发人员团队的资源规划考虑了响应bug所需的工作量。 我们建议保留一小部分SRE和开发人员团队的时间来响应出现的生产bug。例如，Google on-call工程师通常不会在轮值期间处理项目工作。相反，他们处理可以改善系统健康状况的bug。确保你的团队常规下处理生产环境bug的优先级高于其他项目工作。SRE经理和技术主管应确保及时处理生产环境bug，必要时要升级到开发人员团队决策者。 当电话报警严重到需要事后调查时，遵循此方法来安排和跟踪后续行动更为重要。（有关详细信息，请参阅第10章。） 数据质量。一旦识别出系统中导致报警的bug，就会出现一些问题： 如何明确先修复那个bug？ 如何得知系统中哪个组件导致大多数报警？ 如何确定on-call工程师为解决这些报警而采取的重复性手动操作？ 如何判断有多少报警仍有未识别的根本原因？ 如何得知哪些bug是真实存在的、最严重的，而不是未确定的？ 答案很简单：收集数据！ 你可能会通过跟踪和收集on-call负载的方式来收集数据，但这种方法是有局限性的。更加可持续的做法是，为你的bug跟踪系统（例如，Jira，Issue-Tracker）中的每个电话报警提交一个bug，当on-call工程师意识到每个报警都是已存在的bug的表征时，需要在监控系统的相关报警和相关bug间建立链接。你将在一列中找到尚未解决的bug列表，以及每个相关联bug的页面列表。 当你拥有有关报警原因的结构化数据，就可以着手分析数据生成报告，这些报告能够回答以下问题： 哪些bug导致大多数的报警？理想情况下，我们会立即回滚并修复bug，但有时候，查找根本原因并部署修复程序需要很长时间，有时忽略关键报警并不是一个合理的选择。例如，上述Connection SRE团队可能会遇到持续的网络拥塞，这种拥塞无法立即解决，但仍需要跟踪问题。为团队收集导致了最多的报警和压力的生产环境问题的数据，支持进行数据驱动的有系统的、有优先级的对话。 系统的哪个组成部分是大多数报警的原因（支付网关，身份验证微服务等）？ 与其他监控数据相关联时，特定报警是否与其他信号相对应（请求量高峰，并发客户会话数，注册次数，提款次数等）？ 将bug数据和报警根本原因数据结构化还有其他好处： 你可以自动填充现有bug列表（即已知bug），这对你所支持的团队可能有益处。 你可以根据每个bug导致的报警数确定bug的优先级。 你所收集的数据质量将决定人工或机器可以做出的决策质量。为了确保高质量的数据，请考虑以下技术： 定义并记录你的团队对报警数据收集的策略和预期。 设置来自监控系统的非呼叫报警，突出显示未处理报警的位置。经理和技术主管应确保达到预期。 当轮岗交接不符合预期时，队友应该相互帮助跟进。积极的评论有“也许跟bug123有关”，“我已经根据你的调查结果提交了bug报告，所以我们可以进一步跟进了”，或“这看起来像我上周三轮岗发生的事情：&amp;lt;报警，bug的链接&amp;gt;”强化预期行为，确保最大化的改进。没人愿意为上一轮岗就已发生的报警再接收一次报警。 警觉。很多时候，团队会因为多次减员而陷入运维过载中。为了避免温水煮青蛙，要注意on-call工程师的健康状况，确保SRE和开发团队始终优先考虑生产环境健康状况。 以下技术可以帮助团队密切关注报警呼叫负载： 在生产会议上（参见“站点可靠性工程”中的“沟通：生产会议”一节，第31章），定期根据收集的结构化数据分析报警呼叫负载的趋势。追踪21天的平均值非常有用。 当呼叫报警负载超过你的团队事先明确的“告警”阈值时，设置针对技术主管或经理的故障单报警。 在SRE团队和开发团队之间定期召开会议，讨论当前的生产状况以及当前SRE为解决的生产环境bug。 on-call灵活性 值班时长 on-call值班期间每天需要处理一个或几个报警，因此值班安排必须是合理可持续的：我们建议将时长限制为12小时。较短的值班时长对on-call工程师的健康是有利的。当在岗时间太长时，团队成员大概率会觉得疲惫，随之而来的是他们在工作中可能会犯错误。如果一直进行on-call工作，大多数人无法保持高质量的产出。许多国家都有关于最长工作时间，休息时间和工作条件的法律。 虽然理想情况下是一直在白天值班，但12小时轮岗制也并不需要全球分布的团队。整夜12小时处于on-call中比on-call24小时或更长时间更好。即使工作在一个地方，你也可以进行12小时轮岗值班。例如，在为期一周的班次中，不是让一名工程师每天24小时on-call，而是两名工程师一人在白天on-call，一人在夜间on-call。 根据我们的经验，如果没有缓解机制，24小时on-call是不可持续的。虽然不理想，但偶尔on-call一整夜至少可以确保你的工程师的休息时间。另一个选择是缩短值班时间——比如3天值班，4天休息。 情景：个人情况的变化 想象一下，你是一个大型服务的on-call团队成员，该服务具有跨越两个站点的24/7跟随太阳模型。为了在提高服务可靠性的同时保持运维负载的可控性，虽然你并不乐意在上午6点可能会接到报警，但你对你和团队正在进行的工作感到满意。 一切都很好…直到某天你才意识到on-call的时间表和你个人生活的需求开始发生冲突。有许多潜在的原因——例如，成为父母，需要短期旅行，休假或生病。 你需要on-call的职责和新的个人日程表能够共存。 许多团队和组织在成熟时都面临这一挑战。随着时间推移，人们的需求发生变化，为了保持多元化团队成员的健康平衡，on-call轮值的需求变的多样化。保持健康，公平以及on-call工作和个人生活健康的平衡的关键在于灵活性。 为满足团队成员的需求，确保覆盖到你的服务或产品，你可以通过多种方式灵活的进行on-call轮转。指定一套全面的，一刀切的指导方针是不可能的。我们鼓励将灵活性作为一项原则，而不是简单的采用此处列举的实例。 自动化on-call时间安排。随着团队的发展，时间表的安排受到以下约束——休假计划，on-call工作日与周末的分布，个人偏好，宗教要求等，时间表的安排变得越来越困难。你无法手动管理此任务，很难找到任何解决方案，更不用说公平的解决方案了。 “公平”并不意味着跨团队成员的每个变化都是一致的。不同的人有不同的需求和不同的偏好。因此，团队应该分享这些偏好并尝试以智能的方式满足这些偏好。团队组成和首选项决定了你的团队是更喜欢统一分发，还是以自定义的方式来满足日程安排首选项。 使用自动化工具来安排on-call班次会更容易。这个自动化工具应该有这些基本特征： 它应该重新安排on-call班次以适应团队成员不断变化的需求。 它应该自动重新平衡报警负载以响应任何更改。 应该尽量通过考虑个人偏好来确保公平，例如“4月份周末不用上学”，以及历史信息，例如最近每位on-call的值班负载。 因此，on-call工程师可以依据on-call班次进行计划，但绝不能改变已经生成的时间表。 时间表既可以是完全自动化的，也可以是团队人员安排。同时，一些团队更愿意让成员明确遵守时间表，而其他团队则对完全自动化的流程感到满意。如果你的需求很复杂，可以选择在内部开发自己的工具，此外也有许多商业和开源软件包可以帮助自动化生产on-call时间表。 短期互换的计划。on-call时间表通常会收到的短期变化请求。没人能在周一就承诺周四肯定不会感冒。或者你可能需要在on-call期间处理无法预料的紧急事务。 你可能还希望因为非常规原因能够在on-call中换岗——例如，允许on-call人员参加运动训练课程。在这种情况下，团队成员可以交换一天的on-call日（例如，周日的一半）。非竞争性的互换通常是更好的选择。 具有严格报警响应SLO的团队需要考虑通勤时间。如果你的报警响应SLO为5分组，而你的通勤时间为30，那么要确保其他人在你上班途中能处理紧急情况。 为了在灵活性方面实现这些目标，我们建议给予团队成员权利更新on-call轮值表。此外，有一个记录下来的策略描述转换如何操作。权利下放的策略包括只有经理可以改变的完全集中的政策，到任何成员都可以改变的完全分散的政策。根据我们的经验，对变更进行同行评审可以在安全性和灵活性之间进行良好的权衡。 长期休息的计划。由于个人情况或职业倦怠的变化，有时团队成员需要停止on-call工作。团队的结构应该能够允许on-call人员暂时不参与值班。 理想情况下，团队规模应该满足在（临时）员工减少时其他成员能够承受增加的运维负担。根据我们的经验，每个站点至少需要五个人进行多站点全天候的on-call，至少需要8个人进行单站点全天候的on-call。因此，假设每个站点需要一名额外的工程师来防止人员减少，每个站点（多站点）最多需要6名工程师，每个站点（单站点）为9名。 兼职工作的时间表计划。on-call工作的时间表看起来是不兼容的，但我们发现如果你采取某些预防措施，on-call工作和兼职工作是能够做到兼容的。以下讨论假设你的on-call成员是兼职工作，他们无法在兼职工作周之外完成值班工作。 兼职工作主要有两种模式： 每周工作减少一天——例如，每周工作4天，而非5天 每天减少工作时间——例如，每天工作6小时，而非8小时 两种模式都可以兼容on-call工作，但需要对on-call时间进行调整。 如果非工作日是一直不变的，那么第一个模式很容易和on-call工作兼容。对应的，你可以采用每周少于7天的on-call时间（例如，周一至周四，或周五至周日），并自动调整时间表以便在兼职工程师非工作时间不会参与on-call工作。 第二种模式可以通过以下几种方式实现： 与另一名工程师分担on-call时间，这样当兼职工程师不在时，仍然有人值班。例如，如果on-call工程师需要从上午9点工作到下午4点，你可以将值班的前半部分（上午9点到下午3点）分配给他们，后半部分（下午3点到晚上9点）可以以相同的方式分配给其他on-call成员。 如果on-call频率不是太高，兼职工程师可以在on-call日工作整整几个小时也是可行的。 如站点可靠性工程的第11章所述，根据当地劳动法和法规，Google SRE会在正常工作时间之外补偿小时工资或休假时间。在确定on-call补偿时，要考虑兼职工程师的时间表。 为了平衡项目时间和on-call时间，工作时间较少的工程师对应的工作内容应该少点。与小型团队相比，较大的团队更容易吸收额外的on-call负载。 on-call团队动态 我们的第一本书谈到了高报警负载和时间压力等压力因素是如何迫使on-call工程师采用基于直觉的未经详细考虑而非基于理性和数据的决策策略（参加该书第11章“安全感”一节）。基于团队心理学的讨论，你如何建立一个积极的动态团队？考虑一个on-call团队，其中包含以下一组假设问题。 情景：“一周生存”的文化 一家公司从几位创始人和少数员工开始，他们都是开发人员，每个人都相互了解，每个人都需要接收报警。 公司规模开始变大。on-call的职责仅限于一小部分更有经验的功能开发人员，因为他们更了解系统。 公司变得更大。他们增加了ops角色来解决可靠性问题。该团队负责生产环境监控，成员主要集中在运维，而非编码。功能开发人员和ops人员轮流进行on-call工作。功能开发人员在维护服务方面有最终决定权，而ops仅限于运维任务。到目前为止，有30名工程师参与on-call工作：25名功能开发人员和5名ops，都位于同一站点。 团队被高报警量所困扰，尽管遵循了本章前面所述的建议，尽量减少报警负载，但团队的士气仍然很低落。由于功能开发人员优先考虑开发新功能，因此on-call的后续工作需要很长时间才能实现。 更糟糕的是，由于功能开发人员关注的是自己子系统的健康状况，尽管团队中其他人提出了投诉，但有位功能开发人员坚持按错误率而非关键模块错误比率来进行报警。这些报警很嘈杂，会有很多误报或者不可执行的报警。 高报警负载对on-call岗的其他成员的影响不会特别大，确实有许多报警，但大多数报警都没有花太多时间来解决。正如一名on-call工程师所说：“我快速浏览一下报警主题，知道它们是重复的。所以我要做的就是忽略它们。” 听起来很熟悉？ Google的一些团队在成熟的早期阶段遇到过类似问题。如果不小心处理，这些问题可能扰乱功能开发团队和运维团队，并阻碍on-call的操作。没有灵丹妙药能解决这些问题，但我们发现了一些特别有用的方法。虽然你的方法可能有所不同，但总体目标应该是相同的：建立积极的团队氛围，避免混乱。 建议一：给你的ops工程师授权。你可以根据本书和站点可靠性工程中列出的指南对运维组织进行重新构建，甚至可以更改名称（SRE或类似名称）来表示角色的更改。重新命名你的运维组织并非灵丹妙药，但它有助于体现别于旧的以操作为中心的模型的新的责任变化。向团队和整个公司明确说明SRE拥有站点操作权限，包括定义可靠性的共享路线图，推动问题的全面解决，维护监控策略等。功能开发人员是必要的协作者，但没有这些权限。 回到我们之前假设的团队，本公告引入了以下运维变化： 操作项仅分配给5个DevOps工程师——即SRE。SRE与项目专家合作——大多为开发人员——来完成这些任务。SRE就前面提到的：“错误率与错误比例”的报警策略与功能开发人员进行协商。 如果可能，鼓励SRE深入研究代码以自行进行更改。他们将代码审查发送给项目专家。这样有利于在SRE之间建立主人翁意识，并在未来的场合提升他们的技能和权威。 通过这种安排，功能开发人员是可靠性功能的明确协作者，且SRE有权利拥有站点以及改进站点的责任。 建议二：改善团队关系。另一种可能的解决方案是建立更强有力的团队关系。Google设置了一个“有趣的预算”，专门用于组织异地活动来加强团队合作。 我们发现，强大的团队关系可以增强团队成员之间的理解和协作精神。因此，工程师修复bug，完成操作项目并且帮助同事的几率更高。例如，假设你关闭了夜间管道工作，但忘记关闭检查管道是否成功运行的监控。结果，同事在凌晨3点收到了报警。如果你和那位同事为处理报警花了点时间，你对这件事感到很抱歉，并在将来对此类操作更加小心。“我要保护我的同事”这一心态会转化成为更富有成效的工作氛围。 我们还发现，无论职称和职能如何，让on-call的所有成员坐在一起，有助于改善团队关系。还可以鼓励团队一起吃午饭，不要低估这些相对简单的变化，它会直接影响团队动力。 结论 SRE on-call与传统的ops角色不同。SRE不仅专注于日常运维，且拥有生产环境权限，并通过定义适当的可靠性阈值，开发自动化工具以及开展战略工程项目来获得更好的生产环境。on-call的站点操作十分重要，公司必须正确处理。 on-call是个人和集体压力的根源。但如果你盯着怪物的眼睛看久了，就会发现智慧。本章阐述了一些关于on-call的案例；希望我们的经验可以帮助他人避免或解决类似的问题。 如果你的on-call团队淹没在无休止的报警中，我们建议你退一步观察更顶层的情况，和其他SRE和合作伙伴团队对比讨论，一旦收集了必要的信息，就要系统的解决问题。对on-call工程师，on-call团队以及整个公司来说，构建合理的on-call机制是值得投入时间的。</summary></entry><entry><title type="html">第七章 简单化</title><link href="http://localhost:4000/sre/2020/01/07/%E7%AE%80%E5%8D%95%E5%8C%96/" rel="alternate" type="text/html" title="第七章 简单化" /><published>2020-01-07T00:00:00+08:00</published><updated>2020-01-07T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/07/%E7%AE%80%E5%8D%95%E5%8C%96</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/07/%E7%AE%80%E5%8D%95%E5%8C%96/">&lt;!-- more --&gt;

&lt;p&gt;简单化是SRE的重要目标，因为它与可靠性密切相关：简单的软件很少出现故障，在故障发生时更容易且迅速地修复。简单的系统更易于理解、维护以及测试。&lt;/p&gt;

&lt;p&gt;对于SRE而言，简单化是一个端到端的目标：它应该超越代码本身，延伸到系统架构以及用于管理软件生命周期的工具和流程中。本章探讨了一些样例，这些样例展示了SRE是如何衡量、思考和鼓励简单化的。&lt;/p&gt;

&lt;h2 id=&quot;衡量复杂度&quot;&gt;衡量复杂度&lt;/h2&gt;

&lt;p&gt;衡量软件系统的复杂度并不是一门绝对的科学。有许多方法可以衡量软件代码的复杂性，大多数是非常客观的。&lt;/p&gt;

&lt;p&gt;最著名且使用最广泛的衡量标准应该是代码圈复杂度，它通过一组特定的语句来衡量不同代码路径的数量。例如，没有循环或条件语句的代码块的圈复杂度（CCN）为1。其实软件社区很擅长测量代码复杂度，并且有许多用于集成开发环境的测量工具（包括Visual Studio，Eclipse和IntelliJ）。我们无法判断所得到的测量复杂度是必然还是偶然的，一种方法的复杂度是如何影响到系统的，以及哪种方法更适合重构。&lt;/p&gt;

&lt;p&gt;另一方面，衡量系统复杂性的正式的方法很少见。&lt;/p&gt;

&lt;p&gt;你可能尝试使用类似CCN的方法来计算不同实体（例如，微服务）的数量以及它们之间可能存在的通信路径。但是，对于大多数较大规模的系统而言，这个数字的增幅十分迅速。&lt;/p&gt;

&lt;p&gt;针对系统级复杂度，有一些更实用的替代度量方法：&lt;/p&gt;

&lt;h3 id=&quot;训练时长&quot;&gt;训练时长&lt;/h3&gt;

&lt;p&gt;新成员多久能参与on-call工作？糟糕的或缺失的文档可能是主观复杂性的重要来源。&lt;/p&gt;

&lt;h3 id=&quot;解释时长&quot;&gt;解释时长&lt;/h3&gt;

&lt;p&gt;向团队新成员解释服务的全面高级视图需要多久（例如，在白板上绘制系统架构图并解释各个组件的功能和依赖关系）？&lt;/p&gt;

&lt;h3 id=&quot;管理多样性&quot;&gt;管理多样性&lt;/h3&gt;

&lt;p&gt;有多少种方法可以在系统的不同部分配置类似的设置？配置是集中存储在一个位置还是存储在多个位置？&lt;/p&gt;

&lt;h3 id=&quot;部署配置的多样性&quot;&gt;部署配置的多样性&lt;/h3&gt;

&lt;p&gt;生产过程中部署了多少唯一的配置（包括二进制文件、二进制版本、标志和环境）？&lt;/p&gt;

&lt;h3 id=&quot;年龄&quot;&gt;年龄&lt;/h3&gt;

&lt;p&gt;系统使用多久了？Hyrum定律指出，随着时间的推移，API的用户依赖于它实现的每个方面，导致了脆弱和不可预测的行为。&lt;/p&gt;

&lt;p&gt;虽然测量复杂度有时是有价值的，但过程很困难。然而以下这些结论是没有争议的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一般而言，除非付出努力补偿，否则现存的软件系统的复杂度将会随时间增加。&lt;/li&gt;
  &lt;li&gt;付出这样的努力是值得的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;简单化是端到端的并且sre会获益于此&quot;&gt;简单化是端到端的，并且SRE会获益于此&lt;/h2&gt;

&lt;p&gt;通常，生产系统不是通过整体的方式设计的；相反，它们是有机地生长。随着团队添加新特性和推出新产品，它们会逐渐积累组件和连接。虽然单个变更可能相对简单，但每个变更都会影响周围的组件。因此，整体复杂度很快就会超出控制。例如，在一个组件中添加重试可能会使数据库过载并使整个系统不稳定，或者使对给定查询在系统中遵循的路径进行推理变得更加困难。&lt;/p&gt;

&lt;p&gt;一般而言，复杂度带来的成本并不直接影响引入它的个人、团队或从经济角度来看的任何角色，复杂度是一种外部特性。相反，复杂性会影响那些继续在其中和周围工作的人。因此，有个拥护端到端系统简单化的支持者十分重要。&lt;/p&gt;

&lt;p&gt;SRE非常适合这个角色，因为他们的工作需要他们将系统作为一个整体来对待。除了维护自己的服务，SRE还必须深入了解与服务有交互的系统。Google的产品开发团队通常无法查看生产范围内的问题，因此他们可以通过咨询SRE来获取系统设计和运营的相关建议。&lt;/p&gt;

&lt;p&gt;（一般说明）读者操作：在工程师第一次加入on-call工作之前，鼓励他们绘制（或重绘）系统架构图。可以在你的文档中保留一组规范的图表：不仅对新加入的工程师非常有帮助，还可以帮助更多有经验的工程师随时跟上系统的变更。&lt;/p&gt;

&lt;p&gt;根据我们的经验，通常产品开发人员的工作局限在子系统或组件中。因此，他们没有形成针对整个系统的思维模式，所在的团队也没有制作系统级别的架构图。系统架构图的价值在于可以将系统交互可视化地呈现给成员，并且帮助成员使用常用词汇来阐明问题。通常，SRE团队都绘制了所有服务的系统级架构图。&lt;/p&gt;

&lt;p&gt;（一般说明）读者操作：SRE要检查所有重要的设计文档，且团队文档中需要说明新设计会如何影响系统结构。如果一个设计会增加系统复杂度，SRE可能会建议选择降低系统复杂度的替代方案。&lt;/p&gt;

&lt;h3 id=&quot;案例学习1端到端api简单化&quot;&gt;案例学习1：端到端API简单化&lt;/h3&gt;

&lt;h4 id=&quot;背景&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;之前章节的一位作者在一家使用键/值包数据结构的核心库的初创公司工作。RPCs（远程过程调用）取一个包并返回一个包；实际参数作为键/值对存储在包中。核心库支持包的常见操作，比如序列化、加密和日志记录。看起来所有的核心库和API都非常简单灵活，对吧？&lt;/p&gt;

&lt;p&gt;遗憾的是，答案是否定的：核心库的客户最终为核心API的抽象化付出了代价。每个服务都需要仔细记录键和值（和值类型）的集合，但通常做法并非如此。此外，随着时间的推移、参数的添加、删除或更改，维护向后/向前的兼容性变得很困难。&lt;/p&gt;

&lt;h4 id=&quot;经验教训&quot;&gt;经验教训&lt;/h4&gt;

&lt;p&gt;类似Google Protocol Buffers或Apache Thrift这样的结构化数据类型看起来可能比它们抽象的通用替代方案更复杂。但是由于它们强制预先设计方案和准备文档，获得了更简单的端到端解决方案。&lt;/p&gt;

&lt;h3 id=&quot;案例学习2项目生命周期复杂度&quot;&gt;案例学习2：项目生命周期复杂度&lt;/h3&gt;

&lt;p&gt;当您查看现有系统，发现它像一团乱麻，您可能希望用一个新的、干净的、简单的系统取而代之，且这个简单的系统能解决相同的问题。不幸的是，在保持现有系统的同时创建新系统的成本可能超乎您的预期。&lt;/p&gt;

&lt;h4 id=&quot;背景-1&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;Borg是Google的内部容器管理系统。运行了大量Linux容器且具有多种使用模式：批处理与生产，管道与服务器等。多年来，随着硬件的变化，功能的增加以及规模的不断扩大，Borg及其周边生态系统在不断的发展壮大。&lt;/p&gt;

&lt;p&gt;Omega旨在成为一个更合理，更清爽的Borg版本，且能支持相同的功能。然而，从Borg到Omega的转变过程产生了一些严重的问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Omega发展的同时，Borg的发展也没有停滞，因此Omega一直在追逐一个变化的目标。&lt;/li&gt;
  &lt;li&gt;事实证明，前期对改善Borg难度的估计太过悲观，而对Omega的期望太过乐观（实际上，外国的月亮未必更圆）。&lt;/li&gt;
  &lt;li&gt;我们对从Borg迁移到Omega的困难没有了然于胸。数百万行配置代码跨越数千个服务和多个SRE团队，这意味着迁移工作在工程和时间维度上成本都是极高的。可能需要数年时间完成迁移，在这期间，我们必须同时支持和维护这两个系统。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;我们决定做什么&quot;&gt;我们决定做什么&lt;/h4&gt;

&lt;p&gt;最后，我们提供了一些在设计Omega回归Borg时出现的想法。我们还使用了很多Omega的概念来启动Kubernetes，一个开源的容器管理系统。&lt;/p&gt;

&lt;h4 id=&quot;经验教训-1&quot;&gt;经验教训&lt;/h4&gt;

&lt;p&gt;在考虑重写时，要考虑整个项目生命周期，包括对移动的目标的开发，完整的迁移计划以及在迁移时间窗口内可能产生的额外成本。具有大量用户的APIs很难迁移。在您投入了相应的努力之前，不要想当然的将预期结果与当前系统进行比较。在确保已经衡量了成本和收益以及没有低估成本的前提下，有时重写是最好的前进方式。&lt;/p&gt;

&lt;h2 id=&quot;重获简单化&quot;&gt;重获简单化&lt;/h2&gt;

&lt;p&gt;大多数的简化工作是从系统中删除元素。简化工作有时很直接（例如，消除对从远程系统获取的未使用数据的依赖）。简化工作有时需要重新设计。例如，系统的两个部分需要访问相同的远程数据。一个更简单的系统可能只需要获取一次数据并转发结果而非获取两次。&lt;/p&gt;

&lt;p&gt;无论什么工作，领导层必须确保优先考虑简化工作。这里的简化指的是效率-而不是节省计算或网络资源，它节省了工程时间和认知负荷。项目成功的简化就如同成功启用了一个有价值的功能，就如同成功的度量并对代码进行了增删。例如，Google的内部网络会为删除大量代码的工程师显示“Zombie Code Slayer”徽章。&lt;/p&gt;

&lt;p&gt;简化是一项功能。您需要明确优先级并给出待简化的项目，同时为SRE预留时间。如果产品开发和SRE人员发现待简化项目对他们的工作没有益处，他们就不会承担这些项目。对于特别复杂的系统或过载的团队而言，可以将简单化作为明确的目标，安排一个独立的时间来完成这项工作。例如，为“简单化”项目保留10%的工程项目时间。&lt;/p&gt;

&lt;p&gt;（一般说明）读者行动：让工程师集体讨论系统中已知的复杂度，并讨论如何简化。&lt;/p&gt;

&lt;p&gt;随着系统复杂度的增加，SRE团队存在分裂的趋势，每个新的团队分别集中运维系统的某一部分。这样的操作有时是必要的，但新团队规模的缩小可能会降低他们推动较大简化项目的动力或能力。可以考虑指定一个小的轮转的SRE团队来维护整个堆栈的工作信息（可能比较浅显），推动整个堆栈的整合和简化。&lt;/p&gt;

&lt;p&gt;如前所述，绘制系统图表的行为可以帮助您理解系统并预测其行为。例如，在绘制系统图表的过程中，你可能需要查找以下内容：&lt;/p&gt;

&lt;h4 id=&quot;放大&quot;&gt;放大&lt;/h4&gt;

&lt;p&gt;当一个调用操作返回一个错误或超时，且在几个级别上进行重试时，会导致RPC的总数相乘。&lt;/p&gt;

&lt;h4 id=&quot;循环依赖&quot;&gt;循环依赖&lt;/h4&gt;

&lt;p&gt;当组件依赖于自身（通常是间接的）时，系统完整性可能会严重受损-整个系统可能无法进行冷启动。&lt;/p&gt;

&lt;h3 id=&quot;案例学习3简化广告网络的展示&quot;&gt;案例学习3：简化广告网络的展示&lt;/h3&gt;

&lt;h4 id=&quot;背景-2&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;Google的广告展示业务有许多关联产品，其中包括一些收购于DoubleClick，AdMob，Invite Media等公司的产品。这些产品必须适用于Google基础架构和现有产品。例如，我们希望使用DFP广告管理系统的网站展示Google AdSense筛选的广告，也希望使用Double Click Bid Manager进行投标时可以通过访问Google Ad Exchange进行实时竞价。&lt;/p&gt;

&lt;p&gt;独立开发的产品形成了难以推理的互连后端系统，很难观察流量在各组件的流通情况，因此不便且无法精确的为每个产品配置合适的容量。为了确保删除了查询流量中的所有无限循环，我们在其中添加了测试。&lt;/p&gt;

&lt;h4 id=&quot;我们决定做什么-1&quot;&gt;我们决定做什么&lt;/h4&gt;

&lt;p&gt;Ads的运维团队自然而然会推动标准化：虽然产品的每个组件都有特定的开发团队，但SRE是服务于整个系统的。我们的首要任务是制订统一的标准，与开发团队合作逐步采用这个标准。这些标准是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;建立一种复制大规模数据集的方法&lt;/li&gt;
  &lt;li&gt;建立一种执行外部数据查找的方法&lt;/li&gt;
  &lt;li&gt;提供用于监控、配置、组态的通用模板&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在此之前，需要为每个产品单独提供前端和拍卖功能。如图7-1所示，当广告请求可能到达两个系统时，需要重写请求以符合第二个系统的要求。过程中，增加了额外的代码和处理，还加大了非预期循环的可能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/7-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图7-1：之前，广告请求可能会同时触及AdMob和AdSense系统 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;为了简化系统，我们为满足所有用例的常用程序增加了逻辑，并且添加了用于保护程序的标志。随着时间的推移，我们删除了标志，将功能整合到较少的的服务器后端中。&lt;/p&gt;

&lt;p&gt;当服务器统一时，拍卖服务器可直接与两个目标服务器通信。如图7-2所示，当多个目标服务器需要查找数据时，查询只需统一在拍卖服务器中进行一次。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/7-2.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图7-2：统一后的拍卖服务器只需执行一次数据查询 &lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&quot;经验教训-2&quot;&gt;经验教训&lt;/h4&gt;

&lt;p&gt;最好将已经在运行的系统逐步集成到你的基础架构中。&lt;/p&gt;

&lt;p&gt;正如在单个程序中存在相似的函数表示“代码气味”来反应更深层次的设计问题一样，单个请求的冗余查询表示“系统气味”。&lt;/p&gt;

&lt;p&gt;当你通过SRE和开发人员的支持建立了有明确定义的标准时，你可以提供更清晰的蓝图以便管理者对高复杂度的系统更认可和鼓励。&lt;/p&gt;

&lt;h3 id=&quot;案例学习4在共享平台上运行数百个微服务&quot;&gt;案例学习4：在共享平台上运行数百个微服务&lt;/h3&gt;

&lt;h4 id=&quot;背景-3&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;在过去15年，Google成功开发了多个垂直类产品（搜索、广告和Gmail，仅举几例），并源源不断的产生了新的重构的系统。其中很多系统都有专门的SRE团队和与之对应的特定领域生产堆栈，包括定制化开发工作流程，持续集成和持续交付（CI/CD）软件周期以及监控。生产堆栈的定制化带来了巨大的维护、开发以及新的SRE成员工作的成本。此外也为团队之间轮转服务（或工程师！）以及新增服务带来困难。&lt;/p&gt;

&lt;h4 id=&quot;我们决定做什么-2&quot;&gt;我们决定做什么&lt;/h4&gt;

&lt;p&gt;负责社交网络领域的一组SRE团队致力于将其服务的生产堆栈融合到一个托管的微服务平台中，由一个SRE团队管理。共享平台是到目前为止的最佳实践，平台会绑定并自动配置一些之前并未充分利用的功能，这些功能可以提高可靠性且便于调试。无论该SRE团队有多熟悉所负责的服务，新增的服务都必须使用通用平台，而旧式服务必须迁移到新平台或逐步被淘汰。&lt;/p&gt;

&lt;p&gt;共享平台在社交网络领域取得成功后，谷歌的其他SRE团队和非SRE团队也开始使用它。&lt;/p&gt;

&lt;h4 id=&quot;设计&quot;&gt;设计&lt;/h4&gt;

&lt;p&gt;要知道，单个整体服务的变化是缓慢的，使用微服务可以迅速更新和部署功能。微服务实现自我管理，而非托管：团队可以有效的管理他们所负责的服务，无需委托个别团队管理和负责。微服务为每个团队提供工作流程工具用于发布、监控等功能。&lt;/p&gt;

&lt;p&gt;微服务提供的工具包括UI，API和SRE以及开发人员常用的命令行交互界面。即使这些工具可能涉及许多底层系统，开发人员的体验感却是统一的。&lt;/p&gt;

&lt;h4 id=&quot;成果&quot;&gt;成果&lt;/h4&gt;

&lt;p&gt;微服务平台的高质量和功能集成带来了意想不到的好处：开发人员团队可以运行数百项服务，而无需任何SRE的深入参与。&lt;/p&gt;

&lt;p&gt;通用平台还改变了SRE和开发人员的关系。Google的SRE团队开始分层的参与到工作中，从咨询和设计审查到深度参与（即SRE承担on-call职责）。&lt;/p&gt;

&lt;h4 id=&quot;经验教训-3&quot;&gt;经验教训&lt;/h4&gt;

&lt;p&gt;从稀疏的或不明确的标准转变为高度标准化的平台是一个长期项目。每个步骤可能都让人觉得是增量式的，但最终，这些步骤可以减少开销并使大规模运行服务成为可能。&lt;/p&gt;

&lt;p&gt;这种转变可以让开发人员看到价值所在。即不要尝试说服人们执行一个只在全部完成后才得到回报的巨大重构工程，而是在每个开发阶段解锁增量生产力。&lt;/p&gt;

&lt;h3 id=&quot;案例研究5pdns不再取决于自身&quot;&gt;案例研究5：pDNS不再取决于自身&lt;/h3&gt;

&lt;h4 id=&quot;背景-4&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;当Google生产的客户要查询服务的IP地址时，通常使用名为Svelte的查找服务。过去，为了找到Svelte的IP地址，客户端使用了名为pDNS（生产DNS）的Google命名服务。通过负载均衡访问pDNS服务，负载均衡使用Svelte查找实际pDNS服务器的IP地址。&lt;/p&gt;

&lt;h4 id=&quot;问题描述&quot;&gt;问题描述&lt;/h4&gt;

&lt;p&gt;pDNS对自身具有传递依赖性，某种程度上说这是无意中引入的，后来被明确为是可靠性问题。由于pDNS服务可复制，且在生产中的始终可以获得打破依赖关系循环所需的数据，因此查找通常不会遇到问题。然而，冷启动是无法做到的。借用一位SRE的话来说，“我们就像穴居人，只能依赖现有篝火来点火。”&lt;/p&gt;

&lt;h4 id=&quot;我们决定做什么-3&quot;&gt;我们决定做什么&lt;/h4&gt;

&lt;p&gt;我们修改了Google生产中的低级组件以便为所有Google生产机器的本地存储附近的Svelte服务器维护当前IP地址列表。除了打破前文所述的循环依赖之外，此举还消除了对大多数其他Google服务的pDNS的隐式依赖。&lt;/p&gt;

&lt;p&gt;为了避免此类问题，我们还引入了一种方法，将允许与pDNS通信的服务集列入白名单，并慢慢减少该集合。因此，生产中每个服务的查找都通过系统且具有更简单更可靠的路径。&lt;/p&gt;

&lt;h4 id=&quot;经验教训-4&quot;&gt;经验教训&lt;/h4&gt;

&lt;p&gt;注意服务的依赖关系 - 使用明确的白名单以防止意外添加。另外，需要注意循环依赖。&lt;/p&gt;

&lt;h3 id=&quot;结论&quot;&gt;结论&lt;/h3&gt;

&lt;p&gt;通常简单的系统往往是可靠的且易于运行的，因此简单化自然而然就是SRE的目标。很难定量衡量分布式系统的简单性（或取逆，即复杂度），但可以挑选和改进合理的替代测量方案。&lt;/p&gt;

&lt;p&gt;SRE对系统有着端到端的理解，在识别，预防和修复复杂度来源方具有优势，在软件设计，系统架构，配置，部署过程或是其他地方，SRE都应该参与设计讨论，提供对成本和效益的独特见解，尤其是简单化。SRE还可以主动制订标准来使生产统一化。&lt;/p&gt;

&lt;p&gt;作为SRE，追求简单化应该是工作的重点内容。我们强烈建议SRE领导层授予SRE团队权利和奖励来推动简单化。系统在不断发展的过程中会不可避免的越来越复杂，因此追求简单化的斗争道路需要持久的关注和付出-但这份追求是值得的。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">简单化是SRE的重要目标，因为它与可靠性密切相关：简单的软件很少出现故障，在故障发生时更容易且迅速地修复。简单的系统更易于理解、维护以及测试。 对于SRE而言，简单化是一个端到端的目标：它应该超越代码本身，延伸到系统架构以及用于管理软件生命周期的工具和流程中。本章探讨了一些样例，这些样例展示了SRE是如何衡量、思考和鼓励简单化的。 衡量复杂度 衡量软件系统的复杂度并不是一门绝对的科学。有许多方法可以衡量软件代码的复杂性，大多数是非常客观的。 最著名且使用最广泛的衡量标准应该是代码圈复杂度，它通过一组特定的语句来衡量不同代码路径的数量。例如，没有循环或条件语句的代码块的圈复杂度（CCN）为1。其实软件社区很擅长测量代码复杂度，并且有许多用于集成开发环境的测量工具（包括Visual Studio，Eclipse和IntelliJ）。我们无法判断所得到的测量复杂度是必然还是偶然的，一种方法的复杂度是如何影响到系统的，以及哪种方法更适合重构。 另一方面，衡量系统复杂性的正式的方法很少见。 你可能尝试使用类似CCN的方法来计算不同实体（例如，微服务）的数量以及它们之间可能存在的通信路径。但是，对于大多数较大规模的系统而言，这个数字的增幅十分迅速。 针对系统级复杂度，有一些更实用的替代度量方法： 训练时长 新成员多久能参与on-call工作？糟糕的或缺失的文档可能是主观复杂性的重要来源。 解释时长 向团队新成员解释服务的全面高级视图需要多久（例如，在白板上绘制系统架构图并解释各个组件的功能和依赖关系）？ 管理多样性 有多少种方法可以在系统的不同部分配置类似的设置？配置是集中存储在一个位置还是存储在多个位置？ 部署配置的多样性 生产过程中部署了多少唯一的配置（包括二进制文件、二进制版本、标志和环境）？ 年龄 系统使用多久了？Hyrum定律指出，随着时间的推移，API的用户依赖于它实现的每个方面，导致了脆弱和不可预测的行为。 虽然测量复杂度有时是有价值的，但过程很困难。然而以下这些结论是没有争议的： 一般而言，除非付出努力补偿，否则现存的软件系统的复杂度将会随时间增加。 付出这样的努力是值得的。 简单化是端到端的，并且SRE会获益于此 通常，生产系统不是通过整体的方式设计的；相反，它们是有机地生长。随着团队添加新特性和推出新产品，它们会逐渐积累组件和连接。虽然单个变更可能相对简单，但每个变更都会影响周围的组件。因此，整体复杂度很快就会超出控制。例如，在一个组件中添加重试可能会使数据库过载并使整个系统不稳定，或者使对给定查询在系统中遵循的路径进行推理变得更加困难。 一般而言，复杂度带来的成本并不直接影响引入它的个人、团队或从经济角度来看的任何角色，复杂度是一种外部特性。相反，复杂性会影响那些继续在其中和周围工作的人。因此，有个拥护端到端系统简单化的支持者十分重要。 SRE非常适合这个角色，因为他们的工作需要他们将系统作为一个整体来对待。除了维护自己的服务，SRE还必须深入了解与服务有交互的系统。Google的产品开发团队通常无法查看生产范围内的问题，因此他们可以通过咨询SRE来获取系统设计和运营的相关建议。 （一般说明）读者操作：在工程师第一次加入on-call工作之前，鼓励他们绘制（或重绘）系统架构图。可以在你的文档中保留一组规范的图表：不仅对新加入的工程师非常有帮助，还可以帮助更多有经验的工程师随时跟上系统的变更。 根据我们的经验，通常产品开发人员的工作局限在子系统或组件中。因此，他们没有形成针对整个系统的思维模式，所在的团队也没有制作系统级别的架构图。系统架构图的价值在于可以将系统交互可视化地呈现给成员，并且帮助成员使用常用词汇来阐明问题。通常，SRE团队都绘制了所有服务的系统级架构图。 （一般说明）读者操作：SRE要检查所有重要的设计文档，且团队文档中需要说明新设计会如何影响系统结构。如果一个设计会增加系统复杂度，SRE可能会建议选择降低系统复杂度的替代方案。 案例学习1：端到端API简单化 背景 之前章节的一位作者在一家使用键/值包数据结构的核心库的初创公司工作。RPCs（远程过程调用）取一个包并返回一个包；实际参数作为键/值对存储在包中。核心库支持包的常见操作，比如序列化、加密和日志记录。看起来所有的核心库和API都非常简单灵活，对吧？ 遗憾的是，答案是否定的：核心库的客户最终为核心API的抽象化付出了代价。每个服务都需要仔细记录键和值（和值类型）的集合，但通常做法并非如此。此外，随着时间的推移、参数的添加、删除或更改，维护向后/向前的兼容性变得很困难。 经验教训 类似Google Protocol Buffers或Apache Thrift这样的结构化数据类型看起来可能比它们抽象的通用替代方案更复杂。但是由于它们强制预先设计方案和准备文档，获得了更简单的端到端解决方案。 案例学习2：项目生命周期复杂度 当您查看现有系统，发现它像一团乱麻，您可能希望用一个新的、干净的、简单的系统取而代之，且这个简单的系统能解决相同的问题。不幸的是，在保持现有系统的同时创建新系统的成本可能超乎您的预期。 背景 Borg是Google的内部容器管理系统。运行了大量Linux容器且具有多种使用模式：批处理与生产，管道与服务器等。多年来，随着硬件的变化，功能的增加以及规模的不断扩大，Borg及其周边生态系统在不断的发展壮大。 Omega旨在成为一个更合理，更清爽的Borg版本，且能支持相同的功能。然而，从Borg到Omega的转变过程产生了一些严重的问题： Omega发展的同时，Borg的发展也没有停滞，因此Omega一直在追逐一个变化的目标。 事实证明，前期对改善Borg难度的估计太过悲观，而对Omega的期望太过乐观（实际上，外国的月亮未必更圆）。 我们对从Borg迁移到Omega的困难没有了然于胸。数百万行配置代码跨越数千个服务和多个SRE团队，这意味着迁移工作在工程和时间维度上成本都是极高的。可能需要数年时间完成迁移，在这期间，我们必须同时支持和维护这两个系统。 我们决定做什么 最后，我们提供了一些在设计Omega回归Borg时出现的想法。我们还使用了很多Omega的概念来启动Kubernetes，一个开源的容器管理系统。 经验教训 在考虑重写时，要考虑整个项目生命周期，包括对移动的目标的开发，完整的迁移计划以及在迁移时间窗口内可能产生的额外成本。具有大量用户的APIs很难迁移。在您投入了相应的努力之前，不要想当然的将预期结果与当前系统进行比较。在确保已经衡量了成本和收益以及没有低估成本的前提下，有时重写是最好的前进方式。 重获简单化 大多数的简化工作是从系统中删除元素。简化工作有时很直接（例如，消除对从远程系统获取的未使用数据的依赖）。简化工作有时需要重新设计。例如，系统的两个部分需要访问相同的远程数据。一个更简单的系统可能只需要获取一次数据并转发结果而非获取两次。 无论什么工作，领导层必须确保优先考虑简化工作。这里的简化指的是效率-而不是节省计算或网络资源，它节省了工程时间和认知负荷。项目成功的简化就如同成功启用了一个有价值的功能，就如同成功的度量并对代码进行了增删。例如，Google的内部网络会为删除大量代码的工程师显示“Zombie Code Slayer”徽章。 简化是一项功能。您需要明确优先级并给出待简化的项目，同时为SRE预留时间。如果产品开发和SRE人员发现待简化项目对他们的工作没有益处，他们就不会承担这些项目。对于特别复杂的系统或过载的团队而言，可以将简单化作为明确的目标，安排一个独立的时间来完成这项工作。例如，为“简单化”项目保留10%的工程项目时间。 （一般说明）读者行动：让工程师集体讨论系统中已知的复杂度，并讨论如何简化。 随着系统复杂度的增加，SRE团队存在分裂的趋势，每个新的团队分别集中运维系统的某一部分。这样的操作有时是必要的，但新团队规模的缩小可能会降低他们推动较大简化项目的动力或能力。可以考虑指定一个小的轮转的SRE团队来维护整个堆栈的工作信息（可能比较浅显），推动整个堆栈的整合和简化。 如前所述，绘制系统图表的行为可以帮助您理解系统并预测其行为。例如，在绘制系统图表的过程中，你可能需要查找以下内容： 放大 当一个调用操作返回一个错误或超时，且在几个级别上进行重试时，会导致RPC的总数相乘。 循环依赖 当组件依赖于自身（通常是间接的）时，系统完整性可能会严重受损-整个系统可能无法进行冷启动。 案例学习3：简化广告网络的展示 背景 Google的广告展示业务有许多关联产品，其中包括一些收购于DoubleClick，AdMob，Invite Media等公司的产品。这些产品必须适用于Google基础架构和现有产品。例如，我们希望使用DFP广告管理系统的网站展示Google AdSense筛选的广告，也希望使用Double Click Bid Manager进行投标时可以通过访问Google Ad Exchange进行实时竞价。 独立开发的产品形成了难以推理的互连后端系统，很难观察流量在各组件的流通情况，因此不便且无法精确的为每个产品配置合适的容量。为了确保删除了查询流量中的所有无限循环，我们在其中添加了测试。 我们决定做什么 Ads的运维团队自然而然会推动标准化：虽然产品的每个组件都有特定的开发团队，但SRE是服务于整个系统的。我们的首要任务是制订统一的标准，与开发团队合作逐步采用这个标准。这些标准是： 建立一种复制大规模数据集的方法 建立一种执行外部数据查找的方法 提供用于监控、配置、组态的通用模板 在此之前，需要为每个产品单独提供前端和拍卖功能。如图7-1所示，当广告请求可能到达两个系统时，需要重写请求以符合第二个系统的要求。过程中，增加了额外的代码和处理，还加大了非预期循环的可能。 图7-1：之前，广告请求可能会同时触及AdMob和AdSense系统 为了简化系统，我们为满足所有用例的常用程序增加了逻辑，并且添加了用于保护程序的标志。随着时间的推移，我们删除了标志，将功能整合到较少的的服务器后端中。 当服务器统一时，拍卖服务器可直接与两个目标服务器通信。如图7-2所示，当多个目标服务器需要查找数据时，查询只需统一在拍卖服务器中进行一次。 图7-2：统一后的拍卖服务器只需执行一次数据查询 经验教训 最好将已经在运行的系统逐步集成到你的基础架构中。 正如在单个程序中存在相似的函数表示“代码气味”来反应更深层次的设计问题一样，单个请求的冗余查询表示“系统气味”。 当你通过SRE和开发人员的支持建立了有明确定义的标准时，你可以提供更清晰的蓝图以便管理者对高复杂度的系统更认可和鼓励。 案例学习4：在共享平台上运行数百个微服务 背景 在过去15年，Google成功开发了多个垂直类产品（搜索、广告和Gmail，仅举几例），并源源不断的产生了新的重构的系统。其中很多系统都有专门的SRE团队和与之对应的特定领域生产堆栈，包括定制化开发工作流程，持续集成和持续交付（CI/CD）软件周期以及监控。生产堆栈的定制化带来了巨大的维护、开发以及新的SRE成员工作的成本。此外也为团队之间轮转服务（或工程师！）以及新增服务带来困难。 我们决定做什么 负责社交网络领域的一组SRE团队致力于将其服务的生产堆栈融合到一个托管的微服务平台中，由一个SRE团队管理。共享平台是到目前为止的最佳实践，平台会绑定并自动配置一些之前并未充分利用的功能，这些功能可以提高可靠性且便于调试。无论该SRE团队有多熟悉所负责的服务，新增的服务都必须使用通用平台，而旧式服务必须迁移到新平台或逐步被淘汰。 共享平台在社交网络领域取得成功后，谷歌的其他SRE团队和非SRE团队也开始使用它。 设计 要知道，单个整体服务的变化是缓慢的，使用微服务可以迅速更新和部署功能。微服务实现自我管理，而非托管：团队可以有效的管理他们所负责的服务，无需委托个别团队管理和负责。微服务为每个团队提供工作流程工具用于发布、监控等功能。 微服务提供的工具包括UI，API和SRE以及开发人员常用的命令行交互界面。即使这些工具可能涉及许多底层系统，开发人员的体验感却是统一的。 成果 微服务平台的高质量和功能集成带来了意想不到的好处：开发人员团队可以运行数百项服务，而无需任何SRE的深入参与。 通用平台还改变了SRE和开发人员的关系。Google的SRE团队开始分层的参与到工作中，从咨询和设计审查到深度参与（即SRE承担on-call职责）。 经验教训 从稀疏的或不明确的标准转变为高度标准化的平台是一个长期项目。每个步骤可能都让人觉得是增量式的，但最终，这些步骤可以减少开销并使大规模运行服务成为可能。 这种转变可以让开发人员看到价值所在。即不要尝试说服人们执行一个只在全部完成后才得到回报的巨大重构工程，而是在每个开发阶段解锁增量生产力。 案例研究5：pDNS不再取决于自身 背景 当Google生产的客户要查询服务的IP地址时，通常使用名为Svelte的查找服务。过去，为了找到Svelte的IP地址，客户端使用了名为pDNS（生产DNS）的Google命名服务。通过负载均衡访问pDNS服务，负载均衡使用Svelte查找实际pDNS服务器的IP地址。 问题描述 pDNS对自身具有传递依赖性，某种程度上说这是无意中引入的，后来被明确为是可靠性问题。由于pDNS服务可复制，且在生产中的始终可以获得打破依赖关系循环所需的数据，因此查找通常不会遇到问题。然而，冷启动是无法做到的。借用一位SRE的话来说，“我们就像穴居人，只能依赖现有篝火来点火。” 我们决定做什么 我们修改了Google生产中的低级组件以便为所有Google生产机器的本地存储附近的Svelte服务器维护当前IP地址列表。除了打破前文所述的循环依赖之外，此举还消除了对大多数其他Google服务的pDNS的隐式依赖。 为了避免此类问题，我们还引入了一种方法，将允许与pDNS通信的服务集列入白名单，并慢慢减少该集合。因此，生产中每个服务的查找都通过系统且具有更简单更可靠的路径。 经验教训 注意服务的依赖关系 - 使用明确的白名单以防止意外添加。另外，需要注意循环依赖。 结论 通常简单的系统往往是可靠的且易于运行的，因此简单化自然而然就是SRE的目标。很难定量衡量分布式系统的简单性（或取逆，即复杂度），但可以挑选和改进合理的替代测量方案。 SRE对系统有着端到端的理解，在识别，预防和修复复杂度来源方具有优势，在软件设计，系统架构，配置，部署过程或是其他地方，SRE都应该参与设计讨论，提供对成本和效益的独特见解，尤其是简单化。SRE还可以主动制订标准来使生产统一化。 作为SRE，追求简单化应该是工作的重点内容。我们强烈建议SRE领导层授予SRE团队权利和奖励来推动简单化。系统在不断发展的过程中会不可避免的越来越复杂，因此追求简单化的斗争道路需要持久的关注和付出-但这份追求是值得的。</summary></entry><entry><title type="html">第六章 减少琐事</title><link href="http://localhost:4000/sre/2020/01/06/%E5%87%8F%E5%B0%91%E7%90%90%E4%BA%8B/" rel="alternate" type="text/html" title="第六章 减少琐事" /><published>2020-01-06T00:00:00+08:00</published><updated>2020-01-06T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/06/%E5%87%8F%E5%B0%91%E7%90%90%E4%BA%8B</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/06/%E5%87%8F%E5%B0%91%E7%90%90%E4%BA%8B/">&lt;!-- more --&gt;
&lt;p&gt;Google SRE的大量时间用于系统优化，通过工程化的方法，与开发一起协同努力，追求卓越。哪怕是很少的性能收益也是值得的。但优化范围不仅局限于服务器资源，SRE的工作耗时也是优化的范畴。首先，SRE的工作不是琐事（关于琐事请参阅《SRE：Google运维解密》第5章内容）。本章我们将琐事定义为：与维护服务相关的，重复的、可预测的、持续的任务流。&lt;/p&gt;

&lt;p&gt;对于产品运维团队来说，琐事不可避免。运维不可避免地需要处理部署、升级、重启、告警等工作。如果没有系统的方法，这些工作很快将耗尽整个团队精力。Google将SRE团队日常操作的耗时占比限制在50%以内（包括琐事和非劳动密集型工作。这样做的原因，请参阅《SRE：Google运维解密》书中第5章内容）。虽然这个目标可能不适合所有团队，但花费在琐事上的时间上限仍然很重要，因为识别和量化琐事是团队时间优化的第一步。&lt;/p&gt;

&lt;h2 id=&quot;琐事的定义&quot;&gt;琐事的定义&lt;/h2&gt;
&lt;p&gt;琐事往往具有如下特征：这在我们的上一本书中有所阐述（《SRE：Google运维解密》译者注）。在这里，我们列举出琐事的特征，并给出了一个具体的例子加以解释：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;手动性&lt;/code&gt;: 当web服务器上的/tmp目录磁盘占用率达到95%时，工程师Anne登录到服务器，在文件系统中查找并删除了无用的日志文件。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;重复性&lt;/code&gt;: 写满/tmp目录的事情不太可能只发生一次，因此我们需要反复处理。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;可以被自动化&lt;/code&gt;: 假设修复文件的工作包括如下几个步骤：“X登录，执行此命令，检查输出，执行命令，并通过命令的输出来判断是否需要重启Y”。这些指令流本质上就是伪代码！在上面的例子中，解决方案实际上已经可以部分自动化了。如果不需要人来运行脚本，可以自动化的检测故障并修复是再好不过了。更进一步，我们可以提交一个补丁使软件不再因为文档损坏的问题而中断。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;非技术性&lt;/code&gt;: “磁盘写满”和“服务宕机”之类的告警会分散工程师的注意力，从而忽略高价值的事情，并可能掩盖其他更严重的告警。大量类似的告警造成的后果会波及到服务的健康状况。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;没有持续的价值&lt;/code&gt;: 完成一项任务会带来一种令人满意的成就感。但长远来看，这种重复的满足感不能给工程师带来持续的价值。比如，处理告警能够确保用户查询持续进行；确保HTTP请求状态码小于400，以便可以让应用提供持续的服务，这些固然很好。然而，今天解决的问题并不能防止将来不再出现类似的问题，所以这样做的回报只是短期的。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;与服务同步增长&lt;/code&gt;: 许多业务工作量的增长速度与基础设施规模的增长速度一样快(或许更快)。例如，你花费在修复硬件故障的时间会随着服务器集群规模的增加而增加。但请注意，相关的辅助任务(例如，软件/配置更改)不一定是这个趋势。
我们并不能将带来琐事的原因规范化和标准化，但是我们需要知道琐事的一些的特征。除上述特征外，还要考虑某项工作对团队士气的影响。人们是乐于完成一项觉得会有回报的任务？还是会处理无益的琐碎和无聊的任务？答案显而易见，琐事会慢慢地降低团队士气——时间往往花在琐事上而不是花在批判性思考或者是表达创造力上了；只有减少琐事，工程师才能更好地将时间用于思考和进行创造的领域。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;案例：人工处理琐事&lt;br /&gt;
作者：John Looney，Facebook资深 SRE&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;哪些工作内容是琐事，通常是模糊的。一个“创造性”的解决方案，可能使问题得到最优解决，因此，SRE团队应奖励那些分析根因并解决问题的人，而不是那些&lt;code class=&quot;highlighter-rouge&quot;&gt;掩盖&lt;/code&gt;问题的人。&lt;/p&gt;

  &lt;p&gt;我加入Google后的第一个任务（2005年4月）是追查一批机器死机原因并修复。如果确认是硬件原因，则转交给硬件技术人员维修。这个任务并没有看似那样的简单，因为我需在截止日期前处理超过20,000台机器。&lt;/p&gt;

  &lt;p&gt;第一台机器死机原因是：Google网络驱动补丁不断打印毫无意义的日志，导致文件系统的根目录写满，类似的一千台机器都是同样的问题。&lt;/p&gt;

  &lt;p&gt;我和同事沟通了解决这个问题的方案：编写一个脚本，ssh到所有异常机器，如果根目录已满，则清空/var/log中大文件日志，并重启syslog。我的同事对此方案不认可，他说最好找到根因并修复。如果&lt;code class=&quot;highlighter-rouge&quot;&gt;掩盖&lt;/code&gt;了问题，在后续一段时间内，可能会引起更多严重性问题。&lt;/p&gt;

  &lt;p&gt;理论上，每台机器每小时的成本约为1美元。我的想法是，成本是运维工作很重要的衡量指标，应该高优让机器提供服务，利用起来。但我没有考虑的是：如果只是解决了这个表象，就没有机会去追查根因。&lt;/p&gt;

  &lt;p&gt;在高级工程师指导下，我翻阅了内核源码，找到导致此问题的可疑代码，并且记录了bug，帮助内核团队完善了他们的测试用例。从成本来看，解决这个网络补丁问题，每花费一小时，Google将为此付出1,000美元。&lt;/p&gt;

  &lt;p&gt;那天晚上就发布了新的内核版本，第二天我就把它升级到所有受影响的机器，内核团队在第二周更新了他们的测试用例。这个问题的处理，我很满意，因为找到了根因并成功修复，而不是每天上班后清理日志。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;琐事的度量&quot;&gt;琐事的度量&lt;/h2&gt;

&lt;p&gt;运维工作是辛苦的。如果你做了一些工作减少了琐事，如何知道你的努力是成功的？许多SRE团队是结合经验和直觉来回答这个问题。经验和直觉会产生好的效果，但是我们还可以将方法上升到一个理论的维度。&lt;/p&gt;

&lt;p&gt;经验和直觉是因人而异、非客观的。根据场景的不同，琐事的定义也不同。比如，同一团队的不同成员会根据工作的投入产出比来判断一件事情是否可以定义为琐事。此外，为了减少琐事所做的工作可能会持续几个季度甚至几年的时间(本章的一些案例研究就证明了这一点)，在此期间团队的人员主要任务可能会发生改变。所以，为了保证减少琐事的工作能够长期进行，一般的，团队必须从几个确定的琐事中选择一个琐事来消灭它。我们应当将这件事上升为一个项目，并且需要建立起这个项目的长期的客观的度量机制以保证投入得到回报。&lt;/p&gt;

&lt;p&gt;在启动项目之前，重要的是分析成本与收益，并确认通过减少琐事所节省的时间(至少)与第一次开发和维护自动化解决方案所投入的时间成正比(图6-1)。从节省的时间与投入的时间的简单比较来看，那些看起来“无利可图”的项目可能仍然值得进行，因为自动化有许多间接或无形的好处。潜在的好处包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;随着业务规模扩大，收益越明显&lt;/li&gt;
  &lt;li&gt;提高团队士气，减少团队流失和成员的厌倦情绪&lt;/li&gt;
  &lt;li&gt;更少的中断性工作，从而提高团队工作效率&lt;/li&gt;
  &lt;li&gt;提高流程清晰度和标准化&lt;/li&gt;
  &lt;li&gt;增强团队成员的技术技能和拥有更全面的职业发展&lt;/li&gt;
  &lt;li&gt;缩短新成员的培训时间&lt;/li&gt;
  &lt;li&gt;减少人为错误导致的问题&lt;/li&gt;
  &lt;li&gt;提高安全性&lt;/li&gt;
  &lt;li&gt;缩短用户投诉的响应时间
&lt;img src=&quot;/blog/something/images/SRE/6-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图 6-1 预测在减少琐事工作上花费的时间，并确保其收益大于投入 &lt;/center&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;琐事的度量方法&quot;&gt;琐事的度量方法&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;识别它。第一本SRE书的第5章提供了如何识别琐事。最能够识别琐事的人取决于团队本身。理想情况下，SRE团队既是利益相关方，也是实际操作方。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;选择适当的计量单位来量化人力成本。我们可以选择“分钟”或者“小时”这么一个客观和普遍能够理解的计量单位。务必还要考虑琐事转自动化的成本。有些人力成本具有分散性和碎片化的特征，所以我们从成员工作的内容来衡量更为合适。度量单位应该要能够很好的度量如下工作：为应用增加的补丁，完成的票证，手动生产环境的变更，电子邮件交换或者是一些对硬件的操作。总的来说，只要度量单位客观，一致且易于理解，它就可以作为工作的衡量标准。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在项目的整个周期内我们需要连续跟踪并记录度量的指标。我们可以使用工具或脚本来简化度量指标的测量过程，使得收集这些测量值不会产生额外的工作。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;琐事分类法&quot;&gt;琐事分类法&lt;/h2&gt;

&lt;p&gt;琐事，就像一座摇摇欲坠的桥梁或一座漏水的大坝，日复一日地隐藏在广阔无垠的大地之中。本节中的分类并不能够详尽无遗，但代表了一些常见的琐事类别。这些类别中有许多类似“正常”的工作，但是它们实际上就属于琐事。&lt;/p&gt;
&lt;h3 id=&quot;商业流程&quot;&gt;商业流程&lt;/h3&gt;

&lt;p&gt;这可能是最常见的琐事来源。也许你的团队管理一些计算机资源——计算、存储、网络、负载平衡器、数据库等，以及为该资源提供支持的硬件资源。你需要处理用户登录、配置修改和计算机安全维护、软件更新以及扩缩容。你还需要最大限度地降低成本避免计算机资源的浪费。你的团队是计算机的人机界面，通常与为其需求提交票证的内部客户进行交互。你的组织甚至可能拥有多个票务系统和工作系统。
票务系统属于“隐藏”一类的琐事，因为其驱动的业务流程通常是我们需要完成的目标。用户得到了他们想要的东西，并且因为琐事往往分散在整个团队中，所以琐事并不能明显地显现出来。在以票据驱动的任何地方，都有可能悄悄地积累这琐事。即使你没有明确的自动化流程，仍然需要执行流程的改进工作，例如简化流程，使其未来更容易做到自动化，同时更加容易管理。&lt;/p&gt;
&lt;h3 id=&quot;工作中断&quot;&gt;工作中断&lt;/h3&gt;

&lt;p&gt;中断是一类为了保证系统运行的时间敏感类任务，简单理解为被其他紧急事情打断。例如，你可能需要通过手动释放磁盘空间或重新启动泄漏内存的应用程序来解决某些资源（磁盘，内存，I/O）的严重短缺。你可能正在提交更换硬盘驱动器，“踢”出无响应的系统或手动调整容量以满足当前或预期的负载请求。通常，中断会将注意力从更重要的工作上移开。&lt;/p&gt;
&lt;h3 id=&quot;流程监督&quot;&gt;流程监督&lt;/h3&gt;

&lt;p&gt;在许多组织中，部署工具从发布到生产需要SRE进行监督。即使有自动化，全面的代码覆盖，代码审查和多种形式的自动化测试，这个过程并不总是顺利进行。根据工具和发布节奏，发布请求、回滚、紧急补丁以及重复或手动配置更改，发布仍产生琐事。&lt;/p&gt;
&lt;h3 id=&quot;服务迁移&quot;&gt;服务迁移&lt;/h3&gt;

&lt;p&gt;服务迁移也是我们经常要处理的一类事情。你可以手动或使用有限的脚本来执行此工作，而且希望只迁移一次。迁移有多种形式，包括有数据存储、云供应商、源代码控制系统、应用程序库和工具的更改。如果你手动迁移大规模的工程，迁移很可能涉及到“琐事”。对于大规模的迁移，你可能倾向于手动执行迁移，因为这是一次性的工作。并且我们甚至会将其视为“项目”的一部分而非“琐事”，但迁移工作的很多特征与“琐事”的特征是吻合的。从技术上讲，修改一个数据库的备份工具以便与另一个数据库可以协同工作是软件开发的范畴，但这项工作本质上只是重构代码，用一个接口替换另一个接口。这项工作是重复的，并且在很大程度上，备份工具的业务价值与之前是相同的。&lt;/p&gt;
&lt;h3 id=&quot;压缩成本和容量规划&quot;&gt;压缩成本和容量规划&lt;/h3&gt;

&lt;p&gt;无论是拥有硬件还是使用基础架构提供商（云），压缩成本和容量规划通常是一些劳动密集型的工作。例如：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在计算、内存或IOPS（每秒输入/输出操作）等资源的未来规划中要确保成本效益和突发情况的扩容能力。这可能转化为采购订单，AWS预留实例或云/基础设施即服务合同协商。&lt;/li&gt;
  &lt;li&gt;应对（并从中恢复）关键的高流量事件，如产品发布或者遇到假期。&lt;/li&gt;
  &lt;li&gt;排查下游和上游服务水平和容量情况。&lt;/li&gt;
  &lt;li&gt;根据专有云服务产品的计费细节优化应用程序（适用于AWS的DynamoDB或适用于GCP的Cloud Datastore）。&lt;/li&gt;
  &lt;li&gt;重构工具以便更好地利用现有资源。&lt;/li&gt;
  &lt;li&gt;处理超预算的资源，无论是基础设施提供商的上游还是与下游客户之间。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;黑盒系统故障排除&quot;&gt;黑盒系统故障排除&lt;/h3&gt;

&lt;p&gt;分布式微服务架构现在很常见。随着系统更加分散，出现了新的故障模式。团队可能没有能力来构建复杂的分布式跟踪，高可靠监控或详细的仪表盘。即使企业确实拥有这些工具，它们也可能不适用于所有系统。故障排除甚至可能需要登录到各个系统并使用脚本工具来对日志进行实时地查询分析。&lt;/p&gt;

&lt;p&gt;故障排除本身并不是坏事，但你应该把精力集中在新的故障模式上，而不是每周都发生的由脆弱系统架构导致的故障。随着可用度为“P”的新关键上游依赖性服务的上线，系统可用性将下降（1-P）倍。一个可用度为4个9的服务增加了9个关键的4个9的核心组件，现在就变为了是一个三个9的服务。&lt;/p&gt;
&lt;h2 id=&quot;琐事管理战略&quot;&gt;琐事管理战略&lt;/h2&gt;

&lt;p&gt;任何规模的生产系统，琐事管理都是至关重要的。一旦确定并量化了琐事，消除琐事的计划就要提上日程。这个工作可能需要数周才能完成，因此制定一个完善的计划是至关重要。首先，从源头上消除琐事是最佳的解决方案，但是对于源头上无法消除的琐事，则需要通过其他方式来消除。在我们深入研究两个案例之前，本节提供了此方面工作的通用性准则。正如下文的两个案例中提到的，琐事的细微差别是因团队而异。但无论如何，一些常见的准则是适用于任何规模或风格的组织。在后续案例中将以具体方式诠释每种策略。&lt;/p&gt;
&lt;h3 id=&quot;琐事的识别与度量&quot;&gt;琐事的识别与度量&lt;/h3&gt;

&lt;p&gt;采用数据驱动的方法来识别琐事，并配合客观的成本控制策略，获得此类项目最优的投入产出比。如果你的团队正在被琐事缠身，并将减少琐事作为了一个长期的项目。Google SRE团队根据多年的经验，在控制项目投入产出比方面是一个不错的借鉴。有关技术和指导，请参见第96页的“量化琐事”一节。&lt;/p&gt;
&lt;h3 id=&quot;让sre从琐事中解脱出来&quot;&gt;让SRE从琐事中解脱出来&lt;/h3&gt;

&lt;p&gt;减少琐事的最佳策略是从源头杜绝琐事。在进行系统设计和为生产环境制定流程之前，工程师要优化产品和系统来减少甚至消除琐事。&lt;/p&gt;

&lt;p&gt;真正了解生产环境痛点和知道导致系统出现琐事原因的那部分人正是SRE，因为只有他们和生产环境紧密联系。SRE应该在与产品开发团队合作的过程中，将自己的运维经验与产品开发团队共享从而开发出人机交互友好型的软件，从源头减少琐事，并且使产品具有更好的扩展性和弹性。&lt;/p&gt;
&lt;h3 id=&quot;拒绝琐事&quot;&gt;拒绝琐事&lt;/h3&gt;

&lt;p&gt;一个被琐事缠身的团队应该尽早的做出“消除琐事”决策。第一种策略是对琐事说“不”！对于每个琐事，量化它并以此为原则决定是否要做，但是根据Google的经验，这一种策略可能会适得其反。另一种策略是故意拖延这些琐事，直到我们可以通过批处理或并行处理来解决它。将琐事集中在一起一并处理它们，这种方式可以减少工作中的中断，并帮助你们识别琐事的特征，并将它们作为下一个消除目标。&lt;/p&gt;
&lt;h3 id=&quot;使用slo减少琐事&quot;&gt;使用SLO减少琐事&lt;/h3&gt;

&lt;p&gt;如第2章所述，服务系统应具有文档化的SLO。明确定义SLO才能使工程师做出明智的决策。例如，如果某项工作即使做也不会减少服务的错误预算，你就可以考虑忽略某项工作。随着服务的增长，专注于整体服务的可用性而不是单个设备的SLO，这样做是非常有利的，也是可持续的。有关编写有效SLO的指导，请参阅第2章。&lt;/p&gt;
&lt;h3 id=&quot;从部分自动化开始&quot;&gt;从部分自动化开始&lt;/h3&gt;

&lt;p&gt;如果你的业务特别复杂，请将“部分自动化”方法视为实现“完全自动化”的临时步骤。在这种方法中，你的服务通常可以通过定义的API接收结构化数据。工程师也可以进行一些操作从而得到想要的结果。虽然这样做需要一些手动的操作，但是这种“幕后工程师”方法是逐步实现全自动化的前提。使用“客户端输入”来统一收集数据；通过确定的请求格式，你可以更容易的以编程的方式对请求进行处理。这种方法让客户也能够明白你需要的信息和指标，并在你完全理解系统服务之前避免使用大型的解决方案而产生的未知问题。&lt;/p&gt;
&lt;h3 id=&quot;提供一种自助的服务方法&quot;&gt;提供一种自助的服务方法&lt;/h3&gt;

&lt;p&gt;一旦你们提供了交互型界面的服务产品，请进一步的为用户提供自助式的服务方法。你可以提供Web表单、二进制、脚本、API，甚至只是告诉用户如何向服务的配置文件发出拉取请求的文档。例如，软件开发工程师要求SRE工程师为其开发工作配置新虚拟机，我们为他们提供一个简单的Web表单或脚本来触发配置，而不是让他们提交相关票证来进行这件事。如果发生了特殊的情况，我们也允许使用“票证”的方式替代自助的服务，这是可接受的。部分自动化是一个良好的开端，但服务SRE工程师应该始终要致力于尽可能让服务自动化起来。&lt;/p&gt;
&lt;h3 id=&quot;获得管理层和同事的支持&quot;&gt;获得管理层和同事的支持&lt;/h3&gt;

&lt;p&gt;在短期内，减少琐事的项目需要投入人力成本，反之会减少处理其他日常任务的人员数量。但长远来看，如果项目达到了减少琐事的目标，团队将更加健康，并有更多的时间进行更重要的工程改进。对于团队中的每个人来说，“减少琐事”作为一个共同的价值目标是很重要的。管理层的支持对于减少工程师的干扰是至关重要。制定琐事评估的客观指标来说明项目的推进情况可以让管理层更加支持项目的进行。&lt;/p&gt;
&lt;h3 id=&quot;减少琐事作为提高服务稳定性一部分&quot;&gt;减少琐事作为提高服务稳定性一部分&lt;/h3&gt;

&lt;p&gt;要为减少琐事的项目创建一个强大的业务案例支持，将你的目标与其他业务目标相结合。如果有一个补充性的目标，例如，安全性、可扩展性或可靠性——这对客户来说是具有吸引力的，他们会更愿意放弃当前充满琐事的系统，转向更加亮眼的新系统。这样来看，减少琐事也可以提高用户服务的质量，这也是另一个角度来看待琐事的认识。&lt;/p&gt;

&lt;p&gt;从简单的琐事开始并持续改善，不要试图设计没有琐事的系统。面对一个充满琐事的系统，首先自动化一些高优先级的项目，然后通过评估这个项目所花费的时间来改进你的解决方案，总结获得的经验和教训。在项目开始之前，选择一个明确的指标，如MTTR（平均修复时间）来评估你的项目的进展和效果。&lt;/p&gt;
&lt;h3 id=&quot;提高系统的一致性&quot;&gt;提高系统的一致性&lt;/h3&gt;
&lt;p&gt;从规模上看，多样化的生产环境是难以管理的。特殊的生产环境容易出错，管理能力会降低，事故的处理能力也会降低。你可以使用“宠物与牛”方法（https://www.engineyard.com/blog/pets-vs-cattle，译者注）来添加系统冗余并在你的生产环境中实施增加一致性的策略。是否选择“牛”取决于组织的需求和规模。将网络链路、交换机、机器、机架，甚至整个集群评估为可互换单元也是合理的。将设备转换为“牛”的理念可能会带来较高的初始成本，但会减少中长期的维护成本，增强灾难恢复能力和提高资源利用能力。为多个设备配置相同的接口意味着它们具有相同的配置，是可互换的，维护成本也就降低了。各种设备的界面一致（转移流量，恢复流量，执行关机等）使系统更加灵活和更加可扩展。Google鼓励各团队将不断发展的内部技术和工具进行统一，并有相应的鼓励机制。无论团队用什么样的方法，但他们不得不承认一些不受支持的工具或遗留的系统是产生琐事的根源。&lt;/p&gt;
&lt;h3 id=&quot;评估自动化带来的风险&quot;&gt;评估自动化带来的风险&lt;/h3&gt;

&lt;p&gt;自动化可以节省人力成本，但是也会出现未知的错误，严重时会造成停机。一般情况下，防御性软件可以控制这类事情的发生。当管理级别的行为被自动化之后，防御性软件会显得至关重要。在执行前应对每项行为的安全性进行评估。在实施自动化时，我们建议采用以下做法：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;防御性地处理用户输入，即使这个输入来自于上游的系统 ——换句话说，要对上下游的输入进行仔细的校验。&lt;/li&gt;
  &lt;li&gt;构建告警机制，使得工程师可以接收到相关告警以进行处理。安全措施可能与命令超时一样简单，也可能是对当前系统指标或当前中断次数的更复杂检查。因此，监控，报警和仪表系统应由机器和操作人员共同使用。&lt;/li&gt;
  &lt;li&gt;请注意，即使是简单的读取操作也可能会导致设备负载过高和触发服务中断。随着自动化的扩展，这些安全检查的工作量是可控的。&lt;/li&gt;
  &lt;li&gt;最大限度地减少因自动化安全检查不完整导致服务中断的影响。如果操作员遇到不安全的情况，自动化操作应该默认为人工操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;琐事自动化之后要做什么&quot;&gt;琐事自动化之后要做什么&lt;/h3&gt;

&lt;p&gt;一旦你可以将一个工作自动化后，这个自动化的工作就值得更深层次的被发掘。进一步的将自动化的任务按照人工处理的流程优化下去。但请注意，自动化不应该让工程师认为任务不会出错。在完成上述优化后，你还可以尝试将自动化的工作分解为可单独实现的组件，并用于创建可组合的软件库，其他自动化项目可在以后重复使用。正如下文中的“数据中心维修案例”研究所示，自动化提供重新评估和简化人工工作流程的机会。&lt;/p&gt;
&lt;h3 id=&quot;使用开源和第三方工具&quot;&gt;使用开源和第三方工具&lt;/h3&gt;

&lt;p&gt;有时你不必做所有的工作来减少琐事。像一次性迁移这样的工作可能自己无法建立定制型的工具，但你可能并不是第一个遇到这个任务的工程师。寻找第三方或开源库以降低开发成本，或者说，至少可以帮助你过渡到部分自动化。&lt;/p&gt;
&lt;h3 id=&quot;反馈并改进&quot;&gt;反馈并改进&lt;/h3&gt;

&lt;p&gt;积极寻求反馈，这些反馈可以来自于工具、工作流程和自动化交互相关的其他人，这是非常重要的。你的用户将根据他们对底层系统的理解将你的工具在不同使用情景下进行使用。你的用户对这些工具越不熟悉，就越要积极地寻求用户的反馈。利用用户调查，用户体验（UX）和其他机制来了解你的工具被如何使用，并整合这些反馈，以便在未来实现更有效的兼容性。
人的输入只是你应该考虑反馈中的一个方面。我们还可以根据延迟，错误率，返工率和节省的人工时间等指标（跨过流程中涉及的所有组）来衡量自动化任务的有效性。能够获得在自动化工作部署之前和之后两种状态的对比是最明确的衡量方式。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;扩展：历史遗留系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;大多数SRE工程师在他们的工作中都会遇到过历史遗留系统。这些旧系统经常在用户体验，安全性、可靠性或可伸缩性方面有问题。他们倾向于将遗留系统看作一个神奇的黑匣子，因为系统“大部分组件是在工作中的”，但很少有人了解它们是如何工作的。贸然的调整它们是可怕的，也是昂贵的，并且保持它们的运行通常需要大量繁琐操作步骤。
远离遗留系统通常遵循以下路径：&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;避免：我们可以为不去解决这个问题找到许多理由：可能是没有资源来替换这个系统；判断业务成本和风险发现不值得替换；可能没有找到商业上更好的解决方案。避免选择的是接受风险并从SRE转向系统管理。&lt;/li&gt;
    &lt;li&gt;封装/扩充：你可以使用SRE来构建一个抽象API的外壳，自动化，配置管理，监视和测试这些遗留系统，这些系统将卸载SA的工作。遗留系统仍然很难改变，但现在你至少可以识别它并在适当时有回滚策略。这种策略仍然可以避免，但这是将风险引入到的更好的系统中。这通常是准备增量替换的权宜之计。&lt;/li&gt;
    &lt;li&gt;替换/重构：替换遗留系统可能需要大量的决心、耐心、沟通成本和文档，最好是逐步进行。一种方法是定义遗留系统公共接口。此策略可帮助你使用发布的工程手段，将用户缓慢、安全地迁移到其他安全的架构中。通常，遗留系统的“规范”实际上只是通过其历史用途来定义，因此有助于构建生产大小的历史预期输入和输出数据集，以建立新系统不会偏离预期行为的信心（或正在以预期的方式发散）。&lt;/li&gt;
    &lt;li&gt;退出/保管所有权：最终，大多数客户或功能被迁移到一个或多个系统。这个迁移需要有激励措施，没有迁移的用户让他们自行维护历史遗留系统，并承担相应责任。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;案例研究&quot;&gt;案例研究&lt;/h2&gt;

&lt;h2 id=&quot;案例研究1利用自动化减少数据中心的工作量&quot;&gt;案例研究1：利用自动化减少数据中心的工作量&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;案例研究1中所应用的减少琐事的战略：&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;SRE工程师从琐事中解脱出来&lt;/li&gt;
    &lt;li&gt;从部分自动化开始&lt;/li&gt;
    &lt;li&gt;提高系统的一致性&lt;/li&gt;
    &lt;li&gt;使用SLO减少琐事&lt;/li&gt;
    &lt;li&gt;评估自动化带来的风险&lt;/li&gt;
    &lt;li&gt;反馈并改进&lt;/li&gt;
    &lt;li&gt;提供一种自助的服务方法&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;背景&quot;&gt;背景&lt;/h3&gt;
&lt;p&gt;此案例来源于Google数据中心。与其他的数据中心类似，Google的计算机连接到交换机，交换机连接到路由器。流量通过链路流入和流出这些路由器，而链路又连接到互联网上的其他路由器。随着谷歌对互联网流量的要求越来越高，服务该流量所需的交换机数量也急剧增加。为了能够应对大流量的情况，我们的数据中心在规模和复杂性方面都有所增长。这种增长迫使数据中心改变了手动维修的旧方法。（从偶尔和有趣到频繁和沉闷的转变。）&lt;/p&gt;

&lt;p&gt;早期，谷歌在运行数据中心时，每个数据中心的网络拓扑都只有少量的网络设备，可以管理大量服务器的流量。单个网络设备故障可能会显著影响网络性能，但是一个小规模的工程师团队就可以处理设备的故障。早期，工程师调试故障设备并手动将流量切换到其他正常组件。而我们下一代的数据中心拥有更多的机器，并引入了折叠Clos拓扑结构的软件定义网络（SDN），交换机数量显著增加。图6-2展示的是一个小型数据中心Clos交换机网络的流量复杂情况。如果将这个比例放大，意味着设备数量更多，发生故障的组件也更多。虽然可以说，每个单独的故障对网络性能的影响比以前更小，但是大量的问题同时并发也会压倒工程师们。调试问题的过程同时也会引入大量新的问题，复杂的布局也让工程师感到困惑：需要检查哪些链接？需要更换哪个线卡？为什么是Stage 2开关，而不是Stage 1或Stage 3开关？关闭交换机会给用户带来哪些问题？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;center&gt;
图6-2. 一个小型Clos网络，Stage1 支持480台机器连接 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;修复故障的线卡是一个随着系统网络增长而任务量不断增长的琐事，因此我们将此作为“数据中心网络修复自动化”项目的第一阶段的目标。本案例阐述了我们如何在第一代线卡（名为Saturn）系统上开始自动化修复的过程，并以此为基础，我们讨论了如何对自动化工作进行改进以适应下一代线卡(Jupiter光纤网络)。&lt;/p&gt;

&lt;p&gt;如图6-3所示，在自动化项目开始之前，数据中心线卡修复工作需要工程师执行如下几个操作：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;确定从故障交换机切走流量是否是安全的。&lt;/li&gt;
  &lt;li&gt;切走流量至其他交换机（“drain”操作）。&lt;/li&gt;
  &lt;li&gt;执行重启或修复（例如更换线卡）。&lt;/li&gt;
  &lt;li&gt;将流量切回至该交换机（“undrain”操作）。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Drain，更换线卡，undrain的工作是不变和重复性质的，是“琐事”的典型范例。这些重复性的工作本身就会带来一些问题——例如，工程师在处理此类故障时会并行处理其他更有挑战性的工作，分心的工程师可能会意外地将未配置的交换机加入网络。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-3.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt; 
图6-3. 自动化之前的数据中心（Saturn）线卡修复工作流程：所有步骤都需要手动工作 &lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&quot;问题陈述&quot;&gt;问题陈述&lt;/h3&gt;
&lt;p&gt;数据中心修复线卡问题具有以下几个维度：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;团队规模增长的速度跟不上系统增长的速度（故障数量也在增长），使得我们无法快速解决问题以防止对系统带来负面影响。&lt;/li&gt;
  &lt;li&gt;人为错误一定会在重复执行的步骤中发生。&lt;/li&gt;
  &lt;li&gt;并非所有线卡故障的影响都是一致的。我们没办法对线卡故障划分优先级。&lt;/li&gt;
  &lt;li&gt;一些故障是暂时的，这时我们会选择直接重新启动线卡或重新安装交换机作为修复过程的第一步。并且，我们可以用编程方式捕获这些问题，如果它再次发生，则进行设备替换。&lt;/li&gt;
  &lt;li&gt;新的拓扑环境要求我们在采取行动之前手动评估隔离容量的风险。每次的人工风险评估都有可能带来人为错误，并可能带来严重影响。系统工程师和技术人员也没有好的方法来判断有多少设备和链接会受到修复过程的影响。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;我们要如何解决这个问题&quot;&gt;我们要如何解决这个问题？&lt;/h3&gt;
&lt;p&gt;为达到最好的效果，我们决定创建一个与现场技术人员配合使用的自动化框架，而不是把每个问题分配给工程师，让其进行风险评估，流量切换，维修和验证等人工操作。&lt;/p&gt;

&lt;h3 id=&quot;自动化的第一步jupiter光纤网络的修复自动化&quot;&gt;自动化的第一步：Jupiter光纤网络的修复自动化&lt;/h3&gt;
&lt;p&gt;我们的最终目标是构建一个能够代替工程师分析和处理故障的网络设备故障检测系统。我们的程序是直接切换流量并告知工程师，而不是向工程师发送“线卡”故障的告警。新系统有一些值得注意的特点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;我们最好是利用现有工具。如图6-3所示，我们的告警已经可以检测到线卡上的问题; 所以我们可以配置告警以触发自动修复。新的工作流程还应改变工单系统，以便支持自动提交的维修请求。&lt;/li&gt;
  &lt;li&gt;我们建立自动风险评估的机制，以防止在流量切换期间意外的隔离设备，并在需要时触发安全机制。此机制可以杜绝人为错误。&lt;/li&gt;
  &lt;li&gt;我们编写程序用于跟踪告警以便作出不同的处理操作：第一次告警仅重启该线卡并重装了软件；第二次出现告警则直接请求更换线卡并告知供应商。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;执行自动化操作&quot;&gt;执行自动化操作&lt;/h3&gt;
&lt;p&gt;新的自动化工作流程（如图6-4所示）进行如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;检测到有问题的线卡，并将故障特征添加到数据库中。&lt;/li&gt;
  &lt;li&gt;维修服务组件会解决问题并对交换机进行维修。该服务还会执行风险评估以确认操作不会隔离任何容量，然后： &lt;br /&gt;
 a. 从故障交换机中切出流量。&lt;br /&gt;
 b. 关闭线卡。&lt;br /&gt;
 c. 如果这是第一次告警，则重新启动线卡，将流量恢复到此交换机。此时，工作流程已完成。&lt;br /&gt;
 d．如果这是第二次失败，则工作流程进行到步骤3。&lt;/li&gt;
  &lt;li&gt;流程管理器检测到新案例并将其发送到故障维修池，供系统工程师处理。&lt;/li&gt;
  &lt;li&gt;系统工程师对故障做出响应，在UI界面中看到红色的“停止”（表示在开始修理之前需要切走流量），并分三步执行修复步骤：&lt;br /&gt;
 a. 系统工程师通过UI界面中的“准备组件”按钮启动流量切换。&lt;br /&gt;
 b. 流量切换完成后表示交换机可操作。&lt;br /&gt;
 c. 关闭交换机并维修线卡。&lt;/li&gt;
  &lt;li&gt;自动修复系统再次启动线卡。完成修复后，启动交换机，待初始化后，流程管理器会触发恢复操作，切回交换机流量并结算故障工单。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-4.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-4.具有自动化功能的Saturn线卡维修工作流程：只需按下按钮即可完成更换线卡等全部手动工作 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;新的自动化系统将团队从大量的琐事中解放出来，使他们有更多时间在其他地方开展更高效的项目：使用下一代Clos拓扑结构Jupiter。&lt;/p&gt;

&lt;h3 id=&quot;自动化项目的第二步saturn线卡修复与jupiter线卡修复&quot;&gt;自动化项目的第二步：Saturn线卡修复与Jupiter线卡修复&lt;/h3&gt;
&lt;p&gt;数据中心的容量需求几乎每12个月翻一番。因此，我们的下一代数据中心结构Jupiter比Google以前的任何数据中心的六倍还要大，所以故障的数量也会增加六倍多。Jupiter提出了自动化故障修复的挑战目标，这个目标的难度在于每层的数千个光纤链路和数百个线路卡都可能出现故障。幸运的是，随着潜在故障点的增加系统也会伴随增加更多的冗余，这有利于我们完成自动化任务。如图6-5所示，我们保留了系统的一些常规工作流程，并添加了一些重要的修改：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在自动切流量和关闭交换机之后，确定我们要更换的硬件，将硬件故障单发送给系统工程师。在这个过程中，切流量的行为是自动的，不需要系统工程师手动按下“预备按钮切流量开关”来完成。&lt;/li&gt;
  &lt;li&gt;我们添加了自动化，用于安装和推送组件更换后的配置。&lt;/li&gt;
  &lt;li&gt;我们启用自动化功能，以便在切回流量之前验证修复是否成功。&lt;/li&gt;
  &lt;li&gt;除非绝对必要，否则我们更关注的是如何恢复流量而不用人为介入。
&lt;img src=&quot;/blog/something/images/SRE/6-5.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-5.左图为Saturn线卡宕机自动化流程，右图为Jupiter线卡宕机自动化流程&lt;/center&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;项目实现&quot;&gt;项目实现&lt;/h3&gt;
&lt;p&gt;我们为Jupiter交换机上的所有的线卡故障采用了简单而统一的工作流程：操作通报，流量切换，开始修复。&lt;/p&gt;

&lt;p&gt;自动化执行如下：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;检测到交换机故障，并将故障特征写到数据库。&lt;/li&gt;
  &lt;li&gt;维修程序开始修复交换机：停止使用交换机，并将停止原因写到数据库中。&lt;br /&gt;
 a. 如果这是六个月内的第二次故障，请执行步骤4。&lt;br /&gt;
 b. 否则，请执行步骤3。&lt;/li&gt;
  &lt;li&gt;尝试（通过两种不同的方法）重启交换机。 &lt;br /&gt;
 a. 如果重启成功，用自动化服务检查健康状态，然后安装并配置交换机使其投入使用；删除修复原因，删除数据库中的故障记录。&lt;br /&gt;
 b. 如果健康检查失败，请升级给技术人员。&lt;/li&gt;
  &lt;li&gt;如果这是第二次故障告警，请将故障案例直接升级给技术人员，向其申请新的硬件设备。硬件更新后，用自动化服务检查健康状态，然后安装并配置交换机使其投入使用。删除修复原因，删除数据库中的故障记录。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种新的工作流程管理完全重写了以前的修复系统。同样的，我们要尽可能利用现有工具：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;配置新交换机（安装和验证）的操作与验证已更换的交换机所需的操作相同。&lt;/li&gt;
  &lt;li&gt;快速部署新的硬件需要以编程的方式进行BERT和cable-audit的能力。在恢复使用之前，我们可以使用该程序在已经修复的链路上运行功能测试。这些测试需要能够识别错误链接以进一步提高修复的效果。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下一步要提升的是自动缓解并修复Jupiter交换机线卡的内存错误。如图6-6所示，在开始自动化修复之前，此工作流程在很大程度上取决于工程师来判定故障是硬件导致还是软件导致，然后再停止使用，重启交换机等工作，并适时地安排修复。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-6.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-6. 自动化之前的Jupiter内存错误修复工作流程 &lt;/center&gt;
我们的自动化过程不再尝试对内存错误进行故障排除从而达到简化修复工作流程的目的（请参阅第119页的“有时不完美的自动化就足够了”，了解为什么这样做是有意义的）。相反，我们处理内存错误的方式与处理线卡故障的方式相同。为了将自动化覆盖到内存错误引起的故障，我们只需在配置文件中添加一个特征，使其对新的故障类型起作用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-7.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-7描述了内存错误的自动化工作流程。&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&quot;经验教训&quot;&gt;经验教训&lt;/h3&gt;
&lt;p&gt;在我们致力于实现网络故障自愈的这些年里，我们学会了如何有效减少琐事。&lt;/p&gt;

&lt;h4 id=&quot;uis-不该引入开销和复杂度&quot;&gt;UIs 不该引入开销和复杂度&lt;/h4&gt;
&lt;p&gt;替换一块Saturn-based线卡需要切走整个交换机的流量。等待备件更换以及工程师支持的时候，过早地执行全部切换操作意味着失去所有线卡的工作能力。我们在UI中增加一个“准备组件”的按钮，以允许技术人员在更换线卡前执行整个交换机的切换流量操作，从而消除了交换机不必要的停机时间（请参阅“按下准备按钮” 切出流量的开关“见图6-5）&lt;/p&gt;

&lt;h4 id=&quot;ui和维修工作的流程引入了许多非预期的问题&quot;&gt;UI和维修工作的流程引入了许多非预期的问题&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;按下切走流量的按钮后，技术人员无法得到流量切换进度的反馈，只能在结果返回后才能进行下一步操作。&lt;/li&gt;
  &lt;li&gt;该按钮可能无法反馈真实的状态。造成的结果是，有时切流量开关出问题但并没有被维修，或者技术人员可能通过其他方式中断了进程但是并没有告知系统。&lt;/li&gt;
  &lt;li&gt;问题出现时，非自动化的组件反馈了一个通用的‘contact engineering’信息。经验不丰富的技术人员无法快速找到可以提供帮助的人，而联系上的工程师并不总能够立即解决问题。
为快速对用户反馈以及因功能复杂性带来的回归问题进行响应，我们设计了更完善的工作流程，来保证按钮的安全性和可用性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;不要依赖人的经验&quot;&gt;不要依赖人的经验&lt;/h4&gt;
&lt;p&gt;我们过分依赖有经验的数据中心技术人员来识别系统中的错误（例如，当程序认为可以安全地进行维修，但实际上交换机并没有完成流量切换的动作）。这些技术人员在没有自动化提示的情况下，还必须手动执行多项工作。&lt;/p&gt;

&lt;p&gt;经验是难以复制的。在一个复杂的情节中，技术人员在等待数据中心维修时，决定启动并发切换来快速进行“按下按钮并等待结果”的操作，从而导致了网络拥塞和用户可见的数据包丢失。我们的软件无法预测并阻止这种行为，因为我们并没有测试过这种自动化。&lt;/p&gt;

&lt;h4 id=&quot;设计可重复使用的组件&quot;&gt;设计可重复使用的组件&lt;/h4&gt;

&lt;p&gt;尽可能避免采用集成化设计。使用组件来构建复杂的自动化工作流，每个组件处理一个独特且定义明确的任务。我们可以轻松地重复使用或调整早期Jupiter自动化的关键组件来用于下一代的软件设计，并且很容易针对已经存在的自动化项目增加新的功能。Jupiter类结构的连续变体可以采用早期已经完成的工作。&lt;/p&gt;
&lt;h4 id=&quot;不要过分分析问题&quot;&gt;不要过分分析问题&lt;/h4&gt;
&lt;p&gt;我们过度分析了Jupiter线卡内存错误问题。我们试图进行精确的问题诊断，我们想区分软件错误（可通过重新启动修复）与硬件错误（需要更换卡），并识别影响流量的错误与未发生的错误。我们花费将近三年（2012-2015）的时间来收集超过650个离散内存错误的数据，然后才意识到这个分析是过头了，或者至少不应该阻塞我们自动化项目的开展。&lt;/p&gt;

&lt;p&gt;一旦我们决定对检测到的任何错误都采取必要的措施，就可以直接使用我们现有的自动化修复技术来实现简单的切换策略、重启以及为修复内存错误而重置交换机。如果问题再次出现，我们可以认为，故障很可能是基于硬件的，并立即要求更换组件。我们花费了整个项目四分之一的时间来收集数据，发现大多数的错误是暂时的 ——大多数交换机在重新启动和重新安装后都恢复了。我们不需要额外的数据来执行修复，因此为了实现这种自动化花费了三年是没有必要的。&lt;/p&gt;

&lt;h4 id=&quot;有时不完美的自动化就已经足够&quot;&gt;有时不完美的自动化就已经足够&lt;/h4&gt;
&lt;p&gt;解除链路之前，通过BERT很容易确认链路状况，但BERT工具不支持网络管理链路。我们将这些链路添加到现有的链路修复自动化中，并允许跳过验证。我们很愿意绕过验证，因为链路并没有承载客户流量，如果验证结果很重要，我们可以稍后添加此功能。&lt;/p&gt;

&lt;h4 id=&quot;保证维修自动化项目的持续性和可继承性&quot;&gt;保证维修自动化项目的持续性和可继承性&lt;/h4&gt;
&lt;p&gt;自动化项目可以有很长的生命周期，需要确保人员的流动不会干扰项目的连续性。 新人工程师应该接受现有系统的培训，以便他们能够修复错误。由于Jupiter线卡部件的短缺，Saturn-based系统在其目标寿命结束后很长一段时间内还是存在的，这要求我们日后在Saturn的生命周期中进行一些改进。&lt;/p&gt;

&lt;p&gt;自动化一旦被采用，在很长的一段时间内将会被依赖使用，并伴随着一些积极和消极的后果。如果可能，以灵活的方式设计你的自动化程序。不灵活的自动化会使系统变更变得难以实现。使用基于策略的自动化可以明确地将意图与通用实现引擎分离，从而使自动化更加可持续的发展。&lt;/p&gt;

&lt;h4 id=&quot;深入开展风险评估和防御措施&quot;&gt;深入开展风险评估和防御措施&lt;/h4&gt;
&lt;p&gt;为Jupiter构建新工具以评估执行切流操作前的风险，而后由于问题的复杂性，我们需要在更深层次的防御上引入二次检查。二次检查设定了受影响链路数量的上限，以及受影响设备的额外限制。一旦超过任一限定值，便会自动触发追踪bug以请求更进一步的检查。我们不断地调整这些限制，以减少误报。最初我们认为二次检查只是一项临时措施，但是在主要风险评估平稳后，该措施已被证明可用于识别由于停电和软件错误导致的维修问题（如请参阅SRE中“自动化：在规模上实现失效”）。&lt;/p&gt;

&lt;h4 id=&quot;失败预算和管理者支持&quot;&gt;失败预算和管理者支持&lt;/h4&gt;
&lt;p&gt;修复自动化有时会失败，尤其是在首次使用时。管理者的支持对于保护项目，并鼓励团队坚持不懈是至关重要的。我们建议为通过自动化技术消除琐事项目设置错误预算。你还需要向外部的其他利益方解释：尽管存在故障风险，但自动化极其重要，并可以持续提高可靠性和效率。&lt;/p&gt;

&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;
&lt;p&gt;最终，复杂场景的自动化是真正需要解决的问题。在引入自动化系统之前要反复对系统进行评审——是否可以先简化系统和工作流？&lt;/p&gt;

&lt;p&gt;要关注自动化工作流程的各个方面，而不仅仅是造成琐事的那部分。和直接参与项目的人员共同开展测试工作，并积极寻求他们的反馈和帮助。如果他们在使用过程中出现操作问题，要想办法使工作界面更清晰，或者增加额外的安全检查。确保自动化不会带来额外的琐事——例如开启了不必要的工单以引起人的注意。给其他团队创造问题将增加自动化推进的难度。&lt;/p&gt;

&lt;h2 id=&quot;案例研究2淘汰以文件为后端的home-directories&quot;&gt;案例研究2：淘汰以文件为后端的Home directories&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;案例研究2中强调了减少琐事的方法：&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;考虑淘汰旧系统&lt;/li&gt;
    &lt;li&gt;将减少琐事作为一项工程&lt;/li&gt;
    &lt;li&gt;获得管理层和同事的支持&lt;/li&gt;
    &lt;li&gt;拒绝琐事&lt;/li&gt;
    &lt;li&gt;从部分自动化开始&lt;/li&gt;
    &lt;li&gt;提供一种自助的服务方法&lt;/li&gt;
    &lt;li&gt;从细微处开始然后改进&lt;/li&gt;
    &lt;li&gt;反馈并改进&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;背景-1&quot;&gt;背景&lt;/h3&gt;
&lt;p&gt;在谷歌的早期，公司数据存储（CDS）SRE团队为所有Google员工提供home目录服务。与企业IT中常见的Active Directory漫游配置文件类似，Google员工可以跨工作站和平台使用相同的home目录。CDS团队还为共享存储空间中的跨团队协作提供“团队共享”服务。我们通过NFS / CIFS（或“文件管理器”）上的Netapp存储设备提供home目录和团队共享。这种存储系统是很昂贵的，但Google员工对此类服务的需求是必须的。&lt;/p&gt;

&lt;h3 id=&quot;问题陈述-1&quot;&gt;问题陈述&lt;/h3&gt;
&lt;p&gt;随着时间的推移，这些文件管理系统解决方案的优势被其他更好的存储解决方案所超越：我们的版本控制系统（Piper / Git-on-borg），Google Drive，Google Team Drive，Google云存储以及全球内部共享分布式ilesystem（x20）。这些替代方案的优越性体验在如下方面：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NFS / CIFS协议并不适用于在WAN上运行，这造成即使有几十毫秒的延迟，用户体验也会迅速降低。这也为远程工作人员或全球分布的团队带来了问题——因为数据只能存在于一个地方。&lt;/li&gt;
  &lt;li&gt;与替代品相比，原系统的设备运行和规模都是昂贵的。&lt;/li&gt;
  &lt;li&gt;要使NFS / CIFS协议与Google的Beyond Corp11网络安全模型兼容，需要做大量的工作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;与本章最相关的是，home目录和团队共享会频繁的使用。存储配置的许多方面都是ticket驱动的。虽然这些工作流程通常是研发人员编写的，但它们代表了相当数量的CDS团队的工作成果。我们花了很多时间创建和配置共享，修改访问权限，解决最终用户问题，以及执行启动和调整以管理容量。除了配置、更新和备份之外，CDS还需要管理专用硬件的配置，机架和布线过程。由于延迟要求，我们经常不得不部署在远程办公室而不是在Google数据中心 – 这有时需要花费团队成员相当长的时间。&lt;/p&gt;

&lt;h3 id=&quot;我们决定做什么&quot;&gt;我们决定做什么&lt;/h3&gt;
&lt;p&gt;首先，收集数据：CDS团队开发了一个名为“Moonwalk”的工具来分析员工使用此的服务的场景。我们确定收集如下通用的指标，如每日活跃用户（DAU）和月活跃用户（MAU），并询问了诸如“哪些用户实际使用他们的home目录？”和“哪些人每天使用此系统？他们最常访问的文件是什么？“Moonwalk与用户调查相结合，验证了文件管理器当前服务的业务需求可以通过低运营开销和成本的可替代方案代替。另一个引人注目的原因促使我们放弃现有的文件系统：如果我们可以将大多数文件管理器用例迁移到G Suite / GCP，那么我们可以利用我们学到的经验来改进这些产品，从而为其他大型企业迁移到GSuite/ GCP提供支持。&lt;/p&gt;

&lt;p&gt;没有一种替代方案可以满足所有当前的文件管理器用例。然而，通过将问题转化为若干小的需求来寻找可替代系统，我们发现少数备选方案可以涵盖我们所有的使用场景。替代解决方案更专业，并且每个解决方案都带来比旧的解决方案更好的用户体验。例如：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;x20&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;全局共享静态文件对于团队来说是很好的方式，比如二进制文件。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;G Suite Team Drive&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;适用于办公文档协作，与NFS相比，用户更能容忍此延迟。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;谷歌的巨像Colossus文件系统&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;比NFS更安全，更可靠地共享大型数据文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Piper/Git-on-Borg&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;可以更好地同步dotfiles（工程师的个性化工具首选项）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;一种新的“历史服务”工具&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;可以托管跨工作站命令行的历史记录&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在我们编制用例并找到替代方案时，旧文件系统的下线计划也已经开展&lt;/p&gt;

&lt;h3 id=&quot;设计与实施&quot;&gt;设计与实施&lt;/h3&gt;
&lt;p&gt;下线旧的文件管理系统是一项持续的、迭代的、需要多年的进行的工作。需要伴随多个子项目的开展：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Moira&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;home目录下线&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Tekmor&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;迁移home目录用户的历史遗留数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Migra&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;团队共享下线&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Azog&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;下线home目录/共享基础架构和相关硬件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-8.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-8。Moira项目的四个阶段 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;本案例研究重点关注第一个项目Moira。后续项目的开展是在Moira的学习和开展的基础上开始的。如图6-8所示，Moira由四个阶段组成。&lt;/p&gt;

&lt;h3 id=&quot;关键组件&quot;&gt;关键组件&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Moonwalk&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;虽然我们有关于用户共享（例如共享大小）的基本统计数据，但我们仍需要了解用户的工作流程，以帮助用户即使在一片反对声中仍然可以做出有利于业务发展的决策。我们建立了一个名为“Moonwalk”的系统来收集和反馈这些信息。&lt;/p&gt;

&lt;p&gt;Moonwalk存储了谁正在访问哪些文件以及何时使用BigQuery的数据，这使我们能够做出统计报告以便更好地了解用户。在BigQuery的帮助下，我们汇总了25亿个文件，共计300 TB的数据的用户访问模式。该数据来自于全球60个地理站点的124个NAS设备，共计600,000个磁盘卷，共收集了60,000名POSIX用户的使用信息。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Moira Portal&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过工单处理来完成home目录下线的想法，在我们庞大的用户基数面前看起来不太现实。我们需要在整个过程中（调查用户，告知项目下线原因，归档数据或迁移到替代系统）尽可能提供低接触服务（低接触服务是指这样的服务模式：销售服务人员在向顾客提供服务时，保持较少的面对面的接触机会。相对于高接触服务而言，低接触服务需要更多的机器和固定资产。因为通常需要由它们来自动完成顾客服务，如自动售货机、自动柜员机、自动加油机等，译者注）。我们的最终要求是：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;描述项目的登录页面&lt;/li&gt;
  &lt;li&gt;不断更新对常见问题的解答&lt;/li&gt;
  &lt;li&gt;与当前用户共享关联状态和使用信息&lt;/li&gt;
  &lt;li&gt;提供请求，停用，存档，删除，扩展或重新激活共享的选项&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;至此，我们的业务逻辑变得相当复杂——因为我们必须考虑许多用户场景。例如，用户可能会从Google离职，短暂离职，或者在诉讼状态（需要保留其拥有的数据）。图6-9提供了一个示例图，说明了其复杂性。
&lt;img src=&quot;/blog/something/images/SRE/6-9.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-9。基于用户场景的业务逻辑 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;为主门户网站提供支持的技术相对简单。基于Flask框架下用Python语言编写，它读取并写入Bigtable，并使用大量后台作业和调度程序来管理其工作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;归档和迁移自动化&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们需要大量的辅助工具来将门户网站和配置管理组合在一起，并与用户进行通信来进行用户查询。我们还需要使用沟通技巧来鉴别出适合进行数据迁移的用户。误报（错误地报告所需行动）或漏报（未通知用户您正在取走某些东西）都是不可接受的，这里发生错误将意味着失去用户可信度和带来客户服务的额外工作。&lt;/p&gt;

&lt;p&gt;我们与其他存储系统所有者合作——为新系统添加我们需要的功能。因此，随着项目的进展，不太成熟的替代品变得更适合了。我们还可以使用和扩展其他团队的工具。例如，我们使用其他团队内部开发的工具将数据从Google云端存储迁移到Google云端硬盘，作为门户网站自动存档功能的一部分。这项工作需要在整个项目期间进行长期的软件开发。我们构建并迭代了每个组件–Moonwalk报告管道，门户和自动化，以便更好地管理下线和归档共享，以响应下一阶段的要求和用户反馈。我们在第三阶段（差不多两年）才接近达到一个功能健全的系统。即便如此，我们还需要额外的工具来处理大约800名用户的“长尾”。这种低速和慢速的方法有一定的好处。因为它允许我们：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;维持一个精炼的团队（平均三名CDS团队成员）&lt;/li&gt;
  &lt;li&gt;减少对用户工作流程的干扰&lt;/li&gt;
  &lt;li&gt;减少Techstop的琐事（谷歌内部技术支持组织）&lt;/li&gt;
  &lt;li&gt;根据需要构建工具，以避免将时间浪费在工程工作中
与所有工程决策一样，存在如下权衡：项目将长期存在，因此团队在设计解决方案时必须忍受与文件管理器相关的琐事。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;该计划于2016年正式完成。在撰写本文时，我们已将home目录从65,000个减少到约50个（目前的Azog项目旨在淘汰这些最后用户并彻底下线文件管理系统的硬件。）我们的用户体验有所改善，CDS已停止运营成本高昂的硬件。&lt;/p&gt;

&lt;h3 id=&quot;经验教训-1&quot;&gt;经验教训&lt;/h3&gt;
&lt;p&gt;虽然没有任何替代方案可以替代Google员工已使用14年之久的文件管理系统，但我们并非没必要进行批量更换。通过有效地将堆栈从通用但有限的文件系统解决方案升级到多个应用程序共同组成的解决方案，我们增加了系统的灵活性，以提高可扩展性、延迟包容度和安全性。Moira团队不得不预测各种用户的行为，并考虑不同阶段的可替代方案。我们必须围绕这些替代方案来调整我们的期望：总的来说，它们可以提供更好的用户体验，但实现这一目标并非是毫无痛苦的。我们学到了以下关于有效减少琐事的策略。&lt;/p&gt;

&lt;h4 id=&quot;发现旧系统的不足并下线昂贵的业务流程&quot;&gt;发现旧系统的不足并下线昂贵的业务流程&lt;/h4&gt;
&lt;p&gt;业务需求不断变化，新的解决方案不断涌现，因此定期评估旧的业务流程是值得的。正如我们在第101页的“琐事管理策略”中所讨论的那样，拒绝琐事（决定不执行）通常是消除它的最简单方法，即使这种方法并不总是快速或简单的。通过用户分析和业务理由来调整你的业务工作内容，而不仅仅是减少琐事。文件管理系统下线的主要业务理由归结为Beyond Corp安全模型的优势。因此，虽然Moira是减少CDS团队琐事的好方法，但强调下线系统的原因如果是考虑到了新系统诸多的安全优势，这些优势将带来更具吸引力的业务需求。&lt;/p&gt;

&lt;h4 id=&quot;构建自助服务接口&quot;&gt;构建自助服务接口&lt;/h4&gt;
&lt;p&gt;我们为Moira建立了一个自定义门户（相对昂贵），但通常有更便宜的选择。Google的许多团队使用版本控制来管理和配置他们的服务，以拉取请求（称为更改列表或CL）的形式处理请求。这种方法几乎不需要服务团队的参与，但为我们提供了代码审查和持续部署的优势，便于验证、测试和部署内部服务配置更改。&lt;/p&gt;

&lt;h4 id=&quot;从人工支持的界面开始&quot;&gt;从人工支持的界面开始&lt;/h4&gt;
&lt;p&gt;在几个方面，Moira团队采用了“幕后工程师”的方法，将自动化与工程师的人工操作相结合。共享请求在路由过程中出现bug，我们的自动化在处理请求时会及时更新。系统还会通知到终端用户，提醒他们解决类似的共性问题。工单可以作为自动化系统的应急的GUI：它们保存工作日志，更新利益相关者的数据，并在自动化出错时提供简单的人工干预机制。在我们的示例中，如果用户需要获得数据迁移工作的帮助，或者如果自动化无法处理其请求，则该错误会自动路由到SRE手动处理的队列中。&lt;/p&gt;

&lt;h4 id=&quot;零接触的自动化&quot;&gt;零接触的自动化&lt;/h4&gt;
&lt;p&gt;自动化系统要求请求合规。Moira的工程师选择重新调整我们的自动化，以专门处理共享的边缘场景请求，或者删除/修改不合格的共享以符合系统的期望。这使我们能够在大多数迁移过程中实现零接触的自动化。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;有趣的事实：在谷歌，通过改变现实去适应代码而不是通过修改代码去适应现实的方式被称为“购买侏儒”。这句话源自一个关于Froogle的故事，Froogle是一个很早就开展的购物搜索引擎服务。
在Froogle的早期阶段，发生过精确匹配搜索“跑鞋”关键字导致返回了garden gnome(花园侏儒, 穿着跑鞋）的严重错误。在几次尝试修复错误失败之后，有人注意到gnome不是批量生产的商品，而是一个带有“一口价”选项的eBay商品。他们购买了这个“花园侏儒”商品后，解决了这个返回错误搜索结果的问题（译者注：相比起修改代码，以更低的成本解决了问题）。（图6-10）。
&lt;img src=&quot;/blog/something/images/SRE/6-10.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-10. 不会消失的花园侏儒 &lt;/center&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;推动新系统的使用&quot;&gt;推动新系统的使用&lt;/h4&gt;
&lt;p&gt;寻找方法来推动新用户采用更好的替代方案。在这种情况下，Moira需要升级系统配置以便应对新的请求和下线旧的系统。在服务设置中，新系统使用最佳实践和用户如何配置系统也很重要。Google团队经常使用codelabs或cookbook为用户提供常见用例设置和指导如何使用他们的服务。因此，大多数用户入手都不需要额外的团队的指导。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;
&lt;p&gt;与生产服务运行相关的琐事会随着系统复杂性和规模的增长而线性增长。自动化通常是消除琐事的黄金法则，并且可以与其他策略相结合。即使一些琐事没必要完全自动化，你也可以通过部分自动化或改变业务流程等策略来减少操作的负担。本章中描述的消除琐事的模式和方法可以推广到其他各种大规模生产服务中。消除琐事可以节省工作时间，以便工程师专注于服务的更重要的方面，并允许团队将手动任务保持在最低限度。随着现代服务架构的复杂性和规模不断增加，此策略尤其重要。&lt;/p&gt;

&lt;p&gt;但是要注意，消除琐事并不总是最好的解决方案。如本章所述，你应该考虑成本，设计、改造和实施自动化解决方案都需要投入成本。一旦决定减少琐事，就必确立目标，进行投资回报率（ROI）分析，风险评估和迭代开发来确定是否减少了工作量。&lt;/p&gt;

&lt;p&gt;琐事通常是从小事开始积累的，并且可以迅速成长最终消耗整个团队的人力资源。SRE团队必须坚持不懈地消除琐事——因为即使减少琐事的项目看起来令人生畏，但其好处通常也是会超过成本的。我们所描述的每个项目都需要各自团队的坚持不懈和奉献精神，他们有时会面对质疑或者需要与制度进行斗争，并且总是面临竞争这种高优先级的任务。我们希望这些案例鼓励你识别工作中的琐事，量化它，然后努力消除它。即使今天不能开展一个大项目，你也可以从一个小概念开始，这可以帮助改变你的团队处理琐事的方式。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">Google SRE的大量时间用于系统优化，通过工程化的方法，与开发一起协同努力，追求卓越。哪怕是很少的性能收益也是值得的。但优化范围不仅局限于服务器资源，SRE的工作耗时也是优化的范畴。首先，SRE的工作不是琐事（关于琐事请参阅《SRE：Google运维解密》第5章内容）。本章我们将琐事定义为：与维护服务相关的，重复的、可预测的、持续的任务流。 对于产品运维团队来说，琐事不可避免。运维不可避免地需要处理部署、升级、重启、告警等工作。如果没有系统的方法，这些工作很快将耗尽整个团队精力。Google将SRE团队日常操作的耗时占比限制在50%以内（包括琐事和非劳动密集型工作。这样做的原因，请参阅《SRE：Google运维解密》书中第5章内容）。虽然这个目标可能不适合所有团队，但花费在琐事上的时间上限仍然很重要，因为识别和量化琐事是团队时间优化的第一步。 琐事的定义 琐事往往具有如下特征：这在我们的上一本书中有所阐述（《SRE：Google运维解密》译者注）。在这里，我们列举出琐事的特征，并给出了一个具体的例子加以解释： 手动性: 当web服务器上的/tmp目录磁盘占用率达到95%时，工程师Anne登录到服务器，在文件系统中查找并删除了无用的日志文件。 重复性: 写满/tmp目录的事情不太可能只发生一次，因此我们需要反复处理。 可以被自动化: 假设修复文件的工作包括如下几个步骤：“X登录，执行此命令，检查输出，执行命令，并通过命令的输出来判断是否需要重启Y”。这些指令流本质上就是伪代码！在上面的例子中，解决方案实际上已经可以部分自动化了。如果不需要人来运行脚本，可以自动化的检测故障并修复是再好不过了。更进一步，我们可以提交一个补丁使软件不再因为文档损坏的问题而中断。 非技术性: “磁盘写满”和“服务宕机”之类的告警会分散工程师的注意力，从而忽略高价值的事情，并可能掩盖其他更严重的告警。大量类似的告警造成的后果会波及到服务的健康状况。 没有持续的价值: 完成一项任务会带来一种令人满意的成就感。但长远来看，这种重复的满足感不能给工程师带来持续的价值。比如，处理告警能够确保用户查询持续进行；确保HTTP请求状态码小于400，以便可以让应用提供持续的服务，这些固然很好。然而，今天解决的问题并不能防止将来不再出现类似的问题，所以这样做的回报只是短期的。 与服务同步增长: 许多业务工作量的增长速度与基础设施规模的增长速度一样快(或许更快)。例如，你花费在修复硬件故障的时间会随着服务器集群规模的增加而增加。但请注意，相关的辅助任务(例如，软件/配置更改)不一定是这个趋势。 我们并不能将带来琐事的原因规范化和标准化，但是我们需要知道琐事的一些的特征。除上述特征外，还要考虑某项工作对团队士气的影响。人们是乐于完成一项觉得会有回报的任务？还是会处理无益的琐碎和无聊的任务？答案显而易见，琐事会慢慢地降低团队士气——时间往往花在琐事上而不是花在批判性思考或者是表达创造力上了；只有减少琐事，工程师才能更好地将时间用于思考和进行创造的领域。 案例：人工处理琐事 作者：John Looney，Facebook资深 SRE 哪些工作内容是琐事，通常是模糊的。一个“创造性”的解决方案，可能使问题得到最优解决，因此，SRE团队应奖励那些分析根因并解决问题的人，而不是那些掩盖问题的人。 我加入Google后的第一个任务（2005年4月）是追查一批机器死机原因并修复。如果确认是硬件原因，则转交给硬件技术人员维修。这个任务并没有看似那样的简单，因为我需在截止日期前处理超过20,000台机器。 第一台机器死机原因是：Google网络驱动补丁不断打印毫无意义的日志，导致文件系统的根目录写满，类似的一千台机器都是同样的问题。 我和同事沟通了解决这个问题的方案：编写一个脚本，ssh到所有异常机器，如果根目录已满，则清空/var/log中大文件日志，并重启syslog。我的同事对此方案不认可，他说最好找到根因并修复。如果掩盖了问题，在后续一段时间内，可能会引起更多严重性问题。 理论上，每台机器每小时的成本约为1美元。我的想法是，成本是运维工作很重要的衡量指标，应该高优让机器提供服务，利用起来。但我没有考虑的是：如果只是解决了这个表象，就没有机会去追查根因。 在高级工程师指导下，我翻阅了内核源码，找到导致此问题的可疑代码，并且记录了bug，帮助内核团队完善了他们的测试用例。从成本来看，解决这个网络补丁问题，每花费一小时，Google将为此付出1,000美元。 那天晚上就发布了新的内核版本，第二天我就把它升级到所有受影响的机器，内核团队在第二周更新了他们的测试用例。这个问题的处理，我很满意，因为找到了根因并成功修复，而不是每天上班后清理日志。 琐事的度量 运维工作是辛苦的。如果你做了一些工作减少了琐事，如何知道你的努力是成功的？许多SRE团队是结合经验和直觉来回答这个问题。经验和直觉会产生好的效果，但是我们还可以将方法上升到一个理论的维度。 经验和直觉是因人而异、非客观的。根据场景的不同，琐事的定义也不同。比如，同一团队的不同成员会根据工作的投入产出比来判断一件事情是否可以定义为琐事。此外，为了减少琐事所做的工作可能会持续几个季度甚至几年的时间(本章的一些案例研究就证明了这一点)，在此期间团队的人员主要任务可能会发生改变。所以，为了保证减少琐事的工作能够长期进行，一般的，团队必须从几个确定的琐事中选择一个琐事来消灭它。我们应当将这件事上升为一个项目，并且需要建立起这个项目的长期的客观的度量机制以保证投入得到回报。 在启动项目之前，重要的是分析成本与收益，并确认通过减少琐事所节省的时间(至少)与第一次开发和维护自动化解决方案所投入的时间成正比(图6-1)。从节省的时间与投入的时间的简单比较来看，那些看起来“无利可图”的项目可能仍然值得进行，因为自动化有许多间接或无形的好处。潜在的好处包括： 随着业务规模扩大，收益越明显 提高团队士气，减少团队流失和成员的厌倦情绪 更少的中断性工作，从而提高团队工作效率 提高流程清晰度和标准化 增强团队成员的技术技能和拥有更全面的职业发展 缩短新成员的培训时间 减少人为错误导致的问题 提高安全性 缩短用户投诉的响应时间 图 6-1 预测在减少琐事工作上花费的时间，并确保其收益大于投入 琐事的度量方法 识别它。第一本SRE书的第5章提供了如何识别琐事。最能够识别琐事的人取决于团队本身。理想情况下，SRE团队既是利益相关方，也是实际操作方。 选择适当的计量单位来量化人力成本。我们可以选择“分钟”或者“小时”这么一个客观和普遍能够理解的计量单位。务必还要考虑琐事转自动化的成本。有些人力成本具有分散性和碎片化的特征，所以我们从成员工作的内容来衡量更为合适。度量单位应该要能够很好的度量如下工作：为应用增加的补丁，完成的票证，手动生产环境的变更，电子邮件交换或者是一些对硬件的操作。总的来说，只要度量单位客观，一致且易于理解，它就可以作为工作的衡量标准。 在项目的整个周期内我们需要连续跟踪并记录度量的指标。我们可以使用工具或脚本来简化度量指标的测量过程，使得收集这些测量值不会产生额外的工作。 琐事分类法 琐事，就像一座摇摇欲坠的桥梁或一座漏水的大坝，日复一日地隐藏在广阔无垠的大地之中。本节中的分类并不能够详尽无遗，但代表了一些常见的琐事类别。这些类别中有许多类似“正常”的工作，但是它们实际上就属于琐事。 商业流程 这可能是最常见的琐事来源。也许你的团队管理一些计算机资源——计算、存储、网络、负载平衡器、数据库等，以及为该资源提供支持的硬件资源。你需要处理用户登录、配置修改和计算机安全维护、软件更新以及扩缩容。你还需要最大限度地降低成本避免计算机资源的浪费。你的团队是计算机的人机界面，通常与为其需求提交票证的内部客户进行交互。你的组织甚至可能拥有多个票务系统和工作系统。 票务系统属于“隐藏”一类的琐事，因为其驱动的业务流程通常是我们需要完成的目标。用户得到了他们想要的东西，并且因为琐事往往分散在整个团队中，所以琐事并不能明显地显现出来。在以票据驱动的任何地方，都有可能悄悄地积累这琐事。即使你没有明确的自动化流程，仍然需要执行流程的改进工作，例如简化流程，使其未来更容易做到自动化，同时更加容易管理。 工作中断 中断是一类为了保证系统运行的时间敏感类任务，简单理解为被其他紧急事情打断。例如，你可能需要通过手动释放磁盘空间或重新启动泄漏内存的应用程序来解决某些资源（磁盘，内存，I/O）的严重短缺。你可能正在提交更换硬盘驱动器，“踢”出无响应的系统或手动调整容量以满足当前或预期的负载请求。通常，中断会将注意力从更重要的工作上移开。 流程监督 在许多组织中，部署工具从发布到生产需要SRE进行监督。即使有自动化，全面的代码覆盖，代码审查和多种形式的自动化测试，这个过程并不总是顺利进行。根据工具和发布节奏，发布请求、回滚、紧急补丁以及重复或手动配置更改，发布仍产生琐事。 服务迁移 服务迁移也是我们经常要处理的一类事情。你可以手动或使用有限的脚本来执行此工作，而且希望只迁移一次。迁移有多种形式，包括有数据存储、云供应商、源代码控制系统、应用程序库和工具的更改。如果你手动迁移大规模的工程，迁移很可能涉及到“琐事”。对于大规模的迁移，你可能倾向于手动执行迁移，因为这是一次性的工作。并且我们甚至会将其视为“项目”的一部分而非“琐事”，但迁移工作的很多特征与“琐事”的特征是吻合的。从技术上讲，修改一个数据库的备份工具以便与另一个数据库可以协同工作是软件开发的范畴，但这项工作本质上只是重构代码，用一个接口替换另一个接口。这项工作是重复的，并且在很大程度上，备份工具的业务价值与之前是相同的。 压缩成本和容量规划 无论是拥有硬件还是使用基础架构提供商（云），压缩成本和容量规划通常是一些劳动密集型的工作。例如： 在计算、内存或IOPS（每秒输入/输出操作）等资源的未来规划中要确保成本效益和突发情况的扩容能力。这可能转化为采购订单，AWS预留实例或云/基础设施即服务合同协商。 应对（并从中恢复）关键的高流量事件，如产品发布或者遇到假期。 排查下游和上游服务水平和容量情况。 根据专有云服务产品的计费细节优化应用程序（适用于AWS的DynamoDB或适用于GCP的Cloud Datastore）。 重构工具以便更好地利用现有资源。 处理超预算的资源，无论是基础设施提供商的上游还是与下游客户之间。 黑盒系统故障排除 分布式微服务架构现在很常见。随着系统更加分散，出现了新的故障模式。团队可能没有能力来构建复杂的分布式跟踪，高可靠监控或详细的仪表盘。即使企业确实拥有这些工具，它们也可能不适用于所有系统。故障排除甚至可能需要登录到各个系统并使用脚本工具来对日志进行实时地查询分析。 故障排除本身并不是坏事，但你应该把精力集中在新的故障模式上，而不是每周都发生的由脆弱系统架构导致的故障。随着可用度为“P”的新关键上游依赖性服务的上线，系统可用性将下降（1-P）倍。一个可用度为4个9的服务增加了9个关键的4个9的核心组件，现在就变为了是一个三个9的服务。 琐事管理战略 任何规模的生产系统，琐事管理都是至关重要的。一旦确定并量化了琐事，消除琐事的计划就要提上日程。这个工作可能需要数周才能完成，因此制定一个完善的计划是至关重要。首先，从源头上消除琐事是最佳的解决方案，但是对于源头上无法消除的琐事，则需要通过其他方式来消除。在我们深入研究两个案例之前，本节提供了此方面工作的通用性准则。正如下文的两个案例中提到的，琐事的细微差别是因团队而异。但无论如何，一些常见的准则是适用于任何规模或风格的组织。在后续案例中将以具体方式诠释每种策略。 琐事的识别与度量 采用数据驱动的方法来识别琐事，并配合客观的成本控制策略，获得此类项目最优的投入产出比。如果你的团队正在被琐事缠身，并将减少琐事作为了一个长期的项目。Google SRE团队根据多年的经验，在控制项目投入产出比方面是一个不错的借鉴。有关技术和指导，请参见第96页的“量化琐事”一节。 让SRE从琐事中解脱出来 减少琐事的最佳策略是从源头杜绝琐事。在进行系统设计和为生产环境制定流程之前，工程师要优化产品和系统来减少甚至消除琐事。 真正了解生产环境痛点和知道导致系统出现琐事原因的那部分人正是SRE，因为只有他们和生产环境紧密联系。SRE应该在与产品开发团队合作的过程中，将自己的运维经验与产品开发团队共享从而开发出人机交互友好型的软件，从源头减少琐事，并且使产品具有更好的扩展性和弹性。 拒绝琐事 一个被琐事缠身的团队应该尽早的做出“消除琐事”决策。第一种策略是对琐事说“不”！对于每个琐事，量化它并以此为原则决定是否要做，但是根据Google的经验，这一种策略可能会适得其反。另一种策略是故意拖延这些琐事，直到我们可以通过批处理或并行处理来解决它。将琐事集中在一起一并处理它们，这种方式可以减少工作中的中断，并帮助你们识别琐事的特征，并将它们作为下一个消除目标。 使用SLO减少琐事 如第2章所述，服务系统应具有文档化的SLO。明确定义SLO才能使工程师做出明智的决策。例如，如果某项工作即使做也不会减少服务的错误预算，你就可以考虑忽略某项工作。随着服务的增长，专注于整体服务的可用性而不是单个设备的SLO，这样做是非常有利的，也是可持续的。有关编写有效SLO的指导，请参阅第2章。 从部分自动化开始 如果你的业务特别复杂，请将“部分自动化”方法视为实现“完全自动化”的临时步骤。在这种方法中，你的服务通常可以通过定义的API接收结构化数据。工程师也可以进行一些操作从而得到想要的结果。虽然这样做需要一些手动的操作，但是这种“幕后工程师”方法是逐步实现全自动化的前提。使用“客户端输入”来统一收集数据；通过确定的请求格式，你可以更容易的以编程的方式对请求进行处理。这种方法让客户也能够明白你需要的信息和指标，并在你完全理解系统服务之前避免使用大型的解决方案而产生的未知问题。 提供一种自助的服务方法 一旦你们提供了交互型界面的服务产品，请进一步的为用户提供自助式的服务方法。你可以提供Web表单、二进制、脚本、API，甚至只是告诉用户如何向服务的配置文件发出拉取请求的文档。例如，软件开发工程师要求SRE工程师为其开发工作配置新虚拟机，我们为他们提供一个简单的Web表单或脚本来触发配置，而不是让他们提交相关票证来进行这件事。如果发生了特殊的情况，我们也允许使用“票证”的方式替代自助的服务，这是可接受的。部分自动化是一个良好的开端，但服务SRE工程师应该始终要致力于尽可能让服务自动化起来。 获得管理层和同事的支持 在短期内，减少琐事的项目需要投入人力成本，反之会减少处理其他日常任务的人员数量。但长远来看，如果项目达到了减少琐事的目标，团队将更加健康，并有更多的时间进行更重要的工程改进。对于团队中的每个人来说，“减少琐事”作为一个共同的价值目标是很重要的。管理层的支持对于减少工程师的干扰是至关重要。制定琐事评估的客观指标来说明项目的推进情况可以让管理层更加支持项目的进行。 减少琐事作为提高服务稳定性一部分 要为减少琐事的项目创建一个强大的业务案例支持，将你的目标与其他业务目标相结合。如果有一个补充性的目标，例如，安全性、可扩展性或可靠性——这对客户来说是具有吸引力的，他们会更愿意放弃当前充满琐事的系统，转向更加亮眼的新系统。这样来看，减少琐事也可以提高用户服务的质量，这也是另一个角度来看待琐事的认识。 从简单的琐事开始并持续改善，不要试图设计没有琐事的系统。面对一个充满琐事的系统，首先自动化一些高优先级的项目，然后通过评估这个项目所花费的时间来改进你的解决方案，总结获得的经验和教训。在项目开始之前，选择一个明确的指标，如MTTR（平均修复时间）来评估你的项目的进展和效果。 提高系统的一致性 从规模上看，多样化的生产环境是难以管理的。特殊的生产环境容易出错，管理能力会降低，事故的处理能力也会降低。你可以使用“宠物与牛”方法（https://www.engineyard.com/blog/pets-vs-cattle，译者注）来添加系统冗余并在你的生产环境中实施增加一致性的策略。是否选择“牛”取决于组织的需求和规模。将网络链路、交换机、机器、机架，甚至整个集群评估为可互换单元也是合理的。将设备转换为“牛”的理念可能会带来较高的初始成本，但会减少中长期的维护成本，增强灾难恢复能力和提高资源利用能力。为多个设备配置相同的接口意味着它们具有相同的配置，是可互换的，维护成本也就降低了。各种设备的界面一致（转移流量，恢复流量，执行关机等）使系统更加灵活和更加可扩展。Google鼓励各团队将不断发展的内部技术和工具进行统一，并有相应的鼓励机制。无论团队用什么样的方法，但他们不得不承认一些不受支持的工具或遗留的系统是产生琐事的根源。 评估自动化带来的风险 自动化可以节省人力成本，但是也会出现未知的错误，严重时会造成停机。一般情况下，防御性软件可以控制这类事情的发生。当管理级别的行为被自动化之后，防御性软件会显得至关重要。在执行前应对每项行为的安全性进行评估。在实施自动化时，我们建议采用以下做法： 防御性地处理用户输入，即使这个输入来自于上游的系统 ——换句话说，要对上下游的输入进行仔细的校验。 构建告警机制，使得工程师可以接收到相关告警以进行处理。安全措施可能与命令超时一样简单，也可能是对当前系统指标或当前中断次数的更复杂检查。因此，监控，报警和仪表系统应由机器和操作人员共同使用。 请注意，即使是简单的读取操作也可能会导致设备负载过高和触发服务中断。随着自动化的扩展，这些安全检查的工作量是可控的。 最大限度地减少因自动化安全检查不完整导致服务中断的影响。如果操作员遇到不安全的情况，自动化操作应该默认为人工操作。 琐事自动化之后要做什么 一旦你可以将一个工作自动化后，这个自动化的工作就值得更深层次的被发掘。进一步的将自动化的任务按照人工处理的流程优化下去。但请注意，自动化不应该让工程师认为任务不会出错。在完成上述优化后，你还可以尝试将自动化的工作分解为可单独实现的组件，并用于创建可组合的软件库，其他自动化项目可在以后重复使用。正如下文中的“数据中心维修案例”研究所示，自动化提供重新评估和简化人工工作流程的机会。 使用开源和第三方工具 有时你不必做所有的工作来减少琐事。像一次性迁移这样的工作可能自己无法建立定制型的工具，但你可能并不是第一个遇到这个任务的工程师。寻找第三方或开源库以降低开发成本，或者说，至少可以帮助你过渡到部分自动化。 反馈并改进 积极寻求反馈，这些反馈可以来自于工具、工作流程和自动化交互相关的其他人，这是非常重要的。你的用户将根据他们对底层系统的理解将你的工具在不同使用情景下进行使用。你的用户对这些工具越不熟悉，就越要积极地寻求用户的反馈。利用用户调查，用户体验（UX）和其他机制来了解你的工具被如何使用，并整合这些反馈，以便在未来实现更有效的兼容性。 人的输入只是你应该考虑反馈中的一个方面。我们还可以根据延迟，错误率，返工率和节省的人工时间等指标（跨过流程中涉及的所有组）来衡量自动化任务的有效性。能够获得在自动化工作部署之前和之后两种状态的对比是最明确的衡量方式。 扩展：历史遗留系统 大多数SRE工程师在他们的工作中都会遇到过历史遗留系统。这些旧系统经常在用户体验，安全性、可靠性或可伸缩性方面有问题。他们倾向于将遗留系统看作一个神奇的黑匣子，因为系统“大部分组件是在工作中的”，但很少有人了解它们是如何工作的。贸然的调整它们是可怕的，也是昂贵的，并且保持它们的运行通常需要大量繁琐操作步骤。 远离遗留系统通常遵循以下路径： 避免：我们可以为不去解决这个问题找到许多理由：可能是没有资源来替换这个系统；判断业务成本和风险发现不值得替换；可能没有找到商业上更好的解决方案。避免选择的是接受风险并从SRE转向系统管理。 封装/扩充：你可以使用SRE来构建一个抽象API的外壳，自动化，配置管理，监视和测试这些遗留系统，这些系统将卸载SA的工作。遗留系统仍然很难改变，但现在你至少可以识别它并在适当时有回滚策略。这种策略仍然可以避免，但这是将风险引入到的更好的系统中。这通常是准备增量替换的权宜之计。 替换/重构：替换遗留系统可能需要大量的决心、耐心、沟通成本和文档，最好是逐步进行。一种方法是定义遗留系统公共接口。此策略可帮助你使用发布的工程手段，将用户缓慢、安全地迁移到其他安全的架构中。通常，遗留系统的“规范”实际上只是通过其历史用途来定义，因此有助于构建生产大小的历史预期输入和输出数据集，以建立新系统不会偏离预期行为的信心（或正在以预期的方式发散）。 退出/保管所有权：最终，大多数客户或功能被迁移到一个或多个系统。这个迁移需要有激励措施，没有迁移的用户让他们自行维护历史遗留系统，并承担相应责任。 案例研究 案例研究1：利用自动化减少数据中心的工作量 案例研究1中所应用的减少琐事的战略： SRE工程师从琐事中解脱出来 从部分自动化开始 提高系统的一致性 使用SLO减少琐事 评估自动化带来的风险 反馈并改进 提供一种自助的服务方法 背景 此案例来源于Google数据中心。与其他的数据中心类似，Google的计算机连接到交换机，交换机连接到路由器。流量通过链路流入和流出这些路由器，而链路又连接到互联网上的其他路由器。随着谷歌对互联网流量的要求越来越高，服务该流量所需的交换机数量也急剧增加。为了能够应对大流量的情况，我们的数据中心在规模和复杂性方面都有所增长。这种增长迫使数据中心改变了手动维修的旧方法。（从偶尔和有趣到频繁和沉闷的转变。） 早期，谷歌在运行数据中心时，每个数据中心的网络拓扑都只有少量的网络设备，可以管理大量服务器的流量。单个网络设备故障可能会显著影响网络性能，但是一个小规模的工程师团队就可以处理设备的故障。早期，工程师调试故障设备并手动将流量切换到其他正常组件。而我们下一代的数据中心拥有更多的机器，并引入了折叠Clos拓扑结构的软件定义网络（SDN），交换机数量显著增加。图6-2展示的是一个小型数据中心Clos交换机网络的流量复杂情况。如果将这个比例放大，意味着设备数量更多，发生故障的组件也更多。虽然可以说，每个单独的故障对网络性能的影响比以前更小，但是大量的问题同时并发也会压倒工程师们。调试问题的过程同时也会引入大量新的问题，复杂的布局也让工程师感到困惑：需要检查哪些链接？需要更换哪个线卡？为什么是Stage 2开关，而不是Stage 1或Stage 3开关？关闭交换机会给用户带来哪些问题？ 图6-2. 一个小型Clos网络，Stage1 支持480台机器连接 修复故障的线卡是一个随着系统网络增长而任务量不断增长的琐事，因此我们将此作为“数据中心网络修复自动化”项目的第一阶段的目标。本案例阐述了我们如何在第一代线卡（名为Saturn）系统上开始自动化修复的过程，并以此为基础，我们讨论了如何对自动化工作进行改进以适应下一代线卡(Jupiter光纤网络)。 如图6-3所示，在自动化项目开始之前，数据中心线卡修复工作需要工程师执行如下几个操作： 确定从故障交换机切走流量是否是安全的。 切走流量至其他交换机（“drain”操作）。 执行重启或修复（例如更换线卡）。 将流量切回至该交换机（“undrain”操作）。 Drain，更换线卡，undrain的工作是不变和重复性质的，是“琐事”的典型范例。这些重复性的工作本身就会带来一些问题——例如，工程师在处理此类故障时会并行处理其他更有挑战性的工作，分心的工程师可能会意外地将未配置的交换机加入网络。 图6-3. 自动化之前的数据中心（Saturn）线卡修复工作流程：所有步骤都需要手动工作 问题陈述 数据中心修复线卡问题具有以下几个维度： 团队规模增长的速度跟不上系统增长的速度（故障数量也在增长），使得我们无法快速解决问题以防止对系统带来负面影响。 人为错误一定会在重复执行的步骤中发生。 并非所有线卡故障的影响都是一致的。我们没办法对线卡故障划分优先级。 一些故障是暂时的，这时我们会选择直接重新启动线卡或重新安装交换机作为修复过程的第一步。并且，我们可以用编程方式捕获这些问题，如果它再次发生，则进行设备替换。 新的拓扑环境要求我们在采取行动之前手动评估隔离容量的风险。每次的人工风险评估都有可能带来人为错误，并可能带来严重影响。系统工程师和技术人员也没有好的方法来判断有多少设备和链接会受到修复过程的影响。 我们要如何解决这个问题？ 为达到最好的效果，我们决定创建一个与现场技术人员配合使用的自动化框架，而不是把每个问题分配给工程师，让其进行风险评估，流量切换，维修和验证等人工操作。 自动化的第一步：Jupiter光纤网络的修复自动化 我们的最终目标是构建一个能够代替工程师分析和处理故障的网络设备故障检测系统。我们的程序是直接切换流量并告知工程师，而不是向工程师发送“线卡”故障的告警。新系统有一些值得注意的特点： 我们最好是利用现有工具。如图6-3所示，我们的告警已经可以检测到线卡上的问题; 所以我们可以配置告警以触发自动修复。新的工作流程还应改变工单系统，以便支持自动提交的维修请求。 我们建立自动风险评估的机制，以防止在流量切换期间意外的隔离设备，并在需要时触发安全机制。此机制可以杜绝人为错误。 我们编写程序用于跟踪告警以便作出不同的处理操作：第一次告警仅重启该线卡并重装了软件；第二次出现告警则直接请求更换线卡并告知供应商。 执行自动化操作 新的自动化工作流程（如图6-4所示）进行如下： 检测到有问题的线卡，并将故障特征添加到数据库中。 维修服务组件会解决问题并对交换机进行维修。该服务还会执行风险评估以确认操作不会隔离任何容量，然后： a. 从故障交换机中切出流量。 b. 关闭线卡。 c. 如果这是第一次告警，则重新启动线卡，将流量恢复到此交换机。此时，工作流程已完成。 d．如果这是第二次失败，则工作流程进行到步骤3。 流程管理器检测到新案例并将其发送到故障维修池，供系统工程师处理。 系统工程师对故障做出响应，在UI界面中看到红色的“停止”（表示在开始修理之前需要切走流量），并分三步执行修复步骤： a. 系统工程师通过UI界面中的“准备组件”按钮启动流量切换。 b. 流量切换完成后表示交换机可操作。 c. 关闭交换机并维修线卡。 自动修复系统再次启动线卡。完成修复后，启动交换机，待初始化后，流程管理器会触发恢复操作，切回交换机流量并结算故障工单。 图6-4.具有自动化功能的Saturn线卡维修工作流程：只需按下按钮即可完成更换线卡等全部手动工作 新的自动化系统将团队从大量的琐事中解放出来，使他们有更多时间在其他地方开展更高效的项目：使用下一代Clos拓扑结构Jupiter。 自动化项目的第二步：Saturn线卡修复与Jupiter线卡修复 数据中心的容量需求几乎每12个月翻一番。因此，我们的下一代数据中心结构Jupiter比Google以前的任何数据中心的六倍还要大，所以故障的数量也会增加六倍多。Jupiter提出了自动化故障修复的挑战目标，这个目标的难度在于每层的数千个光纤链路和数百个线路卡都可能出现故障。幸运的是，随着潜在故障点的增加系统也会伴随增加更多的冗余，这有利于我们完成自动化任务。如图6-5所示，我们保留了系统的一些常规工作流程，并添加了一些重要的修改： 在自动切流量和关闭交换机之后，确定我们要更换的硬件，将硬件故障单发送给系统工程师。在这个过程中，切流量的行为是自动的，不需要系统工程师手动按下“预备按钮切流量开关”来完成。 我们添加了自动化，用于安装和推送组件更换后的配置。 我们启用自动化功能，以便在切回流量之前验证修复是否成功。 除非绝对必要，否则我们更关注的是如何恢复流量而不用人为介入。 图6-5.左图为Saturn线卡宕机自动化流程，右图为Jupiter线卡宕机自动化流程 项目实现 我们为Jupiter交换机上的所有的线卡故障采用了简单而统一的工作流程：操作通报，流量切换，开始修复。 自动化执行如下： 检测到交换机故障，并将故障特征写到数据库。 维修程序开始修复交换机：停止使用交换机，并将停止原因写到数据库中。 a. 如果这是六个月内的第二次故障，请执行步骤4。 b. 否则，请执行步骤3。 尝试（通过两种不同的方法）重启交换机。 a. 如果重启成功，用自动化服务检查健康状态，然后安装并配置交换机使其投入使用；删除修复原因，删除数据库中的故障记录。 b. 如果健康检查失败，请升级给技术人员。 如果这是第二次故障告警，请将故障案例直接升级给技术人员，向其申请新的硬件设备。硬件更新后，用自动化服务检查健康状态，然后安装并配置交换机使其投入使用。删除修复原因，删除数据库中的故障记录。 这种新的工作流程管理完全重写了以前的修复系统。同样的，我们要尽可能利用现有工具： 配置新交换机（安装和验证）的操作与验证已更换的交换机所需的操作相同。 快速部署新的硬件需要以编程的方式进行BERT和cable-audit的能力。在恢复使用之前，我们可以使用该程序在已经修复的链路上运行功能测试。这些测试需要能够识别错误链接以进一步提高修复的效果。 下一步要提升的是自动缓解并修复Jupiter交换机线卡的内存错误。如图6-6所示，在开始自动化修复之前，此工作流程在很大程度上取决于工程师来判定故障是硬件导致还是软件导致，然后再停止使用，重启交换机等工作，并适时地安排修复。 图6-6. 自动化之前的Jupiter内存错误修复工作流程 我们的自动化过程不再尝试对内存错误进行故障排除从而达到简化修复工作流程的目的（请参阅第119页的“有时不完美的自动化就足够了”，了解为什么这样做是有意义的）。相反，我们处理内存错误的方式与处理线卡故障的方式相同。为了将自动化覆盖到内存错误引起的故障，我们只需在配置文件中添加一个特征，使其对新的故障类型起作用。 图6-7描述了内存错误的自动化工作流程。 经验教训 在我们致力于实现网络故障自愈的这些年里，我们学会了如何有效减少琐事。 UIs 不该引入开销和复杂度 替换一块Saturn-based线卡需要切走整个交换机的流量。等待备件更换以及工程师支持的时候，过早地执行全部切换操作意味着失去所有线卡的工作能力。我们在UI中增加一个“准备组件”的按钮，以允许技术人员在更换线卡前执行整个交换机的切换流量操作，从而消除了交换机不必要的停机时间（请参阅“按下准备按钮” 切出流量的开关“见图6-5） UI和维修工作的流程引入了许多非预期的问题 按下切走流量的按钮后，技术人员无法得到流量切换进度的反馈，只能在结果返回后才能进行下一步操作。 该按钮可能无法反馈真实的状态。造成的结果是，有时切流量开关出问题但并没有被维修，或者技术人员可能通过其他方式中断了进程但是并没有告知系统。 问题出现时，非自动化的组件反馈了一个通用的‘contact engineering’信息。经验不丰富的技术人员无法快速找到可以提供帮助的人，而联系上的工程师并不总能够立即解决问题。 为快速对用户反馈以及因功能复杂性带来的回归问题进行响应，我们设计了更完善的工作流程，来保证按钮的安全性和可用性。 不要依赖人的经验 我们过分依赖有经验的数据中心技术人员来识别系统中的错误（例如，当程序认为可以安全地进行维修，但实际上交换机并没有完成流量切换的动作）。这些技术人员在没有自动化提示的情况下，还必须手动执行多项工作。 经验是难以复制的。在一个复杂的情节中，技术人员在等待数据中心维修时，决定启动并发切换来快速进行“按下按钮并等待结果”的操作，从而导致了网络拥塞和用户可见的数据包丢失。我们的软件无法预测并阻止这种行为，因为我们并没有测试过这种自动化。 设计可重复使用的组件 尽可能避免采用集成化设计。使用组件来构建复杂的自动化工作流，每个组件处理一个独特且定义明确的任务。我们可以轻松地重复使用或调整早期Jupiter自动化的关键组件来用于下一代的软件设计，并且很容易针对已经存在的自动化项目增加新的功能。Jupiter类结构的连续变体可以采用早期已经完成的工作。 不要过分分析问题 我们过度分析了Jupiter线卡内存错误问题。我们试图进行精确的问题诊断，我们想区分软件错误（可通过重新启动修复）与硬件错误（需要更换卡），并识别影响流量的错误与未发生的错误。我们花费将近三年（2012-2015）的时间来收集超过650个离散内存错误的数据，然后才意识到这个分析是过头了，或者至少不应该阻塞我们自动化项目的开展。 一旦我们决定对检测到的任何错误都采取必要的措施，就可以直接使用我们现有的自动化修复技术来实现简单的切换策略、重启以及为修复内存错误而重置交换机。如果问题再次出现，我们可以认为，故障很可能是基于硬件的，并立即要求更换组件。我们花费了整个项目四分之一的时间来收集数据，发现大多数的错误是暂时的 ——大多数交换机在重新启动和重新安装后都恢复了。我们不需要额外的数据来执行修复，因此为了实现这种自动化花费了三年是没有必要的。 有时不完美的自动化就已经足够 解除链路之前，通过BERT很容易确认链路状况，但BERT工具不支持网络管理链路。我们将这些链路添加到现有的链路修复自动化中，并允许跳过验证。我们很愿意绕过验证，因为链路并没有承载客户流量，如果验证结果很重要，我们可以稍后添加此功能。 保证维修自动化项目的持续性和可继承性 自动化项目可以有很长的生命周期，需要确保人员的流动不会干扰项目的连续性。 新人工程师应该接受现有系统的培训，以便他们能够修复错误。由于Jupiter线卡部件的短缺，Saturn-based系统在其目标寿命结束后很长一段时间内还是存在的，这要求我们日后在Saturn的生命周期中进行一些改进。 自动化一旦被采用，在很长的一段时间内将会被依赖使用，并伴随着一些积极和消极的后果。如果可能，以灵活的方式设计你的自动化程序。不灵活的自动化会使系统变更变得难以实现。使用基于策略的自动化可以明确地将意图与通用实现引擎分离，从而使自动化更加可持续的发展。 深入开展风险评估和防御措施 为Jupiter构建新工具以评估执行切流操作前的风险，而后由于问题的复杂性，我们需要在更深层次的防御上引入二次检查。二次检查设定了受影响链路数量的上限，以及受影响设备的额外限制。一旦超过任一限定值，便会自动触发追踪bug以请求更进一步的检查。我们不断地调整这些限制，以减少误报。最初我们认为二次检查只是一项临时措施，但是在主要风险评估平稳后，该措施已被证明可用于识别由于停电和软件错误导致的维修问题（如请参阅SRE中“自动化：在规模上实现失效”）。 失败预算和管理者支持 修复自动化有时会失败，尤其是在首次使用时。管理者的支持对于保护项目，并鼓励团队坚持不懈是至关重要的。我们建议为通过自动化技术消除琐事项目设置错误预算。你还需要向外部的其他利益方解释：尽管存在故障风险，但自动化极其重要，并可以持续提高可靠性和效率。 总结 最终，复杂场景的自动化是真正需要解决的问题。在引入自动化系统之前要反复对系统进行评审——是否可以先简化系统和工作流？ 要关注自动化工作流程的各个方面，而不仅仅是造成琐事的那部分。和直接参与项目的人员共同开展测试工作，并积极寻求他们的反馈和帮助。如果他们在使用过程中出现操作问题，要想办法使工作界面更清晰，或者增加额外的安全检查。确保自动化不会带来额外的琐事——例如开启了不必要的工单以引起人的注意。给其他团队创造问题将增加自动化推进的难度。 案例研究2：淘汰以文件为后端的Home directories 案例研究2中强调了减少琐事的方法： 考虑淘汰旧系统 将减少琐事作为一项工程 获得管理层和同事的支持 拒绝琐事 从部分自动化开始 提供一种自助的服务方法 从细微处开始然后改进 反馈并改进 背景 在谷歌的早期，公司数据存储（CDS）SRE团队为所有Google员工提供home目录服务。与企业IT中常见的Active Directory漫游配置文件类似，Google员工可以跨工作站和平台使用相同的home目录。CDS团队还为共享存储空间中的跨团队协作提供“团队共享”服务。我们通过NFS / CIFS（或“文件管理器”）上的Netapp存储设备提供home目录和团队共享。这种存储系统是很昂贵的，但Google员工对此类服务的需求是必须的。 问题陈述 随着时间的推移，这些文件管理系统解决方案的优势被其他更好的存储解决方案所超越：我们的版本控制系统（Piper / Git-on-borg），Google Drive，Google Team Drive，Google云存储以及全球内部共享分布式ilesystem（x20）。这些替代方案的优越性体验在如下方面： NFS / CIFS协议并不适用于在WAN上运行，这造成即使有几十毫秒的延迟，用户体验也会迅速降低。这也为远程工作人员或全球分布的团队带来了问题——因为数据只能存在于一个地方。 与替代品相比，原系统的设备运行和规模都是昂贵的。 要使NFS / CIFS协议与Google的Beyond Corp11网络安全模型兼容，需要做大量的工作。 与本章最相关的是，home目录和团队共享会频繁的使用。存储配置的许多方面都是ticket驱动的。虽然这些工作流程通常是研发人员编写的，但它们代表了相当数量的CDS团队的工作成果。我们花了很多时间创建和配置共享，修改访问权限，解决最终用户问题，以及执行启动和调整以管理容量。除了配置、更新和备份之外，CDS还需要管理专用硬件的配置，机架和布线过程。由于延迟要求，我们经常不得不部署在远程办公室而不是在Google数据中心 – 这有时需要花费团队成员相当长的时间。 我们决定做什么 首先，收集数据：CDS团队开发了一个名为“Moonwalk”的工具来分析员工使用此的服务的场景。我们确定收集如下通用的指标，如每日活跃用户（DAU）和月活跃用户（MAU），并询问了诸如“哪些用户实际使用他们的home目录？”和“哪些人每天使用此系统？他们最常访问的文件是什么？“Moonwalk与用户调查相结合，验证了文件管理器当前服务的业务需求可以通过低运营开销和成本的可替代方案代替。另一个引人注目的原因促使我们放弃现有的文件系统：如果我们可以将大多数文件管理器用例迁移到G Suite / GCP，那么我们可以利用我们学到的经验来改进这些产品，从而为其他大型企业迁移到GSuite/ GCP提供支持。 没有一种替代方案可以满足所有当前的文件管理器用例。然而，通过将问题转化为若干小的需求来寻找可替代系统，我们发现少数备选方案可以涵盖我们所有的使用场景。替代解决方案更专业，并且每个解决方案都带来比旧的解决方案更好的用户体验。例如： x20 全局共享静态文件对于团队来说是很好的方式，比如二进制文件。 G Suite Team Drive 适用于办公文档协作，与NFS相比，用户更能容忍此延迟。 谷歌的巨像Colossus文件系统 比NFS更安全，更可靠地共享大型数据文件 Piper/Git-on-Borg 可以更好地同步dotfiles（工程师的个性化工具首选项） 一种新的“历史服务”工具 可以托管跨工作站命令行的历史记录 在我们编制用例并找到替代方案时，旧文件系统的下线计划也已经开展 设计与实施 下线旧的文件管理系统是一项持续的、迭代的、需要多年的进行的工作。需要伴随多个子项目的开展： Moira home目录下线 Tekmor 迁移home目录用户的历史遗留数据 Migra 团队共享下线 Azog 下线home目录/共享基础架构和相关硬件 图6-8。Moira项目的四个阶段 本案例研究重点关注第一个项目Moira。后续项目的开展是在Moira的学习和开展的基础上开始的。如图6-8所示，Moira由四个阶段组成。 关键组件 Moonwalk 虽然我们有关于用户共享（例如共享大小）的基本统计数据，但我们仍需要了解用户的工作流程，以帮助用户即使在一片反对声中仍然可以做出有利于业务发展的决策。我们建立了一个名为“Moonwalk”的系统来收集和反馈这些信息。 Moonwalk存储了谁正在访问哪些文件以及何时使用BigQuery的数据，这使我们能够做出统计报告以便更好地了解用户。在BigQuery的帮助下，我们汇总了25亿个文件，共计300 TB的数据的用户访问模式。该数据来自于全球60个地理站点的124个NAS设备，共计600,000个磁盘卷，共收集了60,000名POSIX用户的使用信息。 Moira Portal 通过工单处理来完成home目录下线的想法，在我们庞大的用户基数面前看起来不太现实。我们需要在整个过程中（调查用户，告知项目下线原因，归档数据或迁移到替代系统）尽可能提供低接触服务（低接触服务是指这样的服务模式：销售服务人员在向顾客提供服务时，保持较少的面对面的接触机会。相对于高接触服务而言，低接触服务需要更多的机器和固定资产。因为通常需要由它们来自动完成顾客服务，如自动售货机、自动柜员机、自动加油机等，译者注）。我们的最终要求是： 描述项目的登录页面 不断更新对常见问题的解答 与当前用户共享关联状态和使用信息 提供请求，停用，存档，删除，扩展或重新激活共享的选项 至此，我们的业务逻辑变得相当复杂——因为我们必须考虑许多用户场景。例如，用户可能会从Google离职，短暂离职，或者在诉讼状态（需要保留其拥有的数据）。图6-9提供了一个示例图，说明了其复杂性。 图6-9。基于用户场景的业务逻辑 为主门户网站提供支持的技术相对简单。基于Flask框架下用Python语言编写，它读取并写入Bigtable，并使用大量后台作业和调度程序来管理其工作。 归档和迁移自动化 我们需要大量的辅助工具来将门户网站和配置管理组合在一起，并与用户进行通信来进行用户查询。我们还需要使用沟通技巧来鉴别出适合进行数据迁移的用户。误报（错误地报告所需行动）或漏报（未通知用户您正在取走某些东西）都是不可接受的，这里发生错误将意味着失去用户可信度和带来客户服务的额外工作。 我们与其他存储系统所有者合作——为新系统添加我们需要的功能。因此，随着项目的进展，不太成熟的替代品变得更适合了。我们还可以使用和扩展其他团队的工具。例如，我们使用其他团队内部开发的工具将数据从Google云端存储迁移到Google云端硬盘，作为门户网站自动存档功能的一部分。这项工作需要在整个项目期间进行长期的软件开发。我们构建并迭代了每个组件–Moonwalk报告管道，门户和自动化，以便更好地管理下线和归档共享，以响应下一阶段的要求和用户反馈。我们在第三阶段（差不多两年）才接近达到一个功能健全的系统。即便如此，我们还需要额外的工具来处理大约800名用户的“长尾”。这种低速和慢速的方法有一定的好处。因为它允许我们： 维持一个精炼的团队（平均三名CDS团队成员） 减少对用户工作流程的干扰 减少Techstop的琐事（谷歌内部技术支持组织） 根据需要构建工具，以避免将时间浪费在工程工作中 与所有工程决策一样，存在如下权衡：项目将长期存在，因此团队在设计解决方案时必须忍受与文件管理器相关的琐事。 该计划于2016年正式完成。在撰写本文时，我们已将home目录从65,000个减少到约50个（目前的Azog项目旨在淘汰这些最后用户并彻底下线文件管理系统的硬件。）我们的用户体验有所改善，CDS已停止运营成本高昂的硬件。 经验教训 虽然没有任何替代方案可以替代Google员工已使用14年之久的文件管理系统，但我们并非没必要进行批量更换。通过有效地将堆栈从通用但有限的文件系统解决方案升级到多个应用程序共同组成的解决方案，我们增加了系统的灵活性，以提高可扩展性、延迟包容度和安全性。Moira团队不得不预测各种用户的行为，并考虑不同阶段的可替代方案。我们必须围绕这些替代方案来调整我们的期望：总的来说，它们可以提供更好的用户体验，但实现这一目标并非是毫无痛苦的。我们学到了以下关于有效减少琐事的策略。 发现旧系统的不足并下线昂贵的业务流程 业务需求不断变化，新的解决方案不断涌现，因此定期评估旧的业务流程是值得的。正如我们在第101页的“琐事管理策略”中所讨论的那样，拒绝琐事（决定不执行）通常是消除它的最简单方法，即使这种方法并不总是快速或简单的。通过用户分析和业务理由来调整你的业务工作内容，而不仅仅是减少琐事。文件管理系统下线的主要业务理由归结为Beyond Corp安全模型的优势。因此，虽然Moira是减少CDS团队琐事的好方法，但强调下线系统的原因如果是考虑到了新系统诸多的安全优势，这些优势将带来更具吸引力的业务需求。 构建自助服务接口 我们为Moira建立了一个自定义门户（相对昂贵），但通常有更便宜的选择。Google的许多团队使用版本控制来管理和配置他们的服务，以拉取请求（称为更改列表或CL）的形式处理请求。这种方法几乎不需要服务团队的参与，但为我们提供了代码审查和持续部署的优势，便于验证、测试和部署内部服务配置更改。 从人工支持的界面开始 在几个方面，Moira团队采用了“幕后工程师”的方法，将自动化与工程师的人工操作相结合。共享请求在路由过程中出现bug，我们的自动化在处理请求时会及时更新。系统还会通知到终端用户，提醒他们解决类似的共性问题。工单可以作为自动化系统的应急的GUI：它们保存工作日志，更新利益相关者的数据，并在自动化出错时提供简单的人工干预机制。在我们的示例中，如果用户需要获得数据迁移工作的帮助，或者如果自动化无法处理其请求，则该错误会自动路由到SRE手动处理的队列中。 零接触的自动化 自动化系统要求请求合规。Moira的工程师选择重新调整我们的自动化，以专门处理共享的边缘场景请求，或者删除/修改不合格的共享以符合系统的期望。这使我们能够在大多数迁移过程中实现零接触的自动化。 有趣的事实：在谷歌，通过改变现实去适应代码而不是通过修改代码去适应现实的方式被称为“购买侏儒”。这句话源自一个关于Froogle的故事，Froogle是一个很早就开展的购物搜索引擎服务。 在Froogle的早期阶段，发生过精确匹配搜索“跑鞋”关键字导致返回了garden gnome(花园侏儒, 穿着跑鞋）的严重错误。在几次尝试修复错误失败之后，有人注意到gnome不是批量生产的商品，而是一个带有“一口价”选项的eBay商品。他们购买了这个“花园侏儒”商品后，解决了这个返回错误搜索结果的问题（译者注：相比起修改代码，以更低的成本解决了问题）。（图6-10）。 图6-10. 不会消失的花园侏儒 推动新系统的使用 寻找方法来推动新用户采用更好的替代方案。在这种情况下，Moira需要升级系统配置以便应对新的请求和下线旧的系统。在服务设置中，新系统使用最佳实践和用户如何配置系统也很重要。Google团队经常使用codelabs或cookbook为用户提供常见用例设置和指导如何使用他们的服务。因此，大多数用户入手都不需要额外的团队的指导。 结论 与生产服务运行相关的琐事会随着系统复杂性和规模的增长而线性增长。自动化通常是消除琐事的黄金法则，并且可以与其他策略相结合。即使一些琐事没必要完全自动化，你也可以通过部分自动化或改变业务流程等策略来减少操作的负担。本章中描述的消除琐事的模式和方法可以推广到其他各种大规模生产服务中。消除琐事可以节省工作时间，以便工程师专注于服务的更重要的方面，并允许团队将手动任务保持在最低限度。随着现代服务架构的复杂性和规模不断增加，此策略尤其重要。 但是要注意，消除琐事并不总是最好的解决方案。如本章所述，你应该考虑成本，设计、改造和实施自动化解决方案都需要投入成本。一旦决定减少琐事，就必确立目标，进行投资回报率（ROI）分析，风险评估和迭代开发来确定是否减少了工作量。 琐事通常是从小事开始积累的，并且可以迅速成长最终消耗整个团队的人力资源。SRE团队必须坚持不懈地消除琐事——因为即使减少琐事的项目看起来令人生畏，但其好处通常也是会超过成本的。我们所描述的每个项目都需要各自团队的坚持不懈和奉献精神，他们有时会面对质疑或者需要与制度进行斗争，并且总是面临竞争这种高优先级的任务。我们希望这些案例鼓励你识别工作中的琐事，量化它，然后努力消除它。即使今天不能开展一个大项目，你也可以从一个小概念开始，这可以帮助改变你的团队处理琐事的方式。</summary></entry><entry><title type="html">第五章 SLO报警</title><link href="http://localhost:4000/sre/2020/01/05/SLO%E6%8A%A5%E8%AD%A6/" rel="alternate" type="text/html" title="第五章 SLO报警" /><published>2020-01-05T00:00:00+08:00</published><updated>2020-01-05T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/05/SLO%E6%8A%A5%E8%AD%A6</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/05/SLO%E6%8A%A5%E8%AD%A6/">&lt;!-- more --&gt;

&lt;p&gt;本章介绍如何在发生重要事件时将SLO转换为可进行配置的报警。我们的第一本SRE和本书都讨论了实施SLO。我们相信，拥有好的SLO可以衡量你平台可靠性，正如你的客户所经历的那样，可以为on-call人员该如何迅速做出响应提供最准确的提示。在这里，我们提供了有关如何将这些SLO转换为报警规则的具体指导，以便你在消耗过多的错误预算之前响应问题。&lt;/p&gt;

&lt;p&gt;我们的示例展示了一系列报警指标和逻辑的复杂实现；讨论他们的功能和缺点。虽然我们的示例使用的是简单的request-driven服务和Prometheus语法，但你可以应用到任何报警框架中。&lt;/p&gt;

&lt;h2 id=&quot;报警注意事项&quot;&gt;报警注意事项&lt;/h2&gt;

&lt;p&gt;为了从服务质量指标（SLI）和错误预算生成报警，需要一种方法将这两个元素组合成一个特定的规则。 你的目标是通知重大事件：消耗大部分错误预算的事件。&lt;/p&gt;

&lt;p&gt;在评估报警策略时，请考虑以下属性：&lt;/p&gt;

&lt;h4 id=&quot;精确度&quot;&gt;精确度&lt;/h4&gt;

&lt;p&gt;检测到的事件比例很重要。 如果每一个报警对应一个重大事件，则精度为100％。请注意，报警可能会在低流量时段对非重要事件变得特别敏感（在第86页的“低流量服务和错误预算报警”中讨论）。&lt;/p&gt;

&lt;h4 id=&quot;召回率&quot;&gt;召回率&lt;/h4&gt;

&lt;p&gt;检测到重大事件的比例。如果每一个重要事件都会发出一次报警，则召回率为100％。&lt;/p&gt;

&lt;h4 id=&quot;检测时间&quot;&gt;检测时间&lt;/h4&gt;

&lt;p&gt;在各种条件下发送报警通知需要多长时间。较长的检测时间会对错误预算产生负面影响。&lt;/p&gt;

&lt;h4 id=&quot;恢复时间&quot;&gt;恢复时间&lt;/h4&gt;

&lt;p&gt;解决问题后报警会持续多长时间。较长的恢复时间可能导致混淆或问题被忽略。&lt;/p&gt;

&lt;h2 id=&quot;重大事件的报警方法&quot;&gt;重大事件的报警方法&lt;/h2&gt;

&lt;p&gt;为SLO构建报警规则可能会变得非常复杂。在这里，我们提出了六种方法来配置重要事件的报警，以提高精度，得到一个可以同时控制精度、召回、检测时间和恢复时间这四个参数选项。以下每种方法都解决了不同的问题，有些方法最终同时解决多个问题。&lt;/p&gt;

&lt;p&gt;前三个不可行的方法对后三个可行报警策略的应用是十分有益的，方法6是最可行和最强烈推荐的选择。第一种方法实现简单但不充分，而最佳方法提供了一个完整的解决方案，可以在长期和短期内保护SLO。&lt;/p&gt;

&lt;p&gt;出于本节讨论的目的，“错误预算”和“错误率”适用于所有SLI，而不仅仅是名称中包含“错误”的SLI。在第20页的“测量内容：使用SLI”一节中，我们建议使用SLI来捕获正常事件与总事件的比率。错误预算给出允许的错误事件的数量，错误率=错误事件/总事件的比率。&lt;/p&gt;

&lt;h3 id=&quot;1-目标错误率slo阈值&quot;&gt;1. 目标错误率≥SLO阈值&lt;/h3&gt;

&lt;p&gt;对于最简单的解决方案，你可以选择一个小的时间窗口（例如，10分钟），并在该窗口内的错误率超过SLO阈值时发出报警。&lt;/p&gt;

&lt;p&gt;例如，如果SLO在30天内为99.9％，则在前10分钟的错误率≥0.1％时发出报警：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- alert: HighErrorRate
  expr: job:slo_errors_per_request:ratio_rate10m{job=&quot;myjob&quot;} &amp;gt;= 0.001
      
      这个10分钟的平均值是用Prometheus的记录法则计算出来的:

    record: job:slo_errors_per_request:ratio_rate10m
    expr:
        sum(rate(slo_errors[10m])) by (job) 
            / 
        sum(rate(slo_requests[10m])) by (job)
    
        如果你不从job中导出slo_errors和slo_requests，则可以通过重命名度量值来创建时间序列：
        record: slo_errors
        expr: http_errors
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;当最近的错误率等于SLO时发出报警意味着系统检测到以下预算消耗：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;           告警窗口/SLO期限
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/5-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图5-1 设有10分钟报警窗口和99.9％SLO的示例服务的检测时间&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;图5-1 显示了设有10分钟报警窗口和99.9％SLO示例服务的检测时间和错误率之间的关系。&lt;/p&gt;
&lt;center&gt; 表5-1 显示了即时错误率过高时发出警报的优缺点&lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;优点&lt;/th&gt;
      &lt;th&gt;缺点&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;检测时间良好：总停机时间为0.6秒(10&lt;em&gt;60&lt;/em&gt;0.1%, 10：分钟；60：秒；0.1%：上文所计算得到的10分钟的平均值)。&lt;/td&gt;
      &lt;td&gt;精度很低：报警会触发许多不会威胁SLO的事件。10分钟的0.1％错误率会发出报警，而每月错误预算仅消耗0.02％。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;此报警会触发任何威胁SLO的事件，表现出良好的召回率。&lt;/td&gt;
      &lt;td&gt;极端情况下，你每天最多可以收到144个报警，即使不需要对此采取任何措施，并且服务仍然符合SLO&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;2-增加报警窗口&quot;&gt;2. 增加报警窗口&lt;/h3&gt;

&lt;p&gt;我们可以通过更改报警窗口的大小重新构建前面的示例，以提高精度。 通过增加窗口大小，你可以在触发报警之前消耗更高的预算。&lt;/p&gt;

&lt;p&gt;为了保持报警的数量可控，你决定仅在事件消耗30天错误预算的5％（一个36小时的窗口）时才收到通知 ：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- alert: HighErrorRate
  expr: job:slo_errors_per_request:ratio_rate36h{job=&quot;myjob&quot;} &amp;gt; 0.001
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;现在，检测时间是：&lt;/p&gt;

&lt;p&gt;表5-2 显示了在较大的时间窗口内错误率过高时发出报警的好处和缺点&lt;/p&gt;
&lt;center&gt; 表5-2 在较大的时间窗口内错误率过高时发出报警的优缺点: &lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;优点&lt;/th&gt;
      &lt;th&gt;缺点&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;检测时间仍然很好：完全停机需要2分10秒(36&lt;em&gt;60&lt;/em&gt;0.1%, 36：小时；60：分钟；0.1%：上文所计算得到的10分钟的平均值)&lt;/td&gt;
      &lt;td&gt;非常差的恢复时间：在100％停机的情况下，报警将在2分钟后触发，并在接下来的36小时内持续&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;比前一个示例更精确：通过确保错误率持续更长时间，报警可能会对错误预算构成重大威胁&lt;/td&gt;
      &lt;td&gt;由于存在大量数据点，因此在较长窗口上计算速率在存储器或I/O操作方面可能代价较大&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;图5-2显示，虽然在36小时内，错误率已降至可忽略不计的水平，但36小时的平均错误率仍高于阈值。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/5-2.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图5-2  36小时内的错误率 &lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-增加报警持续时间&quot;&gt;3. 增加报警持续时间&lt;/h3&gt;

&lt;p&gt;大多数监控系统允许你将持续时间参数添加到报警规则，因此报警不会被触发，除非该值在一段时间内保持在阈值之上。你可能想使用此参数作为低成本方式增加更长的窗口：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; - alert: HighErrorRate
   expr: job:slo_errors_per_request:ratio_rate1m{job=&quot;myjob&quot;} &amp;gt; 0.001
   for: 1h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;表5-3显示了使用持续时间参数进行报警的优缺点。&lt;/p&gt;
&lt;center&gt; 表5-3 使用持续时间参数进行报警的优缺点 &lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;优点&lt;/th&gt;
      &lt;th&gt;缺点&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;报警可以更高精度。在触发之前需要持续的错误率意味着报警更可能对应于重大事件。&lt;/td&gt;
      &lt;td&gt;召回率和检测时间不佳：由于持续时间不随事件严重程度而变化，因此在100%服务中断一小时后才会发出报警，0.2%服务中断也会有相同的检测时间。100％的中断将消耗30天预算的140％。即使度量标准暂时返回到SLO内的级别，持续时间计时器也将重置。 当SLI在满足于不满足SLO之间波动时可能永远不会发出报警。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;由于表5-3中列出的原因，我们不建议将持续时间用作基于SLO的报警标准的一部分&lt;/p&gt;

&lt;p&gt;图5-3显示了在报警触发前持续10分钟的服务窗口内5分钟内的平均错误率。每隔10分钟出现的持续5分钟的错误高峰永远不会触发报警，但是总体上看出错率是35%。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/5-3.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图5-3 每隔10分钟出现100%的错误 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;每个峰值消耗了30天预算的近12％，但报警从未触发。&lt;/p&gt;

&lt;h3 id=&quot;4-关于消耗率的报警&quot;&gt;4. 关于消耗率的报警&lt;/h3&gt;

&lt;p&gt;要改进以前的解决方案，你需要创建具有良好检测时间和高精度的报警规则。为此， 你可以引入消耗速率以减小窗口大小，同时保持报警预算花费不变。&lt;/p&gt;

&lt;p&gt;消耗速率是指相对于SLO，服务消耗错误预算的速度。 图5-4显示了消耗率和错误预算之间的关系。&lt;/p&gt;

&lt;p&gt;示例服务使用了100%消耗率，这意味着它正在消耗错误预算，其速率使您在SLO的时间窗口结束时只剩下0的错误预（请参阅第一本书中的第4章）。 在30天的时间窗口内SLO为99.9％，持续的0.1％错误率正好可以在一个月内消耗完所有错误预算：消耗率为100%。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/5-4.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图5-4 相对于消耗率的错误预算 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;表5-4显示了消耗速率，相应的错误率以及耗尽SLO预算所需的时间。&lt;/p&gt;
&lt;center&gt; 表5-4 消耗率和耗尽错误预算所需的时间 &lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Burn rate&lt;/th&gt;
      &lt;th&gt;Error rate for a 99.9% SLO&lt;/th&gt;
      &lt;th&gt;Time to exhaustion&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.1%&lt;/td&gt;
      &lt;td&gt;30days&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.2%&lt;/td&gt;
      &lt;td&gt;15days&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;1%&lt;/td&gt;
      &lt;td&gt;3days&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1000&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
      &lt;td&gt;43minutes&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;通过将报警窗口定为一小时，并设定当消耗掉当月5％的错误预算时发出告警，你可以得到用于报警的消耗率阈值。对于基于消耗率的报警，报警触发所需的时间为：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(（1 - SLO）/错误率）*报警窗口大小*消耗率
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;报警触发时消耗的错误预算为：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;消耗率*报警窗口大小/SLO期限
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;1小时的告警窗口消耗掉30天错误预算的5%时，错误预算的消耗率为36。报警规则现在变为：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- alert: HighErrorRate
  expr: job:slo_errors_per_request:ratio_rate1h{job=&quot;myjob&quot;} &amp;gt; 36 * 0.001
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;表5-5 显示了基于消耗率的报警的优缺点。&lt;/p&gt;
&lt;center&gt; 表5-5 基于消耗率报警的优缺点 &lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;优点&lt;/th&gt;
      &lt;th&gt;缺点&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;良好的精确度：此策略选择大部分错误预算支出以提醒发出报警。更短的时间窗口，计算起来代价较小。 检测时间好。更好的恢复时间：58分钟。&lt;/td&gt;
      &lt;td&gt;低召回率：35%（与上下文的36相对）的消耗率永远不会发出报警，但会在20.5小时内消耗掉所有30天的误差预算。重置时间：58分钟仍然太长&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;5-多次消耗率报警&quot;&gt;5. 多次消耗率报警&lt;/h3&gt;

&lt;p&gt;你的报警逻辑可以使用多个消耗速率和时间窗口，并在消耗速率超过指定阈值时触发报警。 此选项保留了消耗率报警的好处，并确保你不会忽略较低（但仍然很重要）的错误率。&lt;/p&gt;

&lt;p&gt;为事件设置工单通知也是一个好主意，这些事件通常会被忽视，但如果不加以控制可能会耗尽你的错误预算 - 例如，三天内预算消耗率为10％。 这种错误率可以捕获重大事件，但由于预算消耗率提供了足够的时间来处理事件，因此你无需紧急通知某人。&lt;/p&gt;

&lt;p&gt;我们建议在一小时内将2％的预算消耗和6小时内的5％预算消耗作为紧急通知是合理起始数量，并在三天内将10％预算消耗作为故障工单报警的良好基准。适当的数字取决于服务和基本的报警负载。对于更繁忙的服务，并且根据周末和假日的待命责任oncall， 可能需要六小时窗口的工单提醒。&lt;/p&gt;

&lt;p&gt;表5-6显示了消耗的SLO预算百分比的相应消耗率和时间窗口。&lt;/p&gt;

&lt;center&gt; 表5-6 建议的时间窗口和消耗率，以消耗SLO预算的百分比 &lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;SLO budget consumption&lt;/th&gt;
      &lt;th&gt;Time window&lt;/th&gt;
      &lt;th&gt;Burn rate&lt;/th&gt;
      &lt;th&gt;Notification&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;2%&lt;/td&gt;
      &lt;td&gt;1hour&lt;/td&gt;
      &lt;td&gt;14.4&lt;/td&gt;
      &lt;td&gt;Page&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5%&lt;/td&gt;
      &lt;td&gt;6hours&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Page&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10%&lt;/td&gt;
      &lt;td&gt;3day&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Ticket&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;报警配置可能类似于如下规则：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    expr: (
            job:slo_errors_per_request:ratio_rate1h{job=&quot;myjob&quot;} &amp;gt; (14.4*0.001)
        or
            job:slo_errors_per_request:ratio_rate6h{job=&quot;myjob&quot;} &amp;gt; (6*0.001)
          )
    severity: page

    expr: job:slo_errors_per_request:ratio_rate3d{job=&quot;myjob&quot;} &amp;gt; 0.001

    severity: ticket
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;图5-5显示了根据错误率的检测时间和报警类型。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/5-5.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图5-5 错误率检测时间和报警类型 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;多个消耗速率允许你根据响应速度调整报警以提供适当的优先级。如果问题在几小时或几天内耗尽错误预算，则发送有效通知是合适的。否则，在下一个工作日处理基于工单的报警通知则更合适。表5-7列出了使用多种消耗率的优缺点。&lt;/p&gt;

&lt;center&gt; 表5-7 使用多种消耗率的优缺点 &lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;优点&lt;/th&gt;
      &lt;th&gt;缺点&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;能够根据关键值调整监控配置以适应多种情况:错误率高时快速报警;如果错误率很低但持续，最终会发出报警。良好的精度，与所有固定预算的部分报警方法一样。 因为是三天的时间窗口，所以有很好的召回率。能够根据人们对防御SLO的反应速度来选择最合适的报警类型&lt;/td&gt;
      &lt;td&gt;更多数据、窗口大小和阈值需要管理和推理。 由于三天的窗口，更长的恢复时间。 如果所有条件均为真，则要避免触发多个报警，你需要实施报警屏蔽。 例如：5分钟内10％的预算支出也意味着5％的预算在6小时内消耗完，2％的预算消耗在1小时之内。此方案将触发三个通知，除非监控系统足够智能以防止它这样做&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;6-多窗口多消耗率报警&quot;&gt;6. 多窗口，多消耗率报警&lt;/h3&gt;

&lt;p&gt;我们可以在第五节中增强多消耗率报警，以便仅在我们仍在快速消耗预算时通知我们 - 从而减少误报的数量。 为此，我们需要添加另一个参数：一个较短的窗口，用于检查在触发报警时是否仍在消耗错误预算。&lt;/p&gt;

&lt;p&gt;一个好的方案是将短窗口设为长窗口持续时间的1/12，如图5-6所示。 该图显示了报警阈值。 在经历了10分钟的15％错误之后，短窗口平均值立即超过报警阈值，并且在5分钟后长窗口平均值超过阈值，此时报警开始触发。 错误停止后5分钟，短窗口平均值降至阈值以下，此时报警停止触发。 错误停止后60分钟，长窗口平均值降至阈值以下。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/5-6.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图 5-6. 报警用的长短窗口 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;例如，你可以在前一小时和前五分钟超过14.4倍消耗率时发送工单级报警。 只有在消耗了2％的预算后，此报警才会触发，但通过在五分钟后停止发送而不是一小时后显示更好的恢复时间：&lt;/p&gt;

&lt;p&gt;我们建议将表5-8中列出的参数作为基于SLO的报警配置的起点。&lt;/p&gt;

&lt;center&gt; 表 5-8. 99.9%的SLO报警配置推荐的参数 &lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Serverity&lt;/th&gt;
      &lt;th&gt;Long window&lt;/th&gt;
      &lt;th&gt;Short window&lt;/th&gt;
      &lt;th&gt;Butn rate&lt;/th&gt;
      &lt;th&gt;budget consumption&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Page&lt;/td&gt;
      &lt;td&gt;1hour&lt;/td&gt;
      &lt;td&gt;5minutes&lt;/td&gt;
      &lt;td&gt;14.4&lt;/td&gt;
      &lt;td&gt;2%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Page&lt;/td&gt;
      &lt;td&gt;6hours&lt;/td&gt;
      &lt;td&gt;30minutes&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;5%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ticket&lt;/td&gt;
      &lt;td&gt;3day&lt;/td&gt;
      &lt;td&gt;6hour&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;10%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;我们发现基于了多消耗率的报警是实现基于SLO的报警的有效方式。表5-9显示了使用多种消耗率和窗口大小的优点和缺点。&lt;/p&gt;

&lt;center&gt; 表5-9 使用多种消耗率和窗口大小的优点和缺点 &lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;优点&lt;/th&gt;
      &lt;th&gt;缺点&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;灵活的报警框架，允许你根据事件的严重性和组织的要求控制报警类型	要指定的参数很多，这可能使报警规则难以管理&lt;/td&gt;
      &lt;td&gt;良好的精度，与所有固定预算的部分报警方法一样。不错的召回率，因为有三天的窗口	有关管理报警规则的更多信息，请参阅第89页的“按比例报警”&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;低流量服务和错误预算报警&quot;&gt;低流量服务和错误预算报警&lt;/h2&gt;

&lt;p&gt;当出现问题需要提供有意义的信息且请求率较高时，详细说明的多窗口、多点消耗方法很有效。但是，这些方法可能会导致接收请求率较低的系统出现问题。如果系统具有较少的用户数或自然的低流量时段（例如夜晚和周末），则可能需要更改你的方法。&lt;/p&gt;

&lt;p&gt;在低流量服务中自动区分不重要事件更加困难。 例如，如果系统每小时收到10个请求，则单个失败的请求会导致每小时错误率为10％。 对于99.9％的SLO，此请求构成1,000x消耗率并立即发出报警，因为它消耗了30天误差预算的13.9％。 此方案在30天内仅允许七个失败的请求。 单个请求可能会因大量短暂且令人厌倦的原因而失败，这些原因与大型系统中断一样，不一定能用成本效益的方式解决。&lt;/p&gt;

&lt;p&gt;最佳解决方案取决于服务的性质：单个请求失败的影响是什么？ 如果失败的请求是一次性的、高价值的、没有重试的请求，那么高可用性目标可能是合适的。从业务角度来看，调查每个失败的请求都是有意义的。但是，在这种情况下，报警系统会延迟通知错误。我们建议使用几个关键选项来处理低流量服务：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;人工生成流量以补偿来自真实用户的信号不足。&lt;/li&gt;
  &lt;li&gt;将较小的服务组合成更大的服务以用于监控目的。&lt;/li&gt;
  &lt;li&gt;修改产品，以便：
    &lt;ul&gt;
      &lt;li&gt;– 需要更多请求才能将单个事件限定为失败。&lt;/li&gt;
      &lt;li&gt;– 单一故障的影响较小。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;人工生成流量&quot;&gt;人工生成流量&lt;/h3&gt;

&lt;p&gt;系统可以模拟用户活动以检查潜在错误和高延迟请求。在没有真实用户的情况下，你的监控系统可以检测到模拟错误和请求，因此你的值班工程师可以在影响太多实际用户之前对问题做出响应。&lt;/p&gt;

&lt;p&gt;人工流量提供更多信号，并允许你重用现有的监控逻辑和SLO值。你甚至可能已经拥有大部分必要的流量生成组件，例如黑盒探测器和集成测试。&lt;/p&gt;

&lt;p&gt;生成人工负载确实有一些缺点。大多数需要SRE支持的服务都很复杂，而且系统控制面很大。理想情况下，系统应进行设计和更改，以便使用人工流量进行监控。 即使是非常重要的服务，你也只能合成用户请求类型总数的一小部分。 对于有状态的服务，更多的状态会加剧这个问题。&lt;/p&gt;

&lt;p&gt;此外，如果问题影响真实用户但不影响人工流量，则成功的人工请求会隐藏真实的用户信号，因此你不会收到用户看到的错误通知。&lt;/p&gt;

&lt;h3 id=&quot;合并服务&quot;&gt;合并服务&lt;/h3&gt;

&lt;p&gt;如果多个低流量服务对一个整体功能有贡献，则将它们的请求组合到单个更高级别的组中可以更精确地检测重要事件并且具有更少的误报。 要使这种方法起作用，服务必须以某种方式相关联 ——你可以组合构成同一产品的一部分的微服务，或者由同一个二进制文件处理的多个请求类型。&lt;/p&gt;

&lt;p&gt;组合服务的缺点是单个服务的完全失败可能不算是重大事件。 通过选择具有共享故障域的服务（例如公共后端数据库），可以增加故障影响整个组的可能性。 你仍然可以使用较长时间的报警，最终可以100%的捕获单个服务故障。&lt;/p&gt;

&lt;h3 id=&quot;进行服务和基础设施更改&quot;&gt;进行服务和基础设施更改&lt;/h3&gt;

&lt;p&gt;对重大事件发出报警旨在提供足够的通知，以便在耗尽整个错误预算之前缓解问题。 如果你无法将监控调整为对短暂事件不太敏感，并且生成人工流量不切实际，则可以考虑更改服务以减少单个失败请求对用户的影响。 例如，你可能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;修改客户端以使用指数退避和抖动进行重试。&lt;/li&gt;
  &lt;li&gt;设置捕获最终执行请求的回退路径，这可以在服务器或客户端上进行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些更改对于高流量系统非常有用，但对于低流量系统更是如此：它们允许在错误预算中有更多的失败事件，更多的普通信号，以及更多的时间在事件变得重要之前对其做出反应。&lt;/p&gt;

&lt;h3 id=&quot;降低slo或增加窗口&quot;&gt;降低SLO或增加窗口&lt;/h3&gt;

&lt;p&gt;你可能还想重新考虑单个故障对错误预算的影响是否准确反映了其对用户的影响。 如果少量错误导致你丢失错误预算，你是否真的需要寻找工程师来立即解决问题？ 如果没有，用户会对较低的SLO同样满意。 通过较低的SLO，工程师只会收到更大的持续中断报警通知。&lt;/p&gt;

&lt;p&gt;一旦你与服务的利益相关者协商降低SLO（例如，将SLO从99.9％降低到99％），实施更改非常简单：如果你已经有系统用于报告，监控和报警，则基于 SLO阈值，只需将新SLO值添加到相关系统即可。&lt;/p&gt;

&lt;p&gt;降低SLO确实有缺点：它涉及产品决策。 更改SLO会影响系统的其他方面，例如对系统行为的期望以及何时制定错误预算策略。 这些其他要求对于产品而言可能比避免一些低信号报警更重要。以类似的方式，增加用于报警逻辑的时间窗口确保触发页面的报警更加重要并且值得关注。在实践中，我们使用以下方法的某种组合来警告低流量服务：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在这样做时产生虚假流量是可能的并且可以实现良好的覆盖&lt;/li&gt;
  &lt;li&gt;修改客户端，以便短暂的故障不太可能导致用户影响&lt;/li&gt;
  &lt;li&gt;聚合共享某种故障模式的较小服务&lt;/li&gt;
  &lt;li&gt;设置SLO阈值与失败请求的实际影响相对应&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;极端可用性目标&quot;&gt;极端可用性目标&lt;/h2&gt;

&lt;p&gt;具有极低或极高可用性目标的服务可能需要特别考虑。 例如，考虑具有90％可用性目标的服务。 表5-8表示了当在一个小时内消耗了2%的错误预算时的报警。因为100%的宕机只会在那个小时消耗掉1.4%的预算，所以这个报警永远不会触发。如果你的错误预算是在很长一段时间内设置的，那么你可能需要调整报警参数。&lt;/p&gt;

&lt;p&gt;对于具有极高可用性目标的服务，100％中断的耗尽时间非常短。 对于每月目标可用性为99.999％的服务，100％的中断将在26秒内耗尽其预算——这比许多监控服务的采集周期小很多，更不用说生成报警时通过电子邮件和短信等通知系统传递它的目的端时间了。 即使报警直接发送到自动解决方案系统，问题也可能完全消耗错误预算，然后才能得到缓解它。&lt;/p&gt;

&lt;p&gt;收到通知说你只剩下26秒的预算并不一定是一个坏策略；这对于保护SLO是没有用的。 防御这种可靠性的唯一方法是设计系统，使100％中断的可能性极低。 这样，你可以在消耗预算之前解决问题。 例如，如果你最初将此更改推广到仅有1％的用户，并以1％的相同速率消耗错误预算，那么现在你要43分钟才能耗尽错误预算。 有关设计此类系统的策略，请参阅第16章。&lt;/p&gt;

&lt;p&gt;大规模报警&lt;/p&gt;

&lt;p&gt;在扩展服务时，请确保报警同样可扩展。 你可能想为各个服务指定自定义报警参数。 如果你的服务包含100个微服务（或等效地，具有100种不同请求类型的单个服务），这种情况很快就会积累起无法衡量的工作和认知负载。&lt;/p&gt;

&lt;p&gt;在这种情况下，我们强烈建议不要为每项服务单独指定报警窗口和消耗率参数，因为这样做很快就会变得势不可挡。确定报警参数后，将它们应用于所有服务。&lt;/p&gt;

&lt;p&gt;管理大量SLO的一种技术是将请求类型分组到大致类似的可用性要求的桶中。 例如，对于具有可用性和延迟SLO的服务，可以将其请求类型分组到以下存储桶中：&lt;/p&gt;

&lt;h4 id=&quot;critical&quot;&gt;CRITICAL&lt;/h4&gt;
&lt;p&gt;对于最重要的请求类型，例如用户登录服务时的请求。&lt;/p&gt;

&lt;h4 id=&quot;high_fast&quot;&gt;HIGH_FAST&lt;/h4&gt;
&lt;p&gt;适用于具有高可用性和低延迟要求的请求。 这些请求涉及核心交互功能，例如当用户点击按钮以查看他们的广告库存本月赚了多少钱。&lt;/p&gt;

&lt;h4 id=&quot;high_slow&quot;&gt;HIGH_SLOW&lt;/h4&gt;
&lt;p&gt;对于重要但对延迟不太敏感的请求，比如用户单击按钮生成过去几年所有广告活动的报告，并且不希望数据立即返回。&lt;/p&gt;

&lt;h4 id=&quot;low&quot;&gt;LOW&lt;/h4&gt;
&lt;p&gt;对于必须具有某些可用性，但是对于用户来说几乎不可见的中断的请求 —— 例如，轮询处理程序以查看可能长时间失败而不会对用户产生影响的帐户通知。&lt;/p&gt;

&lt;h4 id=&quot;no_slo&quot;&gt;NO_SLO&lt;/h4&gt;
&lt;p&gt;对于用户完全不可见的功能 ——例如，暗启动或显式位于任何SLO之外的alpha功能。&lt;/p&gt;

&lt;p&gt;通过对请求进行分组而不是在所有请求类型上放置唯一可用性和延迟目标，可以将请求分组到五个存储桶中，如表5-10中的示例所示。&lt;/p&gt;

&lt;center&gt; 表5-10  根据类似的可用性要求和阈值请求类桶 &lt;/center&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Request class&lt;/th&gt;
      &lt;th&gt;Availability&lt;/th&gt;
      &lt;th&gt;Latency @ 90%&lt;/th&gt;
      &lt;th&gt;Latency@99%&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;CRITICAL&lt;/td&gt;
      &lt;td&gt;99.99%&lt;/td&gt;
      &lt;td&gt;100 ms&lt;/td&gt;
      &lt;td&gt;200 ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HIGH_Fast&lt;/td&gt;
      &lt;td&gt;99.9%&lt;/td&gt;
      &lt;td&gt;100 ms&lt;/td&gt;
      &lt;td&gt;200 ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;HIGH_SLOW&lt;/td&gt;
      &lt;td&gt;99.9%&lt;/td&gt;
      &lt;td&gt;1000 ms&lt;/td&gt;
      &lt;td&gt;5000 ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOW&lt;/td&gt;
      &lt;td&gt;99%&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NO_SLO&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;这些存储桶提供了足够的保真度来保护用户的满意度，但与一个更复杂、管理成本更高的系统相比，它的工作量更小，而且可能更精确地反映用户体验。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;p&gt;如果你设置的SLOs是有意义的、可理解的，并且可度量，那么你可以配置报警，只有在错误预算中存在可操作的、特定的威胁时才通知值班人员。&lt;/p&gt;

&lt;p&gt;用于警告重大事件的技术包括从错误率高于SLO阈值时发出报警，到使用多级消耗率和窗口大小。 在大多数情况下，我们认为多窗口、多消耗率报警技术是保护应用程序SLO的最佳方法。&lt;/p&gt;

&lt;p&gt;我们希望我们提供了为你自己的应用程序和组织做出正确配置决策时所需的环境和工具。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">本章介绍如何在发生重要事件时将SLO转换为可进行配置的报警。我们的第一本SRE和本书都讨论了实施SLO。我们相信，拥有好的SLO可以衡量你平台可靠性，正如你的客户所经历的那样，可以为on-call人员该如何迅速做出响应提供最准确的提示。在这里，我们提供了有关如何将这些SLO转换为报警规则的具体指导，以便你在消耗过多的错误预算之前响应问题。 我们的示例展示了一系列报警指标和逻辑的复杂实现；讨论他们的功能和缺点。虽然我们的示例使用的是简单的request-driven服务和Prometheus语法，但你可以应用到任何报警框架中。 报警注意事项 为了从服务质量指标（SLI）和错误预算生成报警，需要一种方法将这两个元素组合成一个特定的规则。 你的目标是通知重大事件：消耗大部分错误预算的事件。 在评估报警策略时，请考虑以下属性： 精确度 检测到的事件比例很重要。 如果每一个报警对应一个重大事件，则精度为100％。请注意，报警可能会在低流量时段对非重要事件变得特别敏感（在第86页的“低流量服务和错误预算报警”中讨论）。 召回率 检测到重大事件的比例。如果每一个重要事件都会发出一次报警，则召回率为100％。 检测时间 在各种条件下发送报警通知需要多长时间。较长的检测时间会对错误预算产生负面影响。 恢复时间 解决问题后报警会持续多长时间。较长的恢复时间可能导致混淆或问题被忽略。 重大事件的报警方法 为SLO构建报警规则可能会变得非常复杂。在这里，我们提出了六种方法来配置重要事件的报警，以提高精度，得到一个可以同时控制精度、召回、检测时间和恢复时间这四个参数选项。以下每种方法都解决了不同的问题，有些方法最终同时解决多个问题。 前三个不可行的方法对后三个可行报警策略的应用是十分有益的，方法6是最可行和最强烈推荐的选择。第一种方法实现简单但不充分，而最佳方法提供了一个完整的解决方案，可以在长期和短期内保护SLO。 出于本节讨论的目的，“错误预算”和“错误率”适用于所有SLI，而不仅仅是名称中包含“错误”的SLI。在第20页的“测量内容：使用SLI”一节中，我们建议使用SLI来捕获正常事件与总事件的比率。错误预算给出允许的错误事件的数量，错误率=错误事件/总事件的比率。 1. 目标错误率≥SLO阈值 对于最简单的解决方案，你可以选择一个小的时间窗口（例如，10分钟），并在该窗口内的错误率超过SLO阈值时发出报警。 例如，如果SLO在30天内为99.9％，则在前10分钟的错误率≥0.1％时发出报警： - alert: HighErrorRate expr: job:slo_errors_per_request:ratio_rate10m{job=&quot;myjob&quot;} &amp;gt;= 0.001 这个10分钟的平均值是用Prometheus的记录法则计算出来的: record: job:slo_errors_per_request:ratio_rate10m expr: sum(rate(slo_errors[10m])) by (job) / sum(rate(slo_requests[10m])) by (job) 如果你不从job中导出slo_errors和slo_requests，则可以通过重命名度量值来创建时间序列： record: slo_errors expr: http_errors 当最近的错误率等于SLO时发出报警意味着系统检测到以下预算消耗： 告警窗口/SLO期限 图5-1 设有10分钟报警窗口和99.9％SLO的示例服务的检测时间 图5-1 显示了设有10分钟报警窗口和99.9％SLO示例服务的检测时间和错误率之间的关系。 表5-1 显示了即时错误率过高时发出警报的优缺点 优点 缺点 检测时间良好：总停机时间为0.6秒(10600.1%, 10：分钟；60：秒；0.1%：上文所计算得到的10分钟的平均值)。 精度很低：报警会触发许多不会威胁SLO的事件。10分钟的0.1％错误率会发出报警，而每月错误预算仅消耗0.02％。 此报警会触发任何威胁SLO的事件，表现出良好的召回率。 极端情况下，你每天最多可以收到144个报警，即使不需要对此采取任何措施，并且服务仍然符合SLO 2. 增加报警窗口 我们可以通过更改报警窗口的大小重新构建前面的示例，以提高精度。 通过增加窗口大小，你可以在触发报警之前消耗更高的预算。 为了保持报警的数量可控，你决定仅在事件消耗30天错误预算的5％（一个36小时的窗口）时才收到通知 ： - alert: HighErrorRate expr: job:slo_errors_per_request:ratio_rate36h{job=&quot;myjob&quot;} &amp;gt; 0.001 现在，检测时间是： 表5-2 显示了在较大的时间窗口内错误率过高时发出报警的好处和缺点 表5-2 在较大的时间窗口内错误率过高时发出报警的优缺点: 优点 缺点 检测时间仍然很好：完全停机需要2分10秒(36600.1%, 36：小时；60：分钟；0.1%：上文所计算得到的10分钟的平均值) 非常差的恢复时间：在100％停机的情况下，报警将在2分钟后触发，并在接下来的36小时内持续 比前一个示例更精确：通过确保错误率持续更长时间，报警可能会对错误预算构成重大威胁 由于存在大量数据点，因此在较长窗口上计算速率在存储器或I/O操作方面可能代价较大 图5-2显示，虽然在36小时内，错误率已降至可忽略不计的水平，但36小时的平均错误率仍高于阈值。 图5-2 36小时内的错误率 3. 增加报警持续时间 大多数监控系统允许你将持续时间参数添加到报警规则，因此报警不会被触发，除非该值在一段时间内保持在阈值之上。你可能想使用此参数作为低成本方式增加更长的窗口： - alert: HighErrorRate expr: job:slo_errors_per_request:ratio_rate1m{job=&quot;myjob&quot;} &amp;gt; 0.001 for: 1h 表5-3显示了使用持续时间参数进行报警的优缺点。 表5-3 使用持续时间参数进行报警的优缺点 优点 缺点 报警可以更高精度。在触发之前需要持续的错误率意味着报警更可能对应于重大事件。 召回率和检测时间不佳：由于持续时间不随事件严重程度而变化，因此在100%服务中断一小时后才会发出报警，0.2%服务中断也会有相同的检测时间。100％的中断将消耗30天预算的140％。即使度量标准暂时返回到SLO内的级别，持续时间计时器也将重置。 当SLI在满足于不满足SLO之间波动时可能永远不会发出报警。 由于表5-3中列出的原因，我们不建议将持续时间用作基于SLO的报警标准的一部分 图5-3显示了在报警触发前持续10分钟的服务窗口内5分钟内的平均错误率。每隔10分钟出现的持续5分钟的错误高峰永远不会触发报警，但是总体上看出错率是35%。 图5-3 每隔10分钟出现100%的错误 每个峰值消耗了30天预算的近12％，但报警从未触发。 4. 关于消耗率的报警 要改进以前的解决方案，你需要创建具有良好检测时间和高精度的报警规则。为此， 你可以引入消耗速率以减小窗口大小，同时保持报警预算花费不变。 消耗速率是指相对于SLO，服务消耗错误预算的速度。 图5-4显示了消耗率和错误预算之间的关系。 示例服务使用了100%消耗率，这意味着它正在消耗错误预算，其速率使您在SLO的时间窗口结束时只剩下0的错误预（请参阅第一本书中的第4章）。 在30天的时间窗口内SLO为99.9％，持续的0.1％错误率正好可以在一个月内消耗完所有错误预算：消耗率为100%。 图5-4 相对于消耗率的错误预算 表5-4显示了消耗速率，相应的错误率以及耗尽SLO预算所需的时间。 表5-4 消耗率和耗尽错误预算所需的时间 Burn rate Error rate for a 99.9% SLO Time to exhaustion 1 0.1% 30days 2 0.2% 15days 10 1% 3days 1000 100% 43minutes 通过将报警窗口定为一小时，并设定当消耗掉当月5％的错误预算时发出告警，你可以得到用于报警的消耗率阈值。对于基于消耗率的报警，报警触发所需的时间为： (（1 - SLO）/错误率）*报警窗口大小*消耗率 报警触发时消耗的错误预算为： 消耗率*报警窗口大小/SLO期限 1小时的告警窗口消耗掉30天错误预算的5%时，错误预算的消耗率为36。报警规则现在变为： - alert: HighErrorRate expr: job:slo_errors_per_request:ratio_rate1h{job=&quot;myjob&quot;} &amp;gt; 36 * 0.001 表5-5 显示了基于消耗率的报警的优缺点。 表5-5 基于消耗率报警的优缺点 优点 缺点 良好的精确度：此策略选择大部分错误预算支出以提醒发出报警。更短的时间窗口，计算起来代价较小。 检测时间好。更好的恢复时间：58分钟。 低召回率：35%（与上下文的36相对）的消耗率永远不会发出报警，但会在20.5小时内消耗掉所有30天的误差预算。重置时间：58分钟仍然太长 5. 多次消耗率报警 你的报警逻辑可以使用多个消耗速率和时间窗口，并在消耗速率超过指定阈值时触发报警。 此选项保留了消耗率报警的好处，并确保你不会忽略较低（但仍然很重要）的错误率。 为事件设置工单通知也是一个好主意，这些事件通常会被忽视，但如果不加以控制可能会耗尽你的错误预算 - 例如，三天内预算消耗率为10％。 这种错误率可以捕获重大事件，但由于预算消耗率提供了足够的时间来处理事件，因此你无需紧急通知某人。 我们建议在一小时内将2％的预算消耗和6小时内的5％预算消耗作为紧急通知是合理起始数量，并在三天内将10％预算消耗作为故障工单报警的良好基准。适当的数字取决于服务和基本的报警负载。对于更繁忙的服务，并且根据周末和假日的待命责任oncall， 可能需要六小时窗口的工单提醒。 表5-6显示了消耗的SLO预算百分比的相应消耗率和时间窗口。 表5-6 建议的时间窗口和消耗率，以消耗SLO预算的百分比 SLO budget consumption Time window Burn rate Notification 2% 1hour 14.4 Page 5% 6hours 6 Page 10% 3day 1 Ticket 报警配置可能类似于如下规则： expr: ( job:slo_errors_per_request:ratio_rate1h{job=&quot;myjob&quot;} &amp;gt; (14.4*0.001) or job:slo_errors_per_request:ratio_rate6h{job=&quot;myjob&quot;} &amp;gt; (6*0.001) ) severity: page expr: job:slo_errors_per_request:ratio_rate3d{job=&quot;myjob&quot;} &amp;gt; 0.001 severity: ticket 图5-5显示了根据错误率的检测时间和报警类型。 图5-5 错误率检测时间和报警类型 多个消耗速率允许你根据响应速度调整报警以提供适当的优先级。如果问题在几小时或几天内耗尽错误预算，则发送有效通知是合适的。否则，在下一个工作日处理基于工单的报警通知则更合适。表5-7列出了使用多种消耗率的优缺点。 表5-7 使用多种消耗率的优缺点 优点 缺点 能够根据关键值调整监控配置以适应多种情况:错误率高时快速报警;如果错误率很低但持续，最终会发出报警。良好的精度，与所有固定预算的部分报警方法一样。 因为是三天的时间窗口，所以有很好的召回率。能够根据人们对防御SLO的反应速度来选择最合适的报警类型 更多数据、窗口大小和阈值需要管理和推理。 由于三天的窗口，更长的恢复时间。 如果所有条件均为真，则要避免触发多个报警，你需要实施报警屏蔽。 例如：5分钟内10％的预算支出也意味着5％的预算在6小时内消耗完，2％的预算消耗在1小时之内。此方案将触发三个通知，除非监控系统足够智能以防止它这样做 6. 多窗口，多消耗率报警 我们可以在第五节中增强多消耗率报警，以便仅在我们仍在快速消耗预算时通知我们 - 从而减少误报的数量。 为此，我们需要添加另一个参数：一个较短的窗口，用于检查在触发报警时是否仍在消耗错误预算。 一个好的方案是将短窗口设为长窗口持续时间的1/12，如图5-6所示。 该图显示了报警阈值。 在经历了10分钟的15％错误之后，短窗口平均值立即超过报警阈值，并且在5分钟后长窗口平均值超过阈值，此时报警开始触发。 错误停止后5分钟，短窗口平均值降至阈值以下，此时报警停止触发。 错误停止后60分钟，长窗口平均值降至阈值以下。 图 5-6. 报警用的长短窗口 例如，你可以在前一小时和前五分钟超过14.4倍消耗率时发送工单级报警。 只有在消耗了2％的预算后，此报警才会触发，但通过在五分钟后停止发送而不是一小时后显示更好的恢复时间： 我们建议将表5-8中列出的参数作为基于SLO的报警配置的起点。 表 5-8. 99.9%的SLO报警配置推荐的参数 Serverity Long window Short window Butn rate budget consumption Page 1hour 5minutes 14.4 2% Page 6hours 30minutes 6 5% Ticket 3day 6hour 1 10% 我们发现基于了多消耗率的报警是实现基于SLO的报警的有效方式。表5-9显示了使用多种消耗率和窗口大小的优点和缺点。 表5-9 使用多种消耗率和窗口大小的优点和缺点 优点 缺点 灵活的报警框架，允许你根据事件的严重性和组织的要求控制报警类型 要指定的参数很多，这可能使报警规则难以管理 良好的精度，与所有固定预算的部分报警方法一样。不错的召回率，因为有三天的窗口 有关管理报警规则的更多信息，请参阅第89页的“按比例报警” 低流量服务和错误预算报警 当出现问题需要提供有意义的信息且请求率较高时，详细说明的多窗口、多点消耗方法很有效。但是，这些方法可能会导致接收请求率较低的系统出现问题。如果系统具有较少的用户数或自然的低流量时段（例如夜晚和周末），则可能需要更改你的方法。 在低流量服务中自动区分不重要事件更加困难。 例如，如果系统每小时收到10个请求，则单个失败的请求会导致每小时错误率为10％。 对于99.9％的SLO，此请求构成1,000x消耗率并立即发出报警，因为它消耗了30天误差预算的13.9％。 此方案在30天内仅允许七个失败的请求。 单个请求可能会因大量短暂且令人厌倦的原因而失败，这些原因与大型系统中断一样，不一定能用成本效益的方式解决。 最佳解决方案取决于服务的性质：单个请求失败的影响是什么？ 如果失败的请求是一次性的、高价值的、没有重试的请求，那么高可用性目标可能是合适的。从业务角度来看，调查每个失败的请求都是有意义的。但是，在这种情况下，报警系统会延迟通知错误。我们建议使用几个关键选项来处理低流量服务： 人工生成流量以补偿来自真实用户的信号不足。 将较小的服务组合成更大的服务以用于监控目的。 修改产品，以便： – 需要更多请求才能将单个事件限定为失败。 – 单一故障的影响较小。 人工生成流量 系统可以模拟用户活动以检查潜在错误和高延迟请求。在没有真实用户的情况下，你的监控系统可以检测到模拟错误和请求，因此你的值班工程师可以在影响太多实际用户之前对问题做出响应。 人工流量提供更多信号，并允许你重用现有的监控逻辑和SLO值。你甚至可能已经拥有大部分必要的流量生成组件，例如黑盒探测器和集成测试。 生成人工负载确实有一些缺点。大多数需要SRE支持的服务都很复杂，而且系统控制面很大。理想情况下，系统应进行设计和更改，以便使用人工流量进行监控。 即使是非常重要的服务，你也只能合成用户请求类型总数的一小部分。 对于有状态的服务，更多的状态会加剧这个问题。 此外，如果问题影响真实用户但不影响人工流量，则成功的人工请求会隐藏真实的用户信号，因此你不会收到用户看到的错误通知。 合并服务 如果多个低流量服务对一个整体功能有贡献，则将它们的请求组合到单个更高级别的组中可以更精确地检测重要事件并且具有更少的误报。 要使这种方法起作用，服务必须以某种方式相关联 ——你可以组合构成同一产品的一部分的微服务，或者由同一个二进制文件处理的多个请求类型。 组合服务的缺点是单个服务的完全失败可能不算是重大事件。 通过选择具有共享故障域的服务（例如公共后端数据库），可以增加故障影响整个组的可能性。 你仍然可以使用较长时间的报警，最终可以100%的捕获单个服务故障。 进行服务和基础设施更改 对重大事件发出报警旨在提供足够的通知，以便在耗尽整个错误预算之前缓解问题。 如果你无法将监控调整为对短暂事件不太敏感，并且生成人工流量不切实际，则可以考虑更改服务以减少单个失败请求对用户的影响。 例如，你可能： 修改客户端以使用指数退避和抖动进行重试。 设置捕获最终执行请求的回退路径，这可以在服务器或客户端上进行。 这些更改对于高流量系统非常有用，但对于低流量系统更是如此：它们允许在错误预算中有更多的失败事件，更多的普通信号，以及更多的时间在事件变得重要之前对其做出反应。 降低SLO或增加窗口 你可能还想重新考虑单个故障对错误预算的影响是否准确反映了其对用户的影响。 如果少量错误导致你丢失错误预算，你是否真的需要寻找工程师来立即解决问题？ 如果没有，用户会对较低的SLO同样满意。 通过较低的SLO，工程师只会收到更大的持续中断报警通知。 一旦你与服务的利益相关者协商降低SLO（例如，将SLO从99.9％降低到99％），实施更改非常简单：如果你已经有系统用于报告，监控和报警，则基于 SLO阈值，只需将新SLO值添加到相关系统即可。 降低SLO确实有缺点：它涉及产品决策。 更改SLO会影响系统的其他方面，例如对系统行为的期望以及何时制定错误预算策略。 这些其他要求对于产品而言可能比避免一些低信号报警更重要。以类似的方式，增加用于报警逻辑的时间窗口确保触发页面的报警更加重要并且值得关注。在实践中，我们使用以下方法的某种组合来警告低流量服务： 在这样做时产生虚假流量是可能的并且可以实现良好的覆盖 修改客户端，以便短暂的故障不太可能导致用户影响 聚合共享某种故障模式的较小服务 设置SLO阈值与失败请求的实际影响相对应 极端可用性目标 具有极低或极高可用性目标的服务可能需要特别考虑。 例如，考虑具有90％可用性目标的服务。 表5-8表示了当在一个小时内消耗了2%的错误预算时的报警。因为100%的宕机只会在那个小时消耗掉1.4%的预算，所以这个报警永远不会触发。如果你的错误预算是在很长一段时间内设置的，那么你可能需要调整报警参数。 对于具有极高可用性目标的服务，100％中断的耗尽时间非常短。 对于每月目标可用性为99.999％的服务，100％的中断将在26秒内耗尽其预算——这比许多监控服务的采集周期小很多，更不用说生成报警时通过电子邮件和短信等通知系统传递它的目的端时间了。 即使报警直接发送到自动解决方案系统，问题也可能完全消耗错误预算，然后才能得到缓解它。 收到通知说你只剩下26秒的预算并不一定是一个坏策略；这对于保护SLO是没有用的。 防御这种可靠性的唯一方法是设计系统，使100％中断的可能性极低。 这样，你可以在消耗预算之前解决问题。 例如，如果你最初将此更改推广到仅有1％的用户，并以1％的相同速率消耗错误预算，那么现在你要43分钟才能耗尽错误预算。 有关设计此类系统的策略，请参阅第16章。 大规模报警 在扩展服务时，请确保报警同样可扩展。 你可能想为各个服务指定自定义报警参数。 如果你的服务包含100个微服务（或等效地，具有100种不同请求类型的单个服务），这种情况很快就会积累起无法衡量的工作和认知负载。 在这种情况下，我们强烈建议不要为每项服务单独指定报警窗口和消耗率参数，因为这样做很快就会变得势不可挡。确定报警参数后，将它们应用于所有服务。 管理大量SLO的一种技术是将请求类型分组到大致类似的可用性要求的桶中。 例如，对于具有可用性和延迟SLO的服务，可以将其请求类型分组到以下存储桶中： CRITICAL 对于最重要的请求类型，例如用户登录服务时的请求。 HIGH_FAST 适用于具有高可用性和低延迟要求的请求。 这些请求涉及核心交互功能，例如当用户点击按钮以查看他们的广告库存本月赚了多少钱。 HIGH_SLOW 对于重要但对延迟不太敏感的请求，比如用户单击按钮生成过去几年所有广告活动的报告，并且不希望数据立即返回。 LOW 对于必须具有某些可用性，但是对于用户来说几乎不可见的中断的请求 —— 例如，轮询处理程序以查看可能长时间失败而不会对用户产生影响的帐户通知。 NO_SLO 对于用户完全不可见的功能 ——例如，暗启动或显式位于任何SLO之外的alpha功能。 通过对请求进行分组而不是在所有请求类型上放置唯一可用性和延迟目标，可以将请求分组到五个存储桶中，如表5-10中的示例所示。 表5-10 根据类似的可用性要求和阈值请求类桶 Request class Availability Latency @ 90% Latency@99% CRITICAL 99.99% 100 ms 200 ms HIGH_Fast 99.9% 100 ms 200 ms HIGH_SLOW 99.9% 1000 ms 5000 ms LOW 99% None None NO_SLO None None None 这些存储桶提供了足够的保真度来保护用户的满意度，但与一个更复杂、管理成本更高的系统相比，它的工作量更小，而且可能更精确地反映用户体验。 结论 如果你设置的SLOs是有意义的、可理解的，并且可度量，那么你可以配置报警，只有在错误预算中存在可操作的、特定的威胁时才通知值班人员。 用于警告重大事件的技术包括从错误率高于SLO阈值时发出报警，到使用多级消耗率和窗口大小。 在大多数情况下，我们认为多窗口、多消耗率报警技术是保护应用程序SLO的最佳方法。 我们希望我们提供了为你自己的应用程序和组织做出正确配置决策时所需的环境和工具。</summary></entry><entry><title type="html">第四章 监控</title><link href="http://localhost:4000/sre/2020/01/04/%E7%9B%91%E6%8E%A7/" rel="alternate" type="text/html" title="第四章 监控" /><published>2020-01-04T00:00:00+08:00</published><updated>2020-01-04T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/04/%E7%9B%91%E6%8E%A7</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/04/%E7%9B%91%E6%8E%A7/">&lt;!-- more --&gt;

&lt;p&gt;监控涉及到多种类型的数据，包括监控指标，纯文本日志，结构化日志，分布式跟踪日志， event introspection。 以上各种数据都有它们各自的用处，但是本章主要讨论监控指标和结构化日志。根据我们的经验，这两种数据最适合SRE的基础监控需求。&lt;/p&gt;

&lt;p&gt;从根本上讲，监控系统应当能够透视系统的内部，当需要判断服务的健康状态和诊断服务问题时，这是最关键的需求。在第一版SRE的第6章中给出了一些基本的监控方法，并且提到SRE监控他们系统的主要目的有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当达到阈值时触发报警&lt;/li&gt;
  &lt;li&gt;诊断和分析服务问题&lt;/li&gt;
  &lt;li&gt;展示系统的可视化信息&lt;/li&gt;
  &lt;li&gt;获取系统资源使用情况或服务健康状况的变化趋势，以便做长期计划&lt;/li&gt;
  &lt;li&gt;比较变更前后的系统变化或一个实验的两组样本的不同&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些用例的不同重要程度能指导你在选择或构建一个监控系统时做出权衡。&lt;/p&gt;

&lt;p&gt;本章讨论Google如何管理监控系统，并提供一些如何选择和运行监控系统的指导意见&lt;/p&gt;

&lt;h2 id=&quot;监控策略的特征&quot;&gt;监控策略的特征&lt;/h2&gt;

&lt;p&gt;在选择监控系统时，理解那些你关心的功能并对它们进行一个优先级的排序很重要。如果你正在评估一个监控系统，本节提到的这些特性可以帮助你思考哪种方案最适合你。如果你已经有一个在运行的监控系统了，你可以考虑使用现有解决方案的一些别的特性。根据你的需求，一个监控系统可能就能解决你所有的问题，也可能你需要组合好几个监控系统。&lt;/p&gt;

&lt;h3 id=&quot;速度&quot;&gt;速度&lt;/h3&gt;

&lt;p&gt;不同的组织对于数据的时效性和获取数据的速度有不同的要求。&lt;/p&gt;

&lt;p&gt;你需要用到数据的时候，数据就应该立马可用。数据的时效性影响的是当系统出错后多长时间监控系统才会通知你。另外，不及时的数据可能会导致因为错误的数据而采取不合适的行动。举个例子，关于事故响应，事故的发生时间和监控系统能够反映出结果的时间间隔如果太长，你可能会认为某一个改动没有产生什么不好的影响，或者你认为这个修改和某一个结果之间没有联系。超过4到5分钟才能获取的数据将会显著地影响你快速响应。&lt;/p&gt;

&lt;p&gt;如果你是基于速度在选择监控系统，那你需要首先确定对速度的需求。当你查询大量数据时，数据的获取速度通常都是个问题。如果需要汇总多个监控系统的数据，图形的加载需要费一些时间。监控系统如果从输入数据生成新的时间序列，则可以对一些通用查询进行预先计算，从而加速图形的加载。&lt;/p&gt;

&lt;h3 id=&quot;计算&quot;&gt;计算&lt;/h3&gt;

&lt;p&gt;很多应用场景都要求支持各种复杂计算。至少，你会希望能从监控系统获取几个月时间跨度的数据。没有长期数据的视图，你很难判断像系统增长性这样的长期趋势。关于粒度，汇总数据对于制作增长计划很有效。详细地存储每一个指标会有助于回答这样的问题：这样的异常行为以前发生过吗？但是，数据可能会耗费昂贵的存储空间，或者难以检索。&lt;/p&gt;

&lt;p&gt;理想的事件或资源消耗的指标是递增的计数器。利用计数器，监控系统可以基于时间做计算——比如，报告每秒的访问量。如果在更长的时间范围做计算，你就可以实现implement the building blocks for SLO burn-based alerting (see Chapter 5).&lt;/p&gt;

&lt;p&gt;最后，支持种类更齐全的统计函数很有用，因为细微的操作可能会掩盖不好的行为。当记录访问延迟时，算数平均数只能告诉你访问比较慢，而支持百分比计算的监控系统能让你一眼就看出是50%,5%还是1%的访问太慢。如果你目前的监控系统不支持直接的百分比计算，你可以采取下面的替代方案：&lt;/p&gt;

&lt;p&gt;通过把每一个访问的时间相加然后除以访问数量，从而计算出平均访问时间
每一个请求写入日志，然后对日志记录进行扫描或取样，从而计算出百分比
你也可以把原始的监控指标写到另一个离线分析系统中，用于生成周报或月报，或者执行一些难以在监控系统中直接进行的复杂的计算。&lt;/p&gt;

&lt;h3 id=&quot;交互界面&quot;&gt;交互界面&lt;/h3&gt;

&lt;p&gt;一个健壮的监控系统应该允许你简洁地在图形（graph）中显示时间序列数据，同时也能将数据结构化到表或各种类型的图表（chart）中。Dashborad是显示监控信息的主要的界面，所以选择显示格式很重要，要能最清晰地显示你所关心的数据。可以选择热力图、直方图和对数刻度图。&lt;/p&gt;

&lt;p&gt;你可能需要为同一数据提供不同的视图给不同的用户。高级管理人员希望看到的信息跟SRE的大不相同。针对使用者专门创建对他们来说有意义的dashborad。每一组dashborad，对同一类型的数据显示要保持一致，这样方便沟通。&lt;/p&gt;

&lt;p&gt;你可能需要实时图形化显示某一指标的不同聚合结果，比如分别根据机器类型、服务器版本或请求类型进行聚合。It’s a good idea for your team to be comfortable with performing ad hoc drill-downs on your data。根据不同的指标对你的数据进行切片，在需要的时候你可以查找数据之间的联系和模式。&lt;/p&gt;

&lt;h3 id=&quot;报警&quot;&gt;报警&lt;/h3&gt;

&lt;p&gt;报警分级很有必要，不同的级别采取不同的响应机制。 给报警设置不同的级别很有用，你可能会开一个工单用于跟踪一个低级别故障，调查可能会持续了一个多小时；100%的错误则属于紧急情况，需要立即响应。&lt;/p&gt;

&lt;p&gt;报警屏蔽功能能避免不必要的分散值班工程师精力的干扰。比如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当所有的服务器都出现同样的高故障率时，可以只针对全局的高故障率报警一次，不需要为每个服务器单独发送报警。&lt;/li&gt;
  &lt;li&gt;当你的服务所依赖的服务报警时，不需要为你的服务发出故障报警。不要忘记，一旦故障事件处理完了，记得恢复被屏蔽的故障报警。
你还需要确保在事件结束后不再屏蔽警报。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;监控系统的可控控制程度决定了你应该使用第三方的监控服务，还是部署和运行自己的监控系统。Google内部开发了自己的监控系统，但是外面有很多开源或商业的监控系统可供选择。&lt;/p&gt;

&lt;h2 id=&quot;监控数据的来源&quot;&gt;监控数据的来源&lt;/h2&gt;

&lt;p&gt;选择何种监控系统也受将要使用的监控数据源的影响。这节讨论两种常用的数据源：日志和指标。还有些其它的有用的监控数据源这里没有讨论的，比如分布式跟踪和运行时introspection.&lt;/p&gt;

&lt;p&gt;指标是属性和事件的数字度量，通常间隔一段固定的时间产生多个数据点。日志是只能追加的事件记录。本章的讨论主要集中在结构化日志，相比纯文本日志结构化日志更易于复杂查询和聚合工具。&lt;/p&gt;

&lt;p&gt;Google基于日志的监控系统处理大量细粒度（highly granular）数据。事件发生到log可见之间不可避免的有些延迟。日志分析并不要求即时性，可以先经过一个批处理系统的处理，然后运行一些特定的查询，并在控制面板展示。比如，可以先使用Cloud Dataflow处理日志，BigQuery做查询，Data Studio做控制面板。&lt;/p&gt;

&lt;p&gt;而我们的基于指标的监控系统，从Google的各种服务处收集了大量的指标，能几乎实时地提供粗粒度的信息。基本上其它基于日志或指标的监控系统有着和Google的系统类似的特性，当然也有例外，比如也有实时日志处理系统，或者细粒度的指标等。&lt;/p&gt;

&lt;p&gt;我们的报警和控制面板通常使用指标数据。指标监控系统的实时性能让工程师快速地发现问题。我们倾向于使用日志监控系统查找问题发生的根本原因，指标常常无法提供这方面的信息。&lt;/p&gt;

&lt;p&gt;报表不要求实时性，我们经常用日志处理系统来生成详细的报表，因为日志几乎总是比指标产生更准确的数据。&lt;/p&gt;

&lt;p&gt;如果你的报警是基于指标的，可能会临时增加一些基于日志的报警，比如你需要在某个异常事件发生时收到报警。就算你有这样的需求我们仍然推荐基于指标的报警系统，其实你可以在某个特殊事件发生时将计数器指标加一，然后创建一个基于这个指标的报警。这样做的好处是把所有报警配置都放在一个地方，便于管理（详见“管理你的监控系统”）。&lt;/p&gt;

&lt;h3 id=&quot;案例&quot;&gt;案例&lt;/h3&gt;

&lt;p&gt;接下来这些真实的例子解释了如何选择不同的监控系统。&lt;/p&gt;

&lt;h4 id=&quot;将信息从日志移到指标&quot;&gt;将信息从日志移到指标&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;。对于App Engine的用户来说，HTTP状态码对于错误诊断来说非常重要，它存在于日志而不是指标中。指标控制面板只能显示一个总的错误率，无法包含错误相关的其它信息，如错误码或者错误发生的原因。因此，诊断一个问题需要这样做：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;检查总体错误率的图表，找到错误发生的时间&lt;/li&gt;
  &lt;li&gt;读取日志文件，查找包含错误信息的日志记录&lt;/li&gt;
  &lt;li&gt;尝试在错误日志和错误图表之间建立联系&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;日志记录无法体现出“数量”，因此很难从日志判断某一个错误是否频繁发生。日志还包含很多其它不相关的信息，让查找错误的根源变得很难。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;建议的解决方案&lt;/strong&gt;。 App Engine开发组决定将HTTP状态码输出成一个指标（比如，requests_total{status=404} 或者 requests_total{status=500})。由于不同HTTP状态码的数量是有限的，这样做不会导致指标数据的数据量大小增长到一个不可接受的水平，但是这些数据可以用于图表和报警。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;。新的指标允许App Engine的开发组升级监控图表，分别显示不同的错误类型。用户则可以基于错误码快速判断可能发生的问题。同时，我们还能为客户端和服务端错误分别设置不同的报警阈值，让报警更加准确。&lt;/p&gt;

&lt;h3 id=&quot;优化日志和指标&quot;&gt;优化日志和指标&lt;/h3&gt;

&lt;p&gt;问题。Ads SRE组维护这近50个服务，由不同的语言和框架开发。SRE组将日志作为检验SLO（SLO compiance）的权威数据源。为了统计错误情况，为每个服务编写了专门的日志处理脚本。这里有一个日志处理脚本的例子：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;If the HTTP status code was in the range (500, 599)

AND the 'SERVER ERROR' field of the log is populated

AND DEBUG cookie was not set as part of the request

AND the url did not contain '/reports'

AND the 'exception' field did not contain 'com.google.ads.PasswordException'

THEN increment the error counter by 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;。脚本很难维护，并且用了一些指标监控系统没有的数据。因为报警由指标驱动，有时候报警可能跟用户无关的错误引发的。每一个报警都需要一个显式分类的步骤来判断它是否跟用户相关，这拖慢了响应速度。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;建议的解决方案&lt;/strong&gt;。 SRE组开发了一个库，植入到业务处理系统中，如果判断出某一个错误会影响用户的请求，将判断的结果写入日志并且导出一个指标数据，提高日志和指标的一致性。如果指标显示某个服务返回了错误，日志包含那个错误以及一些请求相关的数据，帮助重现和诊断错误。日志中任何SLO相关的错误同时也会改变SLI指标，SRE组可以据此创建报警。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结果&lt;/strong&gt;。通过创建一个跨服务的统一控制接口，运维组重用了工具和报警逻辑，避免为不同的服务重复开发运维系统。去除了复杂的、服务特定的日志处理逻辑，所有的服务都从中受益，获得了扩展性的提升。一旦报警跟SLO直接绑定就变得清晰可执行了，因此错误报警显著减少&lt;/p&gt;

&lt;h3 id=&quot;为数据源保留日志&quot;&gt;为数据源保留日志&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;。 在诊断生产环境中的问题时，运维组经常会查看受影响的实体ID，判断对用户的影响和问题的根源。在App Engine早期的时候，这样的调查工作需要用到只有日志中才有的数据。处理每个事故，运维组不得不执行一些一次性的查询，这就增加了事故恢复的时间——需要几分钟来编写正确的查询，然后花些时间查询日志。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;建议的解决方案&lt;/em&gt;。 起初，运维组讨论了是否应该用指标来代替日志。然而，实体ID可能有几百万种不同的值，要把它们做成指标不太可行。最终，运维组决定编写一个脚本来执行那些一次性的查询，并在报警邮件中说明需要运行哪个脚本。在需要的时候，运维人员可以直接把脚本拷贝到命令行执行。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;结果&lt;/em&gt;。运维组不再需要花太多精力管理一次性查询，获取结果的速度也更快了（尽管还没到指标查询那么快）。他们还有一个备用方案：报警发生时自动运行脚本，用一个小服务器定时查询日志，能获得半实时(semi-fresh)的数据。&lt;/p&gt;

&lt;h2 id=&quot;管理你的监控系统&quot;&gt;管理你的监控系统&lt;/h2&gt;

&lt;p&gt;你的监控系统与你运行的任何其他服务一样重要。 因此，应该给予适当的关注。&lt;/p&gt;

&lt;h3 id=&quot;将配置视为代码&quot;&gt;将配置视为代码&lt;/h3&gt;

&lt;p&gt;将系统配置作为代码处理并存储在版本控制系统中是常见的做法，好处也很明显：更改历史记录，从特定更改到任务跟踪系统的链接，更简单的回滚和格式检查，以及强制执行的代码审查过程。&lt;/p&gt;

&lt;p&gt;我们强烈建议你将监控配置视为代码（有关配置的更多信息，请参阅第14章）。 支持文本缩进配置的监控系统比仅提供Web UI或CRUD样式API的系统更好。对于很多开源程序，这是标准的配置方法，它们只从配置文件读取信息。 一些第三方配置解决方案（如grafanalib）为传统UI配置增加了文本配置的方式。&lt;/p&gt;

&lt;h3 id=&quot;鼓励一致性&quot;&gt;鼓励一致性&lt;/h3&gt;

&lt;p&gt;大型公司会有多个工程团队使用监控系统，他们需要寻求一种良好的平衡：集中式的监控系统保证了一致性，但另一方面，各个团队可能希望自己能完全决定配置的设计。&lt;/p&gt;

&lt;p&gt;正确的解决方案取决于你的组织。 谷歌的方法随着时间的推移逐渐发展为基于单一框架的集中式监控服务。 说这套解决方案适用于我们有几个原因。 单一的框架使工程师在换团队时能够更容易上手，并使调试过程中的协作变得更加容易。 我们还提供集中式仪表盘服务，每个团队的仪表盘对于其它团队都是可发现和可访问的。 如果你很容易了解其他团队的仪表盘，则可以更快地调试你们的问题。&lt;/p&gt;

&lt;p&gt;如果可能，尽量让基本监控覆盖很容易做。 如果你的所有服务都导出一组一致的基本指标，则可以在整个组织中自动收集这些指标，并提供一组一致的仪表盘。 这也意味着任何新组件都自动具有基本监视功能。公司的许多团队 - 甚至是非工程团队 - 都可以使用这些监控数据。&lt;/p&gt;

&lt;h3 id=&quot;倾向于松耦合&quot;&gt;倾向于松耦合&lt;/h3&gt;

&lt;p&gt;随着业务需求的变化，一年后您的生产系统看起来会有所不同。 同样，你的监控系统需要随着时间的推移而发展，因为它监控的服务会出现不同的故障模式。&lt;/p&gt;

&lt;p&gt;我们建议保持监控系统的组件松耦合。 你应该有稳定的接口来配置每个组件和传递监控数据。 但负责收集、存储、警告和可视化的组件应该相互独立。 稳定的接口使得更换任何给定组件更容易，如果你有更好的替代组件的话。&lt;/p&gt;

&lt;p&gt;将功能拆分为单个组件在开源世界中变得越来越流行。 十年前，像Zabbix这样的监控系统将所有功能集中到一个组件中。现在的设计通常将收集和规则评估（使用Prometheus服务器之类的解决方案）、长期时间序列存储（InfluxDB）、警报聚合（Alertmanager）和仪表盘（Grafana）等拆分开来。&lt;/p&gt;

&lt;p&gt;在本书撰写时，至少有两种流行用于检测软件和输出指标的开放标准：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;statsd&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;度量聚合守护进程最初由Etsy编写，现在移植到大多数编程语言中。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prometheus&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;一种开源监控解决方案，具有灵活的数据模型，支持指标标签和强大的直方图功能。 Prometheus正在被标准化为OpenMetrics，其他系统现在也采用Prometheus格式。&lt;/p&gt;

&lt;p&gt;独立的仪表盘系统可以使用多个数据源的数据，对服务状态进行集中的统一的展示。 谷歌最近在实践中看到了这一好处：我们的旧监控系统（Borgmon3）将仪表盘与报警规则放在同一配置中。在迁移到新系统（Monarch）时，我们决定将仪表盘移动到单独的服务（Viceroy）中。 由于Viceroy不是Borgmon或Monarch的组成部分，因此Monarch的功能要求较少。 由于用户可以使用Viceroy显示来自两个监控系统的数据，因此他们可以逐渐从Borgmon迁移到Monarch。&lt;/p&gt;

&lt;h2 id=&quot;有目的的指标&quot;&gt;有目的的指标&lt;/h2&gt;

&lt;p&gt;第5章介绍了在系统的错误数阈值快接近时如何使用SLI指标进行监控和报警。 SLI指标是您在基于SLO的警报触发时要检查的第一个指标。 SLI指标应显示在服务仪表盘的显眼位置，最好位于其首页上。&lt;/p&gt;

&lt;p&gt;在调查SLO报警触发的原因时，你很可能无法从SLO仪表盘获得足够的信息。 这些仪表盘告诉你SLO被触发了，但不会告诉你为什么。 监控仪表盘还应显示其它哪些数据？&lt;/p&gt;

&lt;p&gt;我们发现了一些指定指标的指导意见。 指标应提供合理的监控，便于调查生产环境的问题，同时提供有服务相关的很多信息。&lt;/p&gt;

&lt;h3 id=&quot;预期的变化&quot;&gt;预期的变化&lt;/h3&gt;

&lt;p&gt;在诊断SLO报警时，你需要能够从告知你有影响用户的问题发生的报警指标切换到能让你知道问题根源的指标。 最近对服务进行的预期更改可能是错误的。添加一些能监视生产环境中的任何改动的指标。对于监控触发条件，我们有如下的建议：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;监控二进制文件的版本。&lt;/li&gt;
  &lt;li&gt;监控命令行参数，尤其是在使用这些参数开启和禁用某些服务功能时。&lt;/li&gt;
  &lt;li&gt;如果配置数据动态推送到你的服务，请监视此动态配置的版本。
如果系统中的任何部分未进行版本控制，你可以监控上次编译或打包的时间戳。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当你尝试将服务中断与上线关联起来时，查看与报警链接的图表/仪表盘要比查看CI / CD（持续集成/持续交付）系统日志更容易。&lt;/p&gt;

&lt;h3 id=&quot;服务依赖&quot;&gt;服务依赖&lt;/h3&gt;

&lt;p&gt;即使你的服务没有更改，其任何依赖项都可能会更改或出现问题，因此你还应该监视来自直接依赖项的响应。&lt;/p&gt;

&lt;p&gt;将每一个服务依赖项返回的字节数、延迟和响应代码都输出。 在选择图标展示指标时，请牢记四个黄金信号。你可以给指标额外定义标签，通过响应代码，RPC（远程过程调用）方法名称和服务依赖项的名称对指标进行进一步细分。&lt;/p&gt;

&lt;p&gt;理想情况下，你可以检测较低级别的RPC客户端库以导出这些度量标准，而不是要求每个RPC客户端库导出它们.检测客户端库提供更高的一致性，并允许你免费监控新的依赖关系。&lt;/p&gt;

&lt;p&gt;你有时会遇到提供非常狭窄的API的依赖项，其中所有功能都可通过名为Get，Query或同样无用的单个RPC获得，并且实际命令被指定为此RPC的参数。 客户端库中的单个检测点与此类依赖关系不符：你将观察到延迟的高度变化和一些百分比的错误，这些错误可能会或可能不会表明此不透明API的某些部分完全失败。 如果这种依赖关系很重要，那么你有几个选项可以很好地监控它：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;导出单独的度量标准以定制依赖关系，以便度量标准可以解压缩它们收到的请求以获取实际信号。&lt;/li&gt;
  &lt;li&gt;要求依赖项所有者执行重写以导出更广泛的API，该API支持跨单独的RPC服务和方法拆分的单独功能。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;资源容量&quot;&gt;资源容量&lt;/h3&gt;

&lt;p&gt;目的是监视和跟踪服务所依赖的每种资源的使用情况。 某些资源具有你不能超过的硬限制，例如分配给你的应用程序的RAM，磁盘或CPU配额。 其他资源（如打开文件描述符，任何线程池中的活动线程，队列中的等待时间或写入的日志量）可能没有明确的硬限制，但仍需要管理。&lt;/p&gt;

&lt;p&gt;根据使用的编程语言不同，你还应该监视些其他资源：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于Java：堆和元空间大小，以及根据垃圾收集类型不同而使用一些特殊指标&lt;/li&gt;
  &lt;li&gt;对于Go：协程的数量
语言本身为跟踪这些资源提供了不同的支持。 除了如第5章所述警告重大事件之外，你可能还需要设置当某些特定资源接近耗尽时触发的报警，例如：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当资源有硬性的上限时&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;当资源使用超过阈值会导致性能下降时&lt;/li&gt;
  &lt;li&gt;你应该对所有的资源都设置监控 - 即使是服务本身能很好地管理的资源。 这些指标对于容量和资源规划至关重要。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;服务流量状况&quot;&gt;服务流量状况&lt;/h3&gt;

&lt;p&gt;最好能增加跟流量相关的指标或指标标签，以便仪表盘按状态代码显示详细的流量情况（除非你的服务用于SLI目的的指标已包含此信息）。以下是一些建议：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于HTTP流量，监视所有响应代码，即使它们没有达到报警的级别，因为某些响应代码可能由不正确的客户端行为触发。&lt;/li&gt;
  &lt;li&gt;如果你对用户设定了频率速率限制或流量限制，监控系统应当统计有多少用户的请求因为流量限制被拒绝了。
流量监控图表可以帮助你确定一个线上的改动在何时导致了显著的错误数量的变化。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;指定有目的指标&quot;&gt;指定有目的指标&lt;/h3&gt;

&lt;p&gt;每个输出的指标都应该有用它的目的性。 不要仅仅因为某些指标很容易产生就轻易地将它们输出。 相反，应该想想输出的这些指标会被如何使用。Metric design, or lack thereof, has implications。&lt;/p&gt;

&lt;p&gt;理想情况下，用于报警的指标只在系统出问题时发生显著的变化，而在系统正常运行时不会变化。 但是，调试用的指标没有这些要求 - 它们旨在告诉我们当报警触发时系统内部发生了什么。 好的调试指标能指示系统中可能导致问题的地方。 编写事后调查时，想一下还有其它哪些指标能帮助你更快地诊断问题。&lt;/p&gt;

&lt;h3 id=&quot;测试报警逻辑&quot;&gt;测试报警逻辑&lt;/h3&gt;

&lt;p&gt;在理想情况下，监控和报警的代码应遵循与系统开发代码相同的测试标准。虽然Prometheus的开发人员正在讨论开发用于监控的单元测试，但目前广泛采用的监控系统还没有单元测试的支持。&lt;/p&gt;

&lt;p&gt;在Google，我们使用领域特定的语言测试我们的监控和警报，该语言允许我们创建合成时间序列（synthetic time series）。 然后，我们根据派生时间序列中的值，或特定报警的触发状态以及标签是否存在来编写测试断言。&lt;/p&gt;

&lt;p&gt;监控和报警通常是一个多阶段过程，因此需要多个单元测试集。 这个领域仍然有待发展，但如果你想在某个时候实施监控测试，我们建议采用三层方法，如图4-1所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/4-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图4-1：报警逻辑测试 &lt;/center&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;二元报告：检查输出的标准的值是否在预期的某些条件下发生变化。&lt;/li&gt;
  &lt;li&gt;监控配置： 确保规则评估产生了预期的结果，并且特定条件会产生预期报警。&lt;/li&gt;
  &lt;li&gt;警报配置： 测试生成的报警是否按照报警的标签值路由到了预定目的地。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果你无法综合测试监控系统，或者你的监控的某一个阶段无法进行测试，可以考虑创建一个运行系统来输出一些公认的指标，例如请求数和错误数。 你可以据此来验证时间序列和报警。 你的报警规则很可能在设置之后的几个月或几年内都不会触发，你需要确信当指标超过某个阈值时，有意义的报警通知会发给正确的工程师。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;p&gt;由于SRE角色负责生产系统的可靠性，SRE通常需要非常熟悉服务的监控系统及其功能。如果没有这方面的知识，SRE可能不知道在哪里查看监控，如何识别异常行为，或者如何在紧急情况下找到所需的信息。&lt;/p&gt;

&lt;p&gt;我们希望通过指出我们认为有用的监控系统功能及其为什么我们认为它们有用，可以帮助你评估监控策略和需求有多匹配，探索你可能能够利用的一些其他功能，并考虑你可能想要做出的改变。也许你会发现将一些指标和日志监控结合起来的策略很有用;具体如何结合取决于应用场景。注意收集的指标要有其目的性，可能是为了更好地进行容量规划，辅助调试或直接通知你发生了什么问题。&lt;/p&gt;

&lt;p&gt;一旦你有了监控系统，需要确保它可见且可用。 为此，我们还建议你测试你的监控设置。良好的监控系统能带来好的回报。好好想想什么样的监控系统最适合你的需求，不断地摸索直到找到最好的那个，这是一笔很值得的投资。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">监控涉及到多种类型的数据，包括监控指标，纯文本日志，结构化日志，分布式跟踪日志， event introspection。 以上各种数据都有它们各自的用处，但是本章主要讨论监控指标和结构化日志。根据我们的经验，这两种数据最适合SRE的基础监控需求。 从根本上讲，监控系统应当能够透视系统的内部，当需要判断服务的健康状态和诊断服务问题时，这是最关键的需求。在第一版SRE的第6章中给出了一些基本的监控方法，并且提到SRE监控他们系统的主要目的有： 当达到阈值时触发报警 诊断和分析服务问题 展示系统的可视化信息 获取系统资源使用情况或服务健康状况的变化趋势，以便做长期计划 比较变更前后的系统变化或一个实验的两组样本的不同 这些用例的不同重要程度能指导你在选择或构建一个监控系统时做出权衡。 本章讨论Google如何管理监控系统，并提供一些如何选择和运行监控系统的指导意见 监控策略的特征 在选择监控系统时，理解那些你关心的功能并对它们进行一个优先级的排序很重要。如果你正在评估一个监控系统，本节提到的这些特性可以帮助你思考哪种方案最适合你。如果你已经有一个在运行的监控系统了，你可以考虑使用现有解决方案的一些别的特性。根据你的需求，一个监控系统可能就能解决你所有的问题，也可能你需要组合好几个监控系统。 速度 不同的组织对于数据的时效性和获取数据的速度有不同的要求。 你需要用到数据的时候，数据就应该立马可用。数据的时效性影响的是当系统出错后多长时间监控系统才会通知你。另外，不及时的数据可能会导致因为错误的数据而采取不合适的行动。举个例子，关于事故响应，事故的发生时间和监控系统能够反映出结果的时间间隔如果太长，你可能会认为某一个改动没有产生什么不好的影响，或者你认为这个修改和某一个结果之间没有联系。超过4到5分钟才能获取的数据将会显著地影响你快速响应。 如果你是基于速度在选择监控系统，那你需要首先确定对速度的需求。当你查询大量数据时，数据的获取速度通常都是个问题。如果需要汇总多个监控系统的数据，图形的加载需要费一些时间。监控系统如果从输入数据生成新的时间序列，则可以对一些通用查询进行预先计算，从而加速图形的加载。 计算 很多应用场景都要求支持各种复杂计算。至少，你会希望能从监控系统获取几个月时间跨度的数据。没有长期数据的视图，你很难判断像系统增长性这样的长期趋势。关于粒度，汇总数据对于制作增长计划很有效。详细地存储每一个指标会有助于回答这样的问题：这样的异常行为以前发生过吗？但是，数据可能会耗费昂贵的存储空间，或者难以检索。 理想的事件或资源消耗的指标是递增的计数器。利用计数器，监控系统可以基于时间做计算——比如，报告每秒的访问量。如果在更长的时间范围做计算，你就可以实现implement the building blocks for SLO burn-based alerting (see Chapter 5). 最后，支持种类更齐全的统计函数很有用，因为细微的操作可能会掩盖不好的行为。当记录访问延迟时，算数平均数只能告诉你访问比较慢，而支持百分比计算的监控系统能让你一眼就看出是50%,5%还是1%的访问太慢。如果你目前的监控系统不支持直接的百分比计算，你可以采取下面的替代方案： 通过把每一个访问的时间相加然后除以访问数量，从而计算出平均访问时间 每一个请求写入日志，然后对日志记录进行扫描或取样，从而计算出百分比 你也可以把原始的监控指标写到另一个离线分析系统中，用于生成周报或月报，或者执行一些难以在监控系统中直接进行的复杂的计算。 交互界面 一个健壮的监控系统应该允许你简洁地在图形（graph）中显示时间序列数据，同时也能将数据结构化到表或各种类型的图表（chart）中。Dashborad是显示监控信息的主要的界面，所以选择显示格式很重要，要能最清晰地显示你所关心的数据。可以选择热力图、直方图和对数刻度图。 你可能需要为同一数据提供不同的视图给不同的用户。高级管理人员希望看到的信息跟SRE的大不相同。针对使用者专门创建对他们来说有意义的dashborad。每一组dashborad，对同一类型的数据显示要保持一致，这样方便沟通。 你可能需要实时图形化显示某一指标的不同聚合结果，比如分别根据机器类型、服务器版本或请求类型进行聚合。It’s a good idea for your team to be comfortable with performing ad hoc drill-downs on your data。根据不同的指标对你的数据进行切片，在需要的时候你可以查找数据之间的联系和模式。 报警 报警分级很有必要，不同的级别采取不同的响应机制。 给报警设置不同的级别很有用，你可能会开一个工单用于跟踪一个低级别故障，调查可能会持续了一个多小时；100%的错误则属于紧急情况，需要立即响应。 报警屏蔽功能能避免不必要的分散值班工程师精力的干扰。比如： 当所有的服务器都出现同样的高故障率时，可以只针对全局的高故障率报警一次，不需要为每个服务器单独发送报警。 当你的服务所依赖的服务报警时，不需要为你的服务发出故障报警。不要忘记，一旦故障事件处理完了，记得恢复被屏蔽的故障报警。 你还需要确保在事件结束后不再屏蔽警报。 监控系统的可控控制程度决定了你应该使用第三方的监控服务，还是部署和运行自己的监控系统。Google内部开发了自己的监控系统，但是外面有很多开源或商业的监控系统可供选择。 监控数据的来源 选择何种监控系统也受将要使用的监控数据源的影响。这节讨论两种常用的数据源：日志和指标。还有些其它的有用的监控数据源这里没有讨论的，比如分布式跟踪和运行时introspection. 指标是属性和事件的数字度量，通常间隔一段固定的时间产生多个数据点。日志是只能追加的事件记录。本章的讨论主要集中在结构化日志，相比纯文本日志结构化日志更易于复杂查询和聚合工具。 Google基于日志的监控系统处理大量细粒度（highly granular）数据。事件发生到log可见之间不可避免的有些延迟。日志分析并不要求即时性，可以先经过一个批处理系统的处理，然后运行一些特定的查询，并在控制面板展示。比如，可以先使用Cloud Dataflow处理日志，BigQuery做查询，Data Studio做控制面板。 而我们的基于指标的监控系统，从Google的各种服务处收集了大量的指标，能几乎实时地提供粗粒度的信息。基本上其它基于日志或指标的监控系统有着和Google的系统类似的特性，当然也有例外，比如也有实时日志处理系统，或者细粒度的指标等。 我们的报警和控制面板通常使用指标数据。指标监控系统的实时性能让工程师快速地发现问题。我们倾向于使用日志监控系统查找问题发生的根本原因，指标常常无法提供这方面的信息。 报表不要求实时性，我们经常用日志处理系统来生成详细的报表，因为日志几乎总是比指标产生更准确的数据。 如果你的报警是基于指标的，可能会临时增加一些基于日志的报警，比如你需要在某个异常事件发生时收到报警。就算你有这样的需求我们仍然推荐基于指标的报警系统，其实你可以在某个特殊事件发生时将计数器指标加一，然后创建一个基于这个指标的报警。这样做的好处是把所有报警配置都放在一个地方，便于管理（详见“管理你的监控系统”）。 案例 接下来这些真实的例子解释了如何选择不同的监控系统。 将信息从日志移到指标 问题。对于App Engine的用户来说，HTTP状态码对于错误诊断来说非常重要，它存在于日志而不是指标中。指标控制面板只能显示一个总的错误率，无法包含错误相关的其它信息，如错误码或者错误发生的原因。因此，诊断一个问题需要这样做： 检查总体错误率的图表，找到错误发生的时间 读取日志文件，查找包含错误信息的日志记录 尝试在错误日志和错误图表之间建立联系 日志记录无法体现出“数量”，因此很难从日志判断某一个错误是否频繁发生。日志还包含很多其它不相关的信息，让查找错误的根源变得很难。 建议的解决方案。 App Engine开发组决定将HTTP状态码输出成一个指标（比如，requests_total{status=404} 或者 requests_total{status=500})。由于不同HTTP状态码的数量是有限的，这样做不会导致指标数据的数据量大小增长到一个不可接受的水平，但是这些数据可以用于图表和报警。 结果。新的指标允许App Engine的开发组升级监控图表，分别显示不同的错误类型。用户则可以基于错误码快速判断可能发生的问题。同时，我们还能为客户端和服务端错误分别设置不同的报警阈值，让报警更加准确。 优化日志和指标 问题。Ads SRE组维护这近50个服务，由不同的语言和框架开发。SRE组将日志作为检验SLO（SLO compiance）的权威数据源。为了统计错误情况，为每个服务编写了专门的日志处理脚本。这里有一个日志处理脚本的例子： If the HTTP status code was in the range (500, 599) AND the 'SERVER ERROR' field of the log is populated AND DEBUG cookie was not set as part of the request AND the url did not contain '/reports' AND the 'exception' field did not contain 'com.google.ads.PasswordException' THEN increment the error counter by 1 问题。脚本很难维护，并且用了一些指标监控系统没有的数据。因为报警由指标驱动，有时候报警可能跟用户无关的错误引发的。每一个报警都需要一个显式分类的步骤来判断它是否跟用户相关，这拖慢了响应速度。 建议的解决方案。 SRE组开发了一个库，植入到业务处理系统中，如果判断出某一个错误会影响用户的请求，将判断的结果写入日志并且导出一个指标数据，提高日志和指标的一致性。如果指标显示某个服务返回了错误，日志包含那个错误以及一些请求相关的数据，帮助重现和诊断错误。日志中任何SLO相关的错误同时也会改变SLI指标，SRE组可以据此创建报警。 结果。通过创建一个跨服务的统一控制接口，运维组重用了工具和报警逻辑，避免为不同的服务重复开发运维系统。去除了复杂的、服务特定的日志处理逻辑，所有的服务都从中受益，获得了扩展性的提升。一旦报警跟SLO直接绑定就变得清晰可执行了，因此错误报警显著减少 为数据源保留日志 问题。 在诊断生产环境中的问题时，运维组经常会查看受影响的实体ID，判断对用户的影响和问题的根源。在App Engine早期的时候，这样的调查工作需要用到只有日志中才有的数据。处理每个事故，运维组不得不执行一些一次性的查询，这就增加了事故恢复的时间——需要几分钟来编写正确的查询，然后花些时间查询日志。 建议的解决方案。 起初，运维组讨论了是否应该用指标来代替日志。然而，实体ID可能有几百万种不同的值，要把它们做成指标不太可行。最终，运维组决定编写一个脚本来执行那些一次性的查询，并在报警邮件中说明需要运行哪个脚本。在需要的时候，运维人员可以直接把脚本拷贝到命令行执行。 结果。运维组不再需要花太多精力管理一次性查询，获取结果的速度也更快了（尽管还没到指标查询那么快）。他们还有一个备用方案：报警发生时自动运行脚本，用一个小服务器定时查询日志，能获得半实时(semi-fresh)的数据。 管理你的监控系统 你的监控系统与你运行的任何其他服务一样重要。 因此，应该给予适当的关注。 将配置视为代码 将系统配置作为代码处理并存储在版本控制系统中是常见的做法，好处也很明显：更改历史记录，从特定更改到任务跟踪系统的链接，更简单的回滚和格式检查，以及强制执行的代码审查过程。 我们强烈建议你将监控配置视为代码（有关配置的更多信息，请参阅第14章）。 支持文本缩进配置的监控系统比仅提供Web UI或CRUD样式API的系统更好。对于很多开源程序，这是标准的配置方法，它们只从配置文件读取信息。 一些第三方配置解决方案（如grafanalib）为传统UI配置增加了文本配置的方式。 鼓励一致性 大型公司会有多个工程团队使用监控系统，他们需要寻求一种良好的平衡：集中式的监控系统保证了一致性，但另一方面，各个团队可能希望自己能完全决定配置的设计。 正确的解决方案取决于你的组织。 谷歌的方法随着时间的推移逐渐发展为基于单一框架的集中式监控服务。 说这套解决方案适用于我们有几个原因。 单一的框架使工程师在换团队时能够更容易上手，并使调试过程中的协作变得更加容易。 我们还提供集中式仪表盘服务，每个团队的仪表盘对于其它团队都是可发现和可访问的。 如果你很容易了解其他团队的仪表盘，则可以更快地调试你们的问题。 如果可能，尽量让基本监控覆盖很容易做。 如果你的所有服务都导出一组一致的基本指标，则可以在整个组织中自动收集这些指标，并提供一组一致的仪表盘。 这也意味着任何新组件都自动具有基本监视功能。公司的许多团队 - 甚至是非工程团队 - 都可以使用这些监控数据。 倾向于松耦合 随着业务需求的变化，一年后您的生产系统看起来会有所不同。 同样，你的监控系统需要随着时间的推移而发展，因为它监控的服务会出现不同的故障模式。 我们建议保持监控系统的组件松耦合。 你应该有稳定的接口来配置每个组件和传递监控数据。 但负责收集、存储、警告和可视化的组件应该相互独立。 稳定的接口使得更换任何给定组件更容易，如果你有更好的替代组件的话。 将功能拆分为单个组件在开源世界中变得越来越流行。 十年前，像Zabbix这样的监控系统将所有功能集中到一个组件中。现在的设计通常将收集和规则评估（使用Prometheus服务器之类的解决方案）、长期时间序列存储（InfluxDB）、警报聚合（Alertmanager）和仪表盘（Grafana）等拆分开来。 在本书撰写时，至少有两种流行用于检测软件和输出指标的开放标准： statsd 度量聚合守护进程最初由Etsy编写，现在移植到大多数编程语言中。 Prometheus 一种开源监控解决方案，具有灵活的数据模型，支持指标标签和强大的直方图功能。 Prometheus正在被标准化为OpenMetrics，其他系统现在也采用Prometheus格式。 独立的仪表盘系统可以使用多个数据源的数据，对服务状态进行集中的统一的展示。 谷歌最近在实践中看到了这一好处：我们的旧监控系统（Borgmon3）将仪表盘与报警规则放在同一配置中。在迁移到新系统（Monarch）时，我们决定将仪表盘移动到单独的服务（Viceroy）中。 由于Viceroy不是Borgmon或Monarch的组成部分，因此Monarch的功能要求较少。 由于用户可以使用Viceroy显示来自两个监控系统的数据，因此他们可以逐渐从Borgmon迁移到Monarch。 有目的的指标 第5章介绍了在系统的错误数阈值快接近时如何使用SLI指标进行监控和报警。 SLI指标是您在基于SLO的警报触发时要检查的第一个指标。 SLI指标应显示在服务仪表盘的显眼位置，最好位于其首页上。 在调查SLO报警触发的原因时，你很可能无法从SLO仪表盘获得足够的信息。 这些仪表盘告诉你SLO被触发了，但不会告诉你为什么。 监控仪表盘还应显示其它哪些数据？ 我们发现了一些指定指标的指导意见。 指标应提供合理的监控，便于调查生产环境的问题，同时提供有服务相关的很多信息。 预期的变化 在诊断SLO报警时，你需要能够从告知你有影响用户的问题发生的报警指标切换到能让你知道问题根源的指标。 最近对服务进行的预期更改可能是错误的。添加一些能监视生产环境中的任何改动的指标。对于监控触发条件，我们有如下的建议： 监控二进制文件的版本。 监控命令行参数，尤其是在使用这些参数开启和禁用某些服务功能时。 如果配置数据动态推送到你的服务，请监视此动态配置的版本。 如果系统中的任何部分未进行版本控制，你可以监控上次编译或打包的时间戳。 当你尝试将服务中断与上线关联起来时，查看与报警链接的图表/仪表盘要比查看CI / CD（持续集成/持续交付）系统日志更容易。 服务依赖 即使你的服务没有更改，其任何依赖项都可能会更改或出现问题，因此你还应该监视来自直接依赖项的响应。 将每一个服务依赖项返回的字节数、延迟和响应代码都输出。 在选择图标展示指标时，请牢记四个黄金信号。你可以给指标额外定义标签，通过响应代码，RPC（远程过程调用）方法名称和服务依赖项的名称对指标进行进一步细分。 理想情况下，你可以检测较低级别的RPC客户端库以导出这些度量标准，而不是要求每个RPC客户端库导出它们.检测客户端库提供更高的一致性，并允许你免费监控新的依赖关系。 你有时会遇到提供非常狭窄的API的依赖项，其中所有功能都可通过名为Get，Query或同样无用的单个RPC获得，并且实际命令被指定为此RPC的参数。 客户端库中的单个检测点与此类依赖关系不符：你将观察到延迟的高度变化和一些百分比的错误，这些错误可能会或可能不会表明此不透明API的某些部分完全失败。 如果这种依赖关系很重要，那么你有几个选项可以很好地监控它： 导出单独的度量标准以定制依赖关系，以便度量标准可以解压缩它们收到的请求以获取实际信号。 要求依赖项所有者执行重写以导出更广泛的API，该API支持跨单独的RPC服务和方法拆分的单独功能。 资源容量 目的是监视和跟踪服务所依赖的每种资源的使用情况。 某些资源具有你不能超过的硬限制，例如分配给你的应用程序的RAM，磁盘或CPU配额。 其他资源（如打开文件描述符，任何线程池中的活动线程，队列中的等待时间或写入的日志量）可能没有明确的硬限制，但仍需要管理。 根据使用的编程语言不同，你还应该监视些其他资源： 对于Java：堆和元空间大小，以及根据垃圾收集类型不同而使用一些特殊指标 对于Go：协程的数量 语言本身为跟踪这些资源提供了不同的支持。 除了如第5章所述警告重大事件之外，你可能还需要设置当某些特定资源接近耗尽时触发的报警，例如： 当资源有硬性的上限时 当资源使用超过阈值会导致性能下降时 你应该对所有的资源都设置监控 - 即使是服务本身能很好地管理的资源。 这些指标对于容量和资源规划至关重要。 服务流量状况 最好能增加跟流量相关的指标或指标标签，以便仪表盘按状态代码显示详细的流量情况（除非你的服务用于SLI目的的指标已包含此信息）。以下是一些建议： 对于HTTP流量，监视所有响应代码，即使它们没有达到报警的级别，因为某些响应代码可能由不正确的客户端行为触发。 如果你对用户设定了频率速率限制或流量限制，监控系统应当统计有多少用户的请求因为流量限制被拒绝了。 流量监控图表可以帮助你确定一个线上的改动在何时导致了显著的错误数量的变化。 指定有目的指标 每个输出的指标都应该有用它的目的性。 不要仅仅因为某些指标很容易产生就轻易地将它们输出。 相反，应该想想输出的这些指标会被如何使用。Metric design, or lack thereof, has implications。 理想情况下，用于报警的指标只在系统出问题时发生显著的变化，而在系统正常运行时不会变化。 但是，调试用的指标没有这些要求 - 它们旨在告诉我们当报警触发时系统内部发生了什么。 好的调试指标能指示系统中可能导致问题的地方。 编写事后调查时，想一下还有其它哪些指标能帮助你更快地诊断问题。 测试报警逻辑 在理想情况下，监控和报警的代码应遵循与系统开发代码相同的测试标准。虽然Prometheus的开发人员正在讨论开发用于监控的单元测试，但目前广泛采用的监控系统还没有单元测试的支持。 在Google，我们使用领域特定的语言测试我们的监控和警报，该语言允许我们创建合成时间序列（synthetic time series）。 然后，我们根据派生时间序列中的值，或特定报警的触发状态以及标签是否存在来编写测试断言。 监控和报警通常是一个多阶段过程，因此需要多个单元测试集。 这个领域仍然有待发展，但如果你想在某个时候实施监控测试，我们建议采用三层方法，如图4-1所示。 图4-1：报警逻辑测试 二元报告：检查输出的标准的值是否在预期的某些条件下发生变化。 监控配置： 确保规则评估产生了预期的结果，并且特定条件会产生预期报警。 警报配置： 测试生成的报警是否按照报警的标签值路由到了预定目的地。 如果你无法综合测试监控系统，或者你的监控的某一个阶段无法进行测试，可以考虑创建一个运行系统来输出一些公认的指标，例如请求数和错误数。 你可以据此来验证时间序列和报警。 你的报警规则很可能在设置之后的几个月或几年内都不会触发，你需要确信当指标超过某个阈值时，有意义的报警通知会发给正确的工程师。 结论 由于SRE角色负责生产系统的可靠性，SRE通常需要非常熟悉服务的监控系统及其功能。如果没有这方面的知识，SRE可能不知道在哪里查看监控，如何识别异常行为，或者如何在紧急情况下找到所需的信息。 我们希望通过指出我们认为有用的监控系统功能及其为什么我们认为它们有用，可以帮助你评估监控策略和需求有多匹配，探索你可能能够利用的一些其他功能，并考虑你可能想要做出的改变。也许你会发现将一些指标和日志监控结合起来的策略很有用;具体如何结合取决于应用场景。注意收集的指标要有其目的性，可能是为了更好地进行容量规划，辅助调试或直接通知你发生了什么问题。 一旦你有了监控系统，需要确保它可见且可用。 为此，我们还建议你测试你的监控设置。良好的监控系统能带来好的回报。好好想想什么样的监控系统最适合你的需求，不断地摸索直到找到最好的那个，这是一笔很值得的投资。</summary></entry><entry><title type="html">第三章 SLO工程案例学习</title><link href="http://localhost:4000/sre/2020/01/03/SLO%E6%A1%88%E4%BE%8B%E5%AD%A6%E4%B9%A0/" rel="alternate" type="text/html" title="第三章 SLO工程案例学习" /><published>2020-01-03T00:00:00+08:00</published><updated>2020-01-03T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/03/SLO%E6%A1%88%E4%BE%8B%E5%AD%A6%E4%B9%A0</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/03/SLO%E6%A1%88%E4%BE%8B%E5%AD%A6%E4%B9%A0/">&lt;blockquote&gt;
  &lt;p&gt;名次解释：&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;SLI：服务质量指标、该服务的某项服务质量的一个具体量化指标。例如：&lt;code class=&quot;highlighter-rouge&quot;&gt;延迟&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;可用性&lt;/code&gt;&lt;/li&gt;
    &lt;li&gt;SLO：服务质量目标、服务的某个SLI的目标值/范围。例如：搜索请求的平均延迟 &amp;lt; 100ms。&lt;/li&gt;
    &lt;li&gt;SLA：服务质量协议、服务与用户之间的一个明确的协议，描述达到/未达到SLO之后的后果。&lt;/li&gt;
    &lt;li&gt;错误预算： 1 - 可靠性目标&lt;/li&gt;
  &lt;/ul&gt;

&lt;/blockquote&gt;

&lt;!-- more --&gt;

&lt;p&gt;尽管SRE的许多原则都是在Google内部形成的，但它的原则早已存在于Google之外。许多Google SRE的标准已被业内多个组织实践应用。&lt;/p&gt;

&lt;p&gt;SLO是SRE模型的基础。自从我们组建了客户可靠性工程（CRE）团队——这是一组帮助Google Cloud Platform（GCP）客户构建更可靠的服务的经验丰富的SRE——几乎与每个客户交互都以SLO开始以SLO结束。&lt;/p&gt;

&lt;p&gt;我们在这里介绍了两个不同行业的公司事迹，概述了他们在与Google CRE团队合作时采纳SLO和基于错误预算的方法的过程。有关SLO和错误预算的讨论，请参阅本书的第2章和第一本书的第3章。&lt;/p&gt;

&lt;h2 id=&quot;evernote的slo故事&quot;&gt;Evernote的SLO故事&lt;/h2&gt;
&lt;p&gt;Evernote是一款跨平台的APP，可帮助个人和团队创建、整合和共享信息。在全球拥有超过2.2亿用户，我们在平台内存储了超过120亿条信息——包括文本笔记、文件和附件/图像。在后台，Evernote服务由750个以上的MySQL实例支持。&lt;/p&gt;

&lt;p&gt;我们向Evernote引入了SLO的概念，并将其作为更广泛的技术改造的一部分，旨在提高工程速度，同时保持服务质量。我们的目标包括：&lt;/p&gt;

&lt;p&gt;将工程重点从数据中心中冗余的繁重工作转移到客户实际关心的产品工程工作上。为此，我们停止运行物理数据中心并转移到公有云。&lt;/p&gt;

&lt;p&gt;调整运维和软件工程师的工作模式，旨在保持整体服务质量的同时提高变更速度。&lt;/p&gt;

&lt;p&gt;改进我们对SLA的看法，以确保我们更加关注故障对庞大的客户群所造成的的影响。&lt;/p&gt;

&lt;p&gt;这些目标对许多行业的组织而言可能都很熟悉。虽然没有一种方法可以全面实现这些类型的变更，但希望我们分享的经验可以为面临类似挑战的人提供有价值的参考意见。&lt;/p&gt;

&lt;h3 id=&quot;为什么evernote采用sre模型&quot;&gt;为什么Evernote采用SRE模型？&lt;/h3&gt;
&lt;p&gt;过渡开始阶段的Evernote的特点是传统的运维和开发分离：运维团队维护生产环境的稳定性，而开发团队的任务是为客户开发新的产品功能。这些目标通常是冲突的：开发团队 感觉被繁琐的流程所束缚，而运维团队又会因新代码在生产环境中引入新的问题变得不满。 当我们在这两个目标之间不断动摇时，运维和开发团队之间蔓延了一种不满和紧张的关系。我们希望达到一个双方都满意的节点，更好地平衡所涉及团队的不同需求。&lt;/p&gt;

&lt;p&gt;在五年多的时间里，我们尝试了各种方式解决这种传统二分法中的差距。在尝试了“你编码，你运行”的开发模式，以及“你编码，我们为你运行”的运维模式之后，我们转向了以SLO为中心的SRE方法。&lt;/p&gt;

&lt;p&gt;那么是什么促使Evernote向这个方向发展呢？&lt;/p&gt;

&lt;p&gt;在Evernote，我们将运维和开发的核心目标视为工程师专业化的独立发展方向。一个方向关注的是近乎7*24小时地持续为客户提供服务。另一个关注的是服务的扩展和发展，以满足客户未来的需求。近年来，这两个方向已经越来越接近，例如SRE和DevOps强调将软件开发应用于运维。（数据中心自动化和公有云的发展进一步推动了这种融合，这两者都为我们提供了一个可以完全由软件控制的数据中心。）另一方面，全栈所有权和持续部署也越来越多地应用于软件开发。&lt;/p&gt;

&lt;p&gt;SRE模型完全接受并包容了运维和开发间的差异，同时鼓励团队朝着共同的目标努力。它并不试图将运维工程师转变为应用程序开发人员，反之亦然。相反，它给出了一个共同的参考框架。根据我们的经验，由于使用错误预算/SLO方法的两个团队在交流时很少带着主观感情，所以在面对同样的情况时通常会做出类似的决定。&lt;/p&gt;

&lt;h3 id=&quot;slo简介正在进行的旅程&quot;&gt;SLO简介：正在进行的旅程&lt;/h3&gt;
&lt;p&gt;旅程的第一步是从物理数据中心迁移到Google云平台。当Evernote服务在GCP上稳定运行后，我们就引入了SLO。我们的目标有两个：&lt;/p&gt;

&lt;p&gt;确保所有团队都在Evernote SLO的新框架内工作。&lt;/p&gt;

&lt;p&gt;将Evernote的SLO纳入我们与Google 云团队的合作中，他们现在负责我们的底层基础架构。由于在整体模型中加入了新的合作伙伴，因此我们需要确保迁移到GCP不会影响我们对用户的承诺。&lt;/p&gt;

&lt;p&gt;在使用SLO约9个月后，Evernote已经开始实践使用其SLO的第3版了！&lt;/p&gt;

&lt;p&gt;在深入了解SLO的技术细节之前，要先从客户的角度开始提问： 你可以提供哪些承诺？与大多数服务类似，Evernote具有许多功能和选项，用户可以通过各种创造性方式使用这些功能和选项。我们希望在一开始就关注最重要和最常见的客户需求：Evernote服务的可用性，以便用户能够访问和同步多个客户端的内容。我们的SLO之旅从这个目标开始。通过关注服务正常运行时间， 我们完成了接入SLO的第一步。使用这种方法，我们可以清楚地表达我们衡量的内容以及衡量方法。&lt;/p&gt;

&lt;p&gt;我们的第一份SLO文件包含以下内容：&lt;/p&gt;

&lt;p&gt;SLO的定义&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;这是一个（服务、系统）可用时间的计算方法：为某些服务或者是方法，在月级别的统计周期内设定了99.95%的可用性。这个数据是我们基于内部客户支持团队、产品团队，尤其重要的是用户共同讨论得来的。我们特意选择将SLO和日历月而不是与滚动的时期进行关联，就是为了使我们在进行服务复查时保持专注有序。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;衡量什么，以及如何衡量它&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;衡量度量

    我们指定了一个服务终点，我们可以调用它来测试服务是否按预期运行。在我们的例子中，我们在服务中内置了一个状态页面，它可以运行我们的大部分堆栈并返回200状态代码（如果一切正常）。

如何度量

    我们想要一个定期调用状态页面的探测器。我们希望探测器完全位于我们的环境之外并独立于我们的环境，因此我们可以测试所有组件，包括负载均衡。我们的目标是确保我们可以统计到GCP服务以及evernote应用任何、所有异常。但是，我们不希望随机网络问题触发误报。我们选择使用专门建立和运行此类探测器的第三方公司。我们选择了Pingdom，但市场上还有很多其他产品。我们按如下方式进行衡量：   
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如何从监控数据计算SLO&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;最后，我们仔细记录了我们如何根据从Pingdom收到的原始数据计算SLO。例如，我们指定了如何考虑维护窗口：我们无法假设我们所有的数亿用户都知道我们发布的维护窗口。因此，不知情的用户会将这些窗口视为通用和无法解释的停机时间，因此我们的SLO计算将维护视为停机时间。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;一旦我们定义了SLO，我们就必须使它发挥最大的价值。 我们希望SLO能够推动软件和运维方面的变革，让我们的客户更快乐并让他们满意。 怎么做到最好？&lt;/p&gt;

&lt;p&gt;探测频率：我们每分钟轮询一次前端节点。&lt;/p&gt;

&lt;p&gt;探测器的位置：此设置是可配置的; 我们目前在北美和欧洲使用多个探测器&lt;/p&gt;

&lt;p&gt;“down”的定义：如果一个探测器检测结果为失败，那么这个节点会被标记为疑似宕机，然后第二个基于不同地理位置独立部署的探测机会进行第二次确认。如果第二次检查同样也失败了，出于计算SLO的目的这个节点会被标记为宕机。只要探测请求持续显示错误，那么这个节点会被持续标记为宕机。&lt;/p&gt;

&lt;p&gt;我们用SLO中有关错误预算的思维为方法来分配下一步工作的需要的资源。举例来说，如果我们没有达成上个月的SLO，这会促使我们高优（对系统、服务）进行目标明确的加固、改进和修复。我们制定最简原则：evernote团队以及google团队共同进行月级别 的SLO目标复查。在这个会议上，我们复核SLO的表现并对所有服务中断行为进行深入研究。基于针对上个月的上述分析而不是根因分析，我们制定了一些改进措施。&lt;/p&gt;

&lt;p&gt;在整个过程中，我们的指导原则是“过犹不及”。即使在SLO还没有达到完美的时候，它也足以在此期间指导我们进行改进。一个“完美”的SLO应该可以衡量每一个与我们服务有关的潜在用户交互设计并且解释所有的边界行为。虽然字面上看起来这是个好主意，但是如果要实现起来却要花费数月的时间去改进服务（如果真的可以这到完美）。相反，我们选择了一个初始SLO，涵盖了大多数（但不是全部）用户交互，这是服务质量的良好代理。&lt;/p&gt;

&lt;p&gt;自从我们开始执行SLO以来，根据从服务复盘以及响应客户有感的宕机事件中得到的启示，我们对SLO做了两次修改。因为我们一开始就没有追求完美SLO，为了适应业务的发展我们乐于做出改变。除了evernote团队与google进行月级别SLO复盘之外，我们也设定了一个6个月的SLO复盘周期，这个周期可以使SLO的维护达到一个平衡：既不会频繁更新，也不会使之过时。在不断修订SLO的过程中，我们也意识到了，期望的衡量标准和可以达到的衡量标准之间的平衡是很重要的。&lt;/p&gt;

&lt;p&gt;自引入SLO以来，我们的运维和开发团队之间的关系有了微妙但显著的改善。现在团队对成功有了共同的衡量标准，那就是：取消对服务质量的人为解释使两个团队达成了共同的观点和标准。在此我们试着举一个例子，2017年当我们不得不在短期内推动多个版本的发布任务时，SLO为我们提供了共同基础。当我们发现一个复杂的bug时，产品开发团队要求我们将常规的周级别发布任务分配到多个独立的发布窗口，每个发布窗口都会对客户产生潜在的影响。通过对问题进行有针对性的SLO计算以及消除方案中的人为主观因素，我们可以更好的量化客户感受并且通过把发布窗口由5个降为2个从而达到了减少了客户痛点的目的。&lt;/p&gt;

&lt;h3 id=&quot;打破客户与云服务商之间的隔阂&quot;&gt;打破客户与云服务商之间的隔阂&lt;/h3&gt;
&lt;p&gt;介于客户和云服务商之间的隔阂看起来是在所难免的。虽然google已经为运行evernote的GCP平台设定了SLO和SLA（服务等级协议），但是evernote有自己的SLO和SLA。期望两个技术团队会将彼此的SLA告知对方看起来是不现实的。&lt;/p&gt;

&lt;p&gt;evernote不希望存在这样的隔阂。当然我们也可以基于自己的SLO和底层的GCP平台的SLA建立起隔离域，相反从一开始我们就希望google可以理解性能表现对我们来说是多重要以及为什么这么重要。我们期望google和我们在目标上达成一致，让两家公司把evernote在可靠性方向的成败当作共同的职责。为了实现这一目标，我们需要一种方法可以：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;达成一致的目标

确保我们的合作伙伴（在此指google）真正清楚我们最关心哪些指标

共担成败
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;大多数服务商都为自己的云服务发布了SLO/SLA。虽然服务运行在此框架下很重要，但这并不能全面的反映我们的服务在云服务商的环境中运行的状况。&lt;/p&gt;

&lt;p&gt;例如，一个给定的云服务商可能在全球运行了数十万台虚拟机，他们为这些虚机的正常运行和可靠性负责。GCP承诺计算引擎（也就是虚机）可以达到99.95%的可靠性。即使当GCP SLO指标显示为绿色的时候（即可靠性高于99.95%），evernote的监控视图的表现可能完全不同：因为我们的虚机在GCP全球总量虚机中仅占有很小的比例，会使导致我们（服务所在）区域成为孤岛（或由于其他原因导致成为孤岛）的故障最终在全球级别的汇总中被忽略。&lt;/p&gt;

&lt;p&gt;为了修正这样的情况，我们将我们的SLO和未达成SLO的实时性能与goolge进行共享。因此，Google CRE团队和Evernote团队基于同样的性能仪表盘展开合作。这看起来似乎是一个很简单的观点，但最终被证明是一种相当有效的、可以形成真正以客户为中心的工作方法。因此，google会向我们提供更明确的环境运行情况通知，而不是那种泛泛的“x服务当前运行缓慢”的通知。举例来说，除了那种泛泛的“今天GCP负载均衡环境运行缓慢”之外，我们还会被告知这个问题已经对evernote的SLO造成了5%的影响。这种关系也有助于google内部团队了解他们的行为和决策是如何影响用户的。&lt;/p&gt;

&lt;p&gt;这种双向关系也为我们提供了一个非常有效的框架来应对重大事件。大多数情况下，P1-P5级别的工单和常规的支持渠道配合使用，产生了很好的效果，使我们能够提供稳定的服务，并与谷歌保持良好的合作关系。但众所周知，当你整个在线服务面临着拓展业务增长的压力的时候，P1级别的工单是不能满足要求的。&lt;/p&gt;

&lt;p&gt;这时，我们与CRE团队共享的SLO和（合作）关系得以实现。我们达成共识，如果SLO影响足够高，双方都会将该问题视为P1级别进行特殊处理。这就意味着evernote和google的cre团队经常要快速组织起一个可以共享的沟通渠道。Google CRE团队监控（管理）我们共同定义和商定的SLO，使我们在优先级和恰当响应方面保持同步。&lt;/p&gt;

&lt;h3 id=&quot;当前状态&quot;&gt;当前状态&lt;/h3&gt;
&lt;p&gt;协调目标&lt;/p&gt;

&lt;p&gt;确保我们的合作伙伴（在本例中为Google）真正了解对我们重要的内容&lt;/p&gt;

&lt;p&gt;分享成功和失败&lt;/p&gt;

&lt;p&gt;在积极使用SLO大约九个月之后，Evernote已经在使用SLO实践的第三版了。下一个版本的SLO会以我们当前简单正常运行时间的SLO为基础进行改进。我们将关注单个API调用和客户端的指标/性能视图，以便更好地表示用户QoS。&lt;/p&gt;

&lt;p&gt;通过提供标准定义的QoS测量方法，SLO使Evernote更关注我们的服务是如何运行的。我们内部或者和谷歌进行以数据为驱动的对话，了解服务中断的影响，这能够推动服务改进，最终建立更强大的支持团队，使客户更满意。&lt;/p&gt;

&lt;h2 id=&quot;home-depot的slo故事&quot;&gt;Home Depot的SLO故事&lt;/h2&gt;
&lt;p&gt;Home Depot（THD）是全球最大的家居装饰零售商：我们在北美拥有2,200多家商店，每家商店都拥有超过35,000种产品（网站上有超过150万种产品）。 我们的基础架构托管各种软件应用程序，支持了近400,000名员工每年处理超过15亿的客户交易。这些商店由全球供应链和每年访问量超过20亿次电子商务网站紧密组成。&lt;/p&gt;

&lt;p&gt;最近为了提高我们软件开发的速度和质量，THD转向敏捷软件开发并改变了我们设计和管理软件的方式。我们从支持大型软件包开发的团队转变为小型独立的微服务架构开发团队。因此，我们的系统现在由一系列不断变更的微服务组成，这些微服务也是通过堆栈整合而成。&lt;/p&gt;

&lt;p&gt;我们向微服务转变的过程中，全栈所有权获得了新的“自由和责任文化”的补充。这种方法使开发人员可以自由地在需要时推送代码，同时也使他们为他们对服务的操作负责。对于这种共同所有权工作模式，运维和开发团队需要达成一种共识，即促进责任制和减少复杂性：SLO。相互依赖的服务需要知道如下信息：&lt;/p&gt;

&lt;p&gt;如果每项服务都能为这些问题提供明确的和一致的答案，那么团队就可以清楚地了解服务的依赖关系，从而达到更好地沟通，增强团队之间的信任和责任感。&lt;/p&gt;

&lt;h3 id=&quot;slo文化项目&quot;&gt;SLO文化项目&lt;/h3&gt;
&lt;p&gt;在我们的服务模式开始转变之前，Home Depot没有SLO文化。监控工具和仪表盘特别多，但都分布在各处，并且不会随着时间的推移记录数据。我们并不总能查出服务中断的根因。我们通常从遇到的服务问题开始排查，直到我们发现问题为止，这浪费了无数个小时。如果服务需要计划停机时间，其依赖服务就会受不了。如果一个团队需要构建一个99.95%的服务，他们不确定有严格依赖的服务能否达到99.99%的标准。这些未知导致我们的软件开发团队和运维团队之间的疑惑和失望。&lt;/p&gt;

&lt;p&gt;我们需要通过建立SLO的共同文化来解决这些问题。因此，需要一个影响人员、流程和技术的总体战略。 我们的努力跨越了四个方面：&lt;/p&gt;

&lt;p&gt;内部名词规定：&lt;/p&gt;

&lt;p&gt;在THD（Home Depot）公司内部定义SLOs。 来说明如何以一致的方式来进行度量。&lt;/p&gt;

&lt;p&gt;福音主义&lt;/p&gt;

&lt;p&gt;在整个公司传播这个词。&lt;/p&gt;

&lt;p&gt;通过给销售提供培训资料，在公司进行路演、内部博客、宣传资料比如T恤和贴纸等方式，传播为什么SLO很重要。&lt;/p&gt;

&lt;p&gt;争取一些早期采用者来实施SLO并向其他人展示其价值。&lt;/p&gt;

&lt;p&gt;建立一个感兴趣的首字母缩略词（VALET;稍后讨论）以帮助传播这个想法。&lt;/p&gt;

&lt;p&gt;创建培训计划（FiRE学院：可靠性工程基础），对开发人员进行培训使其了解SLO和其他可靠性概念。&lt;/p&gt;

&lt;p&gt;自动化&lt;/p&gt;

&lt;p&gt;为了降低指标收集的难度，用一个指标收集平台去自动收集生产环境中的服务的服务等级指标。这些SLI以后可以更容易地转换为SLO。&lt;/p&gt;

&lt;p&gt;激励&lt;/p&gt;

&lt;p&gt;为所有开发经理制定年度目标，为其服务设置和衡量SLO。&lt;/p&gt;

&lt;p&gt;每个人达成共识很重要。我们还希望保持这个框架尽可能简单，以帮助这个想法更快地传播。为了开始，我们仔细研究了我们在各种服务中监控的指标，并发现了一些模式。每项服务都会监控某种形式的流量、延迟、错误和利用率指标，这些指标与Google SRE的四个黄金指标密切相关。此外，许多服务都可以从错误中明显监控正常运行时间或可用性。很遗憾，整体来看，并不是所有类型的采集项都统一添加了监控、统一了命名、或者有足够的监控数据。&lt;/p&gt;

&lt;p&gt;我们的服务都没有SLO。我们的生产系统与面向客户的SLO最接近的指标是（用户）支持数据。通过跟踪商店内咨询台接收到的支持电话数量，是我们评价部署在我们商店的应用可靠性的主要（大多数时候是唯一）方法。&lt;/p&gt;

&lt;h3 id=&quot;我们的第一套slo&quot;&gt;我们的第一套SLO&lt;/h3&gt;
&lt;p&gt;我们不能对一个可度量系统的每个方面都创建SLOs，因此我们必须确定系统的哪些指标或SLIS应该具有SLOs。&lt;/p&gt;

&lt;p&gt;API调用的可用性和延迟
我们决定对微服务之间的API调用设置可用性和延迟SLOs。例如，Cart微服务调用Inventory微服务。针对那些API调用，Inventory微服务发布了SLOs，Cart微服务（以及需要Inventory的其他微服务）可以获取这些SLOs并以此决定Inventory微服务是否能满足可靠性要求 基础设施利用/基础设施利用率。&lt;/p&gt;

&lt;p&gt;基础设施利用率
THD团队通过不同的方式来衡量基础设施利用率，而最典型的衡量标准是分钟级别的实时基础设施利用率。我们基于某些原因并不会设置这种利用率SLOs。首先，微服务并非十分关注这个指标-只要服务可以承载流量，服务器正常运行、响应速度很快、不抛错误，且并不会耗尽容量，那么你的用户就不会真正关心利用率。此外，计划迁移服务到云端意味着资源利用率不是重点，这时我们要关注的是成本规划，而不是容量规划。（我们仍然需要监控利用率并执行容量规划，但不需要将其包括在我们的SLO框架内。）&lt;/p&gt;

&lt;p&gt;流量
由于THD没有进行容量规划的传统，因此我们需要一种机制，该机制能让开发和运维团队就其服务可以承载的流量进行沟通。流量通常被定义为对服务的请求，但我们需要确定是否应该跟踪平均每秒请求数，每秒峰值请求数或报告时间段内的请求数。最终我们决定跟踪这三项，并给每项服务选择最合适的指标。我们讨论是否为流量设置SLO的原因在于这个指标是由用户行为决定的，而非我们可控的内部因素决定。我们要讨论是否为流量设置SLO，因为流量的衡量跟用户行为密切相关，我们可控的内部因素无法发挥决定作用。 最终我们认为，作为零售商，我们需要为应对黑色星期五这样的活动流量峰值增加服务的规模，并根据预期的峰值流量设置SLO。&lt;/p&gt;

&lt;p&gt;延迟
我们给每个服务定义了延迟SLO并确定其最佳的衡量方式。这里我们只要求服务应该通过黑盒监控来补充我们常见的白盒性能监控，以捕获由网络或诸如缓存以及微服务外部代理失效等层面的问题。并且，我们认为，采用百分位数比算术平均值更合适。服务最少需要达到90％的目标，而面向用户的服务则最好达到95%或99%的目标。&lt;/p&gt;

&lt;p&gt;错误
错误解释起来有点复杂。由于我们主要处理Web服务，因此我们必须将错误内容以及返回结果标准化。如果Web服务发生错误，我们自然会对HTTP响应代码进行标准化：&lt;/p&gt;

&lt;p&gt;. 在服务的返回内容中，不应该用2xx来标记错误; 相反，一个错误应该抛出4xx或5xx。
 . 由服务端问题（如内存不足）引起的错误应该抛出5xx错误。
 . 客户端错误（如发送错误格式的请求）应该抛出4xx错误.&lt;/p&gt;

&lt;p&gt;一番考虑后，我们决定跟踪4xx和5xx错误，但仅使用5xx错误来设置SLOs。与定义其他相关SLO的方法类似，我们采用通用形式来定义错误SLO，以便不同环境中的不同应用都可以使用该SLO。例如，除HTTP错误外，定义一个批处理服务的错误，可能是该服务无法处理记录的个数。&lt;/p&gt;

&lt;p&gt;工单
正如前面提到的，工单最初是我们评估大多数生产软件的主要方式。由于历史原因，在我们其他的SLOs中，我们决定继续跟踪工单。你可以将该指标视为类似于“软件操作级别”的指标。&lt;/p&gt;

&lt;p&gt;VALET
我们将新的SLOs概括为一个更简易的缩略词：VALET。&lt;/p&gt;

&lt;p&gt;容量（流量）&lt;/p&gt;

&lt;p&gt;服务可以处理多少业务量？&lt;/p&gt;

&lt;p&gt;可用性&lt;/p&gt;

&lt;p&gt;需要的时候服务是否正在运行？&lt;/p&gt;

&lt;p&gt;延迟&lt;/p&gt;

&lt;p&gt;使用时服务是否快速响应？&lt;/p&gt;

&lt;p&gt;错误&lt;/p&gt;

&lt;p&gt;使用时服务是否会抛出错误？&lt;/p&gt;

&lt;p&gt;工单&lt;/p&gt;

&lt;p&gt;服务是否需要人为干预才能完成请求？&lt;/p&gt;

&lt;h3 id=&quot;推广slos&quot;&gt;推广SLOs&lt;/h3&gt;
&lt;p&gt;凭借这样一个易于记忆的缩略词，我们开始在企业内部推广SLOs：&lt;/p&gt;

&lt;p&gt;. 为何SLOs如此重要&lt;/p&gt;

&lt;p&gt;. SLOs是怎样与我们的“自由和责任”文化相契合的&lt;/p&gt;

&lt;p&gt;. 应该衡量什么&lt;/p&gt;

&lt;p&gt;. 如何处理结果&lt;/p&gt;

&lt;p&gt;因为开发人员现在要负责维护他们自己的软件，因此他们需要建立SLOs以体现他们开发和维护软件可靠性的能力，针对面向用户的服务，他们需要同服务使用者和产品经理进行交流。然而，他们中多数人并不熟悉诸如SLAs和SLOs这样的概念，因此他们需要接受VALET框架方面的培训。&lt;/p&gt;

&lt;p&gt;由于我们需要获得强有力的支持来推广SLOs，因此一开始我们可以面向高级领导者进行SLOs的推广讲解。然后逐个向开发团队讲述SLOs的价值观。我们鼓励团队从他们自定义的度量跟踪机制（通常是人为制定）转向VALET框架。为了保持这种推广态势，我们每周发送一份VALET格式的SLO报告给高层领导，这份报告结合了可靠性理念和从内部事件中吸取的经验。这也有助于构建业务指标，例如在VALET框架下，创建的采购订单（流量）或支付订单失败（错误）。&lt;/p&gt;

&lt;p&gt;我们还以多种方式扩展了我们的推广渠道：&lt;/p&gt;

&lt;p&gt;. 我们建立了一个内部WordPress网站来托管有关VALET和可靠性的博客，并将其链接到相关资源。&lt;/p&gt;

&lt;p&gt;. 我们组织内部技术讲座（包括Google SRE演讲嘉宾），讨论了通用可靠性概念以及如何使用VALET进行度量。&lt;/p&gt;

&lt;p&gt;. 我们开展了一系列VALET培训研讨会（之后将演变为FiRE学院），并向所有想参加的人开放，这些研讨会持续了好几个月。&lt;/p&gt;

&lt;p&gt;. 我们甚至制作了VALET笔记本电脑贴纸和文化衫，用来支持全面的内部推广活动。&lt;/p&gt;

&lt;p&gt;很快，公司里的每个人都知道了VALET这一概念，并且我们的SLOs新文化开始在公司占据主流。对开发负责人来讲，实施SLO甚至已正式成为其年度绩效评估指标。虽然大约有50项服务正在按周级别获取并报告其SLOs，但我们会将这些指标存储在电子表格中。虽然VALET的思想已经非常流行，但为了让其更广泛地被接纳，我们仍然需要自动化技术来进行数据的收集。&lt;/p&gt;

&lt;h3 id=&quot;自动化valet数据收集&quot;&gt;自动化VALET数据收集&lt;/h3&gt;
&lt;p&gt;虽然我们的SLO文化现在有了强大的立足点，但自动化VALET数据收集将加速SLO的应用。&lt;/p&gt;

&lt;p&gt;TPS报告
我们构建了一个框架来自动捕获部署到新GCP环境的任何服务的VALET数据。我们将此框架称为TPS报告，这是我们用于数量和性能测试的术语（每秒交易次数），当然，也是为了满足多个管理者想要查看这些数据的想法。 我们在GCP的BigQuery数据库平台之上构建了TPS Reports框架。我们的Web服务前端生成的所有日志都被输入BigQuery以供TPS Reports处理。当然也包括来自各种监控系统的指标，例如Stackdriver的可用性指标。&lt;/p&gt;

&lt;p&gt;TPS报告将这些数据转换为任何人都可以查询的每小时VALET指标。新创建的服务自动注册到TPS报告中，因此可以立即查询。由于数据全部存储在BigQuery中，因此我们可以跨时间帧有效地报告VALET指标。我们使用此数据构建了各种自动报告和警报。 最有趣的集成是一个聊天机器人，让我们直接在商业聊天平台上报告服务的VALET。例如，任何服务都可以显示过去一小时的VALET，前一周的VALET，未达成SLO的服务以及聊天频道内的各种其他值得引起关注的数据。&lt;/p&gt;

&lt;p&gt;VALET服务
我们的下一步是创建一个VALET应用程序来存储和报告SLO数据。因为SLO最适合用作趋势工具，所以该服务以每日、每周和每月粒度跟踪SLO。请注意，我们的SLO是一种趋势工具，我们可以将其用于错误预估，但不直接连接到我们的监控系统。相反，我们有各种不同的监控平台，每个平台都有自己的警报。这些监控系统每天汇总其SLO并发布到VALET服务以进行趋势分析。此设置的缺点是监控系统中设置的警报阈值未与SLO集成。 但是，我们可以根据需要灵活地更换监控系统。&lt;/p&gt;

&lt;p&gt;预计需要将VALET与未在GCP中运行的其他应用程序集成，我们创建了一个VALET集成层，该层提供API来收集聚合的VALET数据以生成服务日报。TPS Reports是第一个与VALET服务集成的系统，我们最终集成了各种本地应用程序平台（占在VALET中注册的服务的一半以上）。&lt;/p&gt;

&lt;p&gt;VALTE 仪表盘
VALET仪表板（如图3-1所示）是我们用于可视化和报告此数据的UI，并且相对简单。 它允许用户：&lt;/p&gt;

&lt;p&gt;图3-1  VALET仪表盘
注册新服务。 这通常意味着将服务分配给一个或多个URL，这些URL可能已经收集了VALET数据。&lt;/p&gt;

&lt;p&gt;为五个VALET类别中的任何一个设置SLO目标。&lt;/p&gt;

&lt;p&gt;在每个VALET类别下添加新的指标类型。 例如，一个服务采集99%的请求所用的延迟，而另一个服务采集90%的请求所用（或两者）的延迟。或者，后端处理系统可以跟踪每日总量（一天内创建的采购订单），而客户服务的前端可以跟踪每秒交易的峰值。&lt;/p&gt;

&lt;p&gt;VALET仪表盘允许用户一次报告许多服务的SLO，并以多种方式对数据进行切片和切块。例如，团队可以查看过去一周未达到SLO的所有服务的统计信息。负责复盘服务性能的团队可以查看其所有服务及其所依赖的服务的延迟。VALET仪表盘将数据存储在一个简单的Cloud SQL数据库中，开发人员使用流行的商业报告工具来构建报告。&lt;/p&gt;

&lt;p&gt;这些报告成为开发人员新的最佳实践的基础：定期对其服务进行SLO审核（通常是每周或每月）。基于这些，开发人员可以创建操作项以使服务回归SLO，或者可能按照需要调整不符合实际的SLO。&lt;/p&gt;

&lt;h3 id=&quot;slos的扩散&quot;&gt;SLOs的扩散&lt;/h3&gt;
&lt;p&gt;一旦SLOs融入到组织的集体思想中，并且具备了有效的自动化技术和报表，那么新的SLOs就可以快速实施。在年初跟踪了约50项服务的SLOs之后，到今年年底我们正在跟踪800项服务的SLOs，每月约有50项新服务在VALET注册。&lt;/p&gt;

&lt;p&gt;由于VALET允许我们在THD中推广SLO的应用，因此自动化开发这项工作是非常有意义的。但是，不具备这种自动化开发能力的公司也不用担心采用SLO会带来的麻烦。虽然自动化为THD提供了额外的收益，但一开始就编写SLO也收益颇多。&lt;/p&gt;

&lt;h3 id=&quot;将valet应用于批处理应用程序&quot;&gt;将VALET应用于批处理应用程序&lt;/h3&gt;
&lt;p&gt;当我们围绕SLO开发强大的报表时，我们发现了VALET的一些其他用途。 经过一些调整，批处理应用程序可以适用此框架，如下所示：&lt;/p&gt;

&lt;p&gt;数量&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;已处理的记录数量
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可用性&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;在一定时间内完成工作的频率（百分比） 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;等待时间&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;作业运行所需的时间 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;错误&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;程序运行失败的记录 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;工单&lt;/p&gt;

&lt;p&gt;操作员必须手动修复数据和重新处理作业的次数&lt;/p&gt;

&lt;h3 id=&quot;在测试中使用valet&quot;&gt;在测试中使用VALET&lt;/h3&gt;
&lt;p&gt;由于在发展SRE文化的同时，我们发现在临时环境中，VALET可以支持我们的破坏性测试（混沌工程）自动化。有了TPS Reports框架，我们就可以自动进行破坏性测试并记录对service’s VALET data造成的影响（希望没有影响）。&lt;/p&gt;

&lt;h3 id=&quot;未来展望&quot;&gt;未来展望&lt;/h3&gt;
&lt;p&gt;通过800个（并且不断增长）服务来收集VALET数据，我们可以拥有大量有用的运营数据。我们对未来有几个期望。&lt;/p&gt;

&lt;p&gt;既然我们正在有效地收集SLO，我们希望使用这些数据来采取行动。我们的下一步是类似于Google的错误预算文化，当服务不在SLO时，团队停止推送新功能（除了提高可靠性相关的）。为了满足业务增长的需求，需要平衡SLO报告的生成频率（周级别或月级别）和SLO标准的更新频率。和许多采用错误预算的公司一样，我们正在权衡滚动窗口与固定窗口的优缺点。&lt;/p&gt;

&lt;p&gt;我们希望进一步优化VALET以跟踪详细的节点和服务的使用者。目前，即使特定服务具有多个节点，我们也只在整个服务中跟踪VALET。因此，很难区分不同的操作（例如，对目录的写入与对目录的读取；虽然我们对这些操作添加了单独的监控和报警，但不跟踪SLO）。同样，我们也很乐意为服务的不同消费者提供对应的VALET结果。&lt;/p&gt;

&lt;p&gt;虽然我们目前在Web服务层跟踪延迟SLO，但我们还希望跟踪最终用户的延迟SLO。此度量将捕获网络延迟和CDN缓存等因素如何影响页面开始呈现和完成呈现所需的时间。&lt;/p&gt;

&lt;p&gt;我们还想将VALET数据扩展到应用程序部署。具体来说，在将更改推广到下一个服务器、域或区域之前，我们希望自动化验证VALET是否在容差范围内。&lt;/p&gt;

&lt;p&gt;我们已经开始收集有关服务依赖性的信息，并且制作了一个可视化图表原型，该图表显示了我们在调用树中未触及到VALET指标的位置。 新兴的网格服务平台将简化这种分析。&lt;/p&gt;

&lt;p&gt;最后，我们坚信服务的SLO应该由服务的业务所有者（通常称为产品经理）根据其业务的重要性来设置。至少，我们希望业务所有者设置服务正常运行时间的最低要求，并将SLO用作产品管理和开发之间的共享目标。虽然技术人员发现VALET很直观，但对于产品经理来说，这个概念并不那么直观。我们正在努力使用与它们相关的术语来简化VALET的概念：我们既简化了正常运行时间的选择数量又提供了示例指标。我们还强调从一个级别转移到另一个级别所需的大量投入。以下是我们可能提供的简化VALET指标的示例：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;. 99.5％：商店员工使用次数很少的应用程序或新服务。    
. 99.9％：适用于THD的大多数非销售系统 
. 99.95％：销售系统（或支持销售系统的服务） 
. 99.99％：共享的基础设施服务
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;以业务术语来衡量指标并在产品和开发之间共享可见目标（SLO！），这种行为将大量减少公司常见的对可靠性的错误预期。&lt;/p&gt;

&lt;h3 id=&quot;概要&quot;&gt;概要&lt;/h3&gt;
&lt;p&gt;向大公司介绍一个新流程，都需要一个好的策略、高管的支持、强大的传播、简单的采用模式以及最重要的耐心，更不用说是一个新文化了。像SLO这样的重大变革可能需要数年才能在公司中牢固地建立起来。我们想强调的是，Home Depot是一家传统企业;如果我们能够成功地引入这么大的变化，那么你也可以。你也不必一次完成这个任务。虽然我们逐步实施SLO，但制定全面的传播策略和明确的激励结构促进了快速转型：我们在不到一年的时间内获得了从0到800的SLO服务支持。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;
&lt;p&gt;SLO和错误预算为解决许多不同问题提供了强大的理论支持。这些来自Evernote和Home Depot的案例研究提供了非常真实的例子，说明如何实施SLO文化可以使产品开发和运维更紧密地结合在一起。这样做可以促进沟通并更好地为制定决策提供信息。它最终将为你的客户带来更好的体验 - 无论这些客户是内部、外部、人类还是其他服务。&lt;/p&gt;

&lt;p&gt;这两个案例研究强调实现SLO文化是一个持续的过程，而不是一次性修复或解决方案。虽然它们共享哲学基础，但THD和Evernote的度量风格、SLIs、SLOs和实现细节明显不同。这两个案例都补充了谷歌对SLOs的看法，说明了SLO实现不一定是Google所特有的。正如这些公司为自己独特的环境量身定制SLO一样，其他公司和组织也可以这样做。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">名次解释： SLI：服务质量指标、该服务的某项服务质量的一个具体量化指标。例如：延迟、可用性 SLO：服务质量目标、服务的某个SLI的目标值/范围。例如：搜索请求的平均延迟 &amp;lt; 100ms。 SLA：服务质量协议、服务与用户之间的一个明确的协议，描述达到/未达到SLO之后的后果。 错误预算： 1 - 可靠性目标 尽管SRE的许多原则都是在Google内部形成的，但它的原则早已存在于Google之外。许多Google SRE的标准已被业内多个组织实践应用。 SLO是SRE模型的基础。自从我们组建了客户可靠性工程（CRE）团队——这是一组帮助Google Cloud Platform（GCP）客户构建更可靠的服务的经验丰富的SRE——几乎与每个客户交互都以SLO开始以SLO结束。 我们在这里介绍了两个不同行业的公司事迹，概述了他们在与Google CRE团队合作时采纳SLO和基于错误预算的方法的过程。有关SLO和错误预算的讨论，请参阅本书的第2章和第一本书的第3章。 Evernote的SLO故事 Evernote是一款跨平台的APP，可帮助个人和团队创建、整合和共享信息。在全球拥有超过2.2亿用户，我们在平台内存储了超过120亿条信息——包括文本笔记、文件和附件/图像。在后台，Evernote服务由750个以上的MySQL实例支持。 我们向Evernote引入了SLO的概念，并将其作为更广泛的技术改造的一部分，旨在提高工程速度，同时保持服务质量。我们的目标包括： 将工程重点从数据中心中冗余的繁重工作转移到客户实际关心的产品工程工作上。为此，我们停止运行物理数据中心并转移到公有云。 调整运维和软件工程师的工作模式，旨在保持整体服务质量的同时提高变更速度。 改进我们对SLA的看法，以确保我们更加关注故障对庞大的客户群所造成的的影响。 这些目标对许多行业的组织而言可能都很熟悉。虽然没有一种方法可以全面实现这些类型的变更，但希望我们分享的经验可以为面临类似挑战的人提供有价值的参考意见。 为什么Evernote采用SRE模型？ 过渡开始阶段的Evernote的特点是传统的运维和开发分离：运维团队维护生产环境的稳定性，而开发团队的任务是为客户开发新的产品功能。这些目标通常是冲突的：开发团队 感觉被繁琐的流程所束缚，而运维团队又会因新代码在生产环境中引入新的问题变得不满。 当我们在这两个目标之间不断动摇时，运维和开发团队之间蔓延了一种不满和紧张的关系。我们希望达到一个双方都满意的节点，更好地平衡所涉及团队的不同需求。 在五年多的时间里，我们尝试了各种方式解决这种传统二分法中的差距。在尝试了“你编码，你运行”的开发模式，以及“你编码，我们为你运行”的运维模式之后，我们转向了以SLO为中心的SRE方法。 那么是什么促使Evernote向这个方向发展呢？ 在Evernote，我们将运维和开发的核心目标视为工程师专业化的独立发展方向。一个方向关注的是近乎7*24小时地持续为客户提供服务。另一个关注的是服务的扩展和发展，以满足客户未来的需求。近年来，这两个方向已经越来越接近，例如SRE和DevOps强调将软件开发应用于运维。（数据中心自动化和公有云的发展进一步推动了这种融合，这两者都为我们提供了一个可以完全由软件控制的数据中心。）另一方面，全栈所有权和持续部署也越来越多地应用于软件开发。 SRE模型完全接受并包容了运维和开发间的差异，同时鼓励团队朝着共同的目标努力。它并不试图将运维工程师转变为应用程序开发人员，反之亦然。相反，它给出了一个共同的参考框架。根据我们的经验，由于使用错误预算/SLO方法的两个团队在交流时很少带着主观感情，所以在面对同样的情况时通常会做出类似的决定。 SLO简介：正在进行的旅程 旅程的第一步是从物理数据中心迁移到Google云平台。当Evernote服务在GCP上稳定运行后，我们就引入了SLO。我们的目标有两个： 确保所有团队都在Evernote SLO的新框架内工作。 将Evernote的SLO纳入我们与Google 云团队的合作中，他们现在负责我们的底层基础架构。由于在整体模型中加入了新的合作伙伴，因此我们需要确保迁移到GCP不会影响我们对用户的承诺。 在使用SLO约9个月后，Evernote已经开始实践使用其SLO的第3版了！ 在深入了解SLO的技术细节之前，要先从客户的角度开始提问： 你可以提供哪些承诺？与大多数服务类似，Evernote具有许多功能和选项，用户可以通过各种创造性方式使用这些功能和选项。我们希望在一开始就关注最重要和最常见的客户需求：Evernote服务的可用性，以便用户能够访问和同步多个客户端的内容。我们的SLO之旅从这个目标开始。通过关注服务正常运行时间， 我们完成了接入SLO的第一步。使用这种方法，我们可以清楚地表达我们衡量的内容以及衡量方法。 我们的第一份SLO文件包含以下内容： SLO的定义 这是一个（服务、系统）可用时间的计算方法：为某些服务或者是方法，在月级别的统计周期内设定了99.95%的可用性。这个数据是我们基于内部客户支持团队、产品团队，尤其重要的是用户共同讨论得来的。我们特意选择将SLO和日历月而不是与滚动的时期进行关联，就是为了使我们在进行服务复查时保持专注有序。 衡量什么，以及如何衡量它 衡量度量 我们指定了一个服务终点，我们可以调用它来测试服务是否按预期运行。在我们的例子中，我们在服务中内置了一个状态页面，它可以运行我们的大部分堆栈并返回200状态代码（如果一切正常）。 如何度量 我们想要一个定期调用状态页面的探测器。我们希望探测器完全位于我们的环境之外并独立于我们的环境，因此我们可以测试所有组件，包括负载均衡。我们的目标是确保我们可以统计到GCP服务以及evernote应用任何、所有异常。但是，我们不希望随机网络问题触发误报。我们选择使用专门建立和运行此类探测器的第三方公司。我们选择了Pingdom，但市场上还有很多其他产品。我们按如下方式进行衡量： 如何从监控数据计算SLO 最后，我们仔细记录了我们如何根据从Pingdom收到的原始数据计算SLO。例如，我们指定了如何考虑维护窗口：我们无法假设我们所有的数亿用户都知道我们发布的维护窗口。因此，不知情的用户会将这些窗口视为通用和无法解释的停机时间，因此我们的SLO计算将维护视为停机时间。 一旦我们定义了SLO，我们就必须使它发挥最大的价值。 我们希望SLO能够推动软件和运维方面的变革，让我们的客户更快乐并让他们满意。 怎么做到最好？ 探测频率：我们每分钟轮询一次前端节点。 探测器的位置：此设置是可配置的; 我们目前在北美和欧洲使用多个探测器 “down”的定义：如果一个探测器检测结果为失败，那么这个节点会被标记为疑似宕机，然后第二个基于不同地理位置独立部署的探测机会进行第二次确认。如果第二次检查同样也失败了，出于计算SLO的目的这个节点会被标记为宕机。只要探测请求持续显示错误，那么这个节点会被持续标记为宕机。 我们用SLO中有关错误预算的思维为方法来分配下一步工作的需要的资源。举例来说，如果我们没有达成上个月的SLO，这会促使我们高优（对系统、服务）进行目标明确的加固、改进和修复。我们制定最简原则：evernote团队以及google团队共同进行月级别 的SLO目标复查。在这个会议上，我们复核SLO的表现并对所有服务中断行为进行深入研究。基于针对上个月的上述分析而不是根因分析，我们制定了一些改进措施。 在整个过程中，我们的指导原则是“过犹不及”。即使在SLO还没有达到完美的时候，它也足以在此期间指导我们进行改进。一个“完美”的SLO应该可以衡量每一个与我们服务有关的潜在用户交互设计并且解释所有的边界行为。虽然字面上看起来这是个好主意，但是如果要实现起来却要花费数月的时间去改进服务（如果真的可以这到完美）。相反，我们选择了一个初始SLO，涵盖了大多数（但不是全部）用户交互，这是服务质量的良好代理。 自从我们开始执行SLO以来，根据从服务复盘以及响应客户有感的宕机事件中得到的启示，我们对SLO做了两次修改。因为我们一开始就没有追求完美SLO，为了适应业务的发展我们乐于做出改变。除了evernote团队与google进行月级别SLO复盘之外，我们也设定了一个6个月的SLO复盘周期，这个周期可以使SLO的维护达到一个平衡：既不会频繁更新，也不会使之过时。在不断修订SLO的过程中，我们也意识到了，期望的衡量标准和可以达到的衡量标准之间的平衡是很重要的。 自引入SLO以来，我们的运维和开发团队之间的关系有了微妙但显著的改善。现在团队对成功有了共同的衡量标准，那就是：取消对服务质量的人为解释使两个团队达成了共同的观点和标准。在此我们试着举一个例子，2017年当我们不得不在短期内推动多个版本的发布任务时，SLO为我们提供了共同基础。当我们发现一个复杂的bug时，产品开发团队要求我们将常规的周级别发布任务分配到多个独立的发布窗口，每个发布窗口都会对客户产生潜在的影响。通过对问题进行有针对性的SLO计算以及消除方案中的人为主观因素，我们可以更好的量化客户感受并且通过把发布窗口由5个降为2个从而达到了减少了客户痛点的目的。 打破客户与云服务商之间的隔阂 介于客户和云服务商之间的隔阂看起来是在所难免的。虽然google已经为运行evernote的GCP平台设定了SLO和SLA（服务等级协议），但是evernote有自己的SLO和SLA。期望两个技术团队会将彼此的SLA告知对方看起来是不现实的。 evernote不希望存在这样的隔阂。当然我们也可以基于自己的SLO和底层的GCP平台的SLA建立起隔离域，相反从一开始我们就希望google可以理解性能表现对我们来说是多重要以及为什么这么重要。我们期望google和我们在目标上达成一致，让两家公司把evernote在可靠性方向的成败当作共同的职责。为了实现这一目标，我们需要一种方法可以： 达成一致的目标 确保我们的合作伙伴（在此指google）真正清楚我们最关心哪些指标 共担成败 大多数服务商都为自己的云服务发布了SLO/SLA。虽然服务运行在此框架下很重要，但这并不能全面的反映我们的服务在云服务商的环境中运行的状况。 例如，一个给定的云服务商可能在全球运行了数十万台虚拟机，他们为这些虚机的正常运行和可靠性负责。GCP承诺计算引擎（也就是虚机）可以达到99.95%的可靠性。即使当GCP SLO指标显示为绿色的时候（即可靠性高于99.95%），evernote的监控视图的表现可能完全不同：因为我们的虚机在GCP全球总量虚机中仅占有很小的比例，会使导致我们（服务所在）区域成为孤岛（或由于其他原因导致成为孤岛）的故障最终在全球级别的汇总中被忽略。 为了修正这样的情况，我们将我们的SLO和未达成SLO的实时性能与goolge进行共享。因此，Google CRE团队和Evernote团队基于同样的性能仪表盘展开合作。这看起来似乎是一个很简单的观点，但最终被证明是一种相当有效的、可以形成真正以客户为中心的工作方法。因此，google会向我们提供更明确的环境运行情况通知，而不是那种泛泛的“x服务当前运行缓慢”的通知。举例来说，除了那种泛泛的“今天GCP负载均衡环境运行缓慢”之外，我们还会被告知这个问题已经对evernote的SLO造成了5%的影响。这种关系也有助于google内部团队了解他们的行为和决策是如何影响用户的。 这种双向关系也为我们提供了一个非常有效的框架来应对重大事件。大多数情况下，P1-P5级别的工单和常规的支持渠道配合使用，产生了很好的效果，使我们能够提供稳定的服务，并与谷歌保持良好的合作关系。但众所周知，当你整个在线服务面临着拓展业务增长的压力的时候，P1级别的工单是不能满足要求的。 这时，我们与CRE团队共享的SLO和（合作）关系得以实现。我们达成共识，如果SLO影响足够高，双方都会将该问题视为P1级别进行特殊处理。这就意味着evernote和google的cre团队经常要快速组织起一个可以共享的沟通渠道。Google CRE团队监控（管理）我们共同定义和商定的SLO，使我们在优先级和恰当响应方面保持同步。 当前状态 协调目标 确保我们的合作伙伴（在本例中为Google）真正了解对我们重要的内容 分享成功和失败 在积极使用SLO大约九个月之后，Evernote已经在使用SLO实践的第三版了。下一个版本的SLO会以我们当前简单正常运行时间的SLO为基础进行改进。我们将关注单个API调用和客户端的指标/性能视图，以便更好地表示用户QoS。 通过提供标准定义的QoS测量方法，SLO使Evernote更关注我们的服务是如何运行的。我们内部或者和谷歌进行以数据为驱动的对话，了解服务中断的影响，这能够推动服务改进，最终建立更强大的支持团队，使客户更满意。 Home Depot的SLO故事 Home Depot（THD）是全球最大的家居装饰零售商：我们在北美拥有2,200多家商店，每家商店都拥有超过35,000种产品（网站上有超过150万种产品）。 我们的基础架构托管各种软件应用程序，支持了近400,000名员工每年处理超过15亿的客户交易。这些商店由全球供应链和每年访问量超过20亿次电子商务网站紧密组成。 最近为了提高我们软件开发的速度和质量，THD转向敏捷软件开发并改变了我们设计和管理软件的方式。我们从支持大型软件包开发的团队转变为小型独立的微服务架构开发团队。因此，我们的系统现在由一系列不断变更的微服务组成，这些微服务也是通过堆栈整合而成。 我们向微服务转变的过程中，全栈所有权获得了新的“自由和责任文化”的补充。这种方法使开发人员可以自由地在需要时推送代码，同时也使他们为他们对服务的操作负责。对于这种共同所有权工作模式，运维和开发团队需要达成一种共识，即促进责任制和减少复杂性：SLO。相互依赖的服务需要知道如下信息： 如果每项服务都能为这些问题提供明确的和一致的答案，那么团队就可以清楚地了解服务的依赖关系，从而达到更好地沟通，增强团队之间的信任和责任感。 SLO文化项目 在我们的服务模式开始转变之前，Home Depot没有SLO文化。监控工具和仪表盘特别多，但都分布在各处，并且不会随着时间的推移记录数据。我们并不总能查出服务中断的根因。我们通常从遇到的服务问题开始排查，直到我们发现问题为止，这浪费了无数个小时。如果服务需要计划停机时间，其依赖服务就会受不了。如果一个团队需要构建一个99.95%的服务，他们不确定有严格依赖的服务能否达到99.99%的标准。这些未知导致我们的软件开发团队和运维团队之间的疑惑和失望。 我们需要通过建立SLO的共同文化来解决这些问题。因此，需要一个影响人员、流程和技术的总体战略。 我们的努力跨越了四个方面： 内部名词规定： 在THD（Home Depot）公司内部定义SLOs。 来说明如何以一致的方式来进行度量。 福音主义 在整个公司传播这个词。 通过给销售提供培训资料，在公司进行路演、内部博客、宣传资料比如T恤和贴纸等方式，传播为什么SLO很重要。 争取一些早期采用者来实施SLO并向其他人展示其价值。 建立一个感兴趣的首字母缩略词（VALET;稍后讨论）以帮助传播这个想法。 创建培训计划（FiRE学院：可靠性工程基础），对开发人员进行培训使其了解SLO和其他可靠性概念。 自动化 为了降低指标收集的难度，用一个指标收集平台去自动收集生产环境中的服务的服务等级指标。这些SLI以后可以更容易地转换为SLO。 激励 为所有开发经理制定年度目标，为其服务设置和衡量SLO。 每个人达成共识很重要。我们还希望保持这个框架尽可能简单，以帮助这个想法更快地传播。为了开始，我们仔细研究了我们在各种服务中监控的指标，并发现了一些模式。每项服务都会监控某种形式的流量、延迟、错误和利用率指标，这些指标与Google SRE的四个黄金指标密切相关。此外，许多服务都可以从错误中明显监控正常运行时间或可用性。很遗憾，整体来看，并不是所有类型的采集项都统一添加了监控、统一了命名、或者有足够的监控数据。 我们的服务都没有SLO。我们的生产系统与面向客户的SLO最接近的指标是（用户）支持数据。通过跟踪商店内咨询台接收到的支持电话数量，是我们评价部署在我们商店的应用可靠性的主要（大多数时候是唯一）方法。 我们的第一套SLO 我们不能对一个可度量系统的每个方面都创建SLOs，因此我们必须确定系统的哪些指标或SLIS应该具有SLOs。 API调用的可用性和延迟 我们决定对微服务之间的API调用设置可用性和延迟SLOs。例如，Cart微服务调用Inventory微服务。针对那些API调用，Inventory微服务发布了SLOs，Cart微服务（以及需要Inventory的其他微服务）可以获取这些SLOs并以此决定Inventory微服务是否能满足可靠性要求 基础设施利用/基础设施利用率。 基础设施利用率 THD团队通过不同的方式来衡量基础设施利用率，而最典型的衡量标准是分钟级别的实时基础设施利用率。我们基于某些原因并不会设置这种利用率SLOs。首先，微服务并非十分关注这个指标-只要服务可以承载流量，服务器正常运行、响应速度很快、不抛错误，且并不会耗尽容量，那么你的用户就不会真正关心利用率。此外，计划迁移服务到云端意味着资源利用率不是重点，这时我们要关注的是成本规划，而不是容量规划。（我们仍然需要监控利用率并执行容量规划，但不需要将其包括在我们的SLO框架内。） 流量 由于THD没有进行容量规划的传统，因此我们需要一种机制，该机制能让开发和运维团队就其服务可以承载的流量进行沟通。流量通常被定义为对服务的请求，但我们需要确定是否应该跟踪平均每秒请求数，每秒峰值请求数或报告时间段内的请求数。最终我们决定跟踪这三项，并给每项服务选择最合适的指标。我们讨论是否为流量设置SLO的原因在于这个指标是由用户行为决定的，而非我们可控的内部因素决定。我们要讨论是否为流量设置SLO，因为流量的衡量跟用户行为密切相关，我们可控的内部因素无法发挥决定作用。 最终我们认为，作为零售商，我们需要为应对黑色星期五这样的活动流量峰值增加服务的规模，并根据预期的峰值流量设置SLO。 延迟 我们给每个服务定义了延迟SLO并确定其最佳的衡量方式。这里我们只要求服务应该通过黑盒监控来补充我们常见的白盒性能监控，以捕获由网络或诸如缓存以及微服务外部代理失效等层面的问题。并且，我们认为，采用百分位数比算术平均值更合适。服务最少需要达到90％的目标，而面向用户的服务则最好达到95%或99%的目标。 错误 错误解释起来有点复杂。由于我们主要处理Web服务，因此我们必须将错误内容以及返回结果标准化。如果Web服务发生错误，我们自然会对HTTP响应代码进行标准化： . 在服务的返回内容中，不应该用2xx来标记错误; 相反，一个错误应该抛出4xx或5xx。 . 由服务端问题（如内存不足）引起的错误应该抛出5xx错误。 . 客户端错误（如发送错误格式的请求）应该抛出4xx错误. 一番考虑后，我们决定跟踪4xx和5xx错误，但仅使用5xx错误来设置SLOs。与定义其他相关SLO的方法类似，我们采用通用形式来定义错误SLO，以便不同环境中的不同应用都可以使用该SLO。例如，除HTTP错误外，定义一个批处理服务的错误，可能是该服务无法处理记录的个数。 工单 正如前面提到的，工单最初是我们评估大多数生产软件的主要方式。由于历史原因，在我们其他的SLOs中，我们决定继续跟踪工单。你可以将该指标视为类似于“软件操作级别”的指标。 VALET 我们将新的SLOs概括为一个更简易的缩略词：VALET。 容量（流量） 服务可以处理多少业务量？ 可用性 需要的时候服务是否正在运行？ 延迟 使用时服务是否快速响应？ 错误 使用时服务是否会抛出错误？ 工单 服务是否需要人为干预才能完成请求？ 推广SLOs 凭借这样一个易于记忆的缩略词，我们开始在企业内部推广SLOs： . 为何SLOs如此重要 . SLOs是怎样与我们的“自由和责任”文化相契合的 . 应该衡量什么 . 如何处理结果 因为开发人员现在要负责维护他们自己的软件，因此他们需要建立SLOs以体现他们开发和维护软件可靠性的能力，针对面向用户的服务，他们需要同服务使用者和产品经理进行交流。然而，他们中多数人并不熟悉诸如SLAs和SLOs这样的概念，因此他们需要接受VALET框架方面的培训。 由于我们需要获得强有力的支持来推广SLOs，因此一开始我们可以面向高级领导者进行SLOs的推广讲解。然后逐个向开发团队讲述SLOs的价值观。我们鼓励团队从他们自定义的度量跟踪机制（通常是人为制定）转向VALET框架。为了保持这种推广态势，我们每周发送一份VALET格式的SLO报告给高层领导，这份报告结合了可靠性理念和从内部事件中吸取的经验。这也有助于构建业务指标，例如在VALET框架下，创建的采购订单（流量）或支付订单失败（错误）。 我们还以多种方式扩展了我们的推广渠道： . 我们建立了一个内部WordPress网站来托管有关VALET和可靠性的博客，并将其链接到相关资源。 . 我们组织内部技术讲座（包括Google SRE演讲嘉宾），讨论了通用可靠性概念以及如何使用VALET进行度量。 . 我们开展了一系列VALET培训研讨会（之后将演变为FiRE学院），并向所有想参加的人开放，这些研讨会持续了好几个月。 . 我们甚至制作了VALET笔记本电脑贴纸和文化衫，用来支持全面的内部推广活动。 很快，公司里的每个人都知道了VALET这一概念，并且我们的SLOs新文化开始在公司占据主流。对开发负责人来讲，实施SLO甚至已正式成为其年度绩效评估指标。虽然大约有50项服务正在按周级别获取并报告其SLOs，但我们会将这些指标存储在电子表格中。虽然VALET的思想已经非常流行，但为了让其更广泛地被接纳，我们仍然需要自动化技术来进行数据的收集。 自动化VALET数据收集 虽然我们的SLO文化现在有了强大的立足点，但自动化VALET数据收集将加速SLO的应用。 TPS报告 我们构建了一个框架来自动捕获部署到新GCP环境的任何服务的VALET数据。我们将此框架称为TPS报告，这是我们用于数量和性能测试的术语（每秒交易次数），当然，也是为了满足多个管理者想要查看这些数据的想法。 我们在GCP的BigQuery数据库平台之上构建了TPS Reports框架。我们的Web服务前端生成的所有日志都被输入BigQuery以供TPS Reports处理。当然也包括来自各种监控系统的指标，例如Stackdriver的可用性指标。 TPS报告将这些数据转换为任何人都可以查询的每小时VALET指标。新创建的服务自动注册到TPS报告中，因此可以立即查询。由于数据全部存储在BigQuery中，因此我们可以跨时间帧有效地报告VALET指标。我们使用此数据构建了各种自动报告和警报。 最有趣的集成是一个聊天机器人，让我们直接在商业聊天平台上报告服务的VALET。例如，任何服务都可以显示过去一小时的VALET，前一周的VALET，未达成SLO的服务以及聊天频道内的各种其他值得引起关注的数据。 VALET服务 我们的下一步是创建一个VALET应用程序来存储和报告SLO数据。因为SLO最适合用作趋势工具，所以该服务以每日、每周和每月粒度跟踪SLO。请注意，我们的SLO是一种趋势工具，我们可以将其用于错误预估，但不直接连接到我们的监控系统。相反，我们有各种不同的监控平台，每个平台都有自己的警报。这些监控系统每天汇总其SLO并发布到VALET服务以进行趋势分析。此设置的缺点是监控系统中设置的警报阈值未与SLO集成。 但是，我们可以根据需要灵活地更换监控系统。 预计需要将VALET与未在GCP中运行的其他应用程序集成，我们创建了一个VALET集成层，该层提供API来收集聚合的VALET数据以生成服务日报。TPS Reports是第一个与VALET服务集成的系统，我们最终集成了各种本地应用程序平台（占在VALET中注册的服务的一半以上）。 VALTE 仪表盘 VALET仪表板（如图3-1所示）是我们用于可视化和报告此数据的UI，并且相对简单。 它允许用户： 图3-1 VALET仪表盘 注册新服务。 这通常意味着将服务分配给一个或多个URL，这些URL可能已经收集了VALET数据。 为五个VALET类别中的任何一个设置SLO目标。 在每个VALET类别下添加新的指标类型。 例如，一个服务采集99%的请求所用的延迟，而另一个服务采集90%的请求所用（或两者）的延迟。或者，后端处理系统可以跟踪每日总量（一天内创建的采购订单），而客户服务的前端可以跟踪每秒交易的峰值。 VALET仪表盘允许用户一次报告许多服务的SLO，并以多种方式对数据进行切片和切块。例如，团队可以查看过去一周未达到SLO的所有服务的统计信息。负责复盘服务性能的团队可以查看其所有服务及其所依赖的服务的延迟。VALET仪表盘将数据存储在一个简单的Cloud SQL数据库中，开发人员使用流行的商业报告工具来构建报告。 这些报告成为开发人员新的最佳实践的基础：定期对其服务进行SLO审核（通常是每周或每月）。基于这些，开发人员可以创建操作项以使服务回归SLO，或者可能按照需要调整不符合实际的SLO。 SLOs的扩散 一旦SLOs融入到组织的集体思想中，并且具备了有效的自动化技术和报表，那么新的SLOs就可以快速实施。在年初跟踪了约50项服务的SLOs之后，到今年年底我们正在跟踪800项服务的SLOs，每月约有50项新服务在VALET注册。 由于VALET允许我们在THD中推广SLO的应用，因此自动化开发这项工作是非常有意义的。但是，不具备这种自动化开发能力的公司也不用担心采用SLO会带来的麻烦。虽然自动化为THD提供了额外的收益，但一开始就编写SLO也收益颇多。 将VALET应用于批处理应用程序 当我们围绕SLO开发强大的报表时，我们发现了VALET的一些其他用途。 经过一些调整，批处理应用程序可以适用此框架，如下所示： 数量 已处理的记录数量 可用性 在一定时间内完成工作的频率（百分比） 等待时间 作业运行所需的时间 错误 程序运行失败的记录 工单 操作员必须手动修复数据和重新处理作业的次数 在测试中使用VALET 由于在发展SRE文化的同时，我们发现在临时环境中，VALET可以支持我们的破坏性测试（混沌工程）自动化。有了TPS Reports框架，我们就可以自动进行破坏性测试并记录对service’s VALET data造成的影响（希望没有影响）。 未来展望 通过800个（并且不断增长）服务来收集VALET数据，我们可以拥有大量有用的运营数据。我们对未来有几个期望。 既然我们正在有效地收集SLO，我们希望使用这些数据来采取行动。我们的下一步是类似于Google的错误预算文化，当服务不在SLO时，团队停止推送新功能（除了提高可靠性相关的）。为了满足业务增长的需求，需要平衡SLO报告的生成频率（周级别或月级别）和SLO标准的更新频率。和许多采用错误预算的公司一样，我们正在权衡滚动窗口与固定窗口的优缺点。 我们希望进一步优化VALET以跟踪详细的节点和服务的使用者。目前，即使特定服务具有多个节点，我们也只在整个服务中跟踪VALET。因此，很难区分不同的操作（例如，对目录的写入与对目录的读取；虽然我们对这些操作添加了单独的监控和报警，但不跟踪SLO）。同样，我们也很乐意为服务的不同消费者提供对应的VALET结果。 虽然我们目前在Web服务层跟踪延迟SLO，但我们还希望跟踪最终用户的延迟SLO。此度量将捕获网络延迟和CDN缓存等因素如何影响页面开始呈现和完成呈现所需的时间。 我们还想将VALET数据扩展到应用程序部署。具体来说，在将更改推广到下一个服务器、域或区域之前，我们希望自动化验证VALET是否在容差范围内。 我们已经开始收集有关服务依赖性的信息，并且制作了一个可视化图表原型，该图表显示了我们在调用树中未触及到VALET指标的位置。 新兴的网格服务平台将简化这种分析。 最后，我们坚信服务的SLO应该由服务的业务所有者（通常称为产品经理）根据其业务的重要性来设置。至少，我们希望业务所有者设置服务正常运行时间的最低要求，并将SLO用作产品管理和开发之间的共享目标。虽然技术人员发现VALET很直观，但对于产品经理来说，这个概念并不那么直观。我们正在努力使用与它们相关的术语来简化VALET的概念：我们既简化了正常运行时间的选择数量又提供了示例指标。我们还强调从一个级别转移到另一个级别所需的大量投入。以下是我们可能提供的简化VALET指标的示例： . 99.5％：商店员工使用次数很少的应用程序或新服务。 . 99.9％：适用于THD的大多数非销售系统 . 99.95％：销售系统（或支持销售系统的服务） . 99.99％：共享的基础设施服务 以业务术语来衡量指标并在产品和开发之间共享可见目标（SLO！），这种行为将大量减少公司常见的对可靠性的错误预期。 概要 向大公司介绍一个新流程，都需要一个好的策略、高管的支持、强大的传播、简单的采用模式以及最重要的耐心，更不用说是一个新文化了。像SLO这样的重大变革可能需要数年才能在公司中牢固地建立起来。我们想强调的是，Home Depot是一家传统企业;如果我们能够成功地引入这么大的变化，那么你也可以。你也不必一次完成这个任务。虽然我们逐步实施SLO，但制定全面的传播策略和明确的激励结构促进了快速转型：我们在不到一年的时间内获得了从0到800的SLO服务支持。 结论 SLO和错误预算为解决许多不同问题提供了强大的理论支持。这些来自Evernote和Home Depot的案例研究提供了非常真实的例子，说明如何实施SLO文化可以使产品开发和运维更紧密地结合在一起。这样做可以促进沟通并更好地为制定决策提供信息。它最终将为你的客户带来更好的体验 - 无论这些客户是内部、外部、人类还是其他服务。 这两个案例研究强调实现SLO文化是一个持续的过程，而不是一次性修复或解决方案。虽然它们共享哲学基础，但THD和Evernote的度量风格、SLIs、SLOs和实现细节明显不同。这两个案例都补充了谷歌对SLOs的看法，说明了SLO实现不一定是Google所特有的。正如这些公司为自己独特的环境量身定制SLO一样，其他公司和组织也可以这样做。</summary></entry></feed>