<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-19T14:44:39+08:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">Blog</title><author><name>ZX</name></author><entry><title type="html">第一篇blog</title><link href="http://localhost:4000/something/2020/03/11/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87/" rel="alternate" type="text/html" title="第一篇blog" /><published>2020-03-11T00:00:00+08:00</published><updated>2020-03-11T00:00:00+08:00</updated><id>http://localhost:4000/something/2020/03/11/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87</id><content type="html" xml:base="http://localhost:4000/something/2020/03/11/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87/">&lt;blockquote&gt;
  &lt;p&gt;如何搭建该网站！.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;准备工作&quot;&gt;准备工作&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;github pages&lt;/li&gt;
  &lt;li&gt;jekyll&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;具体过程&quot;&gt;具体过程&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;github pages 创建
  首先新建仓库， 需要以&lt;code class=&quot;highlighter-rouge&quot;&gt;username.github.io&lt;/code&gt;作为仓库名。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;jekyll
  选择模版，可以在&lt;a href=&quot;http://jekyllthemes.org/&quot;&gt;Jekyll Themes&lt;/a&gt;选择自己喜欢的模版。我选择的是 &lt;a href=&quot;http://jekyllthemes.org/themes/jekyll-theme-next/&quot;&gt;Next&lt;/a&gt;
模版，之所以选择该模版，是因为可以按照&lt;a href=&quot;http://theme-next.simpleyyt.com/&quot;&gt;Next 使用文档&lt;/a&gt;一步一步进行操作，对于小白
来说比较友好。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;遇到问题&quot;&gt;遇到问题&lt;/h2&gt;
&lt;h3 id=&quot;jekyll安装-psmac-os&quot;&gt;jekyll安装， ps：mac os&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ruby版本过低
  解决方案，安装rvm更新ruby，Rvm是一个命令行工具，可以管理多个版本的Ruby。&lt;a href=&quot;https://www.jianshu.com/p/f56addf0c870&quot;&gt;Mac使用RVM更新Ruby&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;权限问题
  使用gem遇到 write permissions for the /usr/bin directory；
    &lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;gem install &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; /usr/local/bin jekyll
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I&lt;/p&gt;</content><author><name>ZX</name></author><category term="something" /><summary type="html">如何搭建该网站！. 准备工作 github pages jekyll 具体过程 github pages 创建 首先新建仓库， 需要以username.github.io作为仓库名。 jekyll 选择模版，可以在Jekyll Themes选择自己喜欢的模版。我选择的是 Next 模版，之所以选择该模版，是因为可以按照Next 使用文档一步一步进行操作，对于小白 来说比较友好。 遇到问题 jekyll安装， ps：mac os ruby版本过低 解决方案，安装rvm更新ruby，Rvm是一个命令行工具，可以管理多个版本的Ruby。Mac使用RVM更新Ruby 权限问题 使用gem遇到 write permissions for the /usr/bin directory； sudo gem install -n /usr/local/bin jekyll I</summary></entry><entry><title type="html">第十六章 灰度部署 (金丝雀部署)</title><link href="http://localhost:4000/sre/2020/01/16/%E7%81%B0%E5%BA%A6%E9%83%A8%E7%BD%B2/" rel="alternate" type="text/html" title="第十六章 灰度部署 (金丝雀部署)" /><published>2020-01-16T00:00:00+08:00</published><updated>2020-01-16T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/16/%E7%81%B0%E5%BA%A6%E9%83%A8%E7%BD%B2</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/16/%E7%81%B0%E5%BA%A6%E9%83%A8%E7%BD%B2/">&lt;blockquote&gt;
  &lt;p&gt;发布工程是一个术语，用来描述从存储库中获取代码发布到生产环境中，之间相关的全部过程和所有组件。自动化发布可以帮助避免许多发布工程的传统缺陷: 重复性和手工任务的辛苦、手动流程的不一致性、无法了解上线的确切状态以及回滚困难。发布工程的自动化已经在其他文献中得到了很好的介绍——例如，关于持续集成和持续交付的书籍(CI/CD)。&lt;br /&gt;
  我们将灰度发布定义为：对服务进行部分且有时间限制的变更部署，并同时进行评估。该评估将帮助我们决定是否继续上线。变更的服务部分是&lt;code class=&quot;highlighter-rouge&quot;&gt;the canary&lt;/code&gt;，服务的其余部分是&lt;code class=&quot;highlighter-rouge&quot;&gt;the control&lt;/code&gt;。支持这种方法的逻辑是，灰度发布通常在线上进行小流量发布，或者影响比&lt;code class=&quot;highlighter-rouge&quot;&gt;the control&lt;/code&gt;部分少得多的用户上。灰度发布是一个有效的A/B测试过程。&lt;br /&gt;
  我们将首先介绍发布工程的基础知识，以及通过自动化发布来建立共享词汇的益处。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- more --&gt;
&lt;h2 id=&quot;发布工程原理&quot;&gt;发布工程原理&lt;/h2&gt;
&lt;p&gt;发布工程的基本原理如下:&lt;/p&gt;
&lt;h5 id=&quot;可再生构建&quot;&gt;可再生构建&lt;/h5&gt;
&lt;p&gt;构建系统应该能够接受构建输入(源代码、资产等)并生成相同结果。与上周相同的输入(构建代码)应该在本周产生相同的输出。&lt;/p&gt;
&lt;h5 id=&quot;自动化构建&quot;&gt;自动化构建&lt;/h5&gt;
&lt;p&gt;一旦代码上传之后，能够自动化生成构建组件并将其上传到存储系统。&lt;/p&gt;
&lt;h5 id=&quot;自动化测试&quot;&gt;自动化测试&lt;/h5&gt;
&lt;p&gt;一旦自动构建系统构建了组件，某种类型的测试套件应该确保它们正常工作。&lt;/p&gt;
&lt;h5 id=&quot;自动化部署&quot;&gt;自动化部署&lt;/h5&gt;
&lt;p&gt;部署应该由计算机执行，而不是人。&lt;/p&gt;
&lt;h5 id=&quot;小型部署&quot;&gt;小型部署&lt;/h5&gt;
&lt;p&gt;构建系统应该支持小的、自包含的更改。&lt;/p&gt;

&lt;p&gt;这些原则为运维人员带来直接收益:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;通过消除手工和重复的任务来减轻工程师的操作负担。&lt;/li&gt;
  &lt;li&gt;强制同行评审和版本控制，因为自动化通常是基于代码的。&lt;/li&gt;
  &lt;li&gt;建立一致的、可重复的、自动化的流程，从而减少错误。&lt;/li&gt;
  &lt;li&gt;添加对发布管道的监控，通过解决以下问题进行测量和持续改进:
    &lt;ul&gt;
      &lt;li&gt;–发布版本需要多长时间生产环境才生效?&lt;/li&gt;
      &lt;li&gt;–发布成功的频率是多少?一个成功的版本是一个没有严重缺陷或SLO违规的、客户可用的版本。&lt;/li&gt;
      &lt;li&gt;–可以做哪些更改来尽早的捕获管道中的缺陷?&lt;/li&gt;
      &lt;li&gt;–哪些步骤可以并行化或进一步优化?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CI/CD与发布自动化相结合可以持续改进开发周期，如图16-1所示。当发布自动化时，你可以更频繁地发布。对于变更率很高的软件来说，更频繁地发布意味着在任何给定的发布工件中捆绑更少的更改。而更小的、自包含的发布工件使得在出现bug时回滚任何给定的发布工件变得成本更低、更容易。更快的发布节奏意味着可以更快地修复bug。
&lt;img src=&quot;/blog/something/images/SRE/16-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;平衡发布速度和可靠性&quot;&gt;平衡发布速度和可靠性&lt;/h2&gt;

&lt;p&gt;快速发布(以下称为&lt;code class=&quot;highlighter-rouge&quot;&gt;发布&lt;/code&gt;)和可靠性常常被视为相反的目标。企业希望以100%的可靠性尽快发布新特性和产品改进!然而这个目标是不可能实现的(因为100%从来不是可靠性的正确目标;参见第2章)，但可以在满足特定产品的特定可靠性目标的同时，尽可能快地进行交付。&lt;/p&gt;

&lt;p&gt;实现这个目标的第一步是了解发布对软件可靠性的影响。在谷歌的经验,大多数事件都是由二进制或配置推送导致的(见附录C)。许多类型的软件更改都可能导致系统故障 - 例如，底层组件的行为更改，依赖关系（例如API）的更改，或DNS等配置更改。&lt;/p&gt;

&lt;p&gt;尽管对软件进行变更存在固有的风险，但是这些变更(bug修复、安全补丁和新特性)对业务的成功是必需的。你可以使用SLOs和错误预算的概念来衡量发布新版本对可靠性的影响，而不是提倡反对变更。你的目标应该是在满足用户期望的可靠性目标的同时尽快发布软件。下一节将讨论如何使用canary流程来实现这些目标。&lt;/p&gt;

&lt;h3 id=&quot;分离变更频率不同的组件&quot;&gt;分离变更频率不同的组件&lt;/h3&gt;

&lt;p&gt;服务由具有不同变更频率的多个组件组成:二进制文件或代码、基础环境(如JVM、内核/OS)、库、服务配置或标志、特性/测试配置和用户配置。如果只有一种发布变更的方法，那么这些组件单独变更会比较困难。&lt;/p&gt;

&lt;p&gt;特性标志或测试框架(如Gertrude、Feature和PlanOut)允许你将特性启动从二进制版本中分离出来。如果二进制版本包含多个特性，你可以通过更改测试配置一次启用一个特性。这样，就没有必要将这些小的变更集合为一个大的变更，或者为每个特性执行单独的版本。更重要的是，如果只有一些新特性的行为不像预期的那样，你可以选择性地禁用这些特性，直到下一个构建/发布周期可以部署新的二进制文件为止。&lt;/p&gt;

&lt;p&gt;你可以将特性标志/试验原则应用于服务的任何类型的更改，而不仅仅是软件版本。&lt;/p&gt;

&lt;h2 id=&quot;canarying是什么&quot;&gt;Canarying是什么？&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Canarying&lt;/code&gt;一词是指将金丝雀带入煤矿以确定该矿是否对人类安全的做法。 由于鸟类比人类更小，呼吸更快，因此它们被危险气体毒害的速度比人类更快。&lt;/p&gt;

&lt;p&gt;即使你的发布管道是完全自动化的，在真正的流量到达服务之前，你依然无法检测到所有与发布相关的缺陷。当一个发布版本准备好部署到生产环境中时，你的测试策略应该充分保证该版本是安全的，并且按预期工作。然而，测试环境与生产环境并不是100%相同的，并且测试不可能会涵盖100％的场景。依然会存在一些会影响生产缺陷。如果一个版本立即部署到系统的全部地方，那么可能存在的缺陷亦将到达系统的全部地方。&lt;/p&gt;

&lt;p&gt;如果你能够快速地检测和解决缺陷，则可以接受此方案。但是，更安全的选择是:首先使用灰度发布向新版本导入一些生产流量。灰度发布允许部署管道在尽可能少地影响你的服务的前提下，更快地检测出问题。&lt;/p&gt;

&lt;h2 id=&quot;发布工程和灰度发布&quot;&gt;发布工程和灰度发布&lt;/h2&gt;

&lt;p&gt;在部署系统的新版本或其关键组件(如配置或数据)时，我们将变更(通常未公开给真实输入的更改，如面向用户的流量、或用户提供的批处理数据)打包。变更会带来新的特性和功能，但也存在部署之后出现问题的风险。我们的目标是通过测试一小部分流量来降低风险，以确保没有任何不良影响。我们将在本章后面讨论评估过程。&lt;/p&gt;

&lt;p&gt;灰度过程还让我们对变更充满信心，因为我们将其暴露给越来越大的流量。为变更引入实际生产流量还使我们能够识别在单元测试或负载测试等测试框架中可能不可见的问题，这些问题通常更为人为。&lt;/p&gt;

&lt;p&gt;我们将使用一个实际的示例来检查灰度过程及其评估，同时避免深入研究统计数据。相反，我们关注点是整个过程和典型的实际考虑。我们使用App Engine上的一个简单应用程序来说明发布的各个方面。&lt;/p&gt;

&lt;h3 id=&quot;灰度发布流程的需求&quot;&gt;灰度发布流程的需求&lt;/h3&gt;

&lt;p&gt;针对特定服务的灰度发布需要特定功能：
将变更通过灰度发布部署到服务全部子集的方法。      &lt;br /&gt;
一个评估过程，用来评估变更是&lt;code class=&quot;highlighter-rouge&quot;&gt;好&lt;/code&gt;还是&lt;code class=&quot;highlighter-rouge&quot;&gt;坏&lt;/code&gt;。
将评估集成到发布过程中。&lt;/p&gt;

&lt;p&gt;最后，当灰度检测到有问题的发布版本，并在没有误报的情况下识别出好的发布版本时，灰度发布展示了它的价值。&lt;/p&gt;

&lt;h3 id=&quot;我们的示例环境&quot;&gt;我们的示例环境&lt;/h3&gt;

&lt;p&gt;我们将使用一个简单的前端web服务应用程序来演示一些灰度发布的概念。该应用程序提供了一个基于http的API，消费者可以使用它来操作各种数据(如产品价格等简单信息)。示例应用程序有一些可调参数，我们可以使用这些参数来模拟各种生产环境，由灰度发布流程进行评估。例如，可以让应用程序为20%的请求返回错误，或者规定5%的请求至少需要两秒钟。&lt;/p&gt;

&lt;p&gt;我们使用部署在谷歌应用程序引擎上的应用程序来演示灰度发布流程，这些原则同样适用于其他环境。虽然示例应用程序是经过设计的，但是在实际场景中，类似的应用程序与我们的示例可以共享灰度发展中使用的指标。&lt;/p&gt;

&lt;p&gt;我们的示例服务有两个可能的版本:当前版本和候选版本。当前版本是当前部署在生产环境中的版本，而候选版本是新构建的版本。使用这两个版本来说明发布概念，以及如何实现灰度发布以使发布过程更安全。&lt;/p&gt;

&lt;h2 id=&quot;回滚部署与简单的canary部署比较&quot;&gt;回滚部署与简单的Canary部署比较&lt;/h2&gt;

&lt;p&gt;我们将在发生中断时根据错误预算节省和一般影响，来对没有灰度发布的部署流程和灰度发布流程进行比较。我们的部署过程以开发环境为基础。一旦我们感觉代码在开发环境中正常工作，我们就将该版本部署到生产环境中。&lt;/p&gt;

&lt;p&gt;在部署之后不久，监视开始报高错误率(参见图16-2，在图16-2中，为了模拟示例服务中的缺陷，对示例应用程序进行配置以使20％的请求失败)。对于示例，假设部署流程不支持回滚到以前已知的配置正常的版本时。修复这些错误的最佳选择就只有在生产版本中查找缺陷，对其进行补救，并在停机期间重新部署一个新版本。这种做法肯定会延长错误对用户的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/16-2.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图 16-2 部署之后错误率增加 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;为了改进这个初始部署过程，我们可以在使用灰度发布来减少推送错误代码所造成的影响。 我们需要一种方法来在小部分生产环境中运行候选版本，而不是一次性部署到生产环境。 然后将一小部分流量发送到该生产环境（the canary金丝雀）并将其与其他部分（the control 主控）进行比较。 使用此方法，我们可以在所有生产受到影响之前发现候选版本中的缺陷。&lt;/p&gt;

&lt;p&gt;我们在示例应用程序中的进行简单灰度发布，在应用程序的特定版本之间分配流量。 您可以使用App Engine或其他任何方法来分割流量（例如负载均衡器上的后端权重，代理配置或循环DNS记录）。&lt;/p&gt;

&lt;p&gt;图16-3显示了当我们使用灰度发布，变更的影响会大大降低;事实上，这些错误几乎不可见!这提出了一个有趣的问题:与总体流量趋势相比，灰度发布的流量趋势很难看到和跟踪。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/16-3.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图 16-3 部署之后错误率增canary部署错误率； 因为进行canary部署的只是系统的一小部分，因此总体错误率降低 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;为了更清楚地了解需要在合理范围内跟踪的错误，我们可以通过App Engine应用程序版本查看关键指标(HTTP响应代码)，如图16-4所示。当我们查看每个版本的分解趋势图时，我们可以清楚地看到新版本引入的错误。我们还可以从图16-4中观察到当前版本提供的错误非常少。&lt;/p&gt;

&lt;p&gt;现在，我们可以根据应用程序版本的HTTP错误率对部署进行调优。如果灰度发布的错误率大于全部系统的错误率，这表明canary部署是&lt;code class=&quot;highlighter-rouge&quot;&gt;糟糕的&lt;/code&gt;。我们应该暂停并回滚部署，或者联系他人来帮助解决问题。如果错误率相似，我们可以正常地进行部署。在图16-4中，我们的canary部署显然很糟糕，我们应该回滚它。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/16-4.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图 16-4 应用程序HTTP响应码； 新版本产生多数错误、当前版本产生小数错误（图中显示10%的log） &lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&quot;canary实施&quot;&gt;Canary实施&lt;/h2&gt;

&lt;p&gt;现在我们已经看到了一个相当简单的canary部署实现，接下来让我们更深入地了解成功的canary流程所需的参数。&lt;/p&gt;

&lt;h3 id=&quot;最小化slos和错误预算的风险&quot;&gt;最小化SLOs和错误预算的风险&lt;/h3&gt;

&lt;p&gt;第2章讨论了SLOs如何反映设计服务可用性的业务需求。这些需求也可以通过canary实现。canary进程的风险仅仅是我们错误预算的一小部分，它受到时间和canary规模大小的限制。&lt;/p&gt;

&lt;p&gt;全局部署会很快将SLO置于危险之中。如果实例中为系统全面部署候选版本，我们将面临20%的请求失败的风险。如果我们使用5%的canary规模，我们将为5%的流量提供20%错误，导致1%的总体错误率(如图16-3所示)。这个策略允许我们保留我们的错误预算—预算的影响与暴露于缺陷的流量的数量成正比。我们可以假设，对于全局部署和灰度部署，检测和回滚花费的时间差不多，但是当我们将灰度发布集成到部署过程中时，我们会以更低的成本获得有关新版本的有价值信息。&lt;/p&gt;

&lt;p&gt;这是一个假设负载均匀的极简模型。它还假设我们可以将整个错误预算用于灰度发布。这里我们只考虑新版本引入的不可用性，而不是实际可用性。我们的模型还假设新版本具有100%的失败率，这是最坏的情况。而进行灰度的部分不会导致线上系统100%不可用。我们还允许在灰度部署期间，整个系统的可用性低于SLO。&lt;/p&gt;

&lt;p&gt;这个模型有明显的缺陷，但它是一个可靠的起点，你可以根据业务需求进行调整。我们建议使用最简单的模型来满足你的技术和业务目标。根据我们的经验，专注于使模型在技术上尽可能正确，常常会导致在建模上的过度投资。对于具有高复杂性的服务，过于复杂的模型可能导致持续的模型调优，而没有真正的好处。&lt;/p&gt;

&lt;h3 id=&quot;选择灰度规模和持续时间&quot;&gt;选择灰度规模和持续时间&lt;/h3&gt;

&lt;p&gt;选择合适的灰度持续时间，需要考虑发布频率。 如果需要每天发布，那么在一次只运行一个灰度的情况下，无法使灰度保持一周,如果每周部署一次，就可以执行较长的灰度发布。 如果持续部署（例如，一天20次），灰度的持续时间必须明显缩短。 在一些说明里，虽然可以同时运行多个灰度，但这样做会增加大量精力来跟踪系统状态。 在任何情况下，需要快速推断系统状态时，同时运行多个灰度会成为问题。如果灰度重叠，同时运行多个灰度也会增加信号污染的风险。我们强烈建议一次只运行一个灰度。&lt;/p&gt;

&lt;p&gt;对于基本的评估，不需要大规模的灰度来检测关键条件。然而，一个有代表性的灰度发布流程需要跨多个维度进行决策:&lt;/p&gt;

&lt;h2 id=&quot;规模和持续时间&quot;&gt;规模和持续时间&lt;/h2&gt;

&lt;p&gt;它的规模应够大，持续时间应够长，足以代表整个部署。仅在接收到少量查询后终止canary部署，对于以具有不同功能的不同查询为特征的系统来说，这无法提供有用的信号。处理率越高，获取代表性样本所需的时间就越少，以确保所观察到的行为实际上是由变更引起的，而不仅仅是随机因素。&lt;/p&gt;

&lt;p&gt;流量&lt;/p&gt;

&lt;p&gt;我们需要在系统上接收足够的流量，以确保它是一个具有代表性的示例，并且系统有机会对输入做出负面反应。通常，请求越均匀，所需要的流量就越少。&lt;/p&gt;

&lt;p&gt;时间点&lt;/p&gt;

&lt;p&gt;性能缺陷通常只在高负载下出现，因此在非高峰时间部署可能不会触发性能相关的缺陷。&lt;/p&gt;

&lt;p&gt;度量指标&lt;/p&gt;

&lt;p&gt;灰度的代表性与我们选择评估的指标密切相关(我们将在本章后面讨论)。我们可以快速评估诸如查询成功之类的琐碎指标，但是其他指标(如队列深度)可能需要更多的时间或较大规模的灰度来提供清晰的信号。&lt;/p&gt;

&lt;p&gt;但问题是，这些要求可能相互冲突。Canarying是一种平衡行为，它通过对最坏情况的冷静分析和系统过去的实际记录来实现。一旦您从过去的灰度中收集了指标，您就可以根据典型的canary评估失败率而不是假想的最坏情况来选择canary参数。&lt;/p&gt;

&lt;h3 id=&quot;选择和评估度量标准&quot;&gt;选择和评估度量标准&lt;/h3&gt;

&lt;p&gt;到目前为止，我们一直在研究成功率，这是评估灰度发布的一个非常清晰和明显的指标。但是直觉上，我们知道这个单一的指标对于有意义的canary流程来说是不够的。如果我们以10倍的延迟为所有请求提供服务，或者在这样做时使用10倍的内存，那么我们可能也会遇到问题。并不是所有的指标都适合评估灰度发布。哪些指标最适合评估灰度发布版本是好是坏?&lt;/p&gt;

&lt;h3 id=&quot;度量标准应指出问题&quot;&gt;度量标准应指出问题&lt;/h3&gt;

&lt;p&gt;首先，指标需要能够指出服务中的问题。这很棘手，因为构成&lt;code class=&quot;highlighter-rouge&quot;&gt;问题&lt;/code&gt;的并不总是客观的。我们可能会认为用户请求失败是有问题的。但是如果一个请求的响应时间增加了10%，或者系统内存增加了10%?，这该如何判断？我们通常建议使用sla作为开始考虑canary指标的地方。良好的服务质量指数往往与服务健康状况密切相关。如果已经使用SLIs来度量SLO是否符合，那么我们可以重用这些工作。&lt;/p&gt;

&lt;p&gt;几乎任何指标在极端情况下都可能出现问题，但是向灰度流程中添加太多的指标也会产生成本。我们需要为每个指标正确定义可接受行为。如果可接受行为定义过于严格，我们会得到大量的误报;也就是说，我们会认为灰度很糟糕，即使实际不是这样。相反，如果对可接受行为的定义过于宽松，我们更有可能忽略掉有问题的灰度部署。正确选择什么是可接受的行为可能会成本较大——既耗时又需要分析。然而，如果做得不好，错误的结果会完全误导你。此外，随着服务、其特性集和行为的发展，您需要定期重新评估期望。&lt;/p&gt;

&lt;p&gt;我们应该根据这些指标多大程度上能够表明系统中实际用户的体验来进行排名，选择排名靠前的几个指标(可能不超过12个)。太多的度量标准会带来递减的回报，并且在某种程度上，收益会被维护它们的成本所抵消，或者在发布过程中如果不维护它们，会对发布结果无法保证100%的信任。&lt;/p&gt;

&lt;p&gt;为了使这个指导原则更加具体，让我们回头再来看示例。它有许多我们可以评估的指标:CPU使用量、内存占用、HTTP返回码(2xx、3xx等等)、响应延迟、正确性等等。在这种情况下，我们最好的度量标准可能是HTTP返回码和响应延迟，因为它们的降级最接近于实际用户影响。在这个场景中，CPU使用率并没有那么有用:资源使用的增加不一定会影响服务，并且可能导致不稳定或嘈杂的canary进程。这会导致操作人员禁用或忽略canary进程，这会首先破坏使用canary进程的目的。对于前端服务，我们直观地知道，响应较慢或响应失败通常会真实反映服务中存在的问题。&lt;/p&gt;

&lt;p&gt;HTTP返回码包含一些有趣的复杂情况，例如状态码404，它告诉我们没有找到资源。这可能是因为用户获得了错误的URL(想象一下在一个流行的论坛上分享了一个错误的URL)，或者因为服务器错误地停止了对资源的服务。通常，我们可以通过排除canary评估中的400级状态码，并添加黑盒监控来测试特定URL的存在，从而解决此类问题。然后，我们可以将黑盒数据作为canary分析的一部分，以帮助将canary流程与奇怪的用户行为隔离开来。&lt;/p&gt;

&lt;p&gt;度量标准应该具有代表性和可归属性&lt;/p&gt;

&lt;p&gt;观察到的指标变化其来源，应该清楚地归因于正在进行的变更，并且不应该受到外部因素的影响。&lt;/p&gt;

&lt;p&gt;在一个大的系统中(例如，许多服务器或许多容器)，我们可能会有外部性——超过连接的机器、运行具有不同性能特征的不同内核的机器，或者网络中过载的机器。此时金丝雀部分和主系统部分之间的差异，既是我们所部署的两个基础设施之间的差异，也会是我们变更导致的差异。&lt;/p&gt;

&lt;p&gt;管理金丝雀是多种力量之间的平衡。增加金丝雀的规模是减少这个问题影响的方法(如前所述)。当我们的系统达到我们认为的合理的金丝雀规模时，我们需要考虑我们选择的指标是否会显示出很大的差异。&lt;/p&gt;

&lt;p&gt;我们还应该知道canary和control环境之间共享的失败域;坏金丝雀会对控制产生负面影响，而系统中的坏行为可能会导致我们错误地评估金丝雀。同样，确保您的度量标准是良好隔离的。考虑一个同时运行我们的应用程序和其他进程的系统。整个系统的CPU使用量的急剧增加会导致糟糕的度量，因为系统中的其他进程(数据库负载、日志轮转等)可能会导致这种增加。更好的度量标准是在处理请求时所花费的CPU时间。更好的度量标准是在服务进程实际计划在CPU上的时间窗口上为处理请求服务所花费的CPU时间。虽然与我们的进程相关的严重超额的机器显然是一个问题(监控应该捕捉到它!)，但它不是由我们正在进行的更改引起的，因此不应该将其标记为金丝雀部署失败。&lt;/p&gt;

&lt;p&gt;金丝雀也需要是可归属的;也就是说，您还应该能够将canary度量与SLIs联系起来。如果一个度量可以在不影响服务的情况下发生巨大变化，那么它不适合用来评估灰度发布。&lt;/p&gt;

&lt;h3 id=&quot;评估前评估后依然是有风险的&quot;&gt;评估前/评估后依然是有风险的&lt;/h3&gt;

&lt;p&gt;canary过程的前后是归因问题的延伸。在这个过程中，旧系统被新系统完全替代，你的canary评估将在一段时间内比较变更之前和之后的系统行为。你可以将此过程称为&lt;code class=&quot;highlighter-rouge&quot;&gt;时空中的canary部署&lt;/code&gt;，在此过程中，您通过分割时间来选择A/B组，而不是通过机器、cookie或其他方法来分割总体。由于时间是观察到的指标变化的最大来源之一，因此很难在评估之前/之后来判断性能是否下降。&lt;/p&gt;

&lt;p&gt;虽然canary部署可能导致降级，但原有系统本身也可能会降级。如果需要长时间运行canary部署，就会变得更加复杂。例如，如果在周一进行发布，可能会将工作日的行为与周末的行为进行比较，从而引入大量噪音。在该示例中，用户可能在周末以不同的方式访问该服务。从而在canary进程中引入噪音。&lt;/p&gt;

&lt;p&gt;评估前/后过程本身引入了一个问题，即大而短的错误率(由前/后评估引入)是否优于小而长的错误率(由一个小金丝雀引入)。如果新版本完全被破坏，我们能多快地检测和恢复? 大规模的金丝雀之前/之后可以更快地检测到问题，但恢复的总体时间可能仍然相当长，与较小的金丝雀类似。在此期间，用户会一直受到影响。&lt;/p&gt;

&lt;h3 id=&quot;使用渐进的灰度会更好&quot;&gt;使用渐进的灰度会更好&lt;/h3&gt;

&lt;p&gt;选择的度量标准即使不符合我们理想中的属性，但仍然很有价值。我们可以通过使用更细微的灰度过程来介绍这些指标。&lt;/p&gt;

&lt;p&gt;我们可以使用包含多个阶段的canary来反映我们对度量的推理能力，而不是简单地评估单个canary阶段。在第一阶段，我们对这个版本没有信心或不了解。因此，我们希望使用一个小的阶段，以尽量减少负面影响。在小型灰度中，我们更喜欢能够最清晰地显示问题的指标——应用程序崩溃、请求失败等等。一旦这一阶段成功地过去，下一阶段将增加灰度规模，从而增强我们分析变化影响的信心。&lt;/p&gt;

&lt;h2 id=&quot;依赖和隔离&quot;&gt;依赖和隔离&lt;/h2&gt;

&lt;p&gt;正在测试的系统不会在完全真空中运行。出于实际原因，灰度和主系统可以共享后端、前端、网络、数据存储和其他基础设施。甚至可能与客户端有非常不明显的交互。例如，假设一个客户端发送了两个连续的请求。第一个请求可以由灰度部分来处理。其响应可能会改变第二个请求的内容，第二个请求可能会落在主系统部分，从而改变主系统的行为。&lt;/p&gt;

&lt;p&gt;不完美的隔离会带来几个后果。最重要的是，我们需要知道，如果灰度过程的结果表明我们应该停止生产变更并调查情况，那么灰度并不一定是错误的。这一事实对于一般的canarying来说是正确的，但是在实践中，它经常由于隔离问题而导致被强制执行。&lt;/p&gt;

&lt;p&gt;此外，不完美的隔离意味着灰度部署的错误行为也会对原始系统产生负面影响。Canarying是A/B比较，A和B有可能同时改变;这可能会导致评估灰度变得混乱。还必须使用绝对度量，例如定义的SLOs，以确保系统正确运行。&lt;/p&gt;

&lt;h2 id=&quot;在非交互系统中进行canarying&quot;&gt;在非交互系统中进行Canarying&lt;/h2&gt;

&lt;p&gt;本章重点讨论了交互式请求/响应系统，它在许多方面是最简单和最常讨论的系统设计。其他系统，如异步处理管道，也同样重要，但有不同的canarying注意事项，我们将简要列举。有关数据处理管道的canarying的更多信息，请参见第13章。&lt;/p&gt;

&lt;p&gt;首先，canary的持续时间和部署本质上依赖于工作单元处理的持续时间。当涉及到交互系统时，我们忽略了这个因素，假设工作单元处理的时间不会超过几秒钟，这比canary的持续时间要短。非交互式系统中的工作单元处理(如呈现管道或视频编码)可能需要更长的时间。因此，确保canary持续时间至少跨越单个工作单元的持续时间。&lt;/p&gt;

&lt;p&gt;对于非交互式系统，隔离可能变得更加复杂。许多管道系统只有一个工作分配程序和一组使用应用程序代码的工作人员。在多阶段管道中，工作单元由工作人员处理，然后返回到池中，由同一工作人员或另一个工作人员执行下一阶段的处理。金丝雀分析有助于确保处理特定工作单元的工人总是从相同的工人池中提取——要么是金丝雀池，要么是控制池。否则，信号就会变得越来越混杂(有关理清信号的需要的更多信息，请参见349页的&lt;code class=&quot;highlighter-rouge&quot;&gt;监视数据的要求&lt;/code&gt;)。&lt;/p&gt;

&lt;p&gt;最后，度量标准的选择可能更加复杂。我们可能感兴趣的是端到端处理工作单元的时间(类似于交互系统中的延迟)，以及处理本身的质量(当然，这是完全特定于应用程序的)。&lt;/p&gt;

&lt;p&gt;考虑到这些警告，canarying的一般概念仍然是可行的，并且适用相同的高级原则。&lt;/p&gt;

&lt;h2 id=&quot;监控要求&quot;&gt;监控要求&lt;/h2&gt;

&lt;p&gt;在评估灰度部署时，您必须能够将部署了灰度的系统与未部署灰度的系统进行比较。通常，这需要在构造监视系统时多加注意—有效的比较非常简单，并且能够产生有意义的结果。&lt;/p&gt;

&lt;p&gt;考虑之前的例子，在5%的规模中进行灰度，错误率为20%。因为监视很可能将系统作为一个整体来观察，所以它只能检测到1%的总体错误率。根据系统的不同，这个信号可能与其他错误源无法区分(参见图16-3)。&lt;/p&gt;

&lt;p&gt;如果我们通过按照服务请求的对象来（金丝雀与主系统）分解指标，(参见图16-4)我们可以清楚地看到主系统与canary之间的错误率，这清楚地说明了全局部署将带来什么。在这里，我们看到，对整个服务的监控不足以分析灰度是否ok。在收集监视数据时，能够执行细粒度的分解非常重要，这些分解使得能够区分金丝雀和主系统的指标。&lt;/p&gt;

&lt;p&gt;收集指标的另一个难点是金丝雀的部署受到设计的时间限制。当度量指标在特定时期内进行聚合时，这可能会导致问题。考虑每小时的度量误差。我们可以通过对过去一小时的请求求和来计算这个度量。如果我们使用这个度量来评估我们的canary，我们可能会遇到问题，如下面的时间表所述:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;某些事件会导致一些错误发生。&lt;/li&gt;
  &lt;li&gt;一只金丝雀被部署在5%的人口中;金丝雀的持续时间是30分钟。&lt;/li&gt;
  &lt;li&gt;canary系统开始监视每小时的错误度量，以确定部署是好是坏。&lt;/li&gt;
  &lt;li&gt;部署被检测为错误，因为每小时的错误度量与控制总体的每小时的错误显著不同。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;此场景是使用每小时计算一次的度量来评估仅30分钟长的部署的结果。因此，canary进程提供了一个非常模糊的信号。当使用度量来评估canary的成功时，确保度量的间隔与canary的持续时间相同或小于持续时间。&lt;/p&gt;

&lt;p&gt;相关概念&lt;/p&gt;

&lt;p&gt;通常，与客户的对话涉及到在生产中使用蓝/绿部署、人工负载生成和/或流量测试。这些概念类似于canarying，因此虽然它们不是严格意义上的金丝雀流程，但亦可使用。&lt;/p&gt;

&lt;h3 id=&quot;蓝绿部署&quot;&gt;蓝/绿部署&lt;/h3&gt;

&lt;p&gt;蓝/绿部署维护系统的两个实例：一个提供流量（绿色），另一个准备提供流量（蓝色）。 在蓝色环境中部署新版本后，将流量切换到其中。切换过程不需要停机，并且回滚只是简单逆转路由器而已。 一个缺点是该设置使用的资源是&lt;code class=&quot;highlighter-rouge&quot;&gt;传统&lt;/code&gt;部署的两倍。在该设置中，您正在有效地执行前/后金丝雀（前面已讨论过）。&lt;/p&gt;

&lt;p&gt;通过同时(而不是分开地)使用蓝/绿部署，您可以或多或少地将蓝色/绿色部署用作常规的金丝雀。在此策略中，您可以将canary部署到blue(备用)实例，并在绿色和蓝色环境之间缓慢地分配流量。您的评估和比较蓝色环境和绿色环境的指标都应该与流量控制相关。这种设置类似于A/B金丝雀，此时绿色环境是主系统，蓝色环境是金丝雀部署，金丝雀数量由发送到每个金丝雀的流量控制。&lt;/p&gt;

&lt;h3 id=&quot;人工负载生成&quot;&gt;人工负载生成&lt;/h3&gt;

&lt;p&gt;与其将实时用户流量暴露给canary部署，还不如在安全性方面犯点错误，使用人工负载。通常，您可以在多个部署阶段(QA、预生产，甚至在生产中)运行负载测试。虽然根据我们的定义，这些操作不符合canarying，但是它们仍然是找到缺陷的可行方法，但需要注意一些事项。&lt;/p&gt;

&lt;p&gt;使用人工负载进行测试可以很好地最大化代码覆盖率，但不能提供良好的状态覆盖率。在可变系统(具有缓存、cookie、请求关联等的系统)中人工模拟负载尤其困难。人工负载也可能无法准确地模拟真实系统中流量变化。有些问题可能只在无人工负载的情况下出现，从而导致覆盖率有所差距。&lt;/p&gt;

&lt;p&gt;人工负载在可变系统中也很难工作。例如，试图在计费系统上生成人工负载可能非常危险:系统可能开始向信用卡供应商发送呼叫，然后信用卡供应商将开始主动向客户收费。虽然我们可以避免测试危险的代码逻辑，但是在这些逻辑上缺乏测试会降低我们的测试覆盖率。&lt;/p&gt;

&lt;h3 id=&quot;流量测试&quot;&gt;流量测试&lt;/h3&gt;

&lt;p&gt;如果人工流量不具有代表性，我们可以复制流量并将其发送到生产系统和测试环境。这种技术被称为流量镜像。生产系统服务于实际流量并响应请求，canary部署服务于副本流量并丢弃响应。您甚至可以将canary响应与实际响应进行比较，并运行进一步的分析。&lt;/p&gt;

&lt;p&gt;这种策略可以提供有代表性的流量，但通常比更直接的canary流程更复杂。在有状态系统中，流量测试也不能充分识别风险;流量副本可能会在看似独立的部署之间引入意外的影响。例如，如果canary部署和生产系统共享一个缓存，人为导致的缓存命中率增加将使canary指标的性能度量无效。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;p&gt;您可以使用许多工具和方法来自动化版本发布，并将canarying引入到发布管道中。没有一种测试方法是万能的，测试策略应该由系统的需求和行为决定。Canarying可以作为一种简单、健壮且易于集成的方法来补充测试。当您及早发现系统缺陷时，用户受到的影响最小。Canarying还可以为频繁发布提供信心，并提高开发速度。正如测试方法必须随着系统需求和设计而发展一样，canarying也必须如此。&lt;/p&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">发布工程是一个术语，用来描述从存储库中获取代码发布到生产环境中，之间相关的全部过程和所有组件。自动化发布可以帮助避免许多发布工程的传统缺陷: 重复性和手工任务的辛苦、手动流程的不一致性、无法了解上线的确切状态以及回滚困难。发布工程的自动化已经在其他文献中得到了很好的介绍——例如，关于持续集成和持续交付的书籍(CI/CD)。 我们将灰度发布定义为：对服务进行部分且有时间限制的变更部署，并同时进行评估。该评估将帮助我们决定是否继续上线。变更的服务部分是the canary，服务的其余部分是the control。支持这种方法的逻辑是，灰度发布通常在线上进行小流量发布，或者影响比the control部分少得多的用户上。灰度发布是一个有效的A/B测试过程。 我们将首先介绍发布工程的基础知识，以及通过自动化发布来建立共享词汇的益处。 发布工程原理 发布工程的基本原理如下: 可再生构建 构建系统应该能够接受构建输入(源代码、资产等)并生成相同结果。与上周相同的输入(构建代码)应该在本周产生相同的输出。 自动化构建 一旦代码上传之后，能够自动化生成构建组件并将其上传到存储系统。 自动化测试 一旦自动构建系统构建了组件，某种类型的测试套件应该确保它们正常工作。 自动化部署 部署应该由计算机执行，而不是人。 小型部署 构建系统应该支持小的、自包含的更改。 这些原则为运维人员带来直接收益: 通过消除手工和重复的任务来减轻工程师的操作负担。 强制同行评审和版本控制，因为自动化通常是基于代码的。 建立一致的、可重复的、自动化的流程，从而减少错误。 添加对发布管道的监控，通过解决以下问题进行测量和持续改进: –发布版本需要多长时间生产环境才生效? –发布成功的频率是多少?一个成功的版本是一个没有严重缺陷或SLO违规的、客户可用的版本。 –可以做哪些更改来尽早的捕获管道中的缺陷? –哪些步骤可以并行化或进一步优化? CI/CD与发布自动化相结合可以持续改进开发周期，如图16-1所示。当发布自动化时，你可以更频繁地发布。对于变更率很高的软件来说，更频繁地发布意味着在任何给定的发布工件中捆绑更少的更改。而更小的、自包含的发布工件使得在出现bug时回滚任何给定的发布工件变得成本更低、更容易。更快的发布节奏意味着可以更快地修复bug。 平衡发布速度和可靠性 快速发布(以下称为发布)和可靠性常常被视为相反的目标。企业希望以100%的可靠性尽快发布新特性和产品改进!然而这个目标是不可能实现的(因为100%从来不是可靠性的正确目标;参见第2章)，但可以在满足特定产品的特定可靠性目标的同时，尽可能快地进行交付。 实现这个目标的第一步是了解发布对软件可靠性的影响。在谷歌的经验,大多数事件都是由二进制或配置推送导致的(见附录C)。许多类型的软件更改都可能导致系统故障 - 例如，底层组件的行为更改，依赖关系（例如API）的更改，或DNS等配置更改。 尽管对软件进行变更存在固有的风险，但是这些变更(bug修复、安全补丁和新特性)对业务的成功是必需的。你可以使用SLOs和错误预算的概念来衡量发布新版本对可靠性的影响，而不是提倡反对变更。你的目标应该是在满足用户期望的可靠性目标的同时尽快发布软件。下一节将讨论如何使用canary流程来实现这些目标。 分离变更频率不同的组件 服务由具有不同变更频率的多个组件组成:二进制文件或代码、基础环境(如JVM、内核/OS)、库、服务配置或标志、特性/测试配置和用户配置。如果只有一种发布变更的方法，那么这些组件单独变更会比较困难。 特性标志或测试框架(如Gertrude、Feature和PlanOut)允许你将特性启动从二进制版本中分离出来。如果二进制版本包含多个特性，你可以通过更改测试配置一次启用一个特性。这样，就没有必要将这些小的变更集合为一个大的变更，或者为每个特性执行单独的版本。更重要的是，如果只有一些新特性的行为不像预期的那样，你可以选择性地禁用这些特性，直到下一个构建/发布周期可以部署新的二进制文件为止。 你可以将特性标志/试验原则应用于服务的任何类型的更改，而不仅仅是软件版本。 Canarying是什么？ Canarying一词是指将金丝雀带入煤矿以确定该矿是否对人类安全的做法。 由于鸟类比人类更小，呼吸更快，因此它们被危险气体毒害的速度比人类更快。 即使你的发布管道是完全自动化的，在真正的流量到达服务之前，你依然无法检测到所有与发布相关的缺陷。当一个发布版本准备好部署到生产环境中时，你的测试策略应该充分保证该版本是安全的，并且按预期工作。然而，测试环境与生产环境并不是100%相同的，并且测试不可能会涵盖100％的场景。依然会存在一些会影响生产缺陷。如果一个版本立即部署到系统的全部地方，那么可能存在的缺陷亦将到达系统的全部地方。 如果你能够快速地检测和解决缺陷，则可以接受此方案。但是，更安全的选择是:首先使用灰度发布向新版本导入一些生产流量。灰度发布允许部署管道在尽可能少地影响你的服务的前提下，更快地检测出问题。 发布工程和灰度发布 在部署系统的新版本或其关键组件(如配置或数据)时，我们将变更(通常未公开给真实输入的更改，如面向用户的流量、或用户提供的批处理数据)打包。变更会带来新的特性和功能，但也存在部署之后出现问题的风险。我们的目标是通过测试一小部分流量来降低风险，以确保没有任何不良影响。我们将在本章后面讨论评估过程。 灰度过程还让我们对变更充满信心，因为我们将其暴露给越来越大的流量。为变更引入实际生产流量还使我们能够识别在单元测试或负载测试等测试框架中可能不可见的问题，这些问题通常更为人为。 我们将使用一个实际的示例来检查灰度过程及其评估，同时避免深入研究统计数据。相反，我们关注点是整个过程和典型的实际考虑。我们使用App Engine上的一个简单应用程序来说明发布的各个方面。 灰度发布流程的需求 针对特定服务的灰度发布需要特定功能： 将变更通过灰度发布部署到服务全部子集的方法。 一个评估过程，用来评估变更是好还是坏。 将评估集成到发布过程中。 最后，当灰度检测到有问题的发布版本，并在没有误报的情况下识别出好的发布版本时，灰度发布展示了它的价值。 我们的示例环境 我们将使用一个简单的前端web服务应用程序来演示一些灰度发布的概念。该应用程序提供了一个基于http的API，消费者可以使用它来操作各种数据(如产品价格等简单信息)。示例应用程序有一些可调参数，我们可以使用这些参数来模拟各种生产环境，由灰度发布流程进行评估。例如，可以让应用程序为20%的请求返回错误，或者规定5%的请求至少需要两秒钟。 我们使用部署在谷歌应用程序引擎上的应用程序来演示灰度发布流程，这些原则同样适用于其他环境。虽然示例应用程序是经过设计的，但是在实际场景中，类似的应用程序与我们的示例可以共享灰度发展中使用的指标。 我们的示例服务有两个可能的版本:当前版本和候选版本。当前版本是当前部署在生产环境中的版本，而候选版本是新构建的版本。使用这两个版本来说明发布概念，以及如何实现灰度发布以使发布过程更安全。 回滚部署与简单的Canary部署比较 我们将在发生中断时根据错误预算节省和一般影响，来对没有灰度发布的部署流程和灰度发布流程进行比较。我们的部署过程以开发环境为基础。一旦我们感觉代码在开发环境中正常工作，我们就将该版本部署到生产环境中。 在部署之后不久，监视开始报高错误率(参见图16-2，在图16-2中，为了模拟示例服务中的缺陷，对示例应用程序进行配置以使20％的请求失败)。对于示例，假设部署流程不支持回滚到以前已知的配置正常的版本时。修复这些错误的最佳选择就只有在生产版本中查找缺陷，对其进行补救，并在停机期间重新部署一个新版本。这种做法肯定会延长错误对用户的影响。 图 16-2 部署之后错误率增加 为了改进这个初始部署过程，我们可以在使用灰度发布来减少推送错误代码所造成的影响。 我们需要一种方法来在小部分生产环境中运行候选版本，而不是一次性部署到生产环境。 然后将一小部分流量发送到该生产环境（the canary金丝雀）并将其与其他部分（the control 主控）进行比较。 使用此方法，我们可以在所有生产受到影响之前发现候选版本中的缺陷。 我们在示例应用程序中的进行简单灰度发布，在应用程序的特定版本之间分配流量。 您可以使用App Engine或其他任何方法来分割流量（例如负载均衡器上的后端权重，代理配置或循环DNS记录）。 图16-3显示了当我们使用灰度发布，变更的影响会大大降低;事实上，这些错误几乎不可见!这提出了一个有趣的问题:与总体流量趋势相比，灰度发布的流量趋势很难看到和跟踪。 图 16-3 部署之后错误率增canary部署错误率； 因为进行canary部署的只是系统的一小部分，因此总体错误率降低 为了更清楚地了解需要在合理范围内跟踪的错误，我们可以通过App Engine应用程序版本查看关键指标(HTTP响应代码)，如图16-4所示。当我们查看每个版本的分解趋势图时，我们可以清楚地看到新版本引入的错误。我们还可以从图16-4中观察到当前版本提供的错误非常少。 现在，我们可以根据应用程序版本的HTTP错误率对部署进行调优。如果灰度发布的错误率大于全部系统的错误率，这表明canary部署是糟糕的。我们应该暂停并回滚部署，或者联系他人来帮助解决问题。如果错误率相似，我们可以正常地进行部署。在图16-4中，我们的canary部署显然很糟糕，我们应该回滚它。 图 16-4 应用程序HTTP响应码； 新版本产生多数错误、当前版本产生小数错误（图中显示10%的log） Canary实施 现在我们已经看到了一个相当简单的canary部署实现，接下来让我们更深入地了解成功的canary流程所需的参数。 最小化SLOs和错误预算的风险 第2章讨论了SLOs如何反映设计服务可用性的业务需求。这些需求也可以通过canary实现。canary进程的风险仅仅是我们错误预算的一小部分，它受到时间和canary规模大小的限制。 全局部署会很快将SLO置于危险之中。如果实例中为系统全面部署候选版本，我们将面临20%的请求失败的风险。如果我们使用5%的canary规模，我们将为5%的流量提供20%错误，导致1%的总体错误率(如图16-3所示)。这个策略允许我们保留我们的错误预算—预算的影响与暴露于缺陷的流量的数量成正比。我们可以假设，对于全局部署和灰度部署，检测和回滚花费的时间差不多，但是当我们将灰度发布集成到部署过程中时，我们会以更低的成本获得有关新版本的有价值信息。 这是一个假设负载均匀的极简模型。它还假设我们可以将整个错误预算用于灰度发布。这里我们只考虑新版本引入的不可用性，而不是实际可用性。我们的模型还假设新版本具有100%的失败率，这是最坏的情况。而进行灰度的部分不会导致线上系统100%不可用。我们还允许在灰度部署期间，整个系统的可用性低于SLO。 这个模型有明显的缺陷，但它是一个可靠的起点，你可以根据业务需求进行调整。我们建议使用最简单的模型来满足你的技术和业务目标。根据我们的经验，专注于使模型在技术上尽可能正确，常常会导致在建模上的过度投资。对于具有高复杂性的服务，过于复杂的模型可能导致持续的模型调优，而没有真正的好处。 选择灰度规模和持续时间 选择合适的灰度持续时间，需要考虑发布频率。 如果需要每天发布，那么在一次只运行一个灰度的情况下，无法使灰度保持一周,如果每周部署一次，就可以执行较长的灰度发布。 如果持续部署（例如，一天20次），灰度的持续时间必须明显缩短。 在一些说明里，虽然可以同时运行多个灰度，但这样做会增加大量精力来跟踪系统状态。 在任何情况下，需要快速推断系统状态时，同时运行多个灰度会成为问题。如果灰度重叠，同时运行多个灰度也会增加信号污染的风险。我们强烈建议一次只运行一个灰度。 对于基本的评估，不需要大规模的灰度来检测关键条件。然而，一个有代表性的灰度发布流程需要跨多个维度进行决策: 规模和持续时间 它的规模应够大，持续时间应够长，足以代表整个部署。仅在接收到少量查询后终止canary部署，对于以具有不同功能的不同查询为特征的系统来说，这无法提供有用的信号。处理率越高，获取代表性样本所需的时间就越少，以确保所观察到的行为实际上是由变更引起的，而不仅仅是随机因素。 流量 我们需要在系统上接收足够的流量，以确保它是一个具有代表性的示例，并且系统有机会对输入做出负面反应。通常，请求越均匀，所需要的流量就越少。 时间点 性能缺陷通常只在高负载下出现，因此在非高峰时间部署可能不会触发性能相关的缺陷。 度量指标 灰度的代表性与我们选择评估的指标密切相关(我们将在本章后面讨论)。我们可以快速评估诸如查询成功之类的琐碎指标，但是其他指标(如队列深度)可能需要更多的时间或较大规模的灰度来提供清晰的信号。 但问题是，这些要求可能相互冲突。Canarying是一种平衡行为，它通过对最坏情况的冷静分析和系统过去的实际记录来实现。一旦您从过去的灰度中收集了指标，您就可以根据典型的canary评估失败率而不是假想的最坏情况来选择canary参数。 选择和评估度量标准 到目前为止，我们一直在研究成功率，这是评估灰度发布的一个非常清晰和明显的指标。但是直觉上，我们知道这个单一的指标对于有意义的canary流程来说是不够的。如果我们以10倍的延迟为所有请求提供服务，或者在这样做时使用10倍的内存，那么我们可能也会遇到问题。并不是所有的指标都适合评估灰度发布。哪些指标最适合评估灰度发布版本是好是坏? 度量标准应指出问题 首先，指标需要能够指出服务中的问题。这很棘手，因为构成问题的并不总是客观的。我们可能会认为用户请求失败是有问题的。但是如果一个请求的响应时间增加了10%，或者系统内存增加了10%?，这该如何判断？我们通常建议使用sla作为开始考虑canary指标的地方。良好的服务质量指数往往与服务健康状况密切相关。如果已经使用SLIs来度量SLO是否符合，那么我们可以重用这些工作。 几乎任何指标在极端情况下都可能出现问题，但是向灰度流程中添加太多的指标也会产生成本。我们需要为每个指标正确定义可接受行为。如果可接受行为定义过于严格，我们会得到大量的误报;也就是说，我们会认为灰度很糟糕，即使实际不是这样。相反，如果对可接受行为的定义过于宽松，我们更有可能忽略掉有问题的灰度部署。正确选择什么是可接受的行为可能会成本较大——既耗时又需要分析。然而，如果做得不好，错误的结果会完全误导你。此外，随着服务、其特性集和行为的发展，您需要定期重新评估期望。 我们应该根据这些指标多大程度上能够表明系统中实际用户的体验来进行排名，选择排名靠前的几个指标(可能不超过12个)。太多的度量标准会带来递减的回报，并且在某种程度上，收益会被维护它们的成本所抵消，或者在发布过程中如果不维护它们，会对发布结果无法保证100%的信任。 为了使这个指导原则更加具体，让我们回头再来看示例。它有许多我们可以评估的指标:CPU使用量、内存占用、HTTP返回码(2xx、3xx等等)、响应延迟、正确性等等。在这种情况下，我们最好的度量标准可能是HTTP返回码和响应延迟，因为它们的降级最接近于实际用户影响。在这个场景中，CPU使用率并没有那么有用:资源使用的增加不一定会影响服务，并且可能导致不稳定或嘈杂的canary进程。这会导致操作人员禁用或忽略canary进程，这会首先破坏使用canary进程的目的。对于前端服务，我们直观地知道，响应较慢或响应失败通常会真实反映服务中存在的问题。 HTTP返回码包含一些有趣的复杂情况，例如状态码404，它告诉我们没有找到资源。这可能是因为用户获得了错误的URL(想象一下在一个流行的论坛上分享了一个错误的URL)，或者因为服务器错误地停止了对资源的服务。通常，我们可以通过排除canary评估中的400级状态码，并添加黑盒监控来测试特定URL的存在，从而解决此类问题。然后，我们可以将黑盒数据作为canary分析的一部分，以帮助将canary流程与奇怪的用户行为隔离开来。 度量标准应该具有代表性和可归属性 观察到的指标变化其来源，应该清楚地归因于正在进行的变更，并且不应该受到外部因素的影响。 在一个大的系统中(例如，许多服务器或许多容器)，我们可能会有外部性——超过连接的机器、运行具有不同性能特征的不同内核的机器，或者网络中过载的机器。此时金丝雀部分和主系统部分之间的差异，既是我们所部署的两个基础设施之间的差异，也会是我们变更导致的差异。 管理金丝雀是多种力量之间的平衡。增加金丝雀的规模是减少这个问题影响的方法(如前所述)。当我们的系统达到我们认为的合理的金丝雀规模时，我们需要考虑我们选择的指标是否会显示出很大的差异。 我们还应该知道canary和control环境之间共享的失败域;坏金丝雀会对控制产生负面影响，而系统中的坏行为可能会导致我们错误地评估金丝雀。同样，确保您的度量标准是良好隔离的。考虑一个同时运行我们的应用程序和其他进程的系统。整个系统的CPU使用量的急剧增加会导致糟糕的度量，因为系统中的其他进程(数据库负载、日志轮转等)可能会导致这种增加。更好的度量标准是在处理请求时所花费的CPU时间。更好的度量标准是在服务进程实际计划在CPU上的时间窗口上为处理请求服务所花费的CPU时间。虽然与我们的进程相关的严重超额的机器显然是一个问题(监控应该捕捉到它!)，但它不是由我们正在进行的更改引起的，因此不应该将其标记为金丝雀部署失败。 金丝雀也需要是可归属的;也就是说，您还应该能够将canary度量与SLIs联系起来。如果一个度量可以在不影响服务的情况下发生巨大变化，那么它不适合用来评估灰度发布。 评估前/评估后依然是有风险的 canary过程的前后是归因问题的延伸。在这个过程中，旧系统被新系统完全替代，你的canary评估将在一段时间内比较变更之前和之后的系统行为。你可以将此过程称为时空中的canary部署，在此过程中，您通过分割时间来选择A/B组，而不是通过机器、cookie或其他方法来分割总体。由于时间是观察到的指标变化的最大来源之一，因此很难在评估之前/之后来判断性能是否下降。 虽然canary部署可能导致降级，但原有系统本身也可能会降级。如果需要长时间运行canary部署，就会变得更加复杂。例如，如果在周一进行发布，可能会将工作日的行为与周末的行为进行比较，从而引入大量噪音。在该示例中，用户可能在周末以不同的方式访问该服务。从而在canary进程中引入噪音。 评估前/后过程本身引入了一个问题，即大而短的错误率(由前/后评估引入)是否优于小而长的错误率(由一个小金丝雀引入)。如果新版本完全被破坏，我们能多快地检测和恢复? 大规模的金丝雀之前/之后可以更快地检测到问题，但恢复的总体时间可能仍然相当长，与较小的金丝雀类似。在此期间，用户会一直受到影响。 使用渐进的灰度会更好 选择的度量标准即使不符合我们理想中的属性，但仍然很有价值。我们可以通过使用更细微的灰度过程来介绍这些指标。 我们可以使用包含多个阶段的canary来反映我们对度量的推理能力，而不是简单地评估单个canary阶段。在第一阶段，我们对这个版本没有信心或不了解。因此，我们希望使用一个小的阶段，以尽量减少负面影响。在小型灰度中，我们更喜欢能够最清晰地显示问题的指标——应用程序崩溃、请求失败等等。一旦这一阶段成功地过去，下一阶段将增加灰度规模，从而增强我们分析变化影响的信心。 依赖和隔离 正在测试的系统不会在完全真空中运行。出于实际原因，灰度和主系统可以共享后端、前端、网络、数据存储和其他基础设施。甚至可能与客户端有非常不明显的交互。例如，假设一个客户端发送了两个连续的请求。第一个请求可以由灰度部分来处理。其响应可能会改变第二个请求的内容，第二个请求可能会落在主系统部分，从而改变主系统的行为。 不完美的隔离会带来几个后果。最重要的是，我们需要知道，如果灰度过程的结果表明我们应该停止生产变更并调查情况，那么灰度并不一定是错误的。这一事实对于一般的canarying来说是正确的，但是在实践中，它经常由于隔离问题而导致被强制执行。 此外，不完美的隔离意味着灰度部署的错误行为也会对原始系统产生负面影响。Canarying是A/B比较，A和B有可能同时改变;这可能会导致评估灰度变得混乱。还必须使用绝对度量，例如定义的SLOs，以确保系统正确运行。 在非交互系统中进行Canarying 本章重点讨论了交互式请求/响应系统，它在许多方面是最简单和最常讨论的系统设计。其他系统，如异步处理管道，也同样重要，但有不同的canarying注意事项，我们将简要列举。有关数据处理管道的canarying的更多信息，请参见第13章。 首先，canary的持续时间和部署本质上依赖于工作单元处理的持续时间。当涉及到交互系统时，我们忽略了这个因素，假设工作单元处理的时间不会超过几秒钟，这比canary的持续时间要短。非交互式系统中的工作单元处理(如呈现管道或视频编码)可能需要更长的时间。因此，确保canary持续时间至少跨越单个工作单元的持续时间。 对于非交互式系统，隔离可能变得更加复杂。许多管道系统只有一个工作分配程序和一组使用应用程序代码的工作人员。在多阶段管道中，工作单元由工作人员处理，然后返回到池中，由同一工作人员或另一个工作人员执行下一阶段的处理。金丝雀分析有助于确保处理特定工作单元的工人总是从相同的工人池中提取——要么是金丝雀池，要么是控制池。否则，信号就会变得越来越混杂(有关理清信号的需要的更多信息，请参见349页的监视数据的要求)。 最后，度量标准的选择可能更加复杂。我们可能感兴趣的是端到端处理工作单元的时间(类似于交互系统中的延迟)，以及处理本身的质量(当然，这是完全特定于应用程序的)。 考虑到这些警告，canarying的一般概念仍然是可行的，并且适用相同的高级原则。 监控要求 在评估灰度部署时，您必须能够将部署了灰度的系统与未部署灰度的系统进行比较。通常，这需要在构造监视系统时多加注意—有效的比较非常简单，并且能够产生有意义的结果。 考虑之前的例子，在5%的规模中进行灰度，错误率为20%。因为监视很可能将系统作为一个整体来观察，所以它只能检测到1%的总体错误率。根据系统的不同，这个信号可能与其他错误源无法区分(参见图16-3)。 如果我们通过按照服务请求的对象来（金丝雀与主系统）分解指标，(参见图16-4)我们可以清楚地看到主系统与canary之间的错误率，这清楚地说明了全局部署将带来什么。在这里，我们看到，对整个服务的监控不足以分析灰度是否ok。在收集监视数据时，能够执行细粒度的分解非常重要，这些分解使得能够区分金丝雀和主系统的指标。 收集指标的另一个难点是金丝雀的部署受到设计的时间限制。当度量指标在特定时期内进行聚合时，这可能会导致问题。考虑每小时的度量误差。我们可以通过对过去一小时的请求求和来计算这个度量。如果我们使用这个度量来评估我们的canary，我们可能会遇到问题，如下面的时间表所述: 某些事件会导致一些错误发生。 一只金丝雀被部署在5%的人口中;金丝雀的持续时间是30分钟。 canary系统开始监视每小时的错误度量，以确定部署是好是坏。 部署被检测为错误，因为每小时的错误度量与控制总体的每小时的错误显著不同。 此场景是使用每小时计算一次的度量来评估仅30分钟长的部署的结果。因此，canary进程提供了一个非常模糊的信号。当使用度量来评估canary的成功时，确保度量的间隔与canary的持续时间相同或小于持续时间。 相关概念 通常，与客户的对话涉及到在生产中使用蓝/绿部署、人工负载生成和/或流量测试。这些概念类似于canarying，因此虽然它们不是严格意义上的金丝雀流程，但亦可使用。 蓝/绿部署 蓝/绿部署维护系统的两个实例：一个提供流量（绿色），另一个准备提供流量（蓝色）。 在蓝色环境中部署新版本后，将流量切换到其中。切换过程不需要停机，并且回滚只是简单逆转路由器而已。 一个缺点是该设置使用的资源是传统部署的两倍。在该设置中，您正在有效地执行前/后金丝雀（前面已讨论过）。 通过同时(而不是分开地)使用蓝/绿部署，您可以或多或少地将蓝色/绿色部署用作常规的金丝雀。在此策略中，您可以将canary部署到blue(备用)实例，并在绿色和蓝色环境之间缓慢地分配流量。您的评估和比较蓝色环境和绿色环境的指标都应该与流量控制相关。这种设置类似于A/B金丝雀，此时绿色环境是主系统，蓝色环境是金丝雀部署，金丝雀数量由发送到每个金丝雀的流量控制。 人工负载生成 与其将实时用户流量暴露给canary部署，还不如在安全性方面犯点错误，使用人工负载。通常，您可以在多个部署阶段(QA、预生产，甚至在生产中)运行负载测试。虽然根据我们的定义，这些操作不符合canarying，但是它们仍然是找到缺陷的可行方法，但需要注意一些事项。 使用人工负载进行测试可以很好地最大化代码覆盖率，但不能提供良好的状态覆盖率。在可变系统(具有缓存、cookie、请求关联等的系统)中人工模拟负载尤其困难。人工负载也可能无法准确地模拟真实系统中流量变化。有些问题可能只在无人工负载的情况下出现，从而导致覆盖率有所差距。 人工负载在可变系统中也很难工作。例如，试图在计费系统上生成人工负载可能非常危险:系统可能开始向信用卡供应商发送呼叫，然后信用卡供应商将开始主动向客户收费。虽然我们可以避免测试危险的代码逻辑，但是在这些逻辑上缺乏测试会降低我们的测试覆盖率。 流量测试 如果人工流量不具有代表性，我们可以复制流量并将其发送到生产系统和测试环境。这种技术被称为流量镜像。生产系统服务于实际流量并响应请求，canary部署服务于副本流量并丢弃响应。您甚至可以将canary响应与实际响应进行比较，并运行进一步的分析。 这种策略可以提供有代表性的流量，但通常比更直接的canary流程更复杂。在有状态系统中，流量测试也不能充分识别风险;流量副本可能会在看似独立的部署之间引入意外的影响。例如，如果canary部署和生产系统共享一个缓存，人为导致的缓存命中率增加将使canary指标的性能度量无效。 结论 您可以使用许多工具和方法来自动化版本发布，并将canarying引入到发布管道中。没有一种测试方法是万能的，测试策略应该由系统的需求和行为决定。Canarying可以作为一种简单、健壮且易于集成的方法来补充测试。当您及早发现系统缺陷时，用户受到的影响最小。Canarying还可以为频繁发布提供信心，并提高开发速度。正如测试方法必须随着系统需求和设计而发展一样，canarying也必须如此。 前言</summary></entry><entry><title type="html">第十四章 系统配置最佳实践</title><link href="http://localhost:4000/sre/2020/01/14/%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/" rel="alternate" type="text/html" title="第十四章 系统配置最佳实践" /><published>2020-01-14T00:00:00+08:00</published><updated>2020-01-14T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/14/%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/14/%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/">&lt;p&gt;系统配置是SRE经常要面对的问题。这是一项繁琐的工作，有时还会让人沮丧，特别是当工程师不熟悉系统，或是系统配置目标不够清晰时这种情况更为明显。系统配置常见的场景如下：在系统初始阶段的配置设计，出现事故时的紧急配置设计。&lt;/p&gt;

&lt;p&gt;本章基于经验和策略，以基础架构系统工程师的角度来进行系统配置，从而达到“安全和可持续”的目标。&lt;/p&gt;

&lt;h2 id=&quot;什么是配置&quot;&gt;什么是配置？&lt;/h2&gt;
&lt;p&gt;系统并不是永恒不变的。不断变化的业务需求、基础架构的需求和其他因素都会导致系统发生变化。当需要对系统快速迭代时（这个过程是昂贵和冗长的，不仅仅包含系统的重构、部署和代码的更改，系统配置的更新也是重要的一部分。由此，我们需要提供一种低开销的和人机界面的方式进行系统配置。SRE会利用此系统进行系统部署、性能调整和事件响应期间的配置变更。&lt;/p&gt;

&lt;p&gt;我们将系统分为三个关键组件：
. 应用程序
. 数据集
. 系统配置&lt;/p&gt;

&lt;p&gt;实际上，我们是无法将以上三个部分清楚的区分。例如，许多系统使用编程语言进行配置。同样，数据集可能包含代码，例如存储SQL的过程，这些代码已经构成了“应用程序”。&lt;/p&gt;

&lt;p&gt;而良好的配置界面可以完成快速、可信和可测试的配置更改。减少错误的发生，降低工程师的学习曲线。&lt;/p&gt;

&lt;h3 id=&quot;配置的可靠性&quot;&gt;配置的可靠性&lt;/h3&gt;

&lt;p&gt;系统最终由人来进行管理和配置。系统配置人机界面的质量会影响团队运行该系统的能力和可靠性。精心设计的配置界面对使用者的影响类似于代码质量对系统可维护性的影响。&lt;/p&gt;

&lt;p&gt;但在一些方面，配置往往与代码有不同的意义。通过代码更改系统功能是一个冗长且复杂的过程，涉及的范围往往是小增量的更改、代码审查和测试。相比之下，更改单个配置选项可能会对功能产生重大影响。例如，一个错误的防火墙配置规则可能会将自己锁到系统之外。而且，配置通常存在于未经测试（甚至是不可测试）的环境中。&lt;/p&gt;

&lt;p&gt;而且，系统配置更改可能需要工程师处于很大的压力下。在故障期间，可以简单安全地调整配置是必不可少的过程。举个航空的例子，早期，飞机的控制界面和指示灯混乱导致了许多安全事故。研究表明，无论飞行员技能或经验如何，操作故障都是频繁的。所以，可靠的配置是至关重要的。&lt;/p&gt;

&lt;h3 id=&quot;原理和机制&quot;&gt;原理和机制&lt;/h3&gt;

&lt;p&gt;在设计新软件或使用现有软件组装新系统时我们需要讨论配置：如何配置它？配置如何加载？我们将配置设为两部分：配置理论和配置机制。&lt;/p&gt;

&lt;p&gt;配置理论适用于完全独立于所选语言和其他机制的配置方面。我们对原理的讨论包括如何构造配置，如何实现抽象，以及如何无缝地支持不同的用例。&lt;/p&gt;

&lt;p&gt;我们对配置机制讨论涵盖了语言设计，部署策略以及与其他系统的交互等方面。本章重点关注机制，部分原因是语言选择已经在整个行业中进行了广泛的讨论。此外，一些特定的组织可能已经具有强大特色化的要求，例如预先存在的配置基础架构，因此配置机制不容易推广。Jsonnet的以下章节给出Jsonnet现有软件中配置机制的案例——特别是语言设计方面的。&lt;/p&gt;

&lt;p&gt;分别讨论理论和机制使我们能够更清楚地进行配置。实际上，如果配置需要大量难以理解的用户输入，那么配置语言（无论是XML还是Lua）等实现的细节也无关紧要。相反，如果必须将它们输入到非常麻烦的界面中，即使最简单的配置输入也会导致问题。比如旧的Linux内核配置的过程是——必须通过一系列命令进行配置设置每个参数。为了进行最简单的校正，用户必须从头开始配置过程。&lt;/p&gt;

&lt;h3 id=&quot;配置理论&quot;&gt;配置理论&lt;/h3&gt;
&lt;p&gt;本节讨论内容基于完整配置实现，因此下文一些观点是对所有配置实现的概况。&lt;/p&gt;

&lt;p&gt;在以下理论观点中，我们的理想配置是不需要任何配置。在理想场景下，新系统部署前，能根据部署信息、负载或其他配置自动生成正确配置。当然，这些实际都不太可能实现。需要指出的是，这个想法指出了配置的目标：避免复杂、多样的配置，向简单化努力。&lt;/p&gt;

&lt;p&gt;历史上，NASA的核心系统提供了大量的控制操作（相当于配置），这需要对操作者进行高强度的训练才能掌握。图14-1，展现了NASA的一名操作者，通过复杂的操作排列进行飞行器控制。在现代工业中，这种培训已不再可行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/14-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图14-1 NASA航天器控制中心的控制面板，说明了配置复杂性 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;尽管这些理想的配置可以减少对操作员的训练，但也让操作员对问题的深入理解降低。然而，随着系统复杂性增加，操作员对系统的认知理解却越来越重要。&lt;/p&gt;

&lt;p&gt;当我们把这些原则应用到Google的系统时，目标是让内部用户使用简单、全面和低成本。&lt;/p&gt;

&lt;h3 id=&quot;与用户交互的配置问题&quot;&gt;与用户交互的配置问题&lt;/h3&gt;

&lt;p&gt;无论你的配置是什么，如何配置，最终都体现在与计算机交互的一个界面，来询问用户接下来如何操作，让用户来选择。无论是XML编辑，还是GUI向导，这种模型都适用。&lt;/p&gt;

&lt;p&gt;在现代软件系统中，我们可以从两个不同的角度来看待这个模型：&lt;/p&gt;

&lt;h4 id=&quot;以基础设施为主&quot;&gt;以基础设施为主：&lt;/h4&gt;

&lt;p&gt;提供尽可能多的配置。这样做使用户能够根据他们的确切需求调整系统。配置越多越好，因为系统可以调整到完美。&lt;/p&gt;

&lt;h4 id=&quot;以用户为主&quot;&gt;以用户为主：&lt;/h4&gt;

&lt;p&gt;首先会询问用户一部分关于基础设施的问题，之后才能回归到实际业务上。这样的配置，越少越好，因为回答问题是很繁琐、麻烦的事情。&lt;/p&gt;

&lt;p&gt;在最初的理论模型中，我们推崇以用户为中心的理念。&lt;/p&gt;

&lt;p&gt;这样的软件设计脱离了实际。以用户为中心的配置，要求你的软件设计需要真正考虑到用户的需求，专注于用户，这就需要对用户的需求进行深入挖掘。相比而言，以基础设施为中心的配置，需要你为用户提供丰富的配置，来达到系统操作的目的。这些模型互相不冲突，但调试他们比较困难。在配置中，选项可枚举是比较好的，而不是设计出一个极其通用的软件，真正做到“开箱即用”。&lt;/p&gt;

&lt;p&gt;实际中，可以通过各种方式（后续章节会介绍）来删除一些配置，让系统的配置从以基础设施为主，向以用户为主进行转变。&lt;/p&gt;

&lt;h3 id=&quot;交互性问题更应接近用户目标&quot;&gt;交互性问题更应接近用户目标&lt;/h3&gt;

&lt;p&gt;当我们以用户为中心进行配置时，提出的交互式问题，需要确保用户能够准确理解。我们可以思考用户输入的本质：一方面，针对用户的每一项提问，要有更少的配置选项；另一方面，用户又想了解系统如何实现他们的需求，这就需要更多的选项。&lt;/p&gt;

&lt;p&gt;让我们以沏茶的过程来比喻如何配置的过程。在配置项比较少的情况下，用户可以要求“热绿茶”，并能基本满足用户需求。相反，配置项很多的情况下，用户可以要求：水量、水温、茶叶品牌、茶叶分量、浸泡时间、茶杯类型。为了获取接近完美的茶，而使用更多配置项，坚持这些细节所付出的成本和代价，可能太大。&lt;/p&gt;

&lt;p&gt;这个比喻对用户和配置系统开发人员都很有帮助。当用户确定操作步骤时，系统应按照他们要求进行。但是当用户只清楚自己目标时，我们的系统可以改进其中的配置步骤，最终实现用户目标即可。因此，预先了解用户目标是很有必要的。&lt;/p&gt;

&lt;p&gt;此理论在实际应用中发挥的价值，以任务调度系统来补充说明。假如你需要运行一个分析进程，通过Kubernetes或Mesos可以实现你的目标，而不需要你花费很长时间去配置细节参数，比如选择哪台物理机运行等。&lt;/p&gt;

&lt;h3 id=&quot;配置的必选项和可选项&quot;&gt;配置的必选项和可选项&lt;/h3&gt;

&lt;p&gt;配置分为两类：必选项和可选项。必选项的配置回答的问题针对核心功能。例如谁为一项手术收费。可选项一般不代表核心功能，但配置这些选项可以提高功能的质量——例如，设置一些工作进程。&lt;/p&gt;

&lt;p&gt;为了保持以用户为中心并确保易用性，你的系统应尽量减少必选配置的数量。这不是一件容易的事，但这很重要。虽然人们可能会争辩说，增加一个或两个小步骤只会增加很少的成本，但工程师的生活往往是一个无穷无尽的单独步骤链。这些小步骤的减少可以显着提高工作效率。&lt;/p&gt;

&lt;p&gt;最初的一组必选配置通常包括您在设计系统时考虑的问题。减少必选项的最简单方法是将它们转换为可选项，这意味着这些默认配置可以安全有效地应用于大多数（如果不是全部）用户。例如，我们可以简单地执行运行，而不是要求用户定义执行是否应该运行。&lt;/p&gt;

&lt;p&gt;虽然默认值通常是保存在代码中的静态值，但并非必须如此。它可以基于系统的其他属性动态确定。利用动态确定可以进一步简化配置。&lt;/p&gt;

&lt;p&gt;对于上下文，请参考以下动态默认示例。计算密集型系统通常可以通过配置控制使用多少计算线程。它的动态缺省配置了与系统（或容器）具有执行核数一样多的线程。在这种情况下，单个静态默认值没有用。动态默认值意味着我们不要求用户给定平台上部署的正确线程数。同样，单独部署在容器中的Java二进制文件可以根据容器中的可用内存自动调整其堆限制。这是两个常见的动态默认值示例。如果您需要限制资源使用，则对能够覆盖配置中的动态默认值非常有用。&lt;/p&gt;

&lt;p&gt;使用动态默认值可能不适用于所有人。随着时间的推移，用户可能更喜欢不同的方法并要求更好地控制动态默认值。如果很多用户反映动态配置有问题，一般来说这表示你的动态配置逻辑可能不再符合当前用户群的要求。需要考虑实施改进，使您的动态默认值无需额外的配置即可运行。如果只有一小部分用户不满意，他们最好手动设置配置选项。更复杂的系统会增加用户的工作量（例如，增加了文档的阅读难度）。&lt;/p&gt;

&lt;p&gt;在为可选项选择默认选项时，无论您选择静态还是动态默认值，请仔细考虑您选择的影响。经验表明，大多数用户会使用默认值，因此默认值的配置既是机会也是责任。你可以巧妙地向人们推进正确的方向，但指定错误的默认值会造成很大的伤害。例如，考虑配置默认值及其在计算机科学之外的影响。器官默认捐献的国家的器官捐献都比例明显高于器官默认不捐献的国家。简单地选择特定的默认值会对整个系统中的医疗选择产生深远的影响。&lt;/p&gt;

&lt;p&gt;一些可选项在没有明确用例的情况下开始。您可能想要完全删除这些问题。大量可选项可能会使用户感到困惑，因此您应该仅在真正需要的情况下添加配置选项。最后，如果您的配置恰好使用了继承的概念，那么能够恢复配置中任何可选项的默认值是很有用的。&lt;/p&gt;

&lt;h3 id=&quot;避免简单&quot;&gt;避免简单&lt;/h3&gt;

&lt;p&gt;到目前为止，我们已经讨论过将系统配置简化为最简单的形式。但是，配置系统也可能需要考虑高级用户。回到我们的茶类比，如果我们真的需要在特定时间内浸泡茶怎么办？&lt;/p&gt;

&lt;p&gt;适应高级用户的一种策略是找到常规用户和高级用户所需要的最低公分母，并将此复杂性作为默认值。缺点是这个决定会影响每个人; 即使是最简单的用例现在也需要以低级别的角度来考虑。&lt;/p&gt;

&lt;p&gt;通过根据默认行为的可选覆盖考虑配置，用户配置“绿茶”，然后添加“将茶浸泡五分钟。”在此模型中，默认配置仍然是高级别并且接近用户的目标，但用户可以微调低级别配置。这种方法并不新颖。我们可以用高级编程语言（如C ++或Java）做类比，程序员能够将代码中的机器（或VM）指令包含在以高级语言编写的代码中。在某些消费者软件中，我们看到具有高级选项的屏幕可以提供比典型视图更精细的控制。&lt;/p&gt;

&lt;p&gt;优化整个配置的总时间是有用的。不仅要考虑配置本身的行为，还要考虑用户在提供许多选项时可能遇到的决策困难，在纠正错误配置所需的时间，由于信心较低而导致的修改配置速度较慢等等。在考虑配置设计备选方案时，如果能够更轻松地支持最常见的用例，则可以选择以较少但难度较大的步骤完成复杂配置的选项。&lt;/p&gt;

&lt;p&gt;如果您发现超过一小部分用户需要复杂配置，则可能错误地识别了常见用例。如果是这样，请重新审视系统的初始产品假设并其他用户进行研究。&lt;/p&gt;

&lt;h2 id=&quot;配置机制&quot;&gt;配置机制&lt;/h2&gt;
&lt;p&gt;到目前为止，我们的讨论涵盖了配置哲学。本节将重点转移到用户如何与配置交互的机制。&lt;/p&gt;

&lt;h3 id=&quot;单独的配置和产生的数据&quot;&gt;单独的配置和产生的数据&lt;/h3&gt;

&lt;p&gt;存储配置的语言是一个不可避免的问题。您可以选择使用INI、YAML或XML文件中的纯数据。或者，配置可以存储在更高级语言中，以允许更灵活的配置。&lt;/p&gt;

&lt;p&gt;从根本上说，向用户提出的所有问题都可以归结为静态信息）。这显然对“应该使用多少线程”等问题的静态回答）“但是，即使”每一个请求都应该使用什么功能？“也只是功能的静态引用。&lt;/p&gt;

&lt;p&gt;要回到配置是代码还是数据这个古老的问题，我们的经验表明，代码和数据都有，但将两者分离是最优的。系统基础结构应该对纯静态数据进行操作，这些数据可以是协议缓冲区、YAML或JSON等格式。这种选择并不意味着用户需要与纯数据进行实际交互。用户可以与生成此数据的高级接口进行交互。然而，这种数据格式可以被API使用，从而允许系统和自动化的进一步堆叠。&lt;/p&gt;

&lt;p&gt;这个高级接口几乎可以是任何东西。它可以是一种高级语言，如基于Python的域特定语言（DSL）、Lua或专用构建语言，如Jsonnet（我们将在第15章中详细讨论）。我们可以把这样的接口看作一个编译，类似于我们如何对待C++代码。高级接口也可能根本不是语言其配置由web UI消化。&lt;/p&gt;

&lt;p&gt;从有意与静态数据表示分离的配置UI开始，意味着系统具有部署的灵活性。不同的组织可能具有不同的文化规范或产品需求（例如使用公司内的特定语言或需要将配置外部化到最终用户），并且这种通用的系统可以适用于支持不同的配置需求。毫不费力地支持多种语言。参见图14-2。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/14-2.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图14-2。配置流具有单独的配置界面和配置数据基础结构。请注意，Web UI通常还会显示当前配置，从而使关系成为双向关系。 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;这种分离对于用户来说是完全不可见的。用户的共同路径可能是在配置语言中编辑文件，而所有其他的事情都发生在幕后。例如，一旦用户向系统提交更改，新存储的配置就自动编译成原始数据&lt;/p&gt;

&lt;p&gt;静态配置数据一旦获得，也可以在数据分析中使用。例如，如果生成的配置数据是JSON格式的，那么可以将其加载到PostgreSQL中，并用数据库查询进行分析。作为基础架构所有者，您可以快速且容易地查询正在使用哪些配置参数以及由谁使用这些配置参数。此查询对于识别您可以删除的特性或测量bugy选项的影响非常有用。&lt;/p&gt;

&lt;p&gt;当使用最终的配置数据时，您会发现存储有关配置如何被摄取的元数据也很有用。例如，如果您知道数据来自Jsonnet中的配置文件，或者您在将数据编译成数据之前已经拥有了原始数据的完整路径，那么您可以跟踪配置作者。&lt;/p&gt;

&lt;p&gt;配置语言是静态数据也是可以接受的。例如，您的基础结构和接口都可能使用普通JSON。但是，要避免接口使用的数据格式和内部使用的数据格式之间的紧密耦合。例如，您可以在内部使用包含配置所消耗的数据结构的数据结构。内部数据结构还可能包含完全特定于实现的数据，这些数据永远不需要在系统外部出现。&lt;/p&gt;

&lt;h3 id=&quot;工具的重要性&quot;&gt;工具的重要性&lt;/h3&gt;

&lt;p&gt;工具可以区分混乱的噩梦和可持续的可伸缩的系统，但在设计配置系统时经常会被忽略。本节讨论优化配置系统应该有的关键工具。&lt;/p&gt;

&lt;h4 id=&quot;语义验证&quot;&gt;语义验证&lt;/h4&gt;

&lt;p&gt;虽然大多数语言提供现成的语法验证，但不要忽视语义验证。即使您的配置在语法上有效，它是否可能做有用的事情？或者用户是否引用了一个不存在的目录（由于打字错误），或者需要比实际数据库多一千倍的RAM（因为单位不是用户所期望的）？&lt;/p&gt;

&lt;p&gt;尽可能验证配置在语义上是否有意义，有助于防止中断并降低运营成本。对于每个可能的错误配置，我们应该问自己，在用户提交配置时是否可以阻止它，而不是在提交更改之后。&lt;/p&gt;

&lt;h4 id=&quot;配置语法&quot;&gt;配置语法&lt;/h4&gt;

&lt;p&gt;虽然能够确保配置满足用户需求，但消除机械障碍也很重要。从语法角度来看，配置语言应该具备一下特点：&lt;/p&gt;

&lt;h5 id=&quot;在编辑器中高亮显示语法在公司内使用&quot;&gt;在编辑器中高亮显示语法（在公司内使用）&lt;/h5&gt;

&lt;p&gt;通常，您已经通过重用现有语言解决了这个问题。但是，特定语言可能具有额外的语法方式，从特定的方面突出显示 。&lt;/p&gt;

&lt;h5 id=&quot;短绒&quot;&gt;短绒&lt;/h5&gt;

&lt;p&gt;使用linter来识别语言使用中的常见不一致。 Pylint是一种流行的语言示例。&lt;/p&gt;

&lt;h5 id=&quot;自动语法格式化&quot;&gt;自动语法格式化&lt;/h5&gt;

&lt;p&gt;内置标准化可最大限度地减少关于格式化的讨论，并在贡献者切换项目时减少认知负荷。标准格式化还可以实现更轻松的自动编辑，这在大型组织中很有用。现有语言中的autoformatters示例包括clang-format和autopep8。&lt;/p&gt;

&lt;p&gt;这些工具使用户能够轻松地编写和编辑配置并确保其语法正确。在面向空白的配置中进行正确的缩进会产生很大的优势。&lt;/p&gt;

&lt;h4 id=&quot;权限和变更跟踪&quot;&gt;权限和变更跟踪&lt;/h4&gt;

&lt;p&gt;由于配置可能会影响公司和机构的关键系统，因此确保良好的用户隔离以及了解系统中发生的变化非常重要。如第10章所述，有效的死后文化可以避免责怪个人。但是，它既可以在事件期间进行，也可以在进行事后调查时知道谁更改了配置，并了解配置更改如何影响系统。无论事故是由于事故还是恶意行为都是如此。&lt;/p&gt;

&lt;p&gt;系统的每个配置代码段都应具有明确的所有者。例如，如果使用配置文件，则其目录可能由单个生产组拥有。如果目录中的文件只能有一个所有者，则跟踪谁进行更改会更容易。&lt;/p&gt;

&lt;p&gt;版本控制配置，无论其执行方式如何，都允许您及时返回以查看在任何给定时间点配置的内容。将配置文件检入版本控制系统（如Subversion或Git）是当今常见的做法，但这种做法对于Web UI或远程API提取的配置同样重要。您可能还希望在配置和正在配置的软件之间实现更紧密的耦合。通过这样做，您可以避免无意中配置软件中尚不可用或不再支持的功能。&lt;/p&gt;

&lt;p&gt;在相关的说明中，将配置和生成的应用程序的更改记录到系统是有用的（有时是必需的）。提交新版本配置的简单操作并不总是意味着直接应用配置（稍后将详细介绍）。如果在事件响应期间怀疑系统配置更改是罪魁祸首，则能够快速确定进行更改的完整配置编辑集非常有用。这样可以实现可靠的回滚，并能够通知其配置受到影响的各方。&lt;/p&gt;

&lt;h4 id=&quot;安全配置变更申请&quot;&gt;安全配置变更申请&lt;/h4&gt;

&lt;p&gt;如前所述，配置是对系统功能进行大量更改的简单方法，但它通常不经过单元测试甚至不易测试。由于我们希望避免可靠性事件，因此我们应该检查配置更改的安全应用是什么。&lt;/p&gt;

&lt;p&gt;要使配置更改安全，它必须具有三个主要属性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;逐步部署，避免全有或全无变化的能力&lt;/li&gt;
  &lt;li&gt;如果证明存在风险，则可以回滚更改&lt;/li&gt;
  &lt;li&gt;如果更改导致失控，则自动回滚（或至少是停止进度的能力）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;部署新配置时，避免全局一次性推送非常重要。相反，逐步推出新配置 - 这样做可以让您在导致100％中断之前检测问题并中止有问题的推送。这就是Kubernetes等工具使用滚动更新策略更新软件或配置而不是一次更新每个集群的原因之一。（有关相关讨论，请参阅第16章。）&lt;/p&gt;

&lt;p&gt;回滚能力对于减少故障持续时间非常重要。回滚有问题的配置可以比临时修复程序更快地缓解故障-对修补程序改进的内在信心肯定较低。&lt;/p&gt;

&lt;p&gt;为了能够向前滚动和回滚配置，它必须是密封的。需要可在其密封环境之外进行更改的外部资源的配置可能很难回滚。例如，存储在引用网络文件系统上的数据的版本控制系统中的配置不是密封的。&lt;/p&gt;

&lt;p&gt;最后但同样重要的是，在处理可能导致操作员控制突然丧失的变更时系统应特别小心。在桌面系统上，如果用户未确认更改，屏幕分辨率更改通常会提示倒计时并重置。这是因为不正确的显示器设置可能会阻止用户恢复更改。同样，系统管理员通常会意外地将自己防火墙移出他们当前正在设置的系统。&lt;/p&gt;

&lt;p&gt;这些原则并非配置所独有，适用于更改已部署系统的其他方法，例如升级二进制文件或推送新数据集。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;p&gt;琐碎的配置更改可能会以极大的方式影响生产系统，因此我们需要刻意设计配置以减轻这些风险。配置设计包含API和UI设计的各个方面，应该是有目的的，而不仅仅是系统实现的副作用。将配置分为哲学和机制有助于我们在设计内部系统时获得清晰度，并使我们能够正确地进行讨论。&lt;/p&gt;

&lt;p&gt;应用这些建议需要时间和勤勉。有关我们如何在实践中应用这些原则的示例，请参阅有关Canary Analysis Service的ACM Queue文章。在设计这个实用的内部系统时，我们花了大约一个月的时间来尝试减少强制性问题并为可选问题找到合适的答案。我们的努力创造了一个简单的配置系统。因为它易于使用，所以它在内部被广泛采用。我们已经看不到用户支持的需求 - 因为用户可以轻松了解系统，他们可以放心地进行更改。当然，我们还没有完全消除错误配置和用户支持，我们也没有想到。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">系统配置是SRE经常要面对的问题。这是一项繁琐的工作，有时还会让人沮丧，特别是当工程师不熟悉系统，或是系统配置目标不够清晰时这种情况更为明显。系统配置常见的场景如下：在系统初始阶段的配置设计，出现事故时的紧急配置设计。 本章基于经验和策略，以基础架构系统工程师的角度来进行系统配置，从而达到“安全和可持续”的目标。 什么是配置？ 系统并不是永恒不变的。不断变化的业务需求、基础架构的需求和其他因素都会导致系统发生变化。当需要对系统快速迭代时（这个过程是昂贵和冗长的，不仅仅包含系统的重构、部署和代码的更改，系统配置的更新也是重要的一部分。由此，我们需要提供一种低开销的和人机界面的方式进行系统配置。SRE会利用此系统进行系统部署、性能调整和事件响应期间的配置变更。 我们将系统分为三个关键组件： . 应用程序 . 数据集 . 系统配置 实际上，我们是无法将以上三个部分清楚的区分。例如，许多系统使用编程语言进行配置。同样，数据集可能包含代码，例如存储SQL的过程，这些代码已经构成了“应用程序”。 而良好的配置界面可以完成快速、可信和可测试的配置更改。减少错误的发生，降低工程师的学习曲线。 配置的可靠性 系统最终由人来进行管理和配置。系统配置人机界面的质量会影响团队运行该系统的能力和可靠性。精心设计的配置界面对使用者的影响类似于代码质量对系统可维护性的影响。 但在一些方面，配置往往与代码有不同的意义。通过代码更改系统功能是一个冗长且复杂的过程，涉及的范围往往是小增量的更改、代码审查和测试。相比之下，更改单个配置选项可能会对功能产生重大影响。例如，一个错误的防火墙配置规则可能会将自己锁到系统之外。而且，配置通常存在于未经测试（甚至是不可测试）的环境中。 而且，系统配置更改可能需要工程师处于很大的压力下。在故障期间，可以简单安全地调整配置是必不可少的过程。举个航空的例子，早期，飞机的控制界面和指示灯混乱导致了许多安全事故。研究表明，无论飞行员技能或经验如何，操作故障都是频繁的。所以，可靠的配置是至关重要的。 原理和机制 在设计新软件或使用现有软件组装新系统时我们需要讨论配置：如何配置它？配置如何加载？我们将配置设为两部分：配置理论和配置机制。 配置理论适用于完全独立于所选语言和其他机制的配置方面。我们对原理的讨论包括如何构造配置，如何实现抽象，以及如何无缝地支持不同的用例。 我们对配置机制讨论涵盖了语言设计，部署策略以及与其他系统的交互等方面。本章重点关注机制，部分原因是语言选择已经在整个行业中进行了广泛的讨论。此外，一些特定的组织可能已经具有强大特色化的要求，例如预先存在的配置基础架构，因此配置机制不容易推广。Jsonnet的以下章节给出Jsonnet现有软件中配置机制的案例——特别是语言设计方面的。 分别讨论理论和机制使我们能够更清楚地进行配置。实际上，如果配置需要大量难以理解的用户输入，那么配置语言（无论是XML还是Lua）等实现的细节也无关紧要。相反，如果必须将它们输入到非常麻烦的界面中，即使最简单的配置输入也会导致问题。比如旧的Linux内核配置的过程是——必须通过一系列命令进行配置设置每个参数。为了进行最简单的校正，用户必须从头开始配置过程。 配置理论 本节讨论内容基于完整配置实现，因此下文一些观点是对所有配置实现的概况。 在以下理论观点中，我们的理想配置是不需要任何配置。在理想场景下，新系统部署前，能根据部署信息、负载或其他配置自动生成正确配置。当然，这些实际都不太可能实现。需要指出的是，这个想法指出了配置的目标：避免复杂、多样的配置，向简单化努力。 历史上，NASA的核心系统提供了大量的控制操作（相当于配置），这需要对操作者进行高强度的训练才能掌握。图14-1，展现了NASA的一名操作者，通过复杂的操作排列进行飞行器控制。在现代工业中，这种培训已不再可行。 图14-1 NASA航天器控制中心的控制面板，说明了配置复杂性 尽管这些理想的配置可以减少对操作员的训练，但也让操作员对问题的深入理解降低。然而，随着系统复杂性增加，操作员对系统的认知理解却越来越重要。 当我们把这些原则应用到Google的系统时，目标是让内部用户使用简单、全面和低成本。 与用户交互的配置问题 无论你的配置是什么，如何配置，最终都体现在与计算机交互的一个界面，来询问用户接下来如何操作，让用户来选择。无论是XML编辑，还是GUI向导，这种模型都适用。 在现代软件系统中，我们可以从两个不同的角度来看待这个模型： 以基础设施为主： 提供尽可能多的配置。这样做使用户能够根据他们的确切需求调整系统。配置越多越好，因为系统可以调整到完美。 以用户为主： 首先会询问用户一部分关于基础设施的问题，之后才能回归到实际业务上。这样的配置，越少越好，因为回答问题是很繁琐、麻烦的事情。 在最初的理论模型中，我们推崇以用户为中心的理念。 这样的软件设计脱离了实际。以用户为中心的配置，要求你的软件设计需要真正考虑到用户的需求，专注于用户，这就需要对用户的需求进行深入挖掘。相比而言，以基础设施为中心的配置，需要你为用户提供丰富的配置，来达到系统操作的目的。这些模型互相不冲突，但调试他们比较困难。在配置中，选项可枚举是比较好的，而不是设计出一个极其通用的软件，真正做到“开箱即用”。 实际中，可以通过各种方式（后续章节会介绍）来删除一些配置，让系统的配置从以基础设施为主，向以用户为主进行转变。 交互性问题更应接近用户目标 当我们以用户为中心进行配置时，提出的交互式问题，需要确保用户能够准确理解。我们可以思考用户输入的本质：一方面，针对用户的每一项提问，要有更少的配置选项；另一方面，用户又想了解系统如何实现他们的需求，这就需要更多的选项。 让我们以沏茶的过程来比喻如何配置的过程。在配置项比较少的情况下，用户可以要求“热绿茶”，并能基本满足用户需求。相反，配置项很多的情况下，用户可以要求：水量、水温、茶叶品牌、茶叶分量、浸泡时间、茶杯类型。为了获取接近完美的茶，而使用更多配置项，坚持这些细节所付出的成本和代价，可能太大。 这个比喻对用户和配置系统开发人员都很有帮助。当用户确定操作步骤时，系统应按照他们要求进行。但是当用户只清楚自己目标时，我们的系统可以改进其中的配置步骤，最终实现用户目标即可。因此，预先了解用户目标是很有必要的。 此理论在实际应用中发挥的价值，以任务调度系统来补充说明。假如你需要运行一个分析进程，通过Kubernetes或Mesos可以实现你的目标，而不需要你花费很长时间去配置细节参数，比如选择哪台物理机运行等。 配置的必选项和可选项 配置分为两类：必选项和可选项。必选项的配置回答的问题针对核心功能。例如谁为一项手术收费。可选项一般不代表核心功能，但配置这些选项可以提高功能的质量——例如，设置一些工作进程。 为了保持以用户为中心并确保易用性，你的系统应尽量减少必选配置的数量。这不是一件容易的事，但这很重要。虽然人们可能会争辩说，增加一个或两个小步骤只会增加很少的成本，但工程师的生活往往是一个无穷无尽的单独步骤链。这些小步骤的减少可以显着提高工作效率。 最初的一组必选配置通常包括您在设计系统时考虑的问题。减少必选项的最简单方法是将它们转换为可选项，这意味着这些默认配置可以安全有效地应用于大多数（如果不是全部）用户。例如，我们可以简单地执行运行，而不是要求用户定义执行是否应该运行。 虽然默认值通常是保存在代码中的静态值，但并非必须如此。它可以基于系统的其他属性动态确定。利用动态确定可以进一步简化配置。 对于上下文，请参考以下动态默认示例。计算密集型系统通常可以通过配置控制使用多少计算线程。它的动态缺省配置了与系统（或容器）具有执行核数一样多的线程。在这种情况下，单个静态默认值没有用。动态默认值意味着我们不要求用户给定平台上部署的正确线程数。同样，单独部署在容器中的Java二进制文件可以根据容器中的可用内存自动调整其堆限制。这是两个常见的动态默认值示例。如果您需要限制资源使用，则对能够覆盖配置中的动态默认值非常有用。 使用动态默认值可能不适用于所有人。随着时间的推移，用户可能更喜欢不同的方法并要求更好地控制动态默认值。如果很多用户反映动态配置有问题，一般来说这表示你的动态配置逻辑可能不再符合当前用户群的要求。需要考虑实施改进，使您的动态默认值无需额外的配置即可运行。如果只有一小部分用户不满意，他们最好手动设置配置选项。更复杂的系统会增加用户的工作量（例如，增加了文档的阅读难度）。 在为可选项选择默认选项时，无论您选择静态还是动态默认值，请仔细考虑您选择的影响。经验表明，大多数用户会使用默认值，因此默认值的配置既是机会也是责任。你可以巧妙地向人们推进正确的方向，但指定错误的默认值会造成很大的伤害。例如，考虑配置默认值及其在计算机科学之外的影响。器官默认捐献的国家的器官捐献都比例明显高于器官默认不捐献的国家。简单地选择特定的默认值会对整个系统中的医疗选择产生深远的影响。 一些可选项在没有明确用例的情况下开始。您可能想要完全删除这些问题。大量可选项可能会使用户感到困惑，因此您应该仅在真正需要的情况下添加配置选项。最后，如果您的配置恰好使用了继承的概念，那么能够恢复配置中任何可选项的默认值是很有用的。 避免简单 到目前为止，我们已经讨论过将系统配置简化为最简单的形式。但是，配置系统也可能需要考虑高级用户。回到我们的茶类比，如果我们真的需要在特定时间内浸泡茶怎么办？ 适应高级用户的一种策略是找到常规用户和高级用户所需要的最低公分母，并将此复杂性作为默认值。缺点是这个决定会影响每个人; 即使是最简单的用例现在也需要以低级别的角度来考虑。 通过根据默认行为的可选覆盖考虑配置，用户配置“绿茶”，然后添加“将茶浸泡五分钟。”在此模型中，默认配置仍然是高级别并且接近用户的目标，但用户可以微调低级别配置。这种方法并不新颖。我们可以用高级编程语言（如C ++或Java）做类比，程序员能够将代码中的机器（或VM）指令包含在以高级语言编写的代码中。在某些消费者软件中，我们看到具有高级选项的屏幕可以提供比典型视图更精细的控制。 优化整个配置的总时间是有用的。不仅要考虑配置本身的行为，还要考虑用户在提供许多选项时可能遇到的决策困难，在纠正错误配置所需的时间，由于信心较低而导致的修改配置速度较慢等等。在考虑配置设计备选方案时，如果能够更轻松地支持最常见的用例，则可以选择以较少但难度较大的步骤完成复杂配置的选项。 如果您发现超过一小部分用户需要复杂配置，则可能错误地识别了常见用例。如果是这样，请重新审视系统的初始产品假设并其他用户进行研究。 配置机制 到目前为止，我们的讨论涵盖了配置哲学。本节将重点转移到用户如何与配置交互的机制。 单独的配置和产生的数据 存储配置的语言是一个不可避免的问题。您可以选择使用INI、YAML或XML文件中的纯数据。或者，配置可以存储在更高级语言中，以允许更灵活的配置。 从根本上说，向用户提出的所有问题都可以归结为静态信息）。这显然对“应该使用多少线程”等问题的静态回答）“但是，即使”每一个请求都应该使用什么功能？“也只是功能的静态引用。 要回到配置是代码还是数据这个古老的问题，我们的经验表明，代码和数据都有，但将两者分离是最优的。系统基础结构应该对纯静态数据进行操作，这些数据可以是协议缓冲区、YAML或JSON等格式。这种选择并不意味着用户需要与纯数据进行实际交互。用户可以与生成此数据的高级接口进行交互。然而，这种数据格式可以被API使用，从而允许系统和自动化的进一步堆叠。 这个高级接口几乎可以是任何东西。它可以是一种高级语言，如基于Python的域特定语言（DSL）、Lua或专用构建语言，如Jsonnet（我们将在第15章中详细讨论）。我们可以把这样的接口看作一个编译，类似于我们如何对待C++代码。高级接口也可能根本不是语言其配置由web UI消化。 从有意与静态数据表示分离的配置UI开始，意味着系统具有部署的灵活性。不同的组织可能具有不同的文化规范或产品需求（例如使用公司内的特定语言或需要将配置外部化到最终用户），并且这种通用的系统可以适用于支持不同的配置需求。毫不费力地支持多种语言。参见图14-2。 图14-2。配置流具有单独的配置界面和配置数据基础结构。请注意，Web UI通常还会显示当前配置，从而使关系成为双向关系。 这种分离对于用户来说是完全不可见的。用户的共同路径可能是在配置语言中编辑文件，而所有其他的事情都发生在幕后。例如，一旦用户向系统提交更改，新存储的配置就自动编译成原始数据 静态配置数据一旦获得，也可以在数据分析中使用。例如，如果生成的配置数据是JSON格式的，那么可以将其加载到PostgreSQL中，并用数据库查询进行分析。作为基础架构所有者，您可以快速且容易地查询正在使用哪些配置参数以及由谁使用这些配置参数。此查询对于识别您可以删除的特性或测量bugy选项的影响非常有用。 当使用最终的配置数据时，您会发现存储有关配置如何被摄取的元数据也很有用。例如，如果您知道数据来自Jsonnet中的配置文件，或者您在将数据编译成数据之前已经拥有了原始数据的完整路径，那么您可以跟踪配置作者。 配置语言是静态数据也是可以接受的。例如，您的基础结构和接口都可能使用普通JSON。但是，要避免接口使用的数据格式和内部使用的数据格式之间的紧密耦合。例如，您可以在内部使用包含配置所消耗的数据结构的数据结构。内部数据结构还可能包含完全特定于实现的数据，这些数据永远不需要在系统外部出现。 工具的重要性 工具可以区分混乱的噩梦和可持续的可伸缩的系统，但在设计配置系统时经常会被忽略。本节讨论优化配置系统应该有的关键工具。 语义验证 虽然大多数语言提供现成的语法验证，但不要忽视语义验证。即使您的配置在语法上有效，它是否可能做有用的事情？或者用户是否引用了一个不存在的目录（由于打字错误），或者需要比实际数据库多一千倍的RAM（因为单位不是用户所期望的）？ 尽可能验证配置在语义上是否有意义，有助于防止中断并降低运营成本。对于每个可能的错误配置，我们应该问自己，在用户提交配置时是否可以阻止它，而不是在提交更改之后。 配置语法 虽然能够确保配置满足用户需求，但消除机械障碍也很重要。从语法角度来看，配置语言应该具备一下特点： 在编辑器中高亮显示语法（在公司内使用） 通常，您已经通过重用现有语言解决了这个问题。但是，特定语言可能具有额外的语法方式，从特定的方面突出显示 。 短绒 使用linter来识别语言使用中的常见不一致。 Pylint是一种流行的语言示例。 自动语法格式化 内置标准化可最大限度地减少关于格式化的讨论，并在贡献者切换项目时减少认知负荷。标准格式化还可以实现更轻松的自动编辑，这在大型组织中很有用。现有语言中的autoformatters示例包括clang-format和autopep8。 这些工具使用户能够轻松地编写和编辑配置并确保其语法正确。在面向空白的配置中进行正确的缩进会产生很大的优势。 权限和变更跟踪 由于配置可能会影响公司和机构的关键系统，因此确保良好的用户隔离以及了解系统中发生的变化非常重要。如第10章所述，有效的死后文化可以避免责怪个人。但是，它既可以在事件期间进行，也可以在进行事后调查时知道谁更改了配置，并了解配置更改如何影响系统。无论事故是由于事故还是恶意行为都是如此。 系统的每个配置代码段都应具有明确的所有者。例如，如果使用配置文件，则其目录可能由单个生产组拥有。如果目录中的文件只能有一个所有者，则跟踪谁进行更改会更容易。 版本控制配置，无论其执行方式如何，都允许您及时返回以查看在任何给定时间点配置的内容。将配置文件检入版本控制系统（如Subversion或Git）是当今常见的做法，但这种做法对于Web UI或远程API提取的配置同样重要。您可能还希望在配置和正在配置的软件之间实现更紧密的耦合。通过这样做，您可以避免无意中配置软件中尚不可用或不再支持的功能。 在相关的说明中，将配置和生成的应用程序的更改记录到系统是有用的（有时是必需的）。提交新版本配置的简单操作并不总是意味着直接应用配置（稍后将详细介绍）。如果在事件响应期间怀疑系统配置更改是罪魁祸首，则能够快速确定进行更改的完整配置编辑集非常有用。这样可以实现可靠的回滚，并能够通知其配置受到影响的各方。 安全配置变更申请 如前所述，配置是对系统功能进行大量更改的简单方法，但它通常不经过单元测试甚至不易测试。由于我们希望避免可靠性事件，因此我们应该检查配置更改的安全应用是什么。 要使配置更改安全，它必须具有三个主要属性： 逐步部署，避免全有或全无变化的能力 如果证明存在风险，则可以回滚更改 如果更改导致失控，则自动回滚（或至少是停止进度的能力） 部署新配置时，避免全局一次性推送非常重要。相反，逐步推出新配置 - 这样做可以让您在导致100％中断之前检测问题并中止有问题的推送。这就是Kubernetes等工具使用滚动更新策略更新软件或配置而不是一次更新每个集群的原因之一。（有关相关讨论，请参阅第16章。） 回滚能力对于减少故障持续时间非常重要。回滚有问题的配置可以比临时修复程序更快地缓解故障-对修补程序改进的内在信心肯定较低。 为了能够向前滚动和回滚配置，它必须是密封的。需要可在其密封环境之外进行更改的外部资源的配置可能很难回滚。例如，存储在引用网络文件系统上的数据的版本控制系统中的配置不是密封的。 最后但同样重要的是，在处理可能导致操作员控制突然丧失的变更时系统应特别小心。在桌面系统上，如果用户未确认更改，屏幕分辨率更改通常会提示倒计时并重置。这是因为不正确的显示器设置可能会阻止用户恢复更改。同样，系统管理员通常会意外地将自己防火墙移出他们当前正在设置的系统。 这些原则并非配置所独有，适用于更改已部署系统的其他方法，例如升级二进制文件或推送新数据集。 结论 琐碎的配置更改可能会以极大的方式影响生产系统，因此我们需要刻意设计配置以减轻这些风险。配置设计包含API和UI设计的各个方面，应该是有目的的，而不仅仅是系统实现的副作用。将配置分为哲学和机制有助于我们在设计内部系统时获得清晰度，并使我们能够正确地进行讨论。 应用这些建议需要时间和勤勉。有关我们如何在实践中应用这些原则的示例，请参阅有关Canary Analysis Service的ACM Queue文章。在设计这个实用的内部系统时，我们花了大约一个月的时间来尝试减少强制性问题并为可选问题找到合适的答案。我们的努力创造了一个简单的配置系统。因为它易于使用，所以它在内部被广泛采用。我们已经看不到用户支持的需求 - 因为用户可以轻松了解系统，他们可以放心地进行更改。当然，我们还没有完全消除错误配置和用户支持，我们也没有想到。</summary></entry><entry><title type="html">第十二章 非抽象大系统介绍设计</title><link href="http://localhost:4000/sre/2020/01/12/%E9%9D%9E%E6%8A%BD%E8%B1%A1%E5%A4%A7%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%BB%8B%E7%BB%8D/" rel="alternate" type="text/html" title="第十二章 非抽象大系统介绍设计" /><published>2020-01-12T00:00:00+08:00</published><updated>2020-01-12T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/12/%E9%9D%9E%E6%8A%BD%E8%B1%A1%E5%A4%A7%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%BB%8B%E7%BB%8D</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/12/%E9%9D%9E%E6%8A%BD%E8%B1%A1%E5%A4%A7%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%BB%8B%E7%BB%8D/">&lt;p&gt;由于负责生产运营和产品工程，SRE工程师处于使业务用例需求和操作成本保持一致的独特地位。产品工程团队可能不知道他们设计的系统的维护成本，特别是当产品团队正在构建一个单独的组件从而产生更大的生态系统时这种可能性更会增加。&lt;/p&gt;

&lt;p&gt;基于谷歌开发系统的经验，我们认为可靠性是任何生产系统的最关键特性。我们发现延迟可靠性设计过程中的问题类似于以更高的成本换取更少的特性。通过遵循作为系统设计和实现的迭代风格，我们得到了健壮的易于升级的低运营成本的系统设计。我们称之为非抽象大系统设计(NALSD)。&lt;/p&gt;

&lt;h2 id=&quot;什么是nasld&quot;&gt;什么是NASLD？&lt;/h2&gt;

&lt;p&gt;这一章介绍了NALSD方法:我们从问题陈述开始，收集需求，而且使迭代设计变得越来越复杂直到我们找到一个可行的解决方案。最终，我们针对多种失效模式得出了一个稳定的系统设计方法，满足我们在迭代时的初始要求和附加条件。&lt;/p&gt;

&lt;p&gt;NALSD描述了SRE的关键技能：评估、设计和评估大型系统的能力。实际上，NALSD将容量规划、组件隔离和优雅的系统降级等元素组合在一起，这些元素对生产系统的高可用性至关重要。谷歌SRE被希望能够从系统的基本白板图开始资源规划，考虑各种扩展和故障领域，并将其设计集中到资源的具体建议中。因为这些系统会随着时间的推移而变化，所以SRE能够分析和评估系统设计的关键方面是至关重要的。&lt;/p&gt;

&lt;h2 id=&quot;为什么非抽象&quot;&gt;为什么非抽象&lt;/h2&gt;

&lt;p&gt;所有系统最终都必须在使用真实网络的真实数据中心的真实计算机上运行。谷歌已经认识到(艰难的)设计分布式系统的人员需要开发并不断地锻炼将白板设计转换为过程中多个步骤的具体资源估计的能力。如果没有这种严格性，创建在现实世界中不能完全转换的系统就太有诱惑力了。&lt;/p&gt;

&lt;p&gt;这种额外的前期工作通常会减少最后一刻的系统设计更改，以应对一些不可预见的物理约束。&lt;/p&gt;

&lt;p&gt;请注意，当我们将这些练习推进到离散结果(例如，机器的数量)时，声音推理和假设的例子比任何最终值都重要。早期的假设对计算结果有很大的影响，而做出完美的假设并不是NALSD的必要条件。这个练习的价值在于将许多不完美但合理的结果结合在一起，从而更好地理解设计。&lt;/p&gt;

&lt;h2 id=&quot;adwords的例子&quot;&gt;AdWords的例子&lt;/h2&gt;

&lt;p&gt;谷歌AdWords服务在谷歌Web搜索中显示文本广告。点击率(CTR)指标告诉广告商他们的广告表现有多好。CTR是点击广告的次数与显示广告的次数之比。&lt;/p&gt;

&lt;p&gt;这个AdWords示例旨在设计一个能够为每个AdWords ad测量和报告准确的CTR的系统。我们需要计算CTR的数据记录在搜索和ad服务系统的日志中。这些日志分别记录每个搜索查询显示的广告和点击的广告。&lt;/p&gt;

&lt;h3 id=&quot;设计过程&quot;&gt;设计过程&lt;/h3&gt;

&lt;p&gt;谷歌使用迭代的方法来设计满足我们目标的系统。每次迭代都定义一个潜在的设计，并检查它的优缺点。这种分析要么提供给下一个迭代，要么指出什么时候设计足够好以至于被推荐。&lt;/p&gt;

&lt;p&gt;总的来说，NALSD过程有两个阶段，每个阶段有两到三个问题。&lt;/p&gt;

&lt;p&gt;在基本设计阶段，我们试图设计出一种符合原则的设计。我们问两个问题:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;这个是可能的吗？&lt;/code&gt;&lt;br /&gt;
       这个设计可能吗?如果我们不需要担心足够的内存、CPU、网络带宽等等，我们会设计什么来满足需求呢?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;我们可以做的更好吗？&lt;/code&gt;&lt;br /&gt;
       对于这样的设计，我们会问:“我们能做得更好吗?”例如，我们能让这个系统有意义地更快、更小、更高效吗?如果设计在O(N)时间内解决了这个问题，我们能否更快地解决这个问题，比如O(ln(N))?&lt;/p&gt;

&lt;p&gt;在下一个阶段中，我们尝试扩展我们的基本设计—例如，通过显著地增加需求。我们问三个问题:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;它是可行的吗？&lt;/code&gt;&lt;br /&gt;
       在资金、硬件等方面受到限制的情况下，是否有可能扩展这种设计?如果有必要，什么样的分布式设计才能满足需求?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;它是弹性的吗？&lt;/code&gt; 
       设计能优雅地失效吗?当这个组件失效时会发生什么?当整个数据中心失效时，系统如何工作?&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;我们如何做的更好？&lt;/code&gt;&lt;br /&gt;
      虽然我们通常以大致的顺序来讨论这些阶段和问题，但在实践中，我们在问题和阶段之间来回切换。例如，在基本的设计阶段，我们经常在我们的头脑中有成长和扩展。有了这些概念，让我们来回顾一下NALSD迭代过程。&lt;/p&gt;

&lt;h3 id=&quot;初始需求&quot;&gt;初始需求&lt;/h3&gt;

&lt;p&gt;每个广告客户可能有多个广告。每个广告都由ad_id键入，并与广告商选择的搜索词列表相关联。&lt;/p&gt;

&lt;p&gt;当向广告客户展示仪表盘时，我们需要知道以下每个信息：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这个搜索词触发这个广告的频率是多少&lt;/li&gt;
  &lt;li&gt;看到广告的人点击了多少次&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有了这些信息，我们可以计算CTR:点击次数除以查看次数。&lt;/p&gt;

&lt;p&gt;我们知道广告商关心两件事:仪表盘能否快速展示并且数据是最新的。因此，在迭代设计时，我们将根据SLOs来考虑我们的需求(详见第2章)：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;99.9%的仪表板查询在&amp;lt; 1秒内完成&lt;/li&gt;
  &lt;li&gt;99.9%的时间，显示的CTR数据少于5分钟。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些SLOs提供了一个合理的目标，我们应该能够始终如一地满足。&lt;/p&gt;

&lt;p&gt;他们还提供了错误预算(参见站点可靠性工程中的第4章)，我们将在每次设计迭代中对我们的解决方案进行比较。&lt;/p&gt;

&lt;p&gt;我们的目标是创建一个能够满足我们的SLOs的系统，并支持数以百万计希望在仪表盘上看到他们点击率的广告客户。对于事务率，我们认为每秒有500,000个搜索查询和10,000个广告点击。&lt;/p&gt;

&lt;h3 id=&quot;一台机器时&quot;&gt;一台机器时&lt;/h3&gt;

&lt;p&gt;最简单的起点是考虑在一台计算机上运行整个应用程序。&lt;/p&gt;

&lt;p&gt;对每个搜索查询，我们记录如下信息：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;time：
       查询发生的时间
query_id:
       唯一的查询标识符（查询ID）
search_term:
       查询的内容
ad_id:
       搜索显示的所有AdWords广告的id
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这些信息一起形成查询日志。用户每次点击广告时，我们都会记录的时间、查询ID和单击日志中的ad_id。&lt;/p&gt;

&lt;p&gt;你可能想知道为什么我们不简单地将search_term添加到单击日志中以减少复杂性。在我们提供的例子任意缩减的范围内，这可能是可行的。然而，在实践中，CTR实际上只是从这些日志中计算出的许多结论之一。单击日志来自url, url具有固有的大小限制，这使分离查询日志成为一个更可扩展的解决方案。我们不会通过在练习中添加额外的ctr-like需求来证明这一点，而是简单地承认这个假设并继续前进。&lt;/p&gt;

&lt;p&gt;仪表盘展示需要来自两个日志的数据。我们需要保证，我们可以实现我们所展示SLO数据的更新时间在一秒钟以内。要实现这种SLO，需要在系统处理大量单击和查询时保持计算CTR的速度不变。&lt;/p&gt;

&lt;p&gt;为了满足在一秒钟内显示仪表盘所要求SLO数据，我们需要快速查找每个给定ad_id的search_term中被单击和显示的query_id的数量。我们可以从查询日志中提取每个search_term和ad_id所显示的query_id的详细信息。CTR仪表盘需要来自ad_ids的查询日志和单击日志的所有记录。&lt;/p&gt;

&lt;p&gt;如果我们有更多的广告商，扫描查询日志和单击日志来生成仪表盘的效率会非常低。因此，我们的设计要求我们的一台机器创建一个合适的数据结构，以便在接收日志时能够进行快速的CTR计算。在一台机器上，使用具有query_id和search_term索引的SQL数据库应该可以在一秒钟内提供答案。通过在query_id上连接这些日志并按search_term分组，我们可以报告每个搜索的CTR。&lt;/p&gt;

&lt;h4 id=&quot;计算&quot;&gt;计算&lt;/h4&gt;

&lt;p&gt;我们需要计算需要多少资源来解析所有日志。为了确定缩放限制，我们需要做一些假设，从查询日志的大小开始：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;time：
      64位整数，8个字节
query_id：
      64位整数，8字节
ad_id：
      3个64位整数，8字节
search_term：
      一个长度为500字节的字符串
其它元数据：
   500 - 1000字节的信息，比如哪台机器提供广告，搜索用的哪种语言，以及搜索词返回的结果
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;为了确保我们不会过早地达到一个阈值，我们将每个查询日志条目整理为2kb。单击日志卷应该比查询日志卷小得多:因为平均CTR为2%(10,000次单击/ 500,000次查询)，所以单击日志将拥有2%与查询日志一样多的记录。请记住，我们选择了大数字来说明这些原则可以实现任意大的扩展。这些估计似乎很大，因为它们本应该如此。&lt;/p&gt;

&lt;p&gt;最后，我们可以使用科学的表示法来限制由单位上的不一致引起的算术错误。24小时内生成的查询日志量为：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    (5 × 105 queries/sec) × (8.64 × 104 seconds/day) × (2 × 103 bytes) = 86.4 TB/day
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;因为我们收到的点击量是查询的2%，而且我们知道数据库索引将增加一些合理的开销，所以我们可以将86.4 TB/天的存储空间增加到100 TB，以存储一天的日志数据。 由于总存储需求约为100 TB，我们需要作出一些新的假设。 这种设计是否仍适用于单台机器？ 虽然有可能 将100 TB的磁盘连接到一台机器上，但是我们可能会受到机器从磁盘读取和写入磁盘的能力限制。&lt;/p&gt;

&lt;p&gt;例如，常见的4 TB HDD可能能够支持每秒200次输入/输出操作（IOPS）。 如果每个日志条目可以存储并以每个日志条目平均一个磁盘写入为索引，我们会看到IOPS是我们查询日志的限制因素：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    (5 × 105 queries/sec) / (200 IOPS/disk) = 2.5 × 103 disks or 2,500 disks
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;即使我们可以以10：1的比例批量查询以限制操作，但在最佳情况下，我们需要数百个HDD。 考虑到查询日志写入只是设计IO要求的一个组成部分，我们需要使用比传统HDD更好地处理高IOPS的解决方案。&lt;/p&gt;

&lt;p&gt;为简单起见，我们将直接评估RAM并跳过评估其它存储介质，例如固态磁盘（SSD）。单台机器无法完全在RAM中处理100TB的占用空间：假设我们的标准机器占用空间为16核，64 GB RAM和1 Gbps网络吞吐量，我们需要：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    (100 TB) / (64 GB RAM/machine) = 1,563 machines
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;评估&quot;&gt;评估&lt;/h4&gt;

&lt;p&gt;暂时忽略我们的计算并想象我们可以将这个设计放在一台机器上，我们真的想要这样实现吗？ 如果我们通过询问当该组件发生故障时会发生什么来测试我们的设计，我们会找出一长串单点故障列表（例如，CPU，内存，存储，电源，网络，冷却）。 如果其中一个组件出现故障，我们是否可以合理地支持我们的SLO？ 几乎可以肯定的是，即使是一个简单的电力循环也会对我们的用户产生极大影响。&lt;/p&gt;

&lt;p&gt;回到我们的计算，我们的单机设计看起来是不可行，但这一步并不浪费时间。我们已经发现了有关如何推理系统约束及其初始要求的有价值的信息。我们需要将设计发展为使用多台机器。&lt;/p&gt;

&lt;h3 id=&quot;分步式系统&quot;&gt;分步式系统&lt;/h3&gt;

&lt;p&gt;我们需要的search_terms位于查询日志中，ad_ids位于单击日志。 既然我们知
道我们需要多台机器，那么什么是连接它们的最佳设计？&lt;/p&gt;

&lt;h4 id=&quot;mapreduce&quot;&gt;MapReduce&lt;/h4&gt;

&lt;p&gt;我们可以使用MapReduce处理和连接日志。 我们可以定期获取累积的查询日志和单击日志，MapReduce将生成由ad_id组织的数据集，显示每个search_term接收到的单击次数。&lt;/p&gt;

&lt;p&gt;MapReduce作为批处理器工作：它的输入是一个大型数据集，它可以使用许多机器通过进程处理该数据并产生结果。一旦所有机器处理完他们的数据，他们的输出就可以合并 - MapReduce可以直接为每个AdWords广告和搜索字词创建每个CRT的摘要。我们可以使用此数据创建我们需要的dashboard。&lt;/p&gt;

&lt;h4 id=&quot;评价&quot;&gt;评价&lt;/h4&gt;

&lt;p&gt;MapReduce是一种广泛使用的计算模型，我们相信它可以横向扩展。 无论我们的查询日志和单击日志输入有多大，添加更多的机器总是能够在不耗尽磁盘空间或RAM的情况下成功完成流程。&lt;/p&gt;

&lt;p&gt;不幸的是，这种类型的批处理过程无法在收到日志的5分钟内满足我们的加入日志可用性的SLO。 要在5分钟内提供结果，我们需要小批量运行MapReduce作业 —— 每次只需几分钟的日志。&lt;/p&gt;

&lt;p&gt;批次的任意和非重叠性质使小批量不切实际。 如果已记录的查询在批处理1中，并且其单击位于批处理2中，则单击和查询将永远不会被连接。 虽然MapReduce可以很好地处理独立批次，但它并未针对此类问题进行优化。 此时，我们可以尝试使用MapReduce找出可能的解决方法。 然而，为简单起见，我们将继续研究另一种解决方案。&lt;/p&gt;

&lt;h4 id=&quot;logjoniner&quot;&gt;LogJoniner&lt;/h4&gt;

&lt;p&gt;用户点击的广告数量明显小于所投放广告的数量。 直观地说，我们需要专注于扩展两者中的较大者：查询日志。 我们通过引入新的分布式系统组件来实现此目的。&lt;/p&gt;

&lt;p&gt;而不是像我们的MapReduce设计那样小批量查找query_id，如果我们创建了一个所有查询的存储，我们可以按需查询query_id，该怎么办？ 我们称之为QueryStore。 它包含查询日志的完整内容，query_id为主键。为了避免重复，我们假设我们从单个机器设计的计算将应用于QueryStore，我们将QueryStore的审查限制为我们已经涵盖的内容。 有关这样的组件如何工作的深入讨论，我们建议您阅读有关Bigtable的内容。&lt;/p&gt;

&lt;p&gt;因为点击日志也有query_id，所以我们的处理循环的规模比现在要小得多：它只需要遍历点击日志并引入所引用的特定查询。 我们将此组件称为LogJoiner。&lt;/p&gt;

&lt;p&gt;如果我们没有找到点击查询（接收查询日志的速度可能会减慢），我们将其搁置一段时间并重试，直到时间限制。 如果我们在该时间限制内找不到查询，我们会放弃该点击日志。&lt;/p&gt;

&lt;p&gt;点击率仪表盘对每个ad_id和search_term需要两个组件：展示次数和点击的广告数量。 ClickMap需要合作伙伴来保存查询，由ad_id组织。 我们称之为QueryMap。 QueryMap直接从查询日志中提供所有数据，并通过ad_id索引条目。&lt;/p&gt;

&lt;p&gt;图12-1描述了数据如何流经系统。&lt;/p&gt;

&lt;p&gt;LogJoiner设计引入了几个新组件：LogJoiner，QueryStore，ClickMap和QueryMap。 我们需要确保这些组件可以扩展。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/something/images/SRE/12-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图12-1 基本的LogJoiner设计; 处理并存储点击数据，以便仪表板可以检索它 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;计算&lt;/p&gt;

&lt;p&gt;根据我们在之前的迭代中执行的计算，我们知道QueryStore将为一天的日志提供大约100 TB的数据。 我们可以删除太旧而不具有价值的数据。&lt;/p&gt;

&lt;p&gt;LogJoiner应该在进入时处理点击并检索相应的点击从QueryStore的日志。&lt;/p&gt;

&lt;p&gt;QueryStore查找会产生额外的网络开销。 对于每个单击日志记录，我们查
找query_id并返回完整的日志记录：&lt;/p&gt;

&lt;p&gt;LogJoiner还会将结果发送到ClickMap。 我们需要存储query_id，ad_id和
time。 search_term、time和query_id都是64位整数，因此数据将小于1 KB：&lt;/p&gt;

&lt;p&gt;总计约400 Mbps是我们机器可管理的数据传输速率。&lt;/p&gt;

&lt;p&gt;ClickMap必须为每次单击存储时间和query_id，但不需要任何其他元数据。我们将忽略ad_id和search_term，因为它们是一个小的线性因素（例如，广告商数量×广告数量×8字节）。 即使是拥有10个广告的1000万广告客户也只有约800 MB。 一天的ClickMap值是：&lt;/p&gt;

&lt;p&gt;我们将ClickMap最多为20 GB /天，以计算任何开销和我们的ad_ids。&lt;/p&gt;

&lt;p&gt;在我们填写QueryMap时，我们需要为显示的每个广告存储query_id。我们需要增加存储空间，因为每个搜索查询可能会点击三个ad_id，因此我们需要记录query_id 最多三个条目：&lt;/p&gt;

&lt;p&gt;2 TB足够小，可以使用硬盘驱动器托管在一台机器上，但我们从单机迭代中知
道，单个小写操作太频繁将无法存储在硬盘驱动器上。 虽然我们可以计算使用
更高IOPS驱动器（例如SSD）的影响，但我们的工作重点是证明系统可以扩展
到任意大的尺寸。在这种情况下，我们需要围绕单个机器的IO限制进行设计。
 因此，缩放设计的下一步是对输入和输出进行分片：将传入的查询日志和单击
日志分成多个流。&lt;/p&gt;

&lt;p&gt;Sharded LogJoiner&lt;/p&gt;

&lt;p&gt;我们在此迭代中的目标是运行多个LogJoiner实例，每个实例位于数据的不同分片上。 为此，我们需要考虑几个因素：&lt;/p&gt;

&lt;p&gt;数据管理&lt;/p&gt;

&lt;p&gt;为了加入查询日志和单击日志，我们必须将每个单击日志记录与query_id上的相应查询日志记录进行匹配。 该设计应该防止网络和磁盘吞吐量在我们扩展时限制我们的设计。&lt;/p&gt;

&lt;p&gt;可靠性&lt;/p&gt;

&lt;p&gt;我们知道机器可能随时出现故障。 当一台机器运行LogJoiner时故障了，我们如何确保我们不会失去正在进行的工作？&lt;/p&gt;

&lt;p&gt;效率&lt;/p&gt;

&lt;p&gt;我们可以做扩容而没有浪费吗？ 我们需要使用最少的资源满足我们的数据管理和可靠性需求。&lt;/p&gt;

&lt;p&gt;我们的LogJoiner设计表明我们可以加入查询日志和点击日志，但产生的数据量非常大。 如果我们将工作划分为基于query_id的分片，我们可以并行运行多个LogJoiner。&lt;/p&gt;

&lt;p&gt;提供了合理数量的LogJoiner实例，如果我们均匀地分发日志，则每个实例仅通过网络接收一小撮信息。 随着点击流量的增加，我们通过添加更多LogJoiner实例来水平扩展，而不是通过使用更多的CPU和RAM来垂直扩展。&lt;/p&gt;

&lt;p&gt;如图12-2所示，为了使LogJoiners收到正确的消息，我们引入了一个名为日志分片器的组件，它将每个日志条目定向到正确的目的地。 对于每条记录，我们的点击日志分片器执行以下操作：&lt;/p&gt;

&lt;p&gt;哈希记录的query_id。
用N（分片数）模数结果并加1以得到介于1和N之间的数字。
在第2步中将记录发送到分片编号。&lt;/p&gt;

&lt;p&gt;图12-2 分片应该如何工作&lt;/p&gt;

&lt;p&gt;现在，每个LogJoiner将获得分解的传入日志的一致子集 query_id，而不是完整的点击日志。&lt;/p&gt;

&lt;p&gt;QueryMap也需要进行分片。 我们知道需要很多硬盘来支持QueryMap所需的IOPS，并且一天的QueryMap（2TB）的大小对于我们的64 GB机器来说太大而无法存储在RAM中。 但是，我们将在ad_id上进行分片，而不是像LogJoiner那样使用query_id进行分片。 ad_id在任何读取或写入之前都是已知的，因此使用与LogJoiner和CTR仪表盘相同的散列方法将提供一致的数据视图。&lt;/p&gt;

&lt;p&gt;为了保持实现的一致性，我们可以将ClickMap的相同日志分片设计重用为QueryMap，因为ClickMap比QueryMap小。&lt;/p&gt;

&lt;p&gt;现在我们知道我们的系统将扩展，我们可以继续解决系统的可靠性问题。 我们的设计必须能够适应LogJoiner故障。 如果LogJoiner在收到日志消息后但在加入日志消息之前失败，则必须重做其所有工作。 这会延迟准确数据到达仪表板，这将影响我们的SLO。&lt;/p&gt;

&lt;p&gt;如果我们的日志分片器进程将重复的日志条目发送到两个分片，则系统可以继续全速执行并处理得到准确的结果，即使LogJoiner失败（可能是因为它所在的机器失败）。&lt;/p&gt;

&lt;p&gt;通过以这种方式复制工作进程，我们减少（但不消除）丢失这些连接日志的机会。 两个分片可能会同时中断并丢失中继日志。 通过分配工作负载以确保在同一台计算机上没有重复的分片，我们可以减轻大部分风险。 如果两台机器同时发生故障并且我们丢失了分片的两个副本，则系统的错误预算（参见第一本SRE手册中的第4章）可以涵盖剩余的风险。 当灾难发生时，我们可以重新处理日志。 仪表板将仅显示短时间内超过5分钟的数据。&lt;/p&gt;

&lt;p&gt;图12-3显示了我们对分片及其副本的设计，其中LogJoiner、ClickMap和QueryMap都构建在两个分片上。&lt;/p&gt;

&lt;p&gt;从连接的日志中，我们可以在每个LogJoiner机器上构建ClickMap。 要显示我们的用户仪表盘，需要组合和查询所有ClickMaps。&lt;/p&gt;

&lt;p&gt;结论&lt;/p&gt;

&lt;p&gt;在一个数据中心中托管分片组件会产生单点故障：如果刚好不幸一对机器或数据中心断开连接，我们将丢失所有ClickMap工作，并且用户仪表盘完全停止工作！我们需要改进我们的设计以使用多个数据中心。&lt;/p&gt;

&lt;p&gt;图12-3。 使用相同的query_id对日志进行分片以复制分片&lt;/p&gt;

&lt;p&gt;多数据中心&lt;/p&gt;

&lt;p&gt;跨不同地理位置的数据中心复制数据使我们的服务基础架构能够承受灾难性故障。如果一个数据中心关闭（例如，由于电源或网络中断），我们可以故障转移到另一个数据中心。要使故障转移起作用，必须在部署系统的所有数据中心中提供ClickMap数据。&lt;/p&gt;

&lt;p&gt;这样的ClickMap甚至可能存在吗？我们不希望将计算需求乘以数据中心的数量，但我们如何才能有效地同步站点之间的工作以确保充分复制而不会产生不必要的重复？&lt;/p&gt;

&lt;p&gt;我们刚刚描述了分布式系统工程中众所周知的共识问题的一个例子。有许多复杂的算法可以解决这个问题，但基本思路是：&lt;/p&gt;

&lt;p&gt;1.一个服务要有三个或五个副本（如ClickMap）。
2.用像Paxos等一致性算法，以确保在发生数据中心级别故障时我们可以可靠地存储计算状态。
3.在参与节点之间实现至少一个网络往返时间以接受写入操作。此要求限制了系统的顺序吞吐量。基于一致性map仍然需要并行写操作。&lt;/p&gt;

&lt;p&gt;按照刚刚列出的步骤，多数据中心设计现在看来原则上是可行的。它还能在实践中发挥作用吗？我们需要哪些类型的资源，以及我们需要多少资源？&lt;/p&gt;

&lt;p&gt;计算&lt;/p&gt;

&lt;p&gt;使用故障隔离数据中心执行Paxos算法的延迟意味着每个操作大约需要25毫秒才能完成。该延迟的假设基于至少相距几百公里的数据中心。因此，就顺序过程而言，我们每25毫秒只能执行一次操作或每秒40次操作。如果我们需要每秒执行10次连续进程（点击日志），我们需要为每个数据中心至少250个进程（由ad_id分片）用于Paxos操作。在实践中，我们希望添加更多进程以提高并行性，以便能处理任何宕机或流量高峰所造成的积压。&lt;/p&gt;

&lt;p&gt;基于我们之前对ClickMap和QueryMap的计算，并使用每秒40次连续操作来估算，我们的多数据中心设计需要多少台新机器？&lt;/p&gt;

&lt;p&gt;因为我们的分片LogJoiner设计为每条日志记录引入了一个副本，所以我们每秒创建ClickMap和QueryMap的事务数量翻了一番：每秒20,000次点击和每秒1,000,000次查询。&lt;/p&gt;

&lt;p&gt;我们可以通过把每秒总查询数除以每秒最大操作数来计算所需的最小进程数或任务数 ：&lt;/p&gt;

&lt;p&gt;每个任务的内存量（2 TB QueryMap的两个副本）：&lt;/p&gt;

&lt;p&gt;每台机器的任务数：&lt;/p&gt;

&lt;p&gt;我们知道我们可以在一台机器上安装许多任务，但我们需要确保我们不会达到
IO的瓶颈。 ClickMap和QueryMap的总网络吞吐量（使用每个条目2 KB的高
估计）：&lt;/p&gt;

&lt;p&gt;每项任务的吞吐量：&lt;/p&gt;

&lt;p&gt;每台机器的吞吐量：&lt;/p&gt;

&lt;p&gt;对于每个任务15MB内存和640Kbps吞吐量的组合是易于管理的。我们在每个数据中心需要大约4 TB的RAM来托管分片的ClickMap和QueryMap。 如果我们每台机器有64 GB的RAM，我们可以只使用64台机器处理数据，这将会只使用每台机器25％的网络带宽。&lt;/p&gt;

&lt;p&gt;评估&lt;/p&gt;

&lt;p&gt;现在我们已经设计了一个多数据中心的系统，让我们来看看数据流是否有意义。&lt;/p&gt;

&lt;p&gt;图12-4显示了整个系统设计。 您可以查看每个搜索查询和广告点击是如何与服务器进行通信的，以及如何收集日志并将其推送到每个组件中。&lt;/p&gt;

&lt;p&gt;我们可以根据我们的需求检查这个系统：&lt;/p&gt;

&lt;p&gt;每秒10,000次广告点击
       LogJoiner可以水平扩展以处理所有日志点击，并将结果存储在ClickMap中。&lt;/p&gt;

&lt;p&gt;每秒500,000次搜索查询
      QueryStore和QueryMap被设计为能够以此速率存储一整天的数据。&lt;/p&gt;

&lt;p&gt;99.9％的仪表盘查询在&amp;lt;1秒内完成&lt;/p&gt;

&lt;p&gt;CTR仪表板从QueryMap和ClickMap获取数据，这些数据由ad_id键入，使此事务变得快速而简单。&lt;/p&gt;

&lt;p&gt;99.9％的时间，显示的CTR数据不到5分钟&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   每个组件都设计为水平扩展，这意味着如果管道处理太慢，添加更多计算机将减少端到端管道延迟。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;我们相信该系统架构可以扩展以满足我们对吞吐量，性能和可靠性的要求。&lt;/p&gt;

&lt;p&gt;图 12-4 多数据中心设计&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;NALSD描述了Google用于生产系统的系统设计的迭代过程。 通过将软件分解为逻辑组件并将这些组件放入具有可靠基础架构的生产生态系统中，我们得出了对数据一致性、系统可用性和资源效率都能达到合适目标的系统。NALSD的实践使我们能够在不重复每次迭代的情况下改进设计。虽然本章中提出的各种设计迭代都满足了我们原始的问题陈述，但每次迭代都揭示了新的要求，我们可以通过扩展我们以前的工作来满足这些要求。&lt;/p&gt;

&lt;p&gt;在整个过程中，我们根据我们对系统增长的预期来分离软件组件。 该策略允许我们独立地扩展系统的不同部分，并消除对单个硬件或单个软件实例的依赖性，从而产生更可靠的系统。&lt;/p&gt;

&lt;p&gt;在整个设计过程中，我们通过对NALSD的四个关键问题进行提问来持续改进每次的迭代：&lt;/p&gt;

&lt;p&gt;可能吗？
       我们可以在没有“魔法”的情况下建造它&lt;/p&gt;

&lt;p&gt;我们可以做得更好吗？
      它是否像我们可以合理地制造一样简单？&lt;/p&gt;

&lt;p&gt;这可行吗？
       它是否符合我们的实际限制（预算，时间等）？&lt;/p&gt;

&lt;p&gt;它有弹性伸缩的吗？
       它会偶尔存在不可避免的中断吗？&lt;/p&gt;

&lt;p&gt;NALSD是一项有学问的技能。与任何技能一样，您需要定期练习以保持熟练程度。 谷歌的经验表明，从抽象需求推理到具体的资源是建立健康和长寿命系统的关键。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">由于负责生产运营和产品工程，SRE工程师处于使业务用例需求和操作成本保持一致的独特地位。产品工程团队可能不知道他们设计的系统的维护成本，特别是当产品团队正在构建一个单独的组件从而产生更大的生态系统时这种可能性更会增加。 基于谷歌开发系统的经验，我们认为可靠性是任何生产系统的最关键特性。我们发现延迟可靠性设计过程中的问题类似于以更高的成本换取更少的特性。通过遵循作为系统设计和实现的迭代风格，我们得到了健壮的易于升级的低运营成本的系统设计。我们称之为非抽象大系统设计(NALSD)。 什么是NASLD？ 这一章介绍了NALSD方法:我们从问题陈述开始，收集需求，而且使迭代设计变得越来越复杂直到我们找到一个可行的解决方案。最终，我们针对多种失效模式得出了一个稳定的系统设计方法，满足我们在迭代时的初始要求和附加条件。 NALSD描述了SRE的关键技能：评估、设计和评估大型系统的能力。实际上，NALSD将容量规划、组件隔离和优雅的系统降级等元素组合在一起，这些元素对生产系统的高可用性至关重要。谷歌SRE被希望能够从系统的基本白板图开始资源规划，考虑各种扩展和故障领域，并将其设计集中到资源的具体建议中。因为这些系统会随着时间的推移而变化，所以SRE能够分析和评估系统设计的关键方面是至关重要的。 为什么非抽象 所有系统最终都必须在使用真实网络的真实数据中心的真实计算机上运行。谷歌已经认识到(艰难的)设计分布式系统的人员需要开发并不断地锻炼将白板设计转换为过程中多个步骤的具体资源估计的能力。如果没有这种严格性，创建在现实世界中不能完全转换的系统就太有诱惑力了。 这种额外的前期工作通常会减少最后一刻的系统设计更改，以应对一些不可预见的物理约束。 请注意，当我们将这些练习推进到离散结果(例如，机器的数量)时，声音推理和假设的例子比任何最终值都重要。早期的假设对计算结果有很大的影响，而做出完美的假设并不是NALSD的必要条件。这个练习的价值在于将许多不完美但合理的结果结合在一起，从而更好地理解设计。 AdWords的例子 谷歌AdWords服务在谷歌Web搜索中显示文本广告。点击率(CTR)指标告诉广告商他们的广告表现有多好。CTR是点击广告的次数与显示广告的次数之比。 这个AdWords示例旨在设计一个能够为每个AdWords ad测量和报告准确的CTR的系统。我们需要计算CTR的数据记录在搜索和ad服务系统的日志中。这些日志分别记录每个搜索查询显示的广告和点击的广告。 设计过程 谷歌使用迭代的方法来设计满足我们目标的系统。每次迭代都定义一个潜在的设计，并检查它的优缺点。这种分析要么提供给下一个迭代，要么指出什么时候设计足够好以至于被推荐。 总的来说，NALSD过程有两个阶段，每个阶段有两到三个问题。 在基本设计阶段，我们试图设计出一种符合原则的设计。我们问两个问题: 这个是可能的吗？ 这个设计可能吗?如果我们不需要担心足够的内存、CPU、网络带宽等等，我们会设计什么来满足需求呢? 我们可以做的更好吗？ 对于这样的设计，我们会问:“我们能做得更好吗?”例如，我们能让这个系统有意义地更快、更小、更高效吗?如果设计在O(N)时间内解决了这个问题，我们能否更快地解决这个问题，比如O(ln(N))? 在下一个阶段中，我们尝试扩展我们的基本设计—例如，通过显著地增加需求。我们问三个问题: 它是可行的吗？ 在资金、硬件等方面受到限制的情况下，是否有可能扩展这种设计?如果有必要，什么样的分布式设计才能满足需求? 它是弹性的吗？ 设计能优雅地失效吗?当这个组件失效时会发生什么?当整个数据中心失效时，系统如何工作? 我们如何做的更好？ 虽然我们通常以大致的顺序来讨论这些阶段和问题，但在实践中，我们在问题和阶段之间来回切换。例如，在基本的设计阶段，我们经常在我们的头脑中有成长和扩展。有了这些概念，让我们来回顾一下NALSD迭代过程。 初始需求 每个广告客户可能有多个广告。每个广告都由ad_id键入，并与广告商选择的搜索词列表相关联。 当向广告客户展示仪表盘时，我们需要知道以下每个信息： 这个搜索词触发这个广告的频率是多少 看到广告的人点击了多少次 有了这些信息，我们可以计算CTR:点击次数除以查看次数。 我们知道广告商关心两件事:仪表盘能否快速展示并且数据是最新的。因此，在迭代设计时，我们将根据SLOs来考虑我们的需求(详见第2章)： 99.9%的仪表板查询在&amp;lt; 1秒内完成 99.9%的时间，显示的CTR数据少于5分钟。 这些SLOs提供了一个合理的目标，我们应该能够始终如一地满足。 他们还提供了错误预算(参见站点可靠性工程中的第4章)，我们将在每次设计迭代中对我们的解决方案进行比较。 我们的目标是创建一个能够满足我们的SLOs的系统，并支持数以百万计希望在仪表盘上看到他们点击率的广告客户。对于事务率，我们认为每秒有500,000个搜索查询和10,000个广告点击。 一台机器时 最简单的起点是考虑在一台计算机上运行整个应用程序。 对每个搜索查询，我们记录如下信息： time： 查询发生的时间 query_id: 唯一的查询标识符（查询ID） search_term: 查询的内容 ad_id: 搜索显示的所有AdWords广告的id 这些信息一起形成查询日志。用户每次点击广告时，我们都会记录的时间、查询ID和单击日志中的ad_id。 你可能想知道为什么我们不简单地将search_term添加到单击日志中以减少复杂性。在我们提供的例子任意缩减的范围内，这可能是可行的。然而，在实践中，CTR实际上只是从这些日志中计算出的许多结论之一。单击日志来自url, url具有固有的大小限制，这使分离查询日志成为一个更可扩展的解决方案。我们不会通过在练习中添加额外的ctr-like需求来证明这一点，而是简单地承认这个假设并继续前进。 仪表盘展示需要来自两个日志的数据。我们需要保证，我们可以实现我们所展示SLO数据的更新时间在一秒钟以内。要实现这种SLO，需要在系统处理大量单击和查询时保持计算CTR的速度不变。 为了满足在一秒钟内显示仪表盘所要求SLO数据，我们需要快速查找每个给定ad_id的search_term中被单击和显示的query_id的数量。我们可以从查询日志中提取每个search_term和ad_id所显示的query_id的详细信息。CTR仪表盘需要来自ad_ids的查询日志和单击日志的所有记录。 如果我们有更多的广告商，扫描查询日志和单击日志来生成仪表盘的效率会非常低。因此，我们的设计要求我们的一台机器创建一个合适的数据结构，以便在接收日志时能够进行快速的CTR计算。在一台机器上，使用具有query_id和search_term索引的SQL数据库应该可以在一秒钟内提供答案。通过在query_id上连接这些日志并按search_term分组，我们可以报告每个搜索的CTR。 计算 我们需要计算需要多少资源来解析所有日志。为了确定缩放限制，我们需要做一些假设，从查询日志的大小开始： time： 64位整数，8个字节 query_id： 64位整数，8字节 ad_id： 3个64位整数，8字节 search_term： 一个长度为500字节的字符串 其它元数据： 500 - 1000字节的信息，比如哪台机器提供广告，搜索用的哪种语言，以及搜索词返回的结果 为了确保我们不会过早地达到一个阈值，我们将每个查询日志条目整理为2kb。单击日志卷应该比查询日志卷小得多:因为平均CTR为2%(10,000次单击/ 500,000次查询)，所以单击日志将拥有2%与查询日志一样多的记录。请记住，我们选择了大数字来说明这些原则可以实现任意大的扩展。这些估计似乎很大，因为它们本应该如此。 最后，我们可以使用科学的表示法来限制由单位上的不一致引起的算术错误。24小时内生成的查询日志量为： (5 × 105 queries/sec) × (8.64 × 104 seconds/day) × (2 × 103 bytes) = 86.4 TB/day 因为我们收到的点击量是查询的2%，而且我们知道数据库索引将增加一些合理的开销，所以我们可以将86.4 TB/天的存储空间增加到100 TB，以存储一天的日志数据。 由于总存储需求约为100 TB，我们需要作出一些新的假设。 这种设计是否仍适用于单台机器？ 虽然有可能 将100 TB的磁盘连接到一台机器上，但是我们可能会受到机器从磁盘读取和写入磁盘的能力限制。 例如，常见的4 TB HDD可能能够支持每秒200次输入/输出操作（IOPS）。 如果每个日志条目可以存储并以每个日志条目平均一个磁盘写入为索引，我们会看到IOPS是我们查询日志的限制因素： (5 × 105 queries/sec) / (200 IOPS/disk) = 2.5 × 103 disks or 2,500 disks 即使我们可以以10：1的比例批量查询以限制操作，但在最佳情况下，我们需要数百个HDD。 考虑到查询日志写入只是设计IO要求的一个组成部分，我们需要使用比传统HDD更好地处理高IOPS的解决方案。 为简单起见，我们将直接评估RAM并跳过评估其它存储介质，例如固态磁盘（SSD）。单台机器无法完全在RAM中处理100TB的占用空间：假设我们的标准机器占用空间为16核，64 GB RAM和1 Gbps网络吞吐量，我们需要： (100 TB) / (64 GB RAM/machine) = 1,563 machines 评估 暂时忽略我们的计算并想象我们可以将这个设计放在一台机器上，我们真的想要这样实现吗？ 如果我们通过询问当该组件发生故障时会发生什么来测试我们的设计，我们会找出一长串单点故障列表（例如，CPU，内存，存储，电源，网络，冷却）。 如果其中一个组件出现故障，我们是否可以合理地支持我们的SLO？ 几乎可以肯定的是，即使是一个简单的电力循环也会对我们的用户产生极大影响。 回到我们的计算，我们的单机设计看起来是不可行，但这一步并不浪费时间。我们已经发现了有关如何推理系统约束及其初始要求的有价值的信息。我们需要将设计发展为使用多台机器。 分步式系统 我们需要的search_terms位于查询日志中，ad_ids位于单击日志。 既然我们知 道我们需要多台机器，那么什么是连接它们的最佳设计？ MapReduce 我们可以使用MapReduce处理和连接日志。 我们可以定期获取累积的查询日志和单击日志，MapReduce将生成由ad_id组织的数据集，显示每个search_term接收到的单击次数。 MapReduce作为批处理器工作：它的输入是一个大型数据集，它可以使用许多机器通过进程处理该数据并产生结果。一旦所有机器处理完他们的数据，他们的输出就可以合并 - MapReduce可以直接为每个AdWords广告和搜索字词创建每个CRT的摘要。我们可以使用此数据创建我们需要的dashboard。 评价 MapReduce是一种广泛使用的计算模型，我们相信它可以横向扩展。 无论我们的查询日志和单击日志输入有多大，添加更多的机器总是能够在不耗尽磁盘空间或RAM的情况下成功完成流程。 不幸的是，这种类型的批处理过程无法在收到日志的5分钟内满足我们的加入日志可用性的SLO。 要在5分钟内提供结果，我们需要小批量运行MapReduce作业 —— 每次只需几分钟的日志。 批次的任意和非重叠性质使小批量不切实际。 如果已记录的查询在批处理1中，并且其单击位于批处理2中，则单击和查询将永远不会被连接。 虽然MapReduce可以很好地处理独立批次，但它并未针对此类问题进行优化。 此时，我们可以尝试使用MapReduce找出可能的解决方法。 然而，为简单起见，我们将继续研究另一种解决方案。 LogJoniner 用户点击的广告数量明显小于所投放广告的数量。 直观地说，我们需要专注于扩展两者中的较大者：查询日志。 我们通过引入新的分布式系统组件来实现此目的。 而不是像我们的MapReduce设计那样小批量查找query_id，如果我们创建了一个所有查询的存储，我们可以按需查询query_id，该怎么办？ 我们称之为QueryStore。 它包含查询日志的完整内容，query_id为主键。为了避免重复，我们假设我们从单个机器设计的计算将应用于QueryStore，我们将QueryStore的审查限制为我们已经涵盖的内容。 有关这样的组件如何工作的深入讨论，我们建议您阅读有关Bigtable的内容。 因为点击日志也有query_id，所以我们的处理循环的规模比现在要小得多：它只需要遍历点击日志并引入所引用的特定查询。 我们将此组件称为LogJoiner。 如果我们没有找到点击查询（接收查询日志的速度可能会减慢），我们将其搁置一段时间并重试，直到时间限制。 如果我们在该时间限制内找不到查询，我们会放弃该点击日志。 点击率仪表盘对每个ad_id和search_term需要两个组件：展示次数和点击的广告数量。 ClickMap需要合作伙伴来保存查询，由ad_id组织。 我们称之为QueryMap。 QueryMap直接从查询日志中提供所有数据，并通过ad_id索引条目。 图12-1描述了数据如何流经系统。 LogJoiner设计引入了几个新组件：LogJoiner，QueryStore，ClickMap和QueryMap。 我们需要确保这些组件可以扩展。 图12-1 基本的LogJoiner设计; 处理并存储点击数据，以便仪表板可以检索它 计算 根据我们在之前的迭代中执行的计算，我们知道QueryStore将为一天的日志提供大约100 TB的数据。 我们可以删除太旧而不具有价值的数据。 LogJoiner应该在进入时处理点击并检索相应的点击从QueryStore的日志。 QueryStore查找会产生额外的网络开销。 对于每个单击日志记录，我们查 找query_id并返回完整的日志记录： LogJoiner还会将结果发送到ClickMap。 我们需要存储query_id，ad_id和 time。 search_term、time和query_id都是64位整数，因此数据将小于1 KB： 总计约400 Mbps是我们机器可管理的数据传输速率。 ClickMap必须为每次单击存储时间和query_id，但不需要任何其他元数据。我们将忽略ad_id和search_term，因为它们是一个小的线性因素（例如，广告商数量×广告数量×8字节）。 即使是拥有10个广告的1000万广告客户也只有约800 MB。 一天的ClickMap值是： 我们将ClickMap最多为20 GB /天，以计算任何开销和我们的ad_ids。 在我们填写QueryMap时，我们需要为显示的每个广告存储query_id。我们需要增加存储空间，因为每个搜索查询可能会点击三个ad_id，因此我们需要记录query_id 最多三个条目： 2 TB足够小，可以使用硬盘驱动器托管在一台机器上，但我们从单机迭代中知 道，单个小写操作太频繁将无法存储在硬盘驱动器上。 虽然我们可以计算使用 更高IOPS驱动器（例如SSD）的影响，但我们的工作重点是证明系统可以扩展 到任意大的尺寸。在这种情况下，我们需要围绕单个机器的IO限制进行设计。 因此，缩放设计的下一步是对输入和输出进行分片：将传入的查询日志和单击 日志分成多个流。 Sharded LogJoiner 我们在此迭代中的目标是运行多个LogJoiner实例，每个实例位于数据的不同分片上。 为此，我们需要考虑几个因素： 数据管理 为了加入查询日志和单击日志，我们必须将每个单击日志记录与query_id上的相应查询日志记录进行匹配。 该设计应该防止网络和磁盘吞吐量在我们扩展时限制我们的设计。 可靠性 我们知道机器可能随时出现故障。 当一台机器运行LogJoiner时故障了，我们如何确保我们不会失去正在进行的工作？ 效率 我们可以做扩容而没有浪费吗？ 我们需要使用最少的资源满足我们的数据管理和可靠性需求。 我们的LogJoiner设计表明我们可以加入查询日志和点击日志，但产生的数据量非常大。 如果我们将工作划分为基于query_id的分片，我们可以并行运行多个LogJoiner。 提供了合理数量的LogJoiner实例，如果我们均匀地分发日志，则每个实例仅通过网络接收一小撮信息。 随着点击流量的增加，我们通过添加更多LogJoiner实例来水平扩展，而不是通过使用更多的CPU和RAM来垂直扩展。 如图12-2所示，为了使LogJoiners收到正确的消息，我们引入了一个名为日志分片器的组件，它将每个日志条目定向到正确的目的地。 对于每条记录，我们的点击日志分片器执行以下操作： 哈希记录的query_id。 用N（分片数）模数结果并加1以得到介于1和N之间的数字。 在第2步中将记录发送到分片编号。 图12-2 分片应该如何工作 现在，每个LogJoiner将获得分解的传入日志的一致子集 query_id，而不是完整的点击日志。 QueryMap也需要进行分片。 我们知道需要很多硬盘来支持QueryMap所需的IOPS，并且一天的QueryMap（2TB）的大小对于我们的64 GB机器来说太大而无法存储在RAM中。 但是，我们将在ad_id上进行分片，而不是像LogJoiner那样使用query_id进行分片。 ad_id在任何读取或写入之前都是已知的，因此使用与LogJoiner和CTR仪表盘相同的散列方法将提供一致的数据视图。 为了保持实现的一致性，我们可以将ClickMap的相同日志分片设计重用为QueryMap，因为ClickMap比QueryMap小。 现在我们知道我们的系统将扩展，我们可以继续解决系统的可靠性问题。 我们的设计必须能够适应LogJoiner故障。 如果LogJoiner在收到日志消息后但在加入日志消息之前失败，则必须重做其所有工作。 这会延迟准确数据到达仪表板，这将影响我们的SLO。 如果我们的日志分片器进程将重复的日志条目发送到两个分片，则系统可以继续全速执行并处理得到准确的结果，即使LogJoiner失败（可能是因为它所在的机器失败）。 通过以这种方式复制工作进程，我们减少（但不消除）丢失这些连接日志的机会。 两个分片可能会同时中断并丢失中继日志。 通过分配工作负载以确保在同一台计算机上没有重复的分片，我们可以减轻大部分风险。 如果两台机器同时发生故障并且我们丢失了分片的两个副本，则系统的错误预算（参见第一本SRE手册中的第4章）可以涵盖剩余的风险。 当灾难发生时，我们可以重新处理日志。 仪表板将仅显示短时间内超过5分钟的数据。 图12-3显示了我们对分片及其副本的设计，其中LogJoiner、ClickMap和QueryMap都构建在两个分片上。 从连接的日志中，我们可以在每个LogJoiner机器上构建ClickMap。 要显示我们的用户仪表盘，需要组合和查询所有ClickMaps。 结论 在一个数据中心中托管分片组件会产生单点故障：如果刚好不幸一对机器或数据中心断开连接，我们将丢失所有ClickMap工作，并且用户仪表盘完全停止工作！我们需要改进我们的设计以使用多个数据中心。 图12-3。 使用相同的query_id对日志进行分片以复制分片 多数据中心 跨不同地理位置的数据中心复制数据使我们的服务基础架构能够承受灾难性故障。如果一个数据中心关闭（例如，由于电源或网络中断），我们可以故障转移到另一个数据中心。要使故障转移起作用，必须在部署系统的所有数据中心中提供ClickMap数据。 这样的ClickMap甚至可能存在吗？我们不希望将计算需求乘以数据中心的数量，但我们如何才能有效地同步站点之间的工作以确保充分复制而不会产生不必要的重复？ 我们刚刚描述了分布式系统工程中众所周知的共识问题的一个例子。有许多复杂的算法可以解决这个问题，但基本思路是： 1.一个服务要有三个或五个副本（如ClickMap）。 2.用像Paxos等一致性算法，以确保在发生数据中心级别故障时我们可以可靠地存储计算状态。 3.在参与节点之间实现至少一个网络往返时间以接受写入操作。此要求限制了系统的顺序吞吐量。基于一致性map仍然需要并行写操作。 按照刚刚列出的步骤，多数据中心设计现在看来原则上是可行的。它还能在实践中发挥作用吗？我们需要哪些类型的资源，以及我们需要多少资源？ 计算 使用故障隔离数据中心执行Paxos算法的延迟意味着每个操作大约需要25毫秒才能完成。该延迟的假设基于至少相距几百公里的数据中心。因此，就顺序过程而言，我们每25毫秒只能执行一次操作或每秒40次操作。如果我们需要每秒执行10次连续进程（点击日志），我们需要为每个数据中心至少250个进程（由ad_id分片）用于Paxos操作。在实践中，我们希望添加更多进程以提高并行性，以便能处理任何宕机或流量高峰所造成的积压。 基于我们之前对ClickMap和QueryMap的计算，并使用每秒40次连续操作来估算，我们的多数据中心设计需要多少台新机器？ 因为我们的分片LogJoiner设计为每条日志记录引入了一个副本，所以我们每秒创建ClickMap和QueryMap的事务数量翻了一番：每秒20,000次点击和每秒1,000,000次查询。 我们可以通过把每秒总查询数除以每秒最大操作数来计算所需的最小进程数或任务数 ： 每个任务的内存量（2 TB QueryMap的两个副本）： 每台机器的任务数： 我们知道我们可以在一台机器上安装许多任务，但我们需要确保我们不会达到 IO的瓶颈。 ClickMap和QueryMap的总网络吞吐量（使用每个条目2 KB的高 估计）： 每项任务的吞吐量： 每台机器的吞吐量： 对于每个任务15MB内存和640Kbps吞吐量的组合是易于管理的。我们在每个数据中心需要大约4 TB的RAM来托管分片的ClickMap和QueryMap。 如果我们每台机器有64 GB的RAM，我们可以只使用64台机器处理数据，这将会只使用每台机器25％的网络带宽。 评估 现在我们已经设计了一个多数据中心的系统，让我们来看看数据流是否有意义。 图12-4显示了整个系统设计。 您可以查看每个搜索查询和广告点击是如何与服务器进行通信的，以及如何收集日志并将其推送到每个组件中。 我们可以根据我们的需求检查这个系统： 每秒10,000次广告点击 LogJoiner可以水平扩展以处理所有日志点击，并将结果存储在ClickMap中。 每秒500,000次搜索查询 QueryStore和QueryMap被设计为能够以此速率存储一整天的数据。 99.9％的仪表盘查询在&amp;lt;1秒内完成 CTR仪表板从QueryMap和ClickMap获取数据，这些数据由ad_id键入，使此事务变得快速而简单。 99.9％的时间，显示的CTR数据不到5分钟 每个组件都设计为水平扩展，这意味着如果管道处理太慢，添加更多计算机将减少端到端管道延迟。 我们相信该系统架构可以扩展以满足我们对吞吐量，性能和可靠性的要求。 图 12-4 多数据中心设计 总结 NALSD描述了Google用于生产系统的系统设计的迭代过程。 通过将软件分解为逻辑组件并将这些组件放入具有可靠基础架构的生产生态系统中，我们得出了对数据一致性、系统可用性和资源效率都能达到合适目标的系统。NALSD的实践使我们能够在不重复每次迭代的情况下改进设计。虽然本章中提出的各种设计迭代都满足了我们原始的问题陈述，但每次迭代都揭示了新的要求，我们可以通过扩展我们以前的工作来满足这些要求。 在整个过程中，我们根据我们对系统增长的预期来分离软件组件。 该策略允许我们独立地扩展系统的不同部分，并消除对单个硬件或单个软件实例的依赖性，从而产生更可靠的系统。 在整个设计过程中，我们通过对NALSD的四个关键问题进行提问来持续改进每次的迭代： 可能吗？ 我们可以在没有“魔法”的情况下建造它 我们可以做得更好吗？ 它是否像我们可以合理地制造一样简单？ 这可行吗？ 它是否符合我们的实际限制（预算，时间等）？ 它有弹性伸缩的吗？ 它会偶尔存在不可避免的中断吗？ NALSD是一项有学问的技能。与任何技能一样，您需要定期练习以保持熟练程度。 谷歌的经验表明，从抽象需求推理到具体的资源是建立健康和长寿命系统的关键。</summary></entry><entry><title type="html">第十一章 应对过载</title><link href="http://localhost:4000/sre/2020/01/11/%E5%BA%94%E5%AF%B9%E8%BF%87%E8%BD%BD/" rel="alternate" type="text/html" title="第十一章 应对过载" /><published>2020-01-11T00:00:00+08:00</published><updated>2020-01-11T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/11/%E5%BA%94%E5%AF%B9%E8%BF%87%E8%BD%BD</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/11/%E5%BA%94%E5%AF%B9%E8%BF%87%E8%BD%BD/">&lt;p&gt;服务永远不会100％可用：比如突发的流量高峰导致系统崩溃；或者船锚意外扯住了跨大西洋的光缆造成网络不可用。作为服务提供者，我们要为客户提供持久稳定的服务。面对这些突发的情况，如何才能使我们的基础设施尽可能具备可靠性？本章介绍了Google应对过载的方法，希望能够使用这些最佳实践来提高服务的可靠性和可用性。多年来，我们发现，没有单一的解决方案来均衡和稳定网络负载。因此，多种工具、技术和策略的组合才是帮助保持我们的服务可靠性的方法。在我们深入讨论本章之前，我们建议阅读《SRE：Google运维解密》第19章《前端服务器的负载均衡》和20《数据中心内部的负载均衡系统》中的内容。&lt;/p&gt;

&lt;h2 id=&quot;google云服务负载均衡&quot;&gt;Google云服务负载均衡&lt;/h2&gt;
&lt;p&gt;目前，大多数公司都没有开发和维护自己的全局负载均衡解决方案，而是选择使用来自更大的公有云供应商提供的负载均衡服务。我们将讨论Google云负载均衡（GCLB）作为大规模负载平衡的具体示例，但我们描述的所有最佳实践的案例几乎也适用于其他云提供商的负载均衡。&lt;/p&gt;

&lt;p&gt;过去18年来，谷歌一直致力于基础设施的建设，以提高服务的可靠性。今天，YouTube使用了我们的系统，地图，Gmail，搜索以及许多其他产品和服务也在用我们的系统。GCLB是我们面向用户的全球负载均衡解决方案。&lt;/p&gt;

&lt;p&gt;本节介绍GCLB的组件以及面对用户的请求时是如何协同工作的。我们跟踪典型的用户请求的全生命周期。Niantic PokémonGO的案例具体描述了GCLB的具体实施。&lt;/p&gt;

&lt;p&gt;我们的第一本SRE书的第19章描述了基于DNS的负载均衡。他是在用户开始连接阶段进行平衡均衡最简单和最有效的方法。我们还讨论了这种方法的一个地域性问题：它依靠客户端来正确地获取DNS记录。而GCLB不使用DNS进行负载均衡。相反，我们使用任播“anycast”，一种将客户端的请求发送到最近的群集，而不依赖DNS地理定位的方法。Google的全局负载均衡知道客户端所在的位置，并将数据包定向到最近的Web服务，从而在使用单个VIP（虚拟IP）时为用户提供低延迟的服务。使用单个VIP意味着我们可以增加DNS记录的生存时间（TTL），从而进一步减少延迟。&lt;/p&gt;

&lt;h3 id=&quot;任播&quot;&gt;任播&lt;/h3&gt;
&lt;p&gt;任播是一种网络寻址和路由方法。它将数据报从发送方路由到一组潜在接收方中拓扑最近的节点。这些接收方都由相同的目标IP地址标识。Google通过我们网络中多个点的边界网关协议（BGP）确定IP的地址。我们依靠BGP路由网格将来自用户的数据包传递到TCP会话最近的前端路由位置。此部署消除了单播IP扩散和为用户找到最接近的目标接口上。但这仍有两个主要问题：&lt;/p&gt;

&lt;p&gt;若附近具有太多用户可能会压倒前端的接口。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BGP路由计算可能会重置连接。&lt;/li&gt;
  &lt;li&gt;考虑这么一个ISP供应商，提供频繁的BGP路由计算以便给用户提供延迟更低的服务。但是每次BGP路由“振荡”时，所有正在进行的TCP流都会被重置，导致得结果就是用户的数据包被重定向到没有TCP会话状态的新的前端。为了解决这些问题，我们利用我们的连接层负载均衡器——Maglev，即使在路由振荡时也能够汇集TCP流。我们将此技术称为“稳定的任播”。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;稳定的任播&quot;&gt;稳定的任播&lt;/h4&gt;

&lt;p&gt;如图11-1所示，Google使用自定义的负载均衡器Maglev实现“稳定的任播”。为了稳定任播，我们为每个Maglev节点提供一种将客户端IP映射到最近的Google前端站点的方法。有时Maglev会为一个靠近另一个前端站点的客户端处理发往任播VIP的数据包。在这种情况下，Maglev会将该数据包转发到最近的前端站点。然后最近的前端站点的Maglev简单地其路由到本地后端。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/11-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图11-1 稳定的任播） &lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&quot;maglev&quot;&gt;Maglev&lt;/h3&gt;

&lt;p&gt;Maglev，如图11-2所示，是Google的定制分布式数据包层负载均衡器。作为云架构不可或缺的部分，Maglev集群管理传入的流量。它们在我们的前端服务器上提供有状态的TCP层负载均衡。Maglev在一些关键方面与其他传统硬件负载均衡器不同点如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;所有发往给定IP地址的数据包可以通过等价多路径（ECMP）均匀的转发到maglev集群中。这使我们只需扩容服务器即可提高maglev集群容量。均匀分布数据包使Maglev集群的冗余度建模为N+1，从而提高了传统负载均衡系统（通常依靠主动/被动提供1+1的冗余）的可用性和可靠性。&lt;/li&gt;
  &lt;li&gt;Maglev是Google自定义解决方案。我们可以对系统进行端到端地控制，这使我们能够快速进行实验和迭代。&lt;/li&gt;
  &lt;li&gt;Maglev在我们数据中心的商用硬件上运行，极大地简化了部署。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/11-2.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图11-2 Maglev &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;maglev数据包传送使用一致型散列和连接跟踪。这些技术在我们的HTTP反向代理（也称为Google前端或GFE）中合并为TCP流，这些代理终止TCP会话。一致型散列和连接跟踪是Maglev按包扩展而不是按连接数扩展的关键。当路由器收到发往Maglev托管的VIP的数据包时，路由器会通过ECMP将数据包转发到集群中的任何的Maglev节点。&lt;/p&gt;

&lt;p&gt;当Maglev收到数据包时，它会计算数据包的5元组散列，并在其连接跟踪表中查找散列值，该表包含最近连接的路由结果。如果Maglev找到匹配项且所选后端服务仍然是健康的，则会重新使用该连接。否则，maglev将回归利用哈希来选择后端。这些技术的组合消除了在各个Maglev机器之间共享连接状态的需要。&lt;/p&gt;

&lt;h3 id=&quot;全球软件负载均衡&quot;&gt;全球软件负载均衡&lt;/h3&gt;

&lt;p&gt;GSLB是谷歌的全球软件负载均衡器。它允许我们平衡集群之间的实时用户流量，以便我们可以将用户需求与可用服务容量相匹配，因此我们可以对用户透明的方式处理服务故障。如图11-3所示，GSLB控制到GFE的连接分配和后端服务请求的分配。GSLB允许我们为后端运行的用户和在不同集群中运行的GFE提供服务。除了前端和后端之间的负载均衡之外，GSLB还了解后端服务的运行状况，并可以自动从失败的集群中摘除流量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/11-3.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图11-3 GSLB &lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&quot;谷歌前端&quot;&gt;谷歌前端&lt;/h3&gt;

&lt;p&gt;如图11-4所示，GFE位于外部和Google服务（网络搜索，图像搜索，Gmail等）之间，并且通常是客户端HTTP(S)请求时遇到的第一个Google服务器。GFE终止客户端的TCP和SSL会话，并检查HTTP标头和URL路径，以确定哪个后端服务应该处理请求。一旦GFE决定将请求发送到某处，它就会重新加密数据并转发请求。有关此加密过程如何工作的详细信息，请参阅白皮书“Google Cloud中的传输加密”。&lt;/p&gt;

&lt;p&gt;GFE还负责对其后端进行健康检查。如果后端服务器返回异常(NACKs请求)或运行健康检查超时，GFE将停止向失败的后端发送流量。我们使用此信号更新GFE后端，而不会影响正常的链接时间。通过将GFE后端置于健康检查失败的模式，同时继续响应正在进行的请求，我们可以在不中断任何用户请求的情况下优雅地从服务中剔除GFE后端。我们称之为“跛脚鸭”模式，我们将在第一本SRE书的第20章中对其进行更详细的讨论。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/11-4.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图11-3 GFE &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;GFE还为所有活跃的后端维护持久的会话，以便在请求到达时就可以立即进行连接。此策略有助于减少用户的延迟，尤其是在我们使用SSL保护GFE和后端之间的连接时。&lt;/p&gt;

&lt;h3 id=&quot;gclb低延迟&quot;&gt;GCLB：低延迟&lt;/h3&gt;

&lt;p&gt;网络配置策略旨在减少用户访问的延迟。因为通过“TTPS协商安全连接”需要在客户端和服务器之间进行两次网络往返，所以我们最大限度地减少“请求时间”的延迟尤为重要。为此，我们将网络边缘扩展到主机Maglev和GFE。这些组件尽可能靠近用户的进行SSL连接，然后通过长期加密连接将请求转发到我们网络中更深层次的后端服务。&lt;/p&gt;

&lt;p&gt;我们在这个组合的Maglev/GFE增强边缘网络上构建了GCLB。当客户创建负载均衡器时，我们会设置任播VIP并对Maglev进行编程，以便在我们网络边缘的GFE上对其进行全局负载均衡。GFE的作用是终止SSL，接受和缓冲HTTP请求，将这些请求转发给客户的后端服务，然后将代理响应回用户。GSLB提供每层之间的粘合剂：它使Maglev能够找到具有可用容量的最近的GFE位置，并使GFE能够将请求路由到具有可用容量的最近的VM实例组。&lt;/p&gt;

&lt;h3 id=&quot;gclb-高可用性&quot;&gt;GCLB: 高可用性&lt;/h3&gt;

&lt;p&gt;为了向我们的客户提供高可用性，GCLB提供99.99％的可用性SLA。此外，GCLB还提供支持工具，使我们的客户能够改进和管理自己的应用程序的可用性。将负载均衡系统视为一种流量管理器是有用的。在正常操作期间，GCLB将流量路由到具有可用容量的最近的后端。当服务的实例失败时，GCLB会检测到故障并将流量路由到健康的实例。金丝雀发布有助于GCLB保持高可用性。金丝雀发布是我们的标准发布程序之一。如第16章所述，这个过程是程序首先发布到极少数的服务器，然后逐渐增加流量并仔细观察系统行为以确认是否需要回滚。这种做法通过在金丝雀发布的早期阶段捕捉异常来减少影响。如果新版本崩溃或无法进行健康检查，负载均衡器则进行重新路由请求。如果检测到非致命的回滚，则可以从负载均衡器中以管理方式删除实例组，而无需触及应用程序的主要版本。&lt;/p&gt;

&lt;h3 id=&quot;案例研究1gclb上的pokemongo&quot;&gt;案例研究1：GCLB上的PokemonGO&lt;/h3&gt;

&lt;p&gt;Niantic在2016年夏天推出了PokémonGO。这是第一款神奇宝贝相关的游戏，也是Niantic与一家大型娱乐公司的第一个项目。这个游戏受到了超过预期的欢迎——那个夏天你会经常看到球员们在虚拟世界中带着神奇宝贝进行决斗。PokémonGO的成功大大超过了Niantic的期望。在发布之前，他们对系统的堆栈进行了负载测试，最多可处理最乐观估算5倍的流量。实际每秒启动请求数（RPS）估计接近50倍——足以应对几乎所有的用户的挑战的堆栈容量。&lt;/p&gt;

&lt;p&gt;为了使游戏体验更加愉快，PokémonGO支持世界范围内的高度互动，并在用户之间进行全球共享。特定区域内的所有玩家都会看到同样的游戏世界，并在这个世界里互相交流。这要求游戏近乎实时的产生并分发状态给所有参与者。为了达到50倍以上RPS的目标，Niantic研发团队需要应对巨大的挑战。此外，谷歌的许多工程师也给他们提供了帮助——为成功发布扩展服务。在迁移到GCLB两天之内，PokémonGO应用程序成为最大的单一GCLB服务，很轻松与其他十大GCLB服务相提并论。&lt;/p&gt;

&lt;p&gt;如图11-5所示，当它推出时，神奇宝贝GO使用了谷歌的区域性网络负载均衡器（NLB），用于对Kubernetes中的入口流量进行负载均衡。每个集群都包含一组Nginx实例，它们用作第7层反向代理，建立SSL，缓冲HTTP请求，执行路由并使用跨应用程序服务器后端的实例进行负载平衡。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/11-5.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图11-5 神奇宝贝GO （预GCLB） &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;NLB负责IP层的负载均衡，因此使用NLB的服务有效地成为Maglev的后端。 在这种情况下，依赖NLB对Niantic有以下问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Nginx后端负责终止客户端的SSL，这需要从客户端设备到Niantic的前端代理两次往返。&lt;/li&gt;
  &lt;li&gt;缓冲来自客户端的HTTP请求的会导致代理层上的资源耗尽，导致客户端只能缓慢发送字节时。&lt;/li&gt;
  &lt;li&gt;数据包级代理无法有效改善SYN Flood等低级网络攻击。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了解决这些问题，Niantic需要在大型边缘网络上使用高级别的代理。使用NLB无法实现此解决方案。&lt;/p&gt;

&lt;h4 id=&quot;迁移到gclb&quot;&gt;迁移到GCLB&lt;/h4&gt;
&lt;p&gt;大型SYN泛洪攻击使PokémonGO迁移到GCLB成为优先事项。这个迁移是Niantic CRE团队与Google SRE团队共同开展。最初的迁移发生在用户使用量的低谷时期。当时，并没有发现问题。然而，随着流量的增加达到峰值，对于Niantic和Google来说都出现了无法预料的问题。Google和Niantic发现客户流量的需求高于以往峰值的200％。Niantic前端代理收到了过多的请求以至于他们无法响应所有的以接收的请求。任何连接都以拒绝方式返回给用户。&lt;/p&gt;

&lt;p&gt;这种流量激增导致了经典的级联故障情景。众多API-Cloud数据存储区，PokémonGO后端和API服务器以及负载均衡系统——超出了Niantic云的可用容量。过载导致Niantic的后端变得极其缓慢，请求超时波及到了负载平衡层。在这种情况下，负载均衡器重试GET请求。极高的请求量和重试机制的组合使得GFE中的SSL客户端代码处于前所未有的水平，因为它试图重新构建对于无响应的后端。这在GFE中引起严重的性能退化，使得GCLB的全球产能有效减少了50％。&lt;/p&gt;

&lt;p&gt;由于后端失败，PokémonGO应用程序试图重试失败的请求用户时，应用程序的重试策略是立即重启，接下来是不断的重启。随着重启的进行，服务有时会返回大量错误报告 - 例如，重新启动共享后端时，这些错误响应有助于有效地同步客户端重试，产生“雷鸣般的”群体问题，其中许多客户请求基本上是相同的时间。如图11-6所示，这些同步请求峰值增加了小到20倍之前的全球RPS峰值。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/11-6.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图11-6 由同步客户机重试引起的流量峰值 &lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&quot;解决问题&quot;&gt;解决问题&lt;/h4&gt;

&lt;p&gt;这些请求峰值与GFE容量回归相结合，导致所有GCLB服务的排队和高延迟。Google的SRE通过执行以下操作来减少对其他GCLB用户的附带损害：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;隔离可以从主负载平衡器池中提供PokémonGO流量的GFE。&lt;/li&gt;
  &lt;li&gt;扩大Pokémon GO 应用的集群池，直到它能够处理峰值流量，尽管性能下降。此操作将容量瓶颈从GFE转移到Niantic堆栈，其中服务器仍在超时，特别是在客户端重试开始同步和峰值时。&lt;/li&gt;
  &lt;li&gt;在Niantic的协助下，TrafficSRE实施了管理覆盖，以限制负载均衡代表PokémonGO接受的流量。该策略包含客户端需求，足以让Niantic重新建立正常连接并开始垂直扩展。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;图11-7显示了最终的网络配置。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/11-7.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图11-7 Pokémon GO GCLB &lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&quot;面向未来&quot;&gt;面向未来&lt;/h4&gt;
&lt;p&gt;在此事件发生后，Google和Niantic都对其系统进行了重大调整。Niantic为其客户端引入了抖动和截断指数降级，从而抑制了级联故障期间经历的大规模同步重试峰值。谷歌将GFE后端视为潜在的重要负载来源，并制定了检测GFE和负载测试由后端缓慢或以其他方式引起的性能下降。最后，两家公司都意识到他们应该尽可能地靠近客户来衡量负载。如果Niantic和Google CRE能够准确预测客户的RPS需求，那么我们会先行扩容给Niantic，这比我们在迁移到GCLB之前所做的要及时的多。&lt;/p&gt;

&lt;h2 id=&quot;自动弹性伸缩&quot;&gt;自动弹性伸缩&lt;/h2&gt;
&lt;p&gt;像GCLB这样的工具可以帮助有效地均衡整个系统的负载，使服务更加稳定可靠。有时根本没有足够的资源来管理现有流量。此时可以使用自动弹性伸缩来扩展系统。无论是增加每台计算机的资源（垂直扩展）还是增加池中的计算机总数（水平扩展），弹性伸缩都是一种功能强大的工具，如果使用得当，可以提高服务可用性和利用率。相反，如果配置错误或误用，弹性伸缩可能会对服务产生负面影响。本节介绍弹性伸缩的一些最佳实践，常见故障模式和当前限制。&lt;/p&gt;

&lt;h3 id=&quot;处理不健康的实例&quot;&gt;处理不健康的实例&lt;/h3&gt;
&lt;p&gt;自动调节所有实例的利用率，无论其状态如何，并假设实例在请求处理效率方面是同质的。当计算机无法提供服务时（称为不健康的实例），自动调节会遇到问题，但仍然会计入平均利用率。在这种情况下，不会发生弹性伸缩。以下情况都可以触发此故障模式，包括：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;需要很长时间准备服务的实例（例如，加载二进制文件或预热时）&lt;/li&gt;
  &lt;li&gt;陷入非保存状态的实例（即僵尸）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们可以使用各种策略改善这种情况。可以组合或单独进行以下改进：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;负载均衡&lt;/code&gt;&lt;br /&gt;
使用负载均衡观察到的容量指标进行自动弹性伸缩。这将自动从平均值中扣除不健康的实例。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;在收集指标之前，请等待新实例稳定下来&lt;/code&gt;&lt;br /&gt;
您可以将autoscaler配置为仅在新实例变为健康状态时收集有关新实例的信息（GCE将此不活动时段称为冷却时段）。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;自动弹性伸缩和自动修复&lt;/code&gt;&lt;br /&gt;
自动修复监视系统实例，并尝试在不健康的情况下重新启动它们。通常，将自动加密器配置为监视实例公开的运行状况指标。如果自动修复检测到实例已关闭或运行状况不佳，则会尝试重新启动。配置时，务必确保在重新启动后为实例留出足够的时间使其恢复正常。&lt;/p&gt;

&lt;p&gt;使用这些解决方案的混合，可以优化水平弹性伸缩以仅跟踪健康的实例。请记住，在运行服务时，自动调节器会不断调整系统实例的大小。创建新实例绝不是即时的（需要一定的时间）。&lt;/p&gt;

&lt;h3 id=&quot;有状态的系统&quot;&gt;有状态的系统&lt;/h3&gt;
&lt;p&gt;有状态的系统将用户会话中的所有请求一致地发送到同一后端服务器。如果这些路径负担过重，添加更多实例（即水平扩展）将无济于事。分散负载的智能任务型路由（例如，使用一致性哈希）是对有状态系统的更好策略。&lt;/p&gt;

&lt;p&gt;垂直自动伸缩在有状态系统中很有用。当与任务型平衡结合使用以均衡系统负载时，垂直自动弹性伸缩可以帮助吸收短期热点。但请谨慎使用此策略：由于垂直自动调节通常在所有实例中均匀分布，因此低流量的实例可能会出现资源的浪费。&lt;/p&gt;

&lt;h3 id=&quot;谨慎的配置&quot;&gt;谨慎的配置&lt;/h3&gt;
&lt;p&gt;使用自动弹性伸缩扩展比使用缩放更重要且风险更小，因为无法扩展可能导致过载和流量下降。按照设计，大多数自动调节器实现对流量的跳跃都比对流量的下降更敏感。扩展时，自动缩放器倾向于快速增加额外的服务容量。按比例缩小时，它们会更加谨慎，并且在缓慢减少资源之前等待更长时间以使其缩放条件成立。&lt;/p&gt;

&lt;p&gt;当服务进一步远离瓶颈时，你可以吸收的负载峰值会增加。我们建议配置自动扩展器，以使服务远离关键系统瓶颈（例如CPU）。自动定标器还需要足够的时间来做出反应，特别是当新实例无法启动并立即投入使用时。我们建议面向用户的服务为过载保护和冗余保留足够的备用容量。&lt;/p&gt;

&lt;h3 id=&quot;设置约束&quot;&gt;设置约束&lt;/h3&gt;
&lt;p&gt;Autoscaler是一款功能强大的工具; 如果配置错误，它可能会造成意想不到的后果。例如，请考虑以下方案：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;您已将自动扩展配置为根据CPU利用率进行扩展。您发布了系统的新版本，其中包含导致服务器在不执行任何工作的情况下使用CPU的错误。Autoscaler通过反复升级此作业来做出反应，直到浪费了所有可用配额。&lt;/li&gt;
  &lt;li&gt;服务中没有任何变化，但依赖性失败。此故障导致所有请求卡在您的服务器上并且永远不会完成，从而消耗资源。Autoscaler将扩大工作范围，导致越来越多的流量卡住。失败的依赖项上的负载增加可以防止依赖项恢复。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;限制允许自动缩放器执行的工作很有用。设置缩放的最小和最大界限，确保您有足够的配额可以缩放到设置的限制。这样做可以防止您耗尽配额并帮助您进行容量规划。&lt;/p&gt;

&lt;h3 id=&quot;支持禁用和手动执行&quot;&gt;支持禁用和手动执行&lt;/h3&gt;
&lt;p&gt;如果您的自动伸缩出现问题，最好使用kill开关。确保工程师了解如何禁用自动弹性伸缩以及如何在必要时手动进行弹性伸缩。自动弹性伸缩终止开关功能应该简单，明显，迅速且运行良好。&lt;/p&gt;

&lt;h3 id=&quot;避免重载后端&quot;&gt;避免重载后端&lt;/h3&gt;
&lt;p&gt;正确配置的自动缩放器将根据流量的增加进行扩展。流量的增加会对堆栈产生影响。后端服务（如数据库）需要吸收服务器可能产生的任何额外负载。因此，在部署自动缩放器之前对后端服务执行详细的依赖性分析是个好主意，特别是因为某些服务可能比其他服务更加线性扩展。确保您的后端有足够的额外容量来提供增加的流量，并且在超载时能够优雅地降级。使用分析中的数据来告知自动缩放器配置的限制。&lt;/p&gt;

&lt;p&gt;服务部署通常运行各种共享配额的微服务。如果微服务扩大以响应流量峰值，它可能会使用大部分配额。如果单个微服务上的流量增加意味着其他微服务上的流量增加，则剩余的微服务将无法增加可用配额。在这种情况下，依赖性分析可以帮助指导您预先实施有限的扩展。或者，您可以为每个微服务实现单独的配额（这可能需要将您的服务拆分为单独的项目）。&lt;/p&gt;

&lt;h3 id=&quot;避免流量不平衡&quot;&gt;避免流量不平衡&lt;/h3&gt;
&lt;p&gt;一些自动调节器（例如，AWS EC2，GCP）可以跨区域实例组（RMiG）平衡实例。除了常规的自动弹性伸缩之外，这些自动编程器还运行一个单独的工作，不断尝试均衡整个区域中每个区域的大小。以这种方式重新平衡避免流量集中在某一区域。如果使用的系统为每个区域分配配额，则此策略可以均衡您的配额使用情况。此外，跨区域的自动调整为故障域提供了更多的多样性。&lt;/p&gt;

&lt;h2 id=&quot;结合管理负载均衡的策略&quot;&gt;结合管理负载均衡的策略&lt;/h2&gt;
&lt;p&gt;如果系统足够复杂，可能需要使用多种负载管理。例如，可以运行多个托管实例组，这些实例组可以按负载进行扩展，但是可以跨多个区域克隆容量; 因此，还需要平衡区域之间的流量。在这种情况下，系统需要同时使用负载均衡和基于负载的自动扩展。&lt;/p&gt;

&lt;p&gt;或者可能在世界各地的三个共同设施中运营一个网站，希望在本地提供延迟服务，但由于部署更多计算机需要数周时间，因此溢出的流量需要定位到其他位置。如果网站在社交媒体上受欢迎，并且突然间流量增加了五倍。因此，实施减载以减少多余的流量。在这种情况下系统需要同时使用负载平衡和减载。&lt;/p&gt;

&lt;p&gt;或者，数据处理管道可能位于一个云区域的Kubernetes集群中。当数据处理速度显着降低时，它会提供更多的pod来处理工作。但是，当数据进入如此之快以至于读取它会导致内存不足或减慢垃圾收集时，pod可能需要临时或永久地释放该负载。在这种情况下，系统需要使用基于负载的自动弹性伸缩和减载技术。&lt;/p&gt;

&lt;p&gt;负载均衡，减载都是为同一目标而设计的系统：均衡和稳定系统负载。由于这些系统通常是分别实现、安装和配置的，因此它们看起来是独立的。但是，如图11-8所示，它们并非完全独立。以下案例研究说明了这些系统如何相互作用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/11-8.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图11-8 一个完整的流量管理系统 &lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&quot;案例研究2减载以应对流量攻击&quot;&gt;案例研究2：减载以应对流量攻击&lt;/h3&gt;

&lt;p&gt;Dressy是一家流量驱动的服装电商销售平台。其开发团队将应用部署到了三个地区已应对单区域故障问题（他们是这样认为的）。&lt;/p&gt;

&lt;p&gt;在一次故障案例中，Dressy的客户服务团队开收到客户投诉：无法访问该应用程序。 Dressy的开发团队调查并注意到一个问题:他们的负载均衡系统将所有用户流量吸引到区域A，即使该区域是满溢的，B和C都是空的（同样的资源规模下），这是令人费解的。故障的时间表（见图11-9）如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在一天开始时，仪表盘流量图显示所有三个集群稳定在90 RPS的访问。&lt;/li&gt;
  &lt;li&gt;在上午10:46，由于购物者开始寻找便宜货，所有三个地区的流量开始上升。&lt;/li&gt;
  &lt;li&gt;在上午11点，区域A在区域B和C之前达到120 RPS。&lt;/li&gt;
  &lt;li&gt;在上午11点10分，A区继续增长到400 RPS，而B和C下降到40 RPS。&lt;/li&gt;
  &lt;li&gt;负载均衡器稳定了流量请求，将大部分的请求指向A区。&lt;/li&gt;
  &lt;li&gt;击中A区的大多数请求都返回503错误。&lt;/li&gt;
  &lt;li&gt;请求到达此群集的用户开始抱怨。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/11-9.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图11-9 区域流量 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;如果开发团队排查负载均衡器的饱和度图表，他们会看到一些意料之外的东西。负载均衡器从Dressy的容器中读取CPU利用率并根据此信息来估计饱和度。而每个请求的CPU利用率在区域A时比B和C都低10倍。负载均衡确定所有区域的负载均等，认为其工作正常。&lt;/p&gt;

&lt;h4 id=&quot;发生了什么事&quot;&gt;发生了什么事？&lt;/h4&gt;
&lt;p&gt;本周早些时候，为了防止级联过载，该团队实现了减载。每当CPU利用率达到某个阈值时，服务器就会为其收到的任何新请求返回错误响应，而不是尝试处理它们。在这一天，A区域略微超过其他区域。每个服务器开始拒绝收到的10％的请求，然后是20％的请求，然后是50％。在此时间范围内，CPU使用率保持不变。&lt;/p&gt;

&lt;p&gt;就负载均衡器系统而言，每个连续丢弃的请求使CPU利用率降低。区域A比区域B和C更有效。它在80％CPU（上限）下服务240RPS，而B和C仅有120 RPS。从逻辑上讲，它决定向A发送更多请求。&lt;/p&gt;

&lt;h4 id=&quot;什么地方出了错&quot;&gt;什么地方出了错？&lt;/h4&gt;
&lt;p&gt;简而言之，负载均衡器不知道“有效”请求是错误的，因为负载减少和负载平衡系统没有通信。每个系统都是由不同的工程师单独添加和启用的。没有人将它们视为一个统一的负载管理系统。&lt;/p&gt;

&lt;h4 id=&quot;经验总结&quot;&gt;经验总结&lt;/h4&gt;
&lt;p&gt;为了有效地管理系统负载，我们需要慎重考虑各个负载管理工具的配置和他们的交互管理。例如，Dressy案例研究中，将错误处理添加到负载均衡器逻辑可以解决问题。假设每个“错误”请求计为120％的CPU利用率（超过100的任何数字都可以）。现在区域A看起来超载了。请求将扩展到B和C，系统将平衡。&lt;/p&gt;

&lt;p&gt;可以使用类似的逻辑将此示例推断为任何负载管理策略的组合。采用新的负载管理工具时，请仔细检查它与系统已使用的其他工具的交互方式，并检测是否有交集的地方。添加监控以检测反馈循环是否存在。确保有紧急关闭触发器以便在负载管理系统之间进行协调，并且如果这些系统的行为严重失控，请考虑添加自动关闭触发器。如果没有预先采取适当的预防措施，以下的预案可能需要的。以下是可能会需要的预防措施，具体取决于负载管理的类型：&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;负载均衡&lt;/code&gt; &lt;br /&gt;
负载均衡通过路由到最靠近用户的位置来最小化延迟。自动弹性伸缩可以与负载均衡协同工作，更靠近用户，然后在那里路由更多流量，从而创建正反馈循环。&lt;/p&gt;

&lt;p&gt;如果需求主要靠近一个位置，那么该位置的负载将会增加，增加该位置的服务容量以应对请求。如果此位置出现故障，其余位置将出现过载，并且可能会丢弃一部分流量。弹性伸缩不是即时的应对措施。您可以通过为每个位置设置最小实例数来避免这种情况，以保留备用容量进行故障转移。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;减载&lt;/code&gt;&lt;br /&gt;
最好设置一个阈值，以便在减载开始之前系统自动调整。否则，如果设置值过，系统可能会开始减少它可能服务的流量。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;使用RPC管理负载&lt;/code&gt;&lt;br /&gt;
处理正确的请求对于提高效率非常重要：不希望自动调整以提供不会使用户受益的请求，或者因为处理不重要的请求而不必要地减少负载。在同时使用自动缩放和减载时，在RPC请求上设置截止日期非常重要。&lt;/p&gt;

&lt;p&gt;进程为所有正在进行的请求保留资源，并在请求完成时释放这些资源。在没有特定截止日期的情况下，系统将为所有正在进行的请求保留资源，直至达到最大可能限制。默认情况下，此截止日期是一个非常大的数字（这取决于语言实现 - 某些语言API在固定时间点工作，而其他语言API在一段时间内工作）。此行为会导致客户端（最终用户）遇到更高的延迟。该服务还存在资源（如内存）耗尽和崩溃的风险。&lt;/p&gt;

&lt;p&gt;为了优雅地处理这种情况，我们建议服务器终止花费太长时间的请求，并且客户端取消对它们不再有用的请求。例如，如果客户端已向用户返回错误，则服务器不应启动昂贵的搜索操作。要设置服务的行为期望，只需在API的.proto文件中提供注释即可建议默认的截止日期。此外，为客户设置有意的截止日期（例如，请参阅我们的博客文章“gRPC和截止日期”）。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;p&gt;根据Google的经验，没有完美的负载管理配置。自动弹性伸缩是一种强大的工具，但很容易出错。除非是详细定制的配置，否则自动伸缩可能会导致灾难性后果 - 例如，当这些工具单独配置时，负载平衡，负载切除和自动扩展之间可能存在灾难性的反馈周期。正如PokémonGO案例那样，当负载管理是基于系统级别的整体架构时，负载管理效果最佳。&lt;/p&gt;

&lt;p&gt;一次又一次案例中，我们已经看到，减载，自动弹性伸缩或限制它们之间没有协调配置时，系统是会出现灾难性故障的。例如，在PokémonGO案例研究中，我们有一个“雷鸣般的群体问题”，即客户端重试以及等待无响应的后端服务器的负载均衡器。要应对服务的过载，需要提前计划以减少潜在问题。缓解策略可能涉及设置标志，更改默认行为，启用日志记录，或流量管理系统的参数值引入制定管理决策中。&lt;/p&gt;

&lt;p&gt;我们希望本章提供的策略和见解可以帮助您，管理自己服务的流量并让用户满意。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">服务永远不会100％可用：比如突发的流量高峰导致系统崩溃；或者船锚意外扯住了跨大西洋的光缆造成网络不可用。作为服务提供者，我们要为客户提供持久稳定的服务。面对这些突发的情况，如何才能使我们的基础设施尽可能具备可靠性？本章介绍了Google应对过载的方法，希望能够使用这些最佳实践来提高服务的可靠性和可用性。多年来，我们发现，没有单一的解决方案来均衡和稳定网络负载。因此，多种工具、技术和策略的组合才是帮助保持我们的服务可靠性的方法。在我们深入讨论本章之前，我们建议阅读《SRE：Google运维解密》第19章《前端服务器的负载均衡》和20《数据中心内部的负载均衡系统》中的内容。 Google云服务负载均衡 目前，大多数公司都没有开发和维护自己的全局负载均衡解决方案，而是选择使用来自更大的公有云供应商提供的负载均衡服务。我们将讨论Google云负载均衡（GCLB）作为大规模负载平衡的具体示例，但我们描述的所有最佳实践的案例几乎也适用于其他云提供商的负载均衡。 过去18年来，谷歌一直致力于基础设施的建设，以提高服务的可靠性。今天，YouTube使用了我们的系统，地图，Gmail，搜索以及许多其他产品和服务也在用我们的系统。GCLB是我们面向用户的全球负载均衡解决方案。 本节介绍GCLB的组件以及面对用户的请求时是如何协同工作的。我们跟踪典型的用户请求的全生命周期。Niantic PokémonGO的案例具体描述了GCLB的具体实施。 我们的第一本SRE书的第19章描述了基于DNS的负载均衡。他是在用户开始连接阶段进行平衡均衡最简单和最有效的方法。我们还讨论了这种方法的一个地域性问题：它依靠客户端来正确地获取DNS记录。而GCLB不使用DNS进行负载均衡。相反，我们使用任播“anycast”，一种将客户端的请求发送到最近的群集，而不依赖DNS地理定位的方法。Google的全局负载均衡知道客户端所在的位置，并将数据包定向到最近的Web服务，从而在使用单个VIP（虚拟IP）时为用户提供低延迟的服务。使用单个VIP意味着我们可以增加DNS记录的生存时间（TTL），从而进一步减少延迟。 任播 任播是一种网络寻址和路由方法。它将数据报从发送方路由到一组潜在接收方中拓扑最近的节点。这些接收方都由相同的目标IP地址标识。Google通过我们网络中多个点的边界网关协议（BGP）确定IP的地址。我们依靠BGP路由网格将来自用户的数据包传递到TCP会话最近的前端路由位置。此部署消除了单播IP扩散和为用户找到最接近的目标接口上。但这仍有两个主要问题： 若附近具有太多用户可能会压倒前端的接口。 BGP路由计算可能会重置连接。 考虑这么一个ISP供应商，提供频繁的BGP路由计算以便给用户提供延迟更低的服务。但是每次BGP路由“振荡”时，所有正在进行的TCP流都会被重置，导致得结果就是用户的数据包被重定向到没有TCP会话状态的新的前端。为了解决这些问题，我们利用我们的连接层负载均衡器——Maglev，即使在路由振荡时也能够汇集TCP流。我们将此技术称为“稳定的任播”。 稳定的任播 如图11-1所示，Google使用自定义的负载均衡器Maglev实现“稳定的任播”。为了稳定任播，我们为每个Maglev节点提供一种将客户端IP映射到最近的Google前端站点的方法。有时Maglev会为一个靠近另一个前端站点的客户端处理发往任播VIP的数据包。在这种情况下，Maglev会将该数据包转发到最近的前端站点。然后最近的前端站点的Maglev简单地其路由到本地后端。 图11-1 稳定的任播） Maglev Maglev，如图11-2所示，是Google的定制分布式数据包层负载均衡器。作为云架构不可或缺的部分，Maglev集群管理传入的流量。它们在我们的前端服务器上提供有状态的TCP层负载均衡。Maglev在一些关键方面与其他传统硬件负载均衡器不同点如下： 所有发往给定IP地址的数据包可以通过等价多路径（ECMP）均匀的转发到maglev集群中。这使我们只需扩容服务器即可提高maglev集群容量。均匀分布数据包使Maglev集群的冗余度建模为N+1，从而提高了传统负载均衡系统（通常依靠主动/被动提供1+1的冗余）的可用性和可靠性。 Maglev是Google自定义解决方案。我们可以对系统进行端到端地控制，这使我们能够快速进行实验和迭代。 Maglev在我们数据中心的商用硬件上运行，极大地简化了部署。 图11-2 Maglev maglev数据包传送使用一致型散列和连接跟踪。这些技术在我们的HTTP反向代理（也称为Google前端或GFE）中合并为TCP流，这些代理终止TCP会话。一致型散列和连接跟踪是Maglev按包扩展而不是按连接数扩展的关键。当路由器收到发往Maglev托管的VIP的数据包时，路由器会通过ECMP将数据包转发到集群中的任何的Maglev节点。 当Maglev收到数据包时，它会计算数据包的5元组散列，并在其连接跟踪表中查找散列值，该表包含最近连接的路由结果。如果Maglev找到匹配项且所选后端服务仍然是健康的，则会重新使用该连接。否则，maglev将回归利用哈希来选择后端。这些技术的组合消除了在各个Maglev机器之间共享连接状态的需要。 全球软件负载均衡 GSLB是谷歌的全球软件负载均衡器。它允许我们平衡集群之间的实时用户流量，以便我们可以将用户需求与可用服务容量相匹配，因此我们可以对用户透明的方式处理服务故障。如图11-3所示，GSLB控制到GFE的连接分配和后端服务请求的分配。GSLB允许我们为后端运行的用户和在不同集群中运行的GFE提供服务。除了前端和后端之间的负载均衡之外，GSLB还了解后端服务的运行状况，并可以自动从失败的集群中摘除流量。 图11-3 GSLB 谷歌前端 如图11-4所示，GFE位于外部和Google服务（网络搜索，图像搜索，Gmail等）之间，并且通常是客户端HTTP(S)请求时遇到的第一个Google服务器。GFE终止客户端的TCP和SSL会话，并检查HTTP标头和URL路径，以确定哪个后端服务应该处理请求。一旦GFE决定将请求发送到某处，它就会重新加密数据并转发请求。有关此加密过程如何工作的详细信息，请参阅白皮书“Google Cloud中的传输加密”。 GFE还负责对其后端进行健康检查。如果后端服务器返回异常(NACKs请求)或运行健康检查超时，GFE将停止向失败的后端发送流量。我们使用此信号更新GFE后端，而不会影响正常的链接时间。通过将GFE后端置于健康检查失败的模式，同时继续响应正在进行的请求，我们可以在不中断任何用户请求的情况下优雅地从服务中剔除GFE后端。我们称之为“跛脚鸭”模式，我们将在第一本SRE书的第20章中对其进行更详细的讨论。 图11-3 GFE GFE还为所有活跃的后端维护持久的会话，以便在请求到达时就可以立即进行连接。此策略有助于减少用户的延迟，尤其是在我们使用SSL保护GFE和后端之间的连接时。 GCLB：低延迟 网络配置策略旨在减少用户访问的延迟。因为通过“TTPS协商安全连接”需要在客户端和服务器之间进行两次网络往返，所以我们最大限度地减少“请求时间”的延迟尤为重要。为此，我们将网络边缘扩展到主机Maglev和GFE。这些组件尽可能靠近用户的进行SSL连接，然后通过长期加密连接将请求转发到我们网络中更深层次的后端服务。 我们在这个组合的Maglev/GFE增强边缘网络上构建了GCLB。当客户创建负载均衡器时，我们会设置任播VIP并对Maglev进行编程，以便在我们网络边缘的GFE上对其进行全局负载均衡。GFE的作用是终止SSL，接受和缓冲HTTP请求，将这些请求转发给客户的后端服务，然后将代理响应回用户。GSLB提供每层之间的粘合剂：它使Maglev能够找到具有可用容量的最近的GFE位置，并使GFE能够将请求路由到具有可用容量的最近的VM实例组。 GCLB: 高可用性 为了向我们的客户提供高可用性，GCLB提供99.99％的可用性SLA。此外，GCLB还提供支持工具，使我们的客户能够改进和管理自己的应用程序的可用性。将负载均衡系统视为一种流量管理器是有用的。在正常操作期间，GCLB将流量路由到具有可用容量的最近的后端。当服务的实例失败时，GCLB会检测到故障并将流量路由到健康的实例。金丝雀发布有助于GCLB保持高可用性。金丝雀发布是我们的标准发布程序之一。如第16章所述，这个过程是程序首先发布到极少数的服务器，然后逐渐增加流量并仔细观察系统行为以确认是否需要回滚。这种做法通过在金丝雀发布的早期阶段捕捉异常来减少影响。如果新版本崩溃或无法进行健康检查，负载均衡器则进行重新路由请求。如果检测到非致命的回滚，则可以从负载均衡器中以管理方式删除实例组，而无需触及应用程序的主要版本。 案例研究1：GCLB上的PokemonGO Niantic在2016年夏天推出了PokémonGO。这是第一款神奇宝贝相关的游戏，也是Niantic与一家大型娱乐公司的第一个项目。这个游戏受到了超过预期的欢迎——那个夏天你会经常看到球员们在虚拟世界中带着神奇宝贝进行决斗。PokémonGO的成功大大超过了Niantic的期望。在发布之前，他们对系统的堆栈进行了负载测试，最多可处理最乐观估算5倍的流量。实际每秒启动请求数（RPS）估计接近50倍——足以应对几乎所有的用户的挑战的堆栈容量。 为了使游戏体验更加愉快，PokémonGO支持世界范围内的高度互动，并在用户之间进行全球共享。特定区域内的所有玩家都会看到同样的游戏世界，并在这个世界里互相交流。这要求游戏近乎实时的产生并分发状态给所有参与者。为了达到50倍以上RPS的目标，Niantic研发团队需要应对巨大的挑战。此外，谷歌的许多工程师也给他们提供了帮助——为成功发布扩展服务。在迁移到GCLB两天之内，PokémonGO应用程序成为最大的单一GCLB服务，很轻松与其他十大GCLB服务相提并论。 如图11-5所示，当它推出时，神奇宝贝GO使用了谷歌的区域性网络负载均衡器（NLB），用于对Kubernetes中的入口流量进行负载均衡。每个集群都包含一组Nginx实例，它们用作第7层反向代理，建立SSL，缓冲HTTP请求，执行路由并使用跨应用程序服务器后端的实例进行负载平衡。 图11-5 神奇宝贝GO （预GCLB） NLB负责IP层的负载均衡，因此使用NLB的服务有效地成为Maglev的后端。 在这种情况下，依赖NLB对Niantic有以下问题： Nginx后端负责终止客户端的SSL，这需要从客户端设备到Niantic的前端代理两次往返。 缓冲来自客户端的HTTP请求的会导致代理层上的资源耗尽，导致客户端只能缓慢发送字节时。 数据包级代理无法有效改善SYN Flood等低级网络攻击。 为了解决这些问题，Niantic需要在大型边缘网络上使用高级别的代理。使用NLB无法实现此解决方案。 迁移到GCLB 大型SYN泛洪攻击使PokémonGO迁移到GCLB成为优先事项。这个迁移是Niantic CRE团队与Google SRE团队共同开展。最初的迁移发生在用户使用量的低谷时期。当时，并没有发现问题。然而，随着流量的增加达到峰值，对于Niantic和Google来说都出现了无法预料的问题。Google和Niantic发现客户流量的需求高于以往峰值的200％。Niantic前端代理收到了过多的请求以至于他们无法响应所有的以接收的请求。任何连接都以拒绝方式返回给用户。 这种流量激增导致了经典的级联故障情景。众多API-Cloud数据存储区，PokémonGO后端和API服务器以及负载均衡系统——超出了Niantic云的可用容量。过载导致Niantic的后端变得极其缓慢，请求超时波及到了负载平衡层。在这种情况下，负载均衡器重试GET请求。极高的请求量和重试机制的组合使得GFE中的SSL客户端代码处于前所未有的水平，因为它试图重新构建对于无响应的后端。这在GFE中引起严重的性能退化，使得GCLB的全球产能有效减少了50％。 由于后端失败，PokémonGO应用程序试图重试失败的请求用户时，应用程序的重试策略是立即重启，接下来是不断的重启。随着重启的进行，服务有时会返回大量错误报告 - 例如，重新启动共享后端时，这些错误响应有助于有效地同步客户端重试，产生“雷鸣般的”群体问题，其中许多客户请求基本上是相同的时间。如图11-6所示，这些同步请求峰值增加了小到20倍之前的全球RPS峰值。 图11-6 由同步客户机重试引起的流量峰值 解决问题 这些请求峰值与GFE容量回归相结合，导致所有GCLB服务的排队和高延迟。Google的SRE通过执行以下操作来减少对其他GCLB用户的附带损害： 隔离可以从主负载平衡器池中提供PokémonGO流量的GFE。 扩大Pokémon GO 应用的集群池，直到它能够处理峰值流量，尽管性能下降。此操作将容量瓶颈从GFE转移到Niantic堆栈，其中服务器仍在超时，特别是在客户端重试开始同步和峰值时。 在Niantic的协助下，TrafficSRE实施了管理覆盖，以限制负载均衡代表PokémonGO接受的流量。该策略包含客户端需求，足以让Niantic重新建立正常连接并开始垂直扩展。 图11-7显示了最终的网络配置。 图11-7 Pokémon GO GCLB 面向未来 在此事件发生后，Google和Niantic都对其系统进行了重大调整。Niantic为其客户端引入了抖动和截断指数降级，从而抑制了级联故障期间经历的大规模同步重试峰值。谷歌将GFE后端视为潜在的重要负载来源，并制定了检测GFE和负载测试由后端缓慢或以其他方式引起的性能下降。最后，两家公司都意识到他们应该尽可能地靠近客户来衡量负载。如果Niantic和Google CRE能够准确预测客户的RPS需求，那么我们会先行扩容给Niantic，这比我们在迁移到GCLB之前所做的要及时的多。 自动弹性伸缩 像GCLB这样的工具可以帮助有效地均衡整个系统的负载，使服务更加稳定可靠。有时根本没有足够的资源来管理现有流量。此时可以使用自动弹性伸缩来扩展系统。无论是增加每台计算机的资源（垂直扩展）还是增加池中的计算机总数（水平扩展），弹性伸缩都是一种功能强大的工具，如果使用得当，可以提高服务可用性和利用率。相反，如果配置错误或误用，弹性伸缩可能会对服务产生负面影响。本节介绍弹性伸缩的一些最佳实践，常见故障模式和当前限制。 处理不健康的实例 自动调节所有实例的利用率，无论其状态如何，并假设实例在请求处理效率方面是同质的。当计算机无法提供服务时（称为不健康的实例），自动调节会遇到问题，但仍然会计入平均利用率。在这种情况下，不会发生弹性伸缩。以下情况都可以触发此故障模式，包括： 需要很长时间准备服务的实例（例如，加载二进制文件或预热时） 陷入非保存状态的实例（即僵尸） 我们可以使用各种策略改善这种情况。可以组合或单独进行以下改进： 负载均衡 使用负载均衡观察到的容量指标进行自动弹性伸缩。这将自动从平均值中扣除不健康的实例。 在收集指标之前，请等待新实例稳定下来 您可以将autoscaler配置为仅在新实例变为健康状态时收集有关新实例的信息（GCE将此不活动时段称为冷却时段）。 自动弹性伸缩和自动修复 自动修复监视系统实例，并尝试在不健康的情况下重新启动它们。通常，将自动加密器配置为监视实例公开的运行状况指标。如果自动修复检测到实例已关闭或运行状况不佳，则会尝试重新启动。配置时，务必确保在重新启动后为实例留出足够的时间使其恢复正常。 使用这些解决方案的混合，可以优化水平弹性伸缩以仅跟踪健康的实例。请记住，在运行服务时，自动调节器会不断调整系统实例的大小。创建新实例绝不是即时的（需要一定的时间）。 有状态的系统 有状态的系统将用户会话中的所有请求一致地发送到同一后端服务器。如果这些路径负担过重，添加更多实例（即水平扩展）将无济于事。分散负载的智能任务型路由（例如，使用一致性哈希）是对有状态系统的更好策略。 垂直自动伸缩在有状态系统中很有用。当与任务型平衡结合使用以均衡系统负载时，垂直自动弹性伸缩可以帮助吸收短期热点。但请谨慎使用此策略：由于垂直自动调节通常在所有实例中均匀分布，因此低流量的实例可能会出现资源的浪费。 谨慎的配置 使用自动弹性伸缩扩展比使用缩放更重要且风险更小，因为无法扩展可能导致过载和流量下降。按照设计，大多数自动调节器实现对流量的跳跃都比对流量的下降更敏感。扩展时，自动缩放器倾向于快速增加额外的服务容量。按比例缩小时，它们会更加谨慎，并且在缓慢减少资源之前等待更长时间以使其缩放条件成立。 当服务进一步远离瓶颈时，你可以吸收的负载峰值会增加。我们建议配置自动扩展器，以使服务远离关键系统瓶颈（例如CPU）。自动定标器还需要足够的时间来做出反应，特别是当新实例无法启动并立即投入使用时。我们建议面向用户的服务为过载保护和冗余保留足够的备用容量。 设置约束 Autoscaler是一款功能强大的工具; 如果配置错误，它可能会造成意想不到的后果。例如，请考虑以下方案： 您已将自动扩展配置为根据CPU利用率进行扩展。您发布了系统的新版本，其中包含导致服务器在不执行任何工作的情况下使用CPU的错误。Autoscaler通过反复升级此作业来做出反应，直到浪费了所有可用配额。 服务中没有任何变化，但依赖性失败。此故障导致所有请求卡在您的服务器上并且永远不会完成，从而消耗资源。Autoscaler将扩大工作范围，导致越来越多的流量卡住。失败的依赖项上的负载增加可以防止依赖项恢复。 限制允许自动缩放器执行的工作很有用。设置缩放的最小和最大界限，确保您有足够的配额可以缩放到设置的限制。这样做可以防止您耗尽配额并帮助您进行容量规划。 支持禁用和手动执行 如果您的自动伸缩出现问题，最好使用kill开关。确保工程师了解如何禁用自动弹性伸缩以及如何在必要时手动进行弹性伸缩。自动弹性伸缩终止开关功能应该简单，明显，迅速且运行良好。 避免重载后端 正确配置的自动缩放器将根据流量的增加进行扩展。流量的增加会对堆栈产生影响。后端服务（如数据库）需要吸收服务器可能产生的任何额外负载。因此，在部署自动缩放器之前对后端服务执行详细的依赖性分析是个好主意，特别是因为某些服务可能比其他服务更加线性扩展。确保您的后端有足够的额外容量来提供增加的流量，并且在超载时能够优雅地降级。使用分析中的数据来告知自动缩放器配置的限制。 服务部署通常运行各种共享配额的微服务。如果微服务扩大以响应流量峰值，它可能会使用大部分配额。如果单个微服务上的流量增加意味着其他微服务上的流量增加，则剩余的微服务将无法增加可用配额。在这种情况下，依赖性分析可以帮助指导您预先实施有限的扩展。或者，您可以为每个微服务实现单独的配额（这可能需要将您的服务拆分为单独的项目）。 避免流量不平衡 一些自动调节器（例如，AWS EC2，GCP）可以跨区域实例组（RMiG）平衡实例。除了常规的自动弹性伸缩之外，这些自动编程器还运行一个单独的工作，不断尝试均衡整个区域中每个区域的大小。以这种方式重新平衡避免流量集中在某一区域。如果使用的系统为每个区域分配配额，则此策略可以均衡您的配额使用情况。此外，跨区域的自动调整为故障域提供了更多的多样性。 结合管理负载均衡的策略 如果系统足够复杂，可能需要使用多种负载管理。例如，可以运行多个托管实例组，这些实例组可以按负载进行扩展，但是可以跨多个区域克隆容量; 因此，还需要平衡区域之间的流量。在这种情况下，系统需要同时使用负载均衡和基于负载的自动扩展。 或者可能在世界各地的三个共同设施中运营一个网站，希望在本地提供延迟服务，但由于部署更多计算机需要数周时间，因此溢出的流量需要定位到其他位置。如果网站在社交媒体上受欢迎，并且突然间流量增加了五倍。因此，实施减载以减少多余的流量。在这种情况下系统需要同时使用负载平衡和减载。 或者，数据处理管道可能位于一个云区域的Kubernetes集群中。当数据处理速度显着降低时，它会提供更多的pod来处理工作。但是，当数据进入如此之快以至于读取它会导致内存不足或减慢垃圾收集时，pod可能需要临时或永久地释放该负载。在这种情况下，系统需要使用基于负载的自动弹性伸缩和减载技术。 负载均衡，减载都是为同一目标而设计的系统：均衡和稳定系统负载。由于这些系统通常是分别实现、安装和配置的，因此它们看起来是独立的。但是，如图11-8所示，它们并非完全独立。以下案例研究说明了这些系统如何相互作用。 图11-8 一个完整的流量管理系统 案例研究2：减载以应对流量攻击 Dressy是一家流量驱动的服装电商销售平台。其开发团队将应用部署到了三个地区已应对单区域故障问题（他们是这样认为的）。 在一次故障案例中，Dressy的客户服务团队开收到客户投诉：无法访问该应用程序。 Dressy的开发团队调查并注意到一个问题:他们的负载均衡系统将所有用户流量吸引到区域A，即使该区域是满溢的，B和C都是空的（同样的资源规模下），这是令人费解的。故障的时间表（见图11-9）如下： 在一天开始时，仪表盘流量图显示所有三个集群稳定在90 RPS的访问。 在上午10:46，由于购物者开始寻找便宜货，所有三个地区的流量开始上升。 在上午11点，区域A在区域B和C之前达到120 RPS。 在上午11点10分，A区继续增长到400 RPS，而B和C下降到40 RPS。 负载均衡器稳定了流量请求，将大部分的请求指向A区。 击中A区的大多数请求都返回503错误。 请求到达此群集的用户开始抱怨。 图11-9 区域流量 如果开发团队排查负载均衡器的饱和度图表，他们会看到一些意料之外的东西。负载均衡器从Dressy的容器中读取CPU利用率并根据此信息来估计饱和度。而每个请求的CPU利用率在区域A时比B和C都低10倍。负载均衡确定所有区域的负载均等，认为其工作正常。 发生了什么事？ 本周早些时候，为了防止级联过载，该团队实现了减载。每当CPU利用率达到某个阈值时，服务器就会为其收到的任何新请求返回错误响应，而不是尝试处理它们。在这一天，A区域略微超过其他区域。每个服务器开始拒绝收到的10％的请求，然后是20％的请求，然后是50％。在此时间范围内，CPU使用率保持不变。 就负载均衡器系统而言，每个连续丢弃的请求使CPU利用率降低。区域A比区域B和C更有效。它在80％CPU（上限）下服务240RPS，而B和C仅有120 RPS。从逻辑上讲，它决定向A发送更多请求。 什么地方出了错？ 简而言之，负载均衡器不知道“有效”请求是错误的，因为负载减少和负载平衡系统没有通信。每个系统都是由不同的工程师单独添加和启用的。没有人将它们视为一个统一的负载管理系统。 经验总结 为了有效地管理系统负载，我们需要慎重考虑各个负载管理工具的配置和他们的交互管理。例如，Dressy案例研究中，将错误处理添加到负载均衡器逻辑可以解决问题。假设每个“错误”请求计为120％的CPU利用率（超过100的任何数字都可以）。现在区域A看起来超载了。请求将扩展到B和C，系统将平衡。 可以使用类似的逻辑将此示例推断为任何负载管理策略的组合。采用新的负载管理工具时，请仔细检查它与系统已使用的其他工具的交互方式，并检测是否有交集的地方。添加监控以检测反馈循环是否存在。确保有紧急关闭触发器以便在负载管理系统之间进行协调，并且如果这些系统的行为严重失控，请考虑添加自动关闭触发器。如果没有预先采取适当的预防措施，以下的预案可能需要的。以下是可能会需要的预防措施，具体取决于负载管理的类型： 负载均衡 负载均衡通过路由到最靠近用户的位置来最小化延迟。自动弹性伸缩可以与负载均衡协同工作，更靠近用户，然后在那里路由更多流量，从而创建正反馈循环。 如果需求主要靠近一个位置，那么该位置的负载将会增加，增加该位置的服务容量以应对请求。如果此位置出现故障，其余位置将出现过载，并且可能会丢弃一部分流量。弹性伸缩不是即时的应对措施。您可以通过为每个位置设置最小实例数来避免这种情况，以保留备用容量进行故障转移。 减载 最好设置一个阈值，以便在减载开始之前系统自动调整。否则，如果设置值过，系统可能会开始减少它可能服务的流量。 使用RPC管理负载 处理正确的请求对于提高效率非常重要：不希望自动调整以提供不会使用户受益的请求，或者因为处理不重要的请求而不必要地减少负载。在同时使用自动缩放和减载时，在RPC请求上设置截止日期非常重要。 进程为所有正在进行的请求保留资源，并在请求完成时释放这些资源。在没有特定截止日期的情况下，系统将为所有正在进行的请求保留资源，直至达到最大可能限制。默认情况下，此截止日期是一个非常大的数字（这取决于语言实现 - 某些语言API在固定时间点工作，而其他语言API在一段时间内工作）。此行为会导致客户端（最终用户）遇到更高的延迟。该服务还存在资源（如内存）耗尽和崩溃的风险。 为了优雅地处理这种情况，我们建议服务器终止花费太长时间的请求，并且客户端取消对它们不再有用的请求。例如，如果客户端已向用户返回错误，则服务器不应启动昂贵的搜索操作。要设置服务的行为期望，只需在API的.proto文件中提供注释即可建议默认的截止日期。此外，为客户设置有意的截止日期（例如，请参阅我们的博客文章“gRPC和截止日期”）。 结论 根据Google的经验，没有完美的负载管理配置。自动弹性伸缩是一种强大的工具，但很容易出错。除非是详细定制的配置，否则自动伸缩可能会导致灾难性后果 - 例如，当这些工具单独配置时，负载平衡，负载切除和自动扩展之间可能存在灾难性的反馈周期。正如PokémonGO案例那样，当负载管理是基于系统级别的整体架构时，负载管理效果最佳。 一次又一次案例中，我们已经看到，减载，自动弹性伸缩或限制它们之间没有协调配置时，系统是会出现灾难性故障的。例如，在PokémonGO案例研究中，我们有一个“雷鸣般的群体问题”，即客户端重试以及等待无响应的后端服务器的负载均衡器。要应对服务的过载，需要提前计划以减少潜在问题。缓解策略可能涉及设置标志，更改默认行为，启用日志记录，或流量管理系统的参数值引入制定管理决策中。 我们希望本章提供的策略和见解可以帮助您，管理自己服务的流量并让用户满意。</summary></entry><entry><title type="html">第十章 事后总结：从失败中学习</title><link href="http://localhost:4000/sre/2020/01/10/%E4%BA%8B%E5%90%8E%E6%80%BB%E7%BB%93-%E4%BB%8E%E5%A4%B1%E8%B4%A5%E4%B8%AD%E5%AD%A6%E4%B9%A0/" rel="alternate" type="text/html" title="第十章 事后总结：从失败中学习" /><published>2020-01-10T00:00:00+08:00</published><updated>2020-01-10T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/10/%E4%BA%8B%E5%90%8E%E6%80%BB%E7%BB%93:%E4%BB%8E%E5%A4%B1%E8%B4%A5%E4%B8%AD%E5%AD%A6%E4%B9%A0</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/10/%E4%BA%8B%E5%90%8E%E6%80%BB%E7%BB%93-%E4%BB%8E%E5%A4%B1%E8%B4%A5%E4%B8%AD%E5%AD%A6%E4%B9%A0/">&lt;!-- more --&gt;

&lt;p&gt;经验表明，真正对事不对人的事后总结文化可以带来更可靠的系统——这也是我们认为这种做法对于创建和维护一个成功的SRE组织十分重要的原因。&lt;/p&gt;

&lt;p&gt;将事后总结引入组织既是一种文化变革，也是一种技术变革。这样的转变似乎是令人生畏的，本章的关键点是，做出这种改变是可能的，不是一个难以克服的挑战。不要指望你的系统能够自行解决问题。你可以从引入一个非常基础的事后调查程序开始，反映和调整你的流程来适应你的组织——跟很多事情一样，没有放之四海而皆准的方法。&lt;/p&gt;

&lt;p&gt;如果总结的很好，可以采取行动并广泛分享，事后总结可以成为积极推动组织变革和防止重复故障的利器。为了可以说明优秀的事后总结的写作原则，本章介绍了一个曾经发生在Google的故障案例进行研究。一个糟糕的事后总结的案例可以突显出为什么“糟糕”的事后总结对于努力创建健康的事后分析文化的组织是有害的。将糟糕的事后总结与实际事后总结进行比较后，可以突出高质量事后总结的原则和最佳实践。&lt;/p&gt;

&lt;p&gt;本章的第二部分分享了我们在实现强大的事后总结文化激励机制以及识别（和补救）文化破裂的预兆方面所学到的知识。&lt;/p&gt;

&lt;p&gt;最后，我们提供了用于引导事后总结文化的工具和模板。&lt;/p&gt;

&lt;p&gt;有关对事不对人的事后总结哲学的全部讨论，请参阅我们的第一本书“站点可靠性工程”中的第15章。&lt;/p&gt;

&lt;h2 id=&quot;案例分析&quot;&gt;案例分析&lt;/h2&gt;

&lt;p&gt;本案例研究的是例行机架退役导致用户服务延迟增加的case。我们的自动化维护系统的一个bug，加上限速不足，导致数千台承载生产流量的服务器同时宕机。&lt;/p&gt;

&lt;p&gt;虽然Google的大多数服务器都位于我们的专有数据中心，但我们在托管设施（或“colos”）中也有机架代理/缓存机器。colos中包含我们代理机器的机架被称为&lt;code class=&quot;highlighter-rouge&quot;&gt;卫星&lt;/code&gt;，由于&lt;code class=&quot;highlighter-rouge&quot;&gt;卫星&lt;/code&gt;经常进行例行维护和升级，所以在任何时间点都会安装或退役许多&lt;code class=&quot;highlighter-rouge&quot;&gt;卫星&lt;/code&gt;机架。在Google，这些维护流程基本是自动化的。&lt;/p&gt;

&lt;p&gt;退役过程使用我们称为diskerase流程覆盖机架中所有驱动器的全部内容。一旦机器进入diskerase，它曾存储的数据将不再可检索。典型的机架退役步骤如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;获取&lt;code class=&quot;highlighter-rouge&quot;&gt;卫星&lt;/code&gt;上所有活跃的机器&lt;br /&gt;
  machines = GetMachines(satellite)&lt;br /&gt;
将所有通过“filter”匹配到的候选机器发送decom&lt;br /&gt;
  SendToDecom(candidates = GetAllSatelliteMachines(), filter=machines)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们的案例研究始于一个标记为退役的&lt;code class=&quot;highlighter-rouge&quot;&gt;卫星&lt;/code&gt;机架。退役过程的diskerase步骤已成功完成，但负责剩余机器退役的自动化失败了。为了调试失败，我们重新尝试了停用过程。第二次退役如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;获取卫星上所有活跃的机器  &lt;br /&gt;
    machines  = GetMachines(satellite)&lt;br /&gt;
 因为decom流已经在运行了，因此”machines”是一个空的列表&lt;br /&gt;
 API bug：anmpty list被视为“无过滤，即为在全部机器上运行”，而不是“不在任何机器上运行”&lt;br /&gt;
 将所有通过“filter”匹配到的候选机器发送到decom&lt;br /&gt;
    SendToDecom(candidates=GetAllSatelliteMachines(), filter=machines)&lt;br /&gt;
 将所有候选机器发送到diskerase。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;几分钟内，全球所有卫星机架的磁盘数据都被删除。这些机器处于不可用状态，无法接受用户的连接，因此后续用户的连接直接路由到我们的数据中心。结果导致用户延迟增加。得益于良好的容量规划，很少有用户注意到这两天我们在受影响的colo机架中重新安装机器。事件发生后，我们花了几周的时间进行审核，并为自动化添加了更多的健全性检查，使我们的退役工作流程具有幂等性（其任意多次执行所产生的影响均与一次执行的影响相同）。&lt;/p&gt;

&lt;p&gt;故障发生的三年后，我们遇到了类似的事件：由于许多卫星被耗尽导致用户延迟增加。根据原始事后总结，本次事件后实施的行动大大减小了二次事故的影响范围和速度。&lt;/p&gt;

&lt;p&gt;假设你是负责为此案例研究撰写事后总结总结的人。你想了解什么，以及你会采取什么行动来防止此类故障再次发生？&lt;/p&gt;

&lt;p&gt;让我们从这个事件的一个糟糕的事后总结示例开始。&lt;/p&gt;

&lt;h2 id=&quot;糟糕的事后总结示例&quot;&gt;糟糕的事后总结示例：&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;-事后总结所有卫星机器进入diskerase流程-&quot;&gt;&lt;center&gt; 事后总结：所有卫星机器进入diskerase流程 &lt;/center&gt;&lt;/h3&gt;
  &lt;p&gt;&lt;strong&gt;2014-8-11&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;负责人&lt;/strong&gt;：maxone@，logantwo@， sydneythree@，dylanfour@&lt;br /&gt;
&lt;strong&gt;与以下人员共享&lt;/strong&gt;：satellite-infra-team@&lt;br /&gt;
&lt;strong&gt;状态&lt;/strong&gt;：不可更改&lt;br /&gt;
&lt;strong&gt;事件日期&lt;/strong&gt;：2014年8月11日&lt;br /&gt;
&lt;strong&gt;发布时间&lt;/strong&gt;：2014年12月30日&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;执行摘要&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;：所有卫星机器都被发送到diskerase，实际导致Google Edge不可用。  &lt;br /&gt;
&lt;strong&gt;根本原因&lt;/strong&gt;：dylanfour@忽略了自动化设置并手动运行了集群启动逻辑，从而触发了现有bug。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;故障摘要&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;故障持续时间&lt;/strong&gt;：40分钟&lt;br /&gt;
&lt;strong&gt;受影响的产品&lt;/strong&gt;：satellite-infra-team   &lt;br /&gt;
&lt;strong&gt;受影响的产品百分比&lt;/strong&gt;：所有卫星集群。 &lt;br /&gt;
&lt;strong&gt;用户影响&lt;/strong&gt;：进入卫星的所有查询都是由Core提供的，导致延迟增加。 &lt;br /&gt;
&lt;strong&gt;收入影响&lt;/strong&gt;：由于查询异常，部分广告未投放。目前确切的收入影响未知。 &lt;br /&gt;
&lt;strong&gt;监控&lt;/strong&gt;：监控报警。 &lt;br /&gt;
&lt;strong&gt;解决方案&lt;/strong&gt;：将流量转移到核心，然后手动修复边缘集群。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;背景（可选）&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;em&gt;用户影响：&lt;/em&gt; &lt;br /&gt;
  进入卫星的所有查询都是由Core提供的，导致延迟增加。&lt;/li&gt;
    &lt;li&gt;&lt;em&gt;收入影响：&lt;/em&gt;  &lt;br /&gt;
  由于查询异常，部分广告未投放。&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;根本原因和触发点&lt;/strong&gt; &lt;br /&gt;
集群开启和关闭的自动化并不意味着是幂等的，该工具有安全措施确保某些步骤不能多次运行。但无法阻止某个人手动多次运行代码，也没有文件提及这个问题。因此，大多数团队成员认为工具失效时，可以多次运行该过程。&lt;/p&gt;

  &lt;p&gt;正好在机架的常规退役期间发生这种情况。机架正在被一个新的基于lota的卫星取代。dylanfour@忽略了已经执行了一次启动并在第一次执行时出现问题。由于粗心以及无知，他们引发了一个错误的交互，即将所有卫星机器分配给了弃用团队。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;为恢复生产投入的努力&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;经验教训&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;事情进展顺利：
      &lt;ul&gt;
        &lt;li&gt;报警及时发现问题。&lt;/li&gt;
        &lt;li&gt;事故处理进展顺利。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;事情进展受挫：
      &lt;ul&gt;
        &lt;li&gt;团队（特别是maxone@，logantwo@）从未编写任何文档告知SRE不要多次运行自动化。&lt;/li&gt;
        &lt;li&gt;on-call处理的不够及时，没有阻止大多数卫星机器被删除。这已经不是第一次出现处理故障不及时的情况了。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;幸运的
      &lt;ul&gt;
        &lt;li&gt;Core能够为原本进入边缘集群的所有流量提供服务。简直无法相信我们这次能幸免于难！&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;行动列表&lt;/strong&gt;&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;完善自动化工具。&lt;/td&gt;
        &lt;td&gt;缓解&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;logantwo@&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;报警改善&lt;/td&gt;
        &lt;td&gt;检测&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;sydneythree@ 需要学习正确的跨站点轮值文档，避免发生重复的问题&lt;/td&gt;
        &lt;td&gt;缓解&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt;BUG6789&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;训练大家不要运行不安全的命令&lt;/td&gt;
        &lt;td&gt;避免&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
        &lt;td&gt; &lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;&lt;strong&gt;名词解释&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;为什么这份总结十分糟糕&quot;&gt;为什么这份总结十分糟糕？&lt;/h3&gt;

&lt;p&gt;示例包含一些我们试图避免的常见故障模式。以下说明如何改进这份事后总结。&lt;/p&gt;

&lt;h4 id=&quot;缺少上下文&quot;&gt;缺少上下文&lt;/h4&gt;

&lt;p&gt;从一开始，作为示例的事后总结引入了特定于流量服务的术语（例如，卫星）和Google机器管理自动化的较低层（例如，“diskerase”）。如果你需要提供其他上下文作为总结的一部分，请在背景或名词解释部分进行添加（可以链接到更长的文档）。上述总结在这两部分都是空白的。&lt;/p&gt;

&lt;p&gt;如果在编写事后总结时没有将内容置于正确的上下文中，文档可能会被误解或被忽略。谨记，你的受众并不只仅仅是有着直接关系的团队。&lt;/p&gt;

&lt;h4 id=&quot;省略了关键细节&quot;&gt;省略了关键细节&lt;/h4&gt;

&lt;p&gt;多个部分包含摘要但缺少重要细节。例如：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题总结&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;针对多个服务的中断，应该提供数字来表示影响范围。示例中唯一的数据是故障的持续时间。没有足够的细节来评估故障的规模或影响。即使没有一个具体值，一个估计的值也比没有数据好。毕竟，你如果不知道如何衡量它，那么你也不知道它是否被修复了！&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;根本原因和触发点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;确定根本原因和触发点是编写事后总结的重要原因之一。示例包含一个描述根本原因和触发点的小段落，但没有探索故障的底层细节。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为恢复生产投入的努力&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对读者来说，事后总结是事件记录。一份好的事后总结能让读者知道发生了什么，该如何减轻问题，以及用户受到了什么影响。很多问题的答案通常在投入恢复的努力中可以找到，但示例中，这部分是空的。&lt;/p&gt;

&lt;p&gt;如果故障值得写入事后总结中，那么你还应该花时间准确获取并记录必要的详细信息。读者可以全面了解停机情况，更重要的是可以了解新事物。&lt;/p&gt;

&lt;h4 id=&quot;缺失了关键行动列表&quot;&gt;缺失了关键行动列表&lt;/h4&gt;

&lt;p&gt;示例的“行动列表”（AIs）部分缺少防止此类故障再次发生的可执行的行动计划。例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;行动列表主要是缓解措施。为了最大程度的降低重复发生中断的几率，还应该包括一些预防性和修复性的操作。一个“预防性的”行动列表说明我们“让人不会轻易出错”。一般来说，试图改变人类行为比改变自动化系统和流程更不靠谱。（正如Dan.Milstein曾打趣道：“让我们为未来做好计划，因为未来的我们跟今天一样愚蠢。”）&lt;/li&gt;
  &lt;li&gt;所有操作项都标记了相同的优先级。没法确认首先要执行的行动。&lt;/li&gt;
  &lt;li&gt;列表前两个操作项用词模糊不清，如“改善”和“完善”。用词不当会很难衡量和理解成功的标准。&lt;/li&gt;
  &lt;li&gt;只有一个操作项标明跟踪bug。如果没有正式的跟踪过程，事后总结的行动列表往往会被遗忘，导致故障再次发生。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;用Google全天候运维副总裁Ben TreynorSloss的话来说：“对我们的用户而言，没有后续行动的事后总结和没有事后总结并无差别。因此，所有影响到用户的事后总结都必须至少有一个与它们相关的P[01]错误。例外情况由我亲自审查，但几乎没有例外。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;适得其反的指责&quot;&gt;适得其反的指责&lt;/h4&gt;
&lt;p&gt;每一次事后总结都可能会陷入相互指责中。来看一些例子：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;事情变得糟糕&lt;/strong&gt;&lt;br /&gt;
整个团队都被指责要对故障负责，尤其是这两名成员（maxone@和logantwo@）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;行动列表&lt;/strong&gt;&lt;br /&gt;
列表的最后一项指向了sydneythree@，负责跨站点轮岗。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;根本原因和触发点&lt;/strong&gt;&lt;br /&gt;
dylanfour@为本次故障全权负责。&lt;/p&gt;

&lt;p&gt;在事后总结中强调个体似乎是个好主意，但实际上，这种做法会导致团队成员不愿承担风险，因为害怕被当众羞辱。他们可能会掩盖那些防止再次发生故障的事实。&lt;/p&gt;

&lt;h4 id=&quot;animated语言&quot;&gt;Animated语言&lt;/h4&gt;
&lt;p&gt;事后总结基于事实，不应该受个人判断和主观语言的影响，应该考虑多种观点并尊重他人。示例中包含了多个反面例子：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;根本原因和触发点&lt;/strong&gt;&lt;br /&gt;
多余的语言（例如，“粗心无知”）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;事情变得糟糕&lt;/strong&gt;&lt;br /&gt;
Animated文字（例如，“这太荒谬了”）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;幸运的&lt;/strong&gt;&lt;br /&gt;
一种夸张的感叹（例如，“无法相信我们能够幸免于难！”）&lt;/p&gt;

&lt;p&gt;动画语言和对事件的戏剧性描述稀释了关键信息，降低了警惕性。应该提供可验证的数据来证明事件的严重性。&lt;/p&gt;

&lt;h4 id=&quot;缺少负责人&quot;&gt;缺少负责人&lt;/h4&gt;
&lt;p&gt;宣布官方所有权会产生问责制，从而促进采取行动。示例中有几个缺少所有权的例子：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;总结列出了四个负责人。理想情况下，负责人是单点联系人，负责事后总结，后续以及完善工作。&lt;/li&gt;
  &lt;li&gt;“行动列表”部分很少甚至没有提及各条目的负责人。没有明确负责人的行动项目不大可能会被解决。
最好的是拥有一个负责人和多个协作者。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;受众有限&quot;&gt;受众有限&lt;/h4&gt;
&lt;p&gt;示例事后总结仅在团队成员之间共享。默认情况下，公司的每个人都应该可以访问该文档。建议尽可能广泛的分享你的事后总结，甚至和客户分享。总结的价值和所创造的经验成正比。能够从过去事件中吸取教训的人越多，重复发生的可能性就越低。周密和诚实的事后总结也是恢复信任的关键。&lt;/p&gt;

&lt;p&gt;随着体验和舒适度的提高，可以将“受众”扩展到非人类。成熟的事后总结文化通常会添加机器可读标签（和其他元数据）以启用下游分析。&lt;/p&gt;

&lt;h4 id=&quot;延迟发布&quot;&gt;延迟发布&lt;/h4&gt;
&lt;p&gt;示例是在事件发生四个月后公布的。在此期间，如果事故再次发生（实际上确实发生过），团队成员可能会忘记总结中提到的关键细节。&lt;/p&gt;

&lt;h2 id=&quot;优秀的事后总结示例&quot;&gt;优秀的事后总结示例&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;这是一份真实的事后总结。个人和团队的名字均为虚构，为了保护敏感容量信息，我们使用了占位符替换实际值。在你为内部分享的事后总结中，应该包含具体的数字。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;-事后总结所有发送到diskerase的卫星机器-&quot;&gt;&lt;center&gt; 事后总结：所有发送到diskerase的卫星机器 &lt;/center&gt;&lt;/h3&gt;
  &lt;p&gt;&lt;strong&gt;2014-8-11&lt;/strong&gt;:&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;负责人&lt;/strong&gt;：&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;事后总结：maxone@，logantwo@，&lt;/li&gt;
    &lt;li&gt;数据中心自动化：sydneythree@，&lt;/li&gt;
    &lt;li&gt;网络：dylanfour@&lt;/li&gt;
    &lt;li&gt;服务器管理：finfive@&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;与以下人员共享&lt;/strong&gt;：all_engineering_employees@google.com@  &lt;br /&gt;
&lt;strong&gt;状态&lt;/strong&gt;：不可更改&lt;br /&gt;
&lt;strong&gt;事件日期&lt;/strong&gt;：2014年8月11日, 星期一，PST8PDT 17:10至17:50   &lt;br /&gt;
&lt;strong&gt;发布时间&lt;/strong&gt;：2014年8月15日, 星期五&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;执行摘要&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;：前端查询丢失。&lt;br /&gt;
部分广告未投放&lt;br /&gt;
近两天由卫星提供的服务延迟增加&lt;br /&gt;
&lt;strong&gt;根本原因&lt;/strong&gt;：自动调节系统中的某个错误导致所有机架的&lt;code class=&quot;highlighter-rouge&quot;&gt;卫星&lt;/code&gt;服务器被发送磁盘diskerase指令。导致所有卫星机器进入退役工作流程，磁盘数据被擦除。致使全球卫星前端中断。。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;故障摘要&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;故障持续时间&lt;/strong&gt;：故障：周一，8月11日，PST8PDT 17:10至17:50。
                     周三，8月13日，07:46进行重建工作，之后故障解除。 &lt;br /&gt;
&lt;strong&gt;受影响的产品&lt;/strong&gt;：前端基础设施，尤其是卫星所在地。  &lt;br /&gt;
&lt;strong&gt;受影响的产品百分比&lt;/strong&gt;：全球——所有由卫星提供服务的流量（占全球查询的60%）。 &lt;br /&gt;
&lt;strong&gt;用户影响&lt;/strong&gt;：[ ]前端查询在40分钟内有所下降（[ ]QPS在此期间处于平均值，占全球流量的百分比[ ]）。所有由卫星提供的服务延迟增加。 &lt;br /&gt;
&lt;strong&gt;收入影响&lt;/strong&gt;：目前未知确切的收入影响。 &lt;br /&gt;
&lt;strong&gt;监控&lt;/strong&gt;：黑盒报警：对于每个卫星，流量团队收到“卫星a12bcd34有过多失败的HTTP请求”报警。 &lt;br /&gt;
&lt;strong&gt;解决方案&lt;/strong&gt;：通过将所有Google的前端流量转移到核心集群，以用户请求的额外延迟为代价，迅速缓解故障。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;背景（可选）&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;影响&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;em&gt;用户影响：&lt;/em&gt;
      &lt;ul&gt;
        &lt;li&gt;[_]个前端查询在40分钟的时间段内丢失，QPS在此期间持平，占全球流量的[ ]%。我们的监控表明这是个大故障；然而数据并不可靠，监控将自身停止监控的但仍在服务的卫星视为请求被拒绝。附录描述了如何估算上述数字。。&lt;/li&gt;
        &lt;li&gt;最近两天所有由卫星提供的服务延迟增加。
          &lt;ul&gt;
            &lt;li&gt;–核心集群附近的RTT峰值[ ]ms&lt;/li&gt;
            &lt;li&gt;–对于更依赖卫星的地点（例如澳大利亚，新西兰，印度），延迟+[ ]ms&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;em&gt;收入影响：&lt;/em&gt;
      &lt;ul&gt;
        &lt;li&gt;由于请求丢失，部分广告未投放。目前尚不清楚确切的收入影响&lt;/li&gt;
        &lt;li&gt;显示和视频：由于日常波动，数据的误差较大，但我们估计故障当天有[ ]%到[ ]%的收入损失。&lt;/li&gt;
        &lt;li&gt;搜索：在17:00到18:00之间，在使用同样的误差时，有[ ]%到[ ]%的损失。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;em&gt;团队影响：&lt;/em&gt;
      &lt;ul&gt;
        &lt;li&gt;流量团队花了大约48小时全力投入重建卫星。&lt;/li&gt;
        &lt;li&gt;因为需要对过载的对等链路进行流量工程设计，NST的故障/报警负载高于正常值。&lt;/li&gt;
        &lt;li&gt;由于GFE的缓存命中率降低，某些服务可能会在前端提供更多响应。
          &lt;ul&gt;
            &lt;li&gt;—例如，请参阅线程[链接]关于[缓存依赖服务]。&lt;/li&gt;
            &lt;li&gt;—[缓存相关服务]在恢复之前，GFE的缓存命中率从[ ]%下降到[ ]%。&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;故障文件&lt;/strong&gt;  &lt;br /&gt;
  [我们的事后总结文档的链接。]&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;根本原因和触发点&lt;/strong&gt; &lt;br /&gt;
流量管理服务器中一个长期存在的输入验证bug，由于手动重新执行a12bcd34卫星的工作流程而被触发的。该bug删除了执行退役操作的机器的约束，发送并停用了所有卫星机器。 
因此，数据中心自动化执行了退役工作流程，擦除了大多数卫星机器的硬盘驱动器，在此之前无法停止这项操作。
Traffic Admin服务器提供ReleaseSatelliteMachines RPC。此处理程序使用三个MDB API调用卫星停用：&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;查找与边缘节点关联的机架名称（例如，a12bcd34 -&amp;gt;）。&lt;/li&gt;
    &lt;li&gt;查找与机架关联的机器名称（-&amp;gt;等）。&lt;/li&gt;
    &lt;li&gt;将这些计算机重新分配给diskerase，间接触发退役工作流程。&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;由于MDB API的行为以及安全检查不是幂等的。如果卫星节点先前已成功发送到decom，则上面步骤2返回一个空列表，步骤3中将其解释为机器主机名上没有约束。
这种危险行为已存在一段时间，但被调用不安全操作的工作流程隐藏：调用RPC的工作流程步骤标记为“运行一次”，意味着工作流引擎一旦成功就不会重新执行RPC。
但是，“运行一次”的语义不适用于工作流的多个实例。当集群启停团队手动启动a12bcd34的另一个工作流时，会触发admin_server bug。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;为恢复生产投入的努力&lt;/strong&gt;&lt;br /&gt;
    [我们的时间线日志的链接已被省略。在真正的事后总结中，这些信息始终包含在内。]&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;经验教训&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;事情进展顺利：
      &lt;ul&gt;
        &lt;li&gt;疏散边缘。核心中的GFE明确的进行容量规划能允许这种情况发生，生产主干也是如此（除了对等链路之外；请参阅下一节中的故障列表）。这种边缘疏散使得流量团队能迅速减缓故障。&lt;/li&gt;
        &lt;li&gt;自动减轻卫星故障。覆盖的线路自动将来自故障卫星的流量拉回到核心集群，并且当检测到异常抖动时会自行排出。&lt;/li&gt;
        &lt;li&gt;尽管可能会造成混乱，但卫星decom/diskerase工作十分高效和迅速。&lt;/li&gt;
        &lt;li&gt;故障通过OMG触发了快速的IMAG响应，并且该工具适用于持续的事件跟踪。跨团队的反应非常棒，OMG帮助大家保持交流。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;事情进展受挫：
      &lt;ul&gt;
        &lt;li&gt;故障
          &lt;ul&gt;
            &lt;li&gt;Traffic Admin服务器没有对其发送到MDB的命令进行适当的健全性检查。所有命令都应该是幂等的，或者至少在重复调用时是自动防故障的。&lt;/li&gt;
            &lt;li&gt;MDB不拒绝缺少主机名约束的所有权更改请求。&lt;/li&gt;
            &lt;li&gt;decom工作流程不与其他数据源（例如，计划的机架decom）交叉检查decom请求。因此，对已清除（地理上不同）的机器的请求没有异议。&lt;/li&gt;
            &lt;li&gt;decom工作流不受速率限制。一旦机器进入decom，磁盘擦除和其他decom步骤以最大速度进行。&lt;/li&gt;
            &lt;li&gt;当卫星停止服务时，由于出口流量转移到不同位置，Google和各国之间的一些对等链路过载，而请求是从核心集群提供服务。导致了在卫星恢复且匹配NST缓解工作之前，选择对等链路的阻塞短暂爆发。&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
        &lt;li&gt;恢复
          &lt;ul&gt;
            &lt;li&gt;重新安装卫星机器的速度很慢且不可靠。在高延迟链路末端传输到卫星时，重新安装使用TFTP传输数据效果不佳。&lt;/li&gt;
            &lt;li&gt;Autoreplacer基础架构无法在故障时设置GFE的[ ]。需要多个SRE并行手动执行设置来匹配自动化设置的速度。以下因素导致自动化的缓慢：
              &lt;ul&gt;
                &lt;li&gt;—SSH超时阻止了Autoreplacer在远程卫星上的操作。&lt;/li&gt;
                &lt;li&gt;—无论机器是否已具有正确的版本，都执行了慢速内核升级过程。&lt;/li&gt;
                &lt;li&gt;—Autoreplacer中的并发回归阻止了每个工作机器运行两个以上的机器设置任务。&lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
            &lt;li&gt;当23%的目标被移除时，没有触发监控配置的安全检查参数（25%变化），当读取相同内容（剩余的29%）时触发了。导致重新启用卫星监控延迟30分钟。&lt;/li&gt;
            &lt;li&gt;“安装人员”有限，因此，变更过程困难又缓慢。&lt;/li&gt;
            &lt;li&gt;使用超级用户权限将机器从diskerase拉回来时留下了很多僵尸进程，导致后续清理困难。&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;幸运的
      &lt;ul&gt;
        &lt;li&gt;核心集群的GFE与卫星GFE的管理方式不同。他们没有受到decom的影响。&lt;/li&gt;
        &lt;li&gt;同样，YouTube的CDN作为独立的基础设施运行，因此没有受到影响。否则故障将更加严重和持久。&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;&lt;strong&gt;行动列表&lt;/strong&gt;  &lt;br /&gt;
由于此事件的广泛性，我们将行动列表分为五个主题：&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;预防/风险教训&lt;/li&gt;
    &lt;li&gt;紧急响应&lt;/li&gt;
    &lt;li&gt;监控/报警&lt;/li&gt;
    &lt;li&gt;卫星/边缘&lt;/li&gt;
    &lt;li&gt;清理/其他&lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;表10-1.预防/风险教训&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;审核所有能够将实时服务器转换为宕机状态的系统（即，不仅仅是维修和diskerase工作流）&lt;/td&gt;
        &lt;td&gt;调查&lt;/td&gt;
        &lt;td&gt;P1&lt;/td&gt;
        &lt;td&gt;sydneythree@&lt;/td&gt;
        &lt;td&gt;BUG1234&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;提交bug，跟踪BUG1234识别出的所有系统的拒绝错误输入实施的情况。&lt;/td&gt;
        &lt;td&gt;预防&lt;/td&gt;
        &lt;td&gt;P1&lt;/td&gt;
        &lt;td&gt;sydneythree@&lt;/td&gt;
        &lt;td&gt;BUG1235&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;禁止任何单个会影响跨命名空间/类边界的服务器操作。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P1&lt;/td&gt;
        &lt;td&gt;maxone@&lt;/td&gt;
        &lt;td&gt;BUG1236&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;流量管理服务器需要进行安全检查才能在超过[ ]节点数的情况下运行。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P1&lt;/td&gt;
        &lt;td&gt;dylanfour@&lt;/td&gt;
        &lt;td&gt;BUG1237&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;流量管理服务器应该依据&lt;安全检查服务&gt;批准破坏性工作。&lt;/安全检查服务&gt;&lt;/td&gt;
        &lt;td&gt;预防&lt;/td&gt;
        &lt;td&gt;P0&lt;/td&gt;
        &lt;td&gt;logantwo@&lt;/td&gt;
        &lt;td&gt;BUG1238&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;MDB应拒绝对非预期的当前约束提供值的操作。&lt;/td&gt;
        &lt;td&gt;预防&lt;/td&gt;
        &lt;td&gt;P0&lt;/td&gt;
        &lt;td&gt;louseven@&lt;/td&gt;
        &lt;td&gt;BUG1239&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;表10-2.紧急响应&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;确保由核心提供的服务不会使出口网络链路过载。&lt;/td&gt;
        &lt;td&gt;修复&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;rileyslx@&lt;/td&gt;
        &lt;td&gt;BUG1240&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;确保在[我们的紧急停止文档]和[我们的升级联系页面]下注明了decom工作流问题。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;logantwo@&lt;/td&gt;
        &lt;td&gt;BUG1241&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;为decom工作流添加一个大红色禁用按钮a&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P0&lt;/td&gt;
        &lt;td&gt;maxone@&lt;/td&gt;
        &lt;td&gt;BUG1242&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;a : 由于灾难性环境中避免进一步损坏的关闭开关（例如，紧急电源关闭按钮）的常见术语。
表10-3.监控/报警&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;监控目标的安全检查，不允许发布无法回滚的变更。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;dylanfour@&lt;/td&gt;
        &lt;td&gt;BUG1243&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;当超过[ ]%的机器退役需要添加报警。16:38机器从卫星上取下，17:10开始世界范围的报警。&lt;/td&gt;
        &lt;td&gt;监测&lt;/td&gt;
        &lt;td&gt;P1&lt;/td&gt;
        &lt;td&gt;rileyslx@&lt;/td&gt;
        &lt;td&gt;BUG1244&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;表10-4.卫星/边缘&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;利用IPXE配合HTTPS可以使重新安装更快更可靠。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;dylanfour@&lt;/td&gt;
        &lt;td&gt;BUG1245&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;表10-5.清理/其他&lt;/p&gt;

  &lt;table&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;行动列表&lt;/th&gt;
        &lt;th&gt;类型&lt;/th&gt;
        &lt;th&gt;优先级&lt;/th&gt;
        &lt;th&gt;负责人&lt;/th&gt;
        &lt;th&gt;bug跟踪&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;在我们的工具中查看与MDB相关的代码，并将管理服务器备份放至unwedge调节中。&lt;/td&gt;
        &lt;td&gt;修复&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;rileyslx@&lt;/td&gt;
        &lt;td&gt;BUG1246&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;安排DiRT测试：在diskerase后带回卫星；对YouTube CDN执行相同操作。&lt;/td&gt;
        &lt;td&gt;减缓&lt;/td&gt;
        &lt;td&gt;P2&lt;/td&gt;
        &lt;td&gt;louseven@&lt;/td&gt;
        &lt;td&gt;BUG1247&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

  &lt;p&gt;&lt;strong&gt;名词解释&lt;/strong&gt; &lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;管理服务器&lt;/code&gt;&lt;br /&gt;
RPC服务器，支持自动化为前端服务基础结构执行特权操作。自动化服务器常参与PCR和集群启停操作。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Autoreplacer&lt;/code&gt;&lt;br /&gt;
将非Borgified服务器从一台机器移动到另一台机器的系统。在机器故障时保持服务运行，并且支持colo重新配置。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Borg&lt;/code&gt;
集群管理系统，旨在管理大规模任务和机器资源。Borg拥有Borg单元中所有机器，并将任务分配给具有可用资源的机器。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Decom&lt;/code&gt;&lt;br /&gt;
退役的缩写。设备的decom是一个与许多运维团队相关的过程。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Diskerase&lt;/code&gt;&lt;br /&gt;
在生产硬盘驱动器离开Google数据中心前安全擦除生产硬盘的过程（以及相关的硬件/软件系统）。diskerase是decom工作流的一个步骤。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GFE（Google前端）&lt;/code&gt;&lt;br /&gt;
外部连接(几乎)所有谷歌服务的服务器。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;*IMAG（Google事件管理）&lt;/code&gt;&lt;br /&gt;
一个程序，一种标准，以一致的方式来处理从系统中断到自然灾害的所有类型的事件——并组织有效的响应。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MDB（机器数据库）&lt;/code&gt;&lt;br /&gt;
事件管理仪表盘/工具，用于跟踪和管理Google所有正在进行的事件的中心位置。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;卫星&lt;/code&gt;&lt;br /&gt;
小巧便宜的机器机架，仅提供提供来自Google网络边缘的非视频、前端流量。几乎没有传统的生产集群基础设施可用于卫星。卫星不同于CDN，它提供来自Google边缘网络的YouTube视频内容，以及来自互联网中更广泛的其他地方的视频内容。YouTube CDN未受此事件的影响。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;附录&lt;/strong&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;为什么释放卫星机器不是幂等的？&lt;/code&gt; &lt;br /&gt;
[该问题的回复已被删除]&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;管理服务器将所有卫星分配给diskerase团队后发生了什么？&lt;/code&gt; &lt;br /&gt;
[该问题的回复已被删除]&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;故障期间真正的QPS损失是多少？&lt;/code&gt; &lt;br /&gt;
[该问题的回复已被删除]&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;IRC日志&lt;/code&gt;&lt;br /&gt;
[IRC日志已被删除]&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;图表&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;更快的延迟统计——卫星曾为我们做过什么？&lt;/code&gt;&lt;br /&gt;
从这次故障经验得到，卫星会在核心集群附近的许多位置产生[ ]ms延迟，离主干更远的位置甚至会达到[ ]ms：&lt;br /&gt;
[图表的解释已被删除]&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;核心与边缘服务负载&lt;/code&gt;&lt;br /&gt;
为重建服务所付出的努力是个很好的例证。边缘服务恢复50%的流量需要大约36小时，恢复到正常的流量水平需要额外的12小时（见图10-1和图10-2）。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;来自流量转换的对等压力&lt;/code&gt; 
 [图表省略]&lt;br /&gt;
 该图显示了由网络区域聚合的数据包丢失情况。在活动期间有一些短的尖峰，但大部分损失发生在卫星覆盖少的各个地区的高峰时刻。&lt;/p&gt;

  &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;人与机器，GFE&lt;/code&gt;&lt;br /&gt;
 [省略了人机与自动机器设置速率的图表说明。]&lt;br /&gt;
&lt;img src=&quot;/blog/something/images/SRE/10-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图10-1.故障期间核心与边缘QPS分布 &lt;/center&gt;&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/10-2.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图10-2.故障期间核心与边缘QPS分布（替代表示） &lt;/center&gt;&lt;/p&gt;

  &lt;h3 id=&quot;为什么这份事后总结示例更好&quot;&gt;为什么这份事后总结示例更好？&lt;/h3&gt;
  &lt;p&gt;这个事后总结符合了好几条写作要求。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;明晰&quot;&gt;明晰&lt;/h3&gt;
&lt;p&gt;事后总结组织的很好，详细解释了关键术语。例如：&lt;/p&gt;

&lt;h4 id=&quot;名词解释&quot;&gt;名词解释&lt;/h4&gt;
&lt;p&gt;一个精心编写的名词解释让事后总结更容易被大众接受和理解。&lt;/p&gt;

&lt;h4 id=&quot;行动列表&quot;&gt;行动列表&lt;/h4&gt;
&lt;p&gt;这是一个涉及许多小的行动列表的大事件。按照主题对操作项进行分组可以更加轻松的分配负责人和优先级。&lt;/p&gt;

&lt;h4 id=&quot;量化指标&quot;&gt;量化指标&lt;/h4&gt;
&lt;p&gt;事后总结提供了相关事件的有用数据，例如缓存命中率、流量级别和影响持续时间。数据的相关部分将显示原始来源的链接。这种数据透明性消除了歧义并为读者提供了上下文参考。&lt;/p&gt;

&lt;h3 id=&quot;具体行动列表&quot;&gt;具体行动列表&lt;/h3&gt;
&lt;p&gt;没有行动列表的事后总结是无效的。这些行动列表有一些显著特点：&lt;/p&gt;

&lt;h4 id=&quot;负责人&quot;&gt;负责人&lt;/h4&gt;
&lt;p&gt;所以操作项都有负责人和bug跟踪号。&lt;/p&gt;

&lt;h4 id=&quot;优先级&quot;&gt;优先级&lt;/h4&gt;
&lt;p&gt;为所有操作项分配优先级。&lt;/p&gt;

&lt;h4 id=&quot;可测性&quot;&gt;可测性&lt;/h4&gt;
&lt;p&gt;操作项具有可验证的最终状态（例如，“当我们的机器中超过X%的机器退役时添加报警”）。&lt;/p&gt;

&lt;h4 id=&quot;预防措施&quot;&gt;预防措施&lt;/h4&gt;
&lt;p&gt;每个操作项“主题”都有预防/缓解操作项，这些操作项有助于避免故障重复发生（例如，“禁止任何单个会影响跨命名空间/类边界的服务器操作。”）&lt;/p&gt;

&lt;h3 id=&quot;不指责&quot;&gt;不指责&lt;/h3&gt;
&lt;p&gt;作者关注的是系统设计中的差距，正是这些差距导致了非预期的故障，例如：&lt;/p&gt;

&lt;h4 id=&quot;事情进展受挫&quot;&gt;事情进展受挫&lt;/h4&gt;
&lt;p&gt;没有任何人或团队因此事件受到指责。&lt;/p&gt;

&lt;h4 id=&quot;根本原因和触发点&quot;&gt;根本原因和触发点&lt;/h4&gt;
&lt;p&gt;关注“什么”出了问题，而不是“谁”造成了这一问题。&lt;/p&gt;

&lt;h4 id=&quot;行动列表-1&quot;&gt;行动列表&lt;/h4&gt;
&lt;p&gt;旨在改善系统而不是改善人。&lt;/p&gt;

&lt;h3 id=&quot;深度&quot;&gt;深度&lt;/h3&gt;
&lt;p&gt;事后调查不仅仅是调查系统故障的近似区域，也研究了多个团队的影响和系统缺陷。尤其：&lt;/p&gt;

&lt;h4 id=&quot;影响&quot;&gt;影响&lt;/h4&gt;
&lt;p&gt;本节包含来自不同视角的大量细节，尽可能的做到平衡，客观。&lt;/p&gt;

&lt;h4 id=&quot;根本原因和触发点-1&quot;&gt;根本原因和触发点&lt;/h4&gt;
&lt;p&gt;本节对事件进行深入研究，找到根本原因和触发点。&lt;/p&gt;

&lt;h4 id=&quot;由数据推及结论&quot;&gt;由数据推及结论&lt;/h4&gt;
&lt;p&gt;提出的所有结论均基于事实和数据。用于得出结论的数据都和文档相关联。&lt;/p&gt;

&lt;h4 id=&quot;其他资源&quot;&gt;其他资源&lt;/h4&gt;
&lt;p&gt;以图表的形式进一步呈现有用的信息。向不熟悉系统的读者解释图表帮助其理解上下文。&lt;/p&gt;

&lt;h3 id=&quot;及时&quot;&gt;及时&lt;/h3&gt;
&lt;p&gt;事件结束后不到一星期就写完并传播了事后总结。快速的事后总结往往更加准确，因为此时任何参与者心理都记着这件事。而受故障影响的人正在等待一个解释，证明你们已经控制了故障。等待的时间越长，他们就越能散发想象，那样对你十分不利。&lt;/p&gt;

&lt;h3 id=&quot;简明&quot;&gt;简明&lt;/h3&gt;
&lt;p&gt;该事件是全球范围的事件，影响多个系统。事后总结记录了且随后分析了大量数据。冗长的数据源（例如聊天记录和系统日志）被抽象化，未经编辑的版本可从主文档中链接到。总体而言，这份总结在冗长和可读性之间取得了平衡。&lt;/p&gt;

&lt;h2 id=&quot;组织激励&quot;&gt;组织激励&lt;/h2&gt;
&lt;p&gt;理想情况下，高级领导应该支持和鼓励有效的事后总结。本节描述了一个组织如何激励健康的事后总结文化。我们着重描述了总结文化失败的征兆，并给出了一些解决方案。同时还提供了工具和模板来简化和自动化事后处理流程。&lt;/p&gt;

&lt;h3 id=&quot;模型以及对事不对人&quot;&gt;模型以及对事不对人&lt;/h3&gt;
&lt;p&gt;为了正确的支持事后总结文化，领导者应始终如一的坚持对事不对人的原则，并在事后讨论中鼓励对事不对人。可以使用一些具体策略来强制组织执行对事不对人这一准则。&lt;/p&gt;

&lt;h4 id=&quot;使用对事不对人的语言&quot;&gt;使用对事不对人的语言&lt;/h4&gt;
&lt;p&gt;指责性的语言会影响团队协作。请考虑以下情况：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sandy 错过了服务Foo培训，且不确定如何运行特定的更新命令。因而导致故障时间的延长。
SRE Jesse [对Sandy的leader说]:“你是经理，为什么不确保每个人都完成培训？”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个交流凸显了一个主要问题，即让收件人处于劣势。更平衡的回应是：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;SRE Jesse [对Sandy的leader]：“看完事后总结，能够注意到on-call错过了一次重要的培训，导致没有更快的解决故障。因此是否应该要求团队成员加入on-call轮转之前都完成此培训？或者是否可以提醒on-call，如果操作卡住可以尽快升级事件。毕竟，升级不是错误——尤其它有助于降低客户的负担！从长远来看，因为一些细节很容易被忘记，因此我们不能完全依赖培训。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;事后总结的创作要包含所有事件参与者&quot;&gt;事后总结的创作要包含所有事件参与者&lt;/h4&gt;
&lt;p&gt;当事后总结是单人或由单个团队编写时，很容易忽略导致故障的关键因素。&lt;/p&gt;

&lt;h4 id=&quot;收集反馈&quot;&gt;收集反馈&lt;/h4&gt;
&lt;p&gt;明确的审查过程和事后计划可以帮助防止指责的语言和观点在组织内传播。有关的结构化审核流程请参阅第221页的“事后检查清单”部分。&lt;/p&gt;

&lt;h3 id=&quot;奖励事后总结&quot;&gt;奖励事后总结&lt;/h3&gt;
&lt;p&gt;事后总结培训是积极推动组织变革和防止重复故障的有效工具。如果写的够好，采取行动并广泛分享，可以考虑以下策略来激励事后总结文化。&lt;/p&gt;

&lt;h4 id=&quot;对完成行动列表进行奖励&quot;&gt;对完成行动列表进行奖励&lt;/h4&gt;
&lt;p&gt;如果你奖励工程师编写事后总结而不是完成了相关的行动列表，那么可能会出现总结中的行动项目未完成的事情。需要在编写事后总结和成功完成行动计划间平衡奖励措施。&lt;/p&gt;

&lt;h4 id=&quot;对积极的组织变革进行奖励&quot;&gt;对积极的组织变革进行奖励&lt;/h4&gt;
&lt;p&gt;你可以将事后总结的重要性作为提高组织影响的依据，通过对标奖金、积极的绩效评估、晋升等作为奖励。来激励并广泛实施事后总结教训。&lt;/p&gt;

&lt;h4 id=&quot;突出提高可靠性&quot;&gt;突出提高可靠性&lt;/h4&gt;
&lt;p&gt;随着时间的推移，有效的事后总结可以减少故障，让系统更加可靠。因此，团队可以专注于特性功能的开发速度，而不是基础架构的修补上。在总结、演示文稿和绩效评估中强调这些改进在本质上是会提供动力的。&lt;/p&gt;

&lt;h4 id=&quot;把事后总结的负责人当做领导者&quot;&gt;把事后总结的负责人当做领导者&lt;/h4&gt;
&lt;p&gt;通过电子邮件或会议完成事后总结，或者通过给作者向受众提供经验教训的机会，能够吸引那些喜欢公共赞誉的人。对于寻求同行认可的工程师而言，可以将负责人设置为某种类型的故障的“专家”。例如，你可能会听到有人说：“和Sara说，她现在是专家了。她参与了事后总结的撰写，并且想出了解决问题的方法！”&lt;/p&gt;

&lt;h4 id=&quot;游戏化&quot;&gt;游戏化&lt;/h4&gt;
&lt;p&gt;一些人会被成就感和更远大的目标所激励，例如修复系统薄弱点和提高可靠性。对于这些人而言，事后总结行动列表的记录或完成所获得的成就已经是奖励了。在Google，我们每年举办两次“FixIt”周。完成最重要的行动列表项目的SRE会收到小额赞赏和吹牛的权利。图10-3显示了一个事后总结排行榜的示例。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/10-3.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图10-3.事后总结排行榜 &lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&quot;公开分享事后总结&quot;&gt;公开分享事后总结&lt;/h3&gt;
&lt;p&gt;为了在组织内保持健康的事后总结文化，要尽可能广泛的分享事后总结。以下策略可以提供一些帮助。&lt;/p&gt;

&lt;h4 id=&quot;在整个组织内分享总结&quot;&gt;在整个组织内分享总结&lt;/h4&gt;
&lt;p&gt;在内部沟通渠道、电子邮件、Slack等中宣传事后总结的可用性。如果你负责一个公司，可以分享一个最近的有趣的事后总结。&lt;/p&gt;

&lt;h4 id=&quot;进行跨团队审核&quot;&gt;进行跨团队审核&lt;/h4&gt;
&lt;p&gt;对事后总结进行跨团队审查。过程中，一个团队过一遍故障，其他团队提出问题并间接学习。在Google，几个办公室都设有非正式的事后总结俱乐部，向所有员工开放。&lt;/p&gt;

&lt;p&gt;此外，由开发人员、SRE和组织领导组成的跨职能小组审核整个事后总结流程。他们每个月都会审查事后总结过程和模板的有效性。&lt;/p&gt;

&lt;h4 id=&quot;进行培训练习&quot;&gt;进行培训练习&lt;/h4&gt;
&lt;p&gt;使用“命运之轮”训练新入职工程师：一群工程师重新扮演事后总结的角色，当时的事故总控负责人也参与其中，确保这次演习尽可能的“真实”。&lt;/p&gt;

&lt;h4 id=&quot;每周总结事件和故障&quot;&gt;每周总结事件和故障&lt;/h4&gt;
&lt;p&gt;每周对过去七天内发生的事件和故障的进行总结，并尽可能广泛的进行分享。&lt;/p&gt;

&lt;h3 id=&quot;响应事后总结文化的失败&quot;&gt;响应事后总结文化的失败&lt;/h3&gt;
&lt;p&gt;事后总结文化的崩溃可能并不明显，以下介绍了常见的故障模式和推荐的解决方案。&lt;/p&gt;

&lt;h4 id=&quot;逃避&quot;&gt;逃避&lt;/h4&gt;
&lt;p&gt;逃避事后总结过程可能是一个组织的事后总结文化失败的征兆，例如，假设SRE 主管 Parker无意中听到以下对话：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;SWE Sam：哇，你听说了这次的大故障了吗？&lt;br /&gt;
SWE Riley：听说了，太可怕了。他们现在得写一个事后总结了。&lt;br /&gt;
SWE Sam：不是吧，还好我没有参与进去。&lt;br /&gt;
SWE Riley：对啊，我是真的不想参与那个讨论会议。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;确保对产生这些抱怨的事件进行高可见度的事后总结可以避免这种逃避。此外，分享高质量的案例并讨论参与的人如何获得奖励将有助于重新团结每个人。。&lt;/p&gt;

&lt;h4 id=&quot;没有强调文化&quot;&gt;没有强调文化&lt;/h4&gt;
&lt;p&gt;当高级管理人员使用责备的语言做出回应可能会使事情更糟糕。假设一个高级领导在会议上对故障做出以下声明：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;VP Ash：我知道应该对事不对人，但有人事先知道这个操作可能会产生问题，你为什么不听那个人的？&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以通过更有建设性的话术来减少损害，例如：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;SRE Dana：我确信每个人的出发点都是好的，所以为了保持对事不对人的准则，我们一般会这样问：是否有任何应该注意到的告警以及我们为何会忽略它们。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于组织而言，立足正确的出发点，并根据现有的最佳信息做出决策，调查误导性信息的来源比分配责任更有帮助。（如果你知道敏捷原则，那么你应该对这个更加清楚。）&lt;/p&gt;

&lt;h4 id=&quot;没有时间写事后总结&quot;&gt;没有时间写事后总结&lt;/h4&gt;
&lt;p&gt;优质的事后总结撰写是需要时间的。当一个团队负担其他任务时，总结的质量会受到影响。低质量的没有完整的行动列表的事后总结会更容易导致故障复发。事后总结是你写给团队未来成员的信件：以免你不小心教给未来的队友一个错误的教训，保持一致的质量标准十分重要。应该优先考虑事后检查工作，跟踪事后完成情况和审查，并让团队有足够的时间来实施相关的行动计划。我们在第220页的“工具和模板”一节中讨论的工具可以帮助完成此项活动。&lt;/p&gt;

&lt;h4 id=&quot;重复故障&quot;&gt;重复故障&lt;/h4&gt;
&lt;p&gt;如果团队在遇到类似故障时采用以前的经验失败了，那么就该深入挖掘了。要考虑以下问题：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;行动列表项目是否需要很长时间才能完成？&lt;/li&gt;
  &lt;li&gt;故障速度是否超过了可靠性的修复速度？&lt;/li&gt;
  &lt;li&gt;最先获得的是正确的行动项目吗？&lt;/li&gt;
  &lt;li&gt;重构的故障服务是否过期？&lt;/li&gt;
  &lt;li&gt;是否把Band-Aids定位成更严重的问题上了？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果你发现了系统性流程或技术问题，应该退一步考虑整体服务运行状况。将每个类似事件的事后总结作者聚集到一起，讨论防止故障重复发生的最佳行动方案。&lt;/p&gt;

&lt;h2 id=&quot;工具和模板&quot;&gt;工具和模板&lt;/h2&gt;
&lt;p&gt;一组工具和模板可以让编写事后总结和管理相关数据变得更轻松，从而引导事后总结文化。在这一领域，你可以利用Google和其他公司提供的大量资源。&lt;/p&gt;

&lt;h3 id=&quot;事后总结模板&quot;&gt;事后总结模板&lt;/h3&gt;
&lt;p&gt;模板可以使编写和分享完整的事后总结更加轻松。使用标准格式可以使非专业的读者更容易理解事后处理过程。你可以自定义模板。例如，获取特定团队的元数据（如数据中心团队的硬件品牌/型号）或受移动团队影响的Android版本可能更有用。随着团队在这方面的成熟，还可以自定义模板。&lt;/p&gt;

&lt;h4 id=&quot;google模板&quot;&gt;Google模板&lt;/h4&gt;
&lt;p&gt;Google已经通过&lt;a href=&quot;http://g.co/SiteReliabilityWorkbookMaterials&quot;&gt;http://g.co/SiteReliabilityWorkbookMaterials&lt;/a&gt;以Google文档格式分享了我们的事后模板。我们在内部主要使用Docs来编写事后总结，可以通过共享编辑权限和注释促进合作。我们的一些内部工具可以使用元数据预填充此模板让事后总结更容易编写。我们利用Google Apps脚本自动化部分创作，并将大量数据捕获到特定的部分和表格中，以便我们的事后总结存储库更容易解析数据。&lt;/p&gt;

&lt;h4 id=&quot;其他行业模板&quot;&gt;其他行业模板&lt;/h4&gt;
&lt;p&gt;其他几家公司和个人分享了他们的事后总结模板：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;报警职责&lt;/li&gt;
  &lt;li&gt;改编原始的Google可靠性站点工程书籍模型&lt;/li&gt;
  &lt;li&gt;GitHub上托管的四个模板列表&lt;/li&gt;
  &lt;li&gt;GitHub用户Julian Dunn&lt;/li&gt;
  &lt;li&gt;服务器故障&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;事后总结工具&quot;&gt;事后总结工具&lt;/h3&gt;
&lt;p&gt;在撰写本文时，Google的事后总结管理工具无法供外部使用（请查看我们的博客获取最新更新）。但是，我们可以解释我们的工具是如何促进事后总结文化的。&lt;/p&gt;

&lt;h4 id=&quot;事件管理工具&quot;&gt;事件管理工具&lt;/h4&gt;
&lt;p&gt;我们的事件管理工具收集并存储大量关于故障的有用数据，并将该数据自动推动到事后总结模板中。我们推送的数据类型包括：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;故障指挥人和其他角色&lt;/li&gt;
  &lt;li&gt;详细的事件时间表和IRC日志&lt;/li&gt;
  &lt;li&gt;受影响的服务和导致根本原因的服务&lt;/li&gt;
  &lt;li&gt;事件严重性&lt;/li&gt;
  &lt;li&gt;事件检测机制&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;事后总结清单&quot;&gt;事后总结清单&lt;/h4&gt;
&lt;p&gt;为了帮助作者确保正确完成事后检查，我们提供了一个事后检查清单，通过关键步骤引导负责人。以下是列表中的一些示例检查：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;对事件影响进行全面评估。&lt;/li&gt;
  &lt;li&gt;进行足够详细的根本原因分析，推动行动列表的规划。&lt;/li&gt;
  &lt;li&gt;确保行动列表项目通过服务技术主管的审查和批准。&lt;/li&gt;
  &lt;li&gt;和更多的组织分享事后总结。
完整的清单可在&lt;a href=&quot;http://g.co/SiteReliabilityWorkbookMaterials&quot;&gt;http://g.co/SiteReliabilityWorkbookMaterials&lt;/a&gt;找到。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;归档事后总结&quot;&gt;归档事后总结&lt;/h4&gt;
&lt;p&gt;我们将事后总结归档在一个名为Requiem的工具中，这样任何Google员工都可以轻松找到它们。我们的事件管理工具会自动将所有事后总结推送到Requiem，组织中的任何人都可以发布他们的事后总结给所有人查看。我们有成千上万的总结存档，可以追溯到2009年。Requiem会解析个人事后总结的元数据，使其可以用于搜索、分析和总结。&lt;/p&gt;

&lt;h4 id=&quot;跟进事后总结&quot;&gt;跟进事后总结&lt;/h4&gt;
&lt;p&gt;我们的事后总结归档在Requiem的数据库中。任何生成的操作项都会在我们的集中式bug跟踪系统中归档为bug。因此，我们可以监控每个事后总结的行动项目的结束与否。通过这种级别的跟踪，可以确保行动项目不会有漏洞以致服务越来越不稳定。图10-4显示了由我们的工具启用的事后总结操作项监控的模型。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/10-4.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图10-4.事后总结行动项目监控 &lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&quot;事后总结分析&quot;&gt;事后总结分析&lt;/h4&gt;
&lt;p&gt;我们的事后总结管理工具将信息存储在数据库中以供分析。团队可以使用这些数据编写有关其事后趋势的总结，并确定易受攻击的系统。这有助于我们发现潜在的不稳定因素或可能被忽略的故障管理障碍。例如，图10-5显示了使用我们的分析工具构建的图表。这些图表显示了我们每个组织每月有多少次事后追踪、事件平均持续时间、检测时间、解决时间和故障半径的趋势。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/10-5.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图10-5.事后总结分析 &lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&quot;其他行业工具&quot;&gt;其他行业工具&lt;/h4&gt;
&lt;p&gt;以下是一些可以帮助你创建、组织和分析事后总结的第三方工具：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;报警职责事后调查&lt;/li&gt;
  &lt;li&gt;Etsy的档案室&lt;/li&gt;
  &lt;li&gt;VictorOps
尽管完全实现自动化编写事后总结是不可能的，但我们发现事后总结模板和工具会使整个流程更加顺畅。这些工具可以节省时间，让作者能够专注于事后的关键部分，例如根本原因的分析和行动项目计划。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;
&lt;p&gt;在培养事后总结文化的持续投资中，能够减少故障，为用户提供更好的体验，以及让依赖你的人对你更加信任。这些实践的应用可以使系统设计更完善、宕机时间更短、工程师工作效率更高且工作更快乐。如果最坏的情况确实发生并且故障再次发生，那么你受到的损失会更小并且恢复的更快，而且有更多的数据帮助健壮生产环境。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">经验表明，真正对事不对人的事后总结文化可以带来更可靠的系统——这也是我们认为这种做法对于创建和维护一个成功的SRE组织十分重要的原因。 将事后总结引入组织既是一种文化变革，也是一种技术变革。这样的转变似乎是令人生畏的，本章的关键点是，做出这种改变是可能的，不是一个难以克服的挑战。不要指望你的系统能够自行解决问题。你可以从引入一个非常基础的事后调查程序开始，反映和调整你的流程来适应你的组织——跟很多事情一样，没有放之四海而皆准的方法。 如果总结的很好，可以采取行动并广泛分享，事后总结可以成为积极推动组织变革和防止重复故障的利器。为了可以说明优秀的事后总结的写作原则，本章介绍了一个曾经发生在Google的故障案例进行研究。一个糟糕的事后总结的案例可以突显出为什么“糟糕”的事后总结对于努力创建健康的事后分析文化的组织是有害的。将糟糕的事后总结与实际事后总结进行比较后，可以突出高质量事后总结的原则和最佳实践。 本章的第二部分分享了我们在实现强大的事后总结文化激励机制以及识别（和补救）文化破裂的预兆方面所学到的知识。 最后，我们提供了用于引导事后总结文化的工具和模板。 有关对事不对人的事后总结哲学的全部讨论，请参阅我们的第一本书“站点可靠性工程”中的第15章。 案例分析 本案例研究的是例行机架退役导致用户服务延迟增加的case。我们的自动化维护系统的一个bug，加上限速不足，导致数千台承载生产流量的服务器同时宕机。 虽然Google的大多数服务器都位于我们的专有数据中心，但我们在托管设施（或“colos”）中也有机架代理/缓存机器。colos中包含我们代理机器的机架被称为卫星，由于卫星经常进行例行维护和升级，所以在任何时间点都会安装或退役许多卫星机架。在Google，这些维护流程基本是自动化的。 退役过程使用我们称为diskerase流程覆盖机架中所有驱动器的全部内容。一旦机器进入diskerase，它曾存储的数据将不再可检索。典型的机架退役步骤如下： 获取卫星上所有活跃的机器 machines = GetMachines(satellite) 将所有通过“filter”匹配到的候选机器发送decom SendToDecom(candidates = GetAllSatelliteMachines(), filter=machines) 我们的案例研究始于一个标记为退役的卫星机架。退役过程的diskerase步骤已成功完成，但负责剩余机器退役的自动化失败了。为了调试失败，我们重新尝试了停用过程。第二次退役如下： 获取卫星上所有活跃的机器 machines = GetMachines(satellite) 因为decom流已经在运行了，因此”machines”是一个空的列表 API bug：anmpty list被视为“无过滤，即为在全部机器上运行”，而不是“不在任何机器上运行” 将所有通过“filter”匹配到的候选机器发送到decom SendToDecom(candidates=GetAllSatelliteMachines(), filter=machines) 将所有候选机器发送到diskerase。 几分钟内，全球所有卫星机架的磁盘数据都被删除。这些机器处于不可用状态，无法接受用户的连接，因此后续用户的连接直接路由到我们的数据中心。结果导致用户延迟增加。得益于良好的容量规划，很少有用户注意到这两天我们在受影响的colo机架中重新安装机器。事件发生后，我们花了几周的时间进行审核，并为自动化添加了更多的健全性检查，使我们的退役工作流程具有幂等性（其任意多次执行所产生的影响均与一次执行的影响相同）。 故障发生的三年后，我们遇到了类似的事件：由于许多卫星被耗尽导致用户延迟增加。根据原始事后总结，本次事件后实施的行动大大减小了二次事故的影响范围和速度。 假设你是负责为此案例研究撰写事后总结总结的人。你想了解什么，以及你会采取什么行动来防止此类故障再次发生？ 让我们从这个事件的一个糟糕的事后总结示例开始。 糟糕的事后总结示例： 事后总结：所有卫星机器进入diskerase流程 2014-8-11: 负责人：maxone@，logantwo@， sydneythree@，dylanfour@ 与以下人员共享：satellite-infra-team@ 状态：不可更改 事件日期：2014年8月11日 发布时间：2014年12月30日 执行摘要 影响：所有卫星机器都被发送到diskerase，实际导致Google Edge不可用。 根本原因：dylanfour@忽略了自动化设置并手动运行了集群启动逻辑，从而触发了现有bug。 故障摘要 故障持续时间：40分钟 受影响的产品：satellite-infra-team 受影响的产品百分比：所有卫星集群。 用户影响：进入卫星的所有查询都是由Core提供的，导致延迟增加。 收入影响：由于查询异常，部分广告未投放。目前确切的收入影响未知。 监控：监控报警。 解决方案：将流量转移到核心，然后手动修复边缘集群。 背景（可选） 影响 用户影响： 进入卫星的所有查询都是由Core提供的，导致延迟增加。 收入影响： 由于查询异常，部分广告未投放。 根本原因和触发点 集群开启和关闭的自动化并不意味着是幂等的，该工具有安全措施确保某些步骤不能多次运行。但无法阻止某个人手动多次运行代码，也没有文件提及这个问题。因此，大多数团队成员认为工具失效时，可以多次运行该过程。 正好在机架的常规退役期间发生这种情况。机架正在被一个新的基于lota的卫星取代。dylanfour@忽略了已经执行了一次启动并在第一次执行时出现问题。由于粗心以及无知，他们引发了一个错误的交互，即将所有卫星机器分配给了弃用团队。 为恢复生产投入的努力 经验教训 事情进展顺利： 报警及时发现问题。 事故处理进展顺利。 事情进展受挫： 团队（特别是maxone@，logantwo@）从未编写任何文档告知SRE不要多次运行自动化。 on-call处理的不够及时，没有阻止大多数卫星机器被删除。这已经不是第一次出现处理故障不及时的情况了。 幸运的 Core能够为原本进入边缘集群的所有流量提供服务。简直无法相信我们这次能幸免于难！ 行动列表 行动列表 类型 优先级 负责人 bug跟踪 完善自动化工具。 缓解 P2 logantwo@   报警改善 检测 P2     sydneythree@ 需要学习正确的跨站点轮值文档，避免发生重复的问题 缓解 P2   BUG6789 训练大家不要运行不安全的命令 避免 P2     名词解释 为什么这份总结十分糟糕？ 示例包含一些我们试图避免的常见故障模式。以下说明如何改进这份事后总结。 缺少上下文 从一开始，作为示例的事后总结引入了特定于流量服务的术语（例如，卫星）和Google机器管理自动化的较低层（例如，“diskerase”）。如果你需要提供其他上下文作为总结的一部分，请在背景或名词解释部分进行添加（可以链接到更长的文档）。上述总结在这两部分都是空白的。 如果在编写事后总结时没有将内容置于正确的上下文中，文档可能会被误解或被忽略。谨记，你的受众并不只仅仅是有着直接关系的团队。 省略了关键细节 多个部分包含摘要但缺少重要细节。例如： 问题总结 针对多个服务的中断，应该提供数字来表示影响范围。示例中唯一的数据是故障的持续时间。没有足够的细节来评估故障的规模或影响。即使没有一个具体值，一个估计的值也比没有数据好。毕竟，你如果不知道如何衡量它，那么你也不知道它是否被修复了！ 根本原因和触发点 确定根本原因和触发点是编写事后总结的重要原因之一。示例包含一个描述根本原因和触发点的小段落，但没有探索故障的底层细节。 为恢复生产投入的努力 对读者来说，事后总结是事件记录。一份好的事后总结能让读者知道发生了什么，该如何减轻问题，以及用户受到了什么影响。很多问题的答案通常在投入恢复的努力中可以找到，但示例中，这部分是空的。 如果故障值得写入事后总结中，那么你还应该花时间准确获取并记录必要的详细信息。读者可以全面了解停机情况，更重要的是可以了解新事物。 缺失了关键行动列表 示例的“行动列表”（AIs）部分缺少防止此类故障再次发生的可执行的行动计划。例如： 行动列表主要是缓解措施。为了最大程度的降低重复发生中断的几率，还应该包括一些预防性和修复性的操作。一个“预防性的”行动列表说明我们“让人不会轻易出错”。一般来说，试图改变人类行为比改变自动化系统和流程更不靠谱。（正如Dan.Milstein曾打趣道：“让我们为未来做好计划，因为未来的我们跟今天一样愚蠢。”） 所有操作项都标记了相同的优先级。没法确认首先要执行的行动。 列表前两个操作项用词模糊不清，如“改善”和“完善”。用词不当会很难衡量和理解成功的标准。 只有一个操作项标明跟踪bug。如果没有正式的跟踪过程，事后总结的行动列表往往会被遗忘，导致故障再次发生。 用Google全天候运维副总裁Ben TreynorSloss的话来说：“对我们的用户而言，没有后续行动的事后总结和没有事后总结并无差别。因此，所有影响到用户的事后总结都必须至少有一个与它们相关的P[01]错误。例外情况由我亲自审查，但几乎没有例外。” 适得其反的指责 每一次事后总结都可能会陷入相互指责中。来看一些例子： 事情变得糟糕 整个团队都被指责要对故障负责，尤其是这两名成员（maxone@和logantwo@）。 行动列表 列表的最后一项指向了sydneythree@，负责跨站点轮岗。 根本原因和触发点 dylanfour@为本次故障全权负责。 在事后总结中强调个体似乎是个好主意，但实际上，这种做法会导致团队成员不愿承担风险，因为害怕被当众羞辱。他们可能会掩盖那些防止再次发生故障的事实。 Animated语言 事后总结基于事实，不应该受个人判断和主观语言的影响，应该考虑多种观点并尊重他人。示例中包含了多个反面例子： 根本原因和触发点 多余的语言（例如，“粗心无知”） 事情变得糟糕 Animated文字（例如，“这太荒谬了”） 幸运的 一种夸张的感叹（例如，“无法相信我们能够幸免于难！”） 动画语言和对事件的戏剧性描述稀释了关键信息，降低了警惕性。应该提供可验证的数据来证明事件的严重性。 缺少负责人 宣布官方所有权会产生问责制，从而促进采取行动。示例中有几个缺少所有权的例子： 总结列出了四个负责人。理想情况下，负责人是单点联系人，负责事后总结，后续以及完善工作。 “行动列表”部分很少甚至没有提及各条目的负责人。没有明确负责人的行动项目不大可能会被解决。 最好的是拥有一个负责人和多个协作者。 受众有限 示例事后总结仅在团队成员之间共享。默认情况下，公司的每个人都应该可以访问该文档。建议尽可能广泛的分享你的事后总结，甚至和客户分享。总结的价值和所创造的经验成正比。能够从过去事件中吸取教训的人越多，重复发生的可能性就越低。周密和诚实的事后总结也是恢复信任的关键。 随着体验和舒适度的提高，可以将“受众”扩展到非人类。成熟的事后总结文化通常会添加机器可读标签（和其他元数据）以启用下游分析。 延迟发布 示例是在事件发生四个月后公布的。在此期间，如果事故再次发生（实际上确实发生过），团队成员可能会忘记总结中提到的关键细节。 优秀的事后总结示例 这是一份真实的事后总结。个人和团队的名字均为虚构，为了保护敏感容量信息，我们使用了占位符替换实际值。在你为内部分享的事后总结中，应该包含具体的数字。 事后总结：所有发送到diskerase的卫星机器 2014-8-11: 负责人： 事后总结：maxone@，logantwo@， 数据中心自动化：sydneythree@， 网络：dylanfour@ 服务器管理：finfive@ 与以下人员共享：all_engineering_employees@google.com@ 状态：不可更改 事件日期：2014年8月11日, 星期一，PST8PDT 17:10至17:50 发布时间：2014年8月15日, 星期五 执行摘要 影响：前端查询丢失。 部分广告未投放 近两天由卫星提供的服务延迟增加 根本原因：自动调节系统中的某个错误导致所有机架的卫星服务器被发送磁盘diskerase指令。导致所有卫星机器进入退役工作流程，磁盘数据被擦除。致使全球卫星前端中断。。 故障摘要 故障持续时间：故障：周一，8月11日，PST8PDT 17:10至17:50。 周三，8月13日，07:46进行重建工作，之后故障解除。 受影响的产品：前端基础设施，尤其是卫星所在地。 受影响的产品百分比：全球——所有由卫星提供服务的流量（占全球查询的60%）。 用户影响：[ ]前端查询在40分钟内有所下降（[ ]QPS在此期间处于平均值，占全球流量的百分比[ ]）。所有由卫星提供的服务延迟增加。 收入影响：目前未知确切的收入影响。 监控：黑盒报警：对于每个卫星，流量团队收到“卫星a12bcd34有过多失败的HTTP请求”报警。 解决方案：通过将所有Google的前端流量转移到核心集群，以用户请求的额外延迟为代价，迅速缓解故障。 背景（可选） 影响 用户影响： [_]个前端查询在40分钟的时间段内丢失，QPS在此期间持平，占全球流量的[ ]%。我们的监控表明这是个大故障；然而数据并不可靠，监控将自身停止监控的但仍在服务的卫星视为请求被拒绝。附录描述了如何估算上述数字。。 最近两天所有由卫星提供的服务延迟增加。 –核心集群附近的RTT峰值[ ]ms –对于更依赖卫星的地点（例如澳大利亚，新西兰，印度），延迟+[ ]ms 收入影响： 由于请求丢失，部分广告未投放。目前尚不清楚确切的收入影响 显示和视频：由于日常波动，数据的误差较大，但我们估计故障当天有[ ]%到[ ]%的收入损失。 搜索：在17:00到18:00之间，在使用同样的误差时，有[ ]%到[ ]%的损失。 团队影响： 流量团队花了大约48小时全力投入重建卫星。 因为需要对过载的对等链路进行流量工程设计，NST的故障/报警负载高于正常值。 由于GFE的缓存命中率降低，某些服务可能会在前端提供更多响应。 —例如，请参阅线程[链接]关于[缓存依赖服务]。 —[缓存相关服务]在恢复之前，GFE的缓存命中率从[ ]%下降到[ ]%。 故障文件 [我们的事后总结文档的链接。] 根本原因和触发点 流量管理服务器中一个长期存在的输入验证bug，由于手动重新执行a12bcd34卫星的工作流程而被触发的。该bug删除了执行退役操作的机器的约束，发送并停用了所有卫星机器。 因此，数据中心自动化执行了退役工作流程，擦除了大多数卫星机器的硬盘驱动器，在此之前无法停止这项操作。 Traffic Admin服务器提供ReleaseSatelliteMachines RPC。此处理程序使用三个MDB API调用卫星停用： 查找与边缘节点关联的机架名称（例如，a12bcd34 -&amp;gt;）。 查找与机架关联的机器名称（-&amp;gt;等）。 将这些计算机重新分配给diskerase，间接触发退役工作流程。 由于MDB API的行为以及安全检查不是幂等的。如果卫星节点先前已成功发送到decom，则上面步骤2返回一个空列表，步骤3中将其解释为机器主机名上没有约束。 这种危险行为已存在一段时间，但被调用不安全操作的工作流程隐藏：调用RPC的工作流程步骤标记为“运行一次”，意味着工作流引擎一旦成功就不会重新执行RPC。 但是，“运行一次”的语义不适用于工作流的多个实例。当集群启停团队手动启动a12bcd34的另一个工作流时，会触发admin_server bug。 为恢复生产投入的努力 [我们的时间线日志的链接已被省略。在真正的事后总结中，这些信息始终包含在内。] 经验教训 事情进展顺利： 疏散边缘。核心中的GFE明确的进行容量规划能允许这种情况发生，生产主干也是如此（除了对等链路之外；请参阅下一节中的故障列表）。这种边缘疏散使得流量团队能迅速减缓故障。 自动减轻卫星故障。覆盖的线路自动将来自故障卫星的流量拉回到核心集群，并且当检测到异常抖动时会自行排出。 尽管可能会造成混乱，但卫星decom/diskerase工作十分高效和迅速。 故障通过OMG触发了快速的IMAG响应，并且该工具适用于持续的事件跟踪。跨团队的反应非常棒，OMG帮助大家保持交流。 事情进展受挫： 故障 Traffic Admin服务器没有对其发送到MDB的命令进行适当的健全性检查。所有命令都应该是幂等的，或者至少在重复调用时是自动防故障的。 MDB不拒绝缺少主机名约束的所有权更改请求。 decom工作流程不与其他数据源（例如，计划的机架decom）交叉检查decom请求。因此，对已清除（地理上不同）的机器的请求没有异议。 decom工作流不受速率限制。一旦机器进入decom，磁盘擦除和其他decom步骤以最大速度进行。 当卫星停止服务时，由于出口流量转移到不同位置，Google和各国之间的一些对等链路过载，而请求是从核心集群提供服务。导致了在卫星恢复且匹配NST缓解工作之前，选择对等链路的阻塞短暂爆发。 恢复 重新安装卫星机器的速度很慢且不可靠。在高延迟链路末端传输到卫星时，重新安装使用TFTP传输数据效果不佳。 Autoreplacer基础架构无法在故障时设置GFE的[ ]。需要多个SRE并行手动执行设置来匹配自动化设置的速度。以下因素导致自动化的缓慢： —SSH超时阻止了Autoreplacer在远程卫星上的操作。 —无论机器是否已具有正确的版本，都执行了慢速内核升级过程。 —Autoreplacer中的并发回归阻止了每个工作机器运行两个以上的机器设置任务。 当23%的目标被移除时，没有触发监控配置的安全检查参数（25%变化），当读取相同内容（剩余的29%）时触发了。导致重新启用卫星监控延迟30分钟。 “安装人员”有限，因此，变更过程困难又缓慢。 使用超级用户权限将机器从diskerase拉回来时留下了很多僵尸进程，导致后续清理困难。 幸运的 核心集群的GFE与卫星GFE的管理方式不同。他们没有受到decom的影响。 同样，YouTube的CDN作为独立的基础设施运行，因此没有受到影响。否则故障将更加严重和持久。 行动列表 由于此事件的广泛性，我们将行动列表分为五个主题： 预防/风险教训 紧急响应 监控/报警 卫星/边缘 清理/其他 表10-1.预防/风险教训 行动列表 类型 优先级 负责人 bug跟踪 审核所有能够将实时服务器转换为宕机状态的系统（即，不仅仅是维修和diskerase工作流） 调查 P1 sydneythree@ BUG1234 提交bug，跟踪BUG1234识别出的所有系统的拒绝错误输入实施的情况。 预防 P1 sydneythree@ BUG1235 禁止任何单个会影响跨命名空间/类边界的服务器操作。 减缓 P1 maxone@ BUG1236 流量管理服务器需要进行安全检查才能在超过[ ]节点数的情况下运行。 减缓 P1 dylanfour@ BUG1237 流量管理服务器应该依据批准破坏性工作。 预防 P0 logantwo@ BUG1238 MDB应拒绝对非预期的当前约束提供值的操作。 预防 P0 louseven@ BUG1239 表10-2.紧急响应 行动列表 类型 优先级 负责人 bug跟踪 确保由核心提供的服务不会使出口网络链路过载。 修复 P2 rileyslx@ BUG1240 确保在[我们的紧急停止文档]和[我们的升级联系页面]下注明了decom工作流问题。 减缓 P2 logantwo@ BUG1241 为decom工作流添加一个大红色禁用按钮a 减缓 P0 maxone@ BUG1242 a : 由于灾难性环境中避免进一步损坏的关闭开关（例如，紧急电源关闭按钮）的常见术语。 表10-3.监控/报警 行动列表 类型 优先级 负责人 bug跟踪 监控目标的安全检查，不允许发布无法回滚的变更。 减缓 P2 dylanfour@ BUG1243 当超过[ ]%的机器退役需要添加报警。16:38机器从卫星上取下，17:10开始世界范围的报警。 监测 P1 rileyslx@ BUG1244 表10-4.卫星/边缘 行动列表 类型 优先级 负责人 bug跟踪 利用IPXE配合HTTPS可以使重新安装更快更可靠。 减缓 P2 dylanfour@ BUG1245 表10-5.清理/其他 行动列表 类型 优先级 负责人 bug跟踪 在我们的工具中查看与MDB相关的代码，并将管理服务器备份放至unwedge调节中。 修复 P2 rileyslx@ BUG1246 安排DiRT测试：在diskerase后带回卫星；对YouTube CDN执行相同操作。 减缓 P2 louseven@ BUG1247 名词解释 管理服务器 RPC服务器，支持自动化为前端服务基础结构执行特权操作。自动化服务器常参与PCR和集群启停操作。 Autoreplacer 将非Borgified服务器从一台机器移动到另一台机器的系统。在机器故障时保持服务运行，并且支持colo重新配置。 Borg 集群管理系统，旨在管理大规模任务和机器资源。Borg拥有Borg单元中所有机器，并将任务分配给具有可用资源的机器。 Decom 退役的缩写。设备的decom是一个与许多运维团队相关的过程。 Diskerase 在生产硬盘驱动器离开Google数据中心前安全擦除生产硬盘的过程（以及相关的硬件/软件系统）。diskerase是decom工作流的一个步骤。 GFE（Google前端） 外部连接(几乎)所有谷歌服务的服务器。 *IMAG（Google事件管理） 一个程序，一种标准，以一致的方式来处理从系统中断到自然灾害的所有类型的事件——并组织有效的响应。 MDB（机器数据库） 事件管理仪表盘/工具，用于跟踪和管理Google所有正在进行的事件的中心位置。 卫星 小巧便宜的机器机架，仅提供提供来自Google网络边缘的非视频、前端流量。几乎没有传统的生产集群基础设施可用于卫星。卫星不同于CDN，它提供来自Google边缘网络的YouTube视频内容，以及来自互联网中更广泛的其他地方的视频内容。YouTube CDN未受此事件的影响。 附录 为什么释放卫星机器不是幂等的？ [该问题的回复已被删除] 管理服务器将所有卫星分配给diskerase团队后发生了什么？ [该问题的回复已被删除] 故障期间真正的QPS损失是多少？ [该问题的回复已被删除] IRC日志 [IRC日志已被删除] 图表 更快的延迟统计——卫星曾为我们做过什么？ 从这次故障经验得到，卫星会在核心集群附近的许多位置产生[ ]ms延迟，离主干更远的位置甚至会达到[ ]ms： [图表的解释已被删除] 核心与边缘服务负载 为重建服务所付出的努力是个很好的例证。边缘服务恢复50%的流量需要大约36小时，恢复到正常的流量水平需要额外的12小时（见图10-1和图10-2）。 来自流量转换的对等压力 [图表省略] 该图显示了由网络区域聚合的数据包丢失情况。在活动期间有一些短的尖峰，但大部分损失发生在卫星覆盖少的各个地区的高峰时刻。 人与机器，GFE [省略了人机与自动机器设置速率的图表说明。] 图10-1.故障期间核心与边缘QPS分布 图10-2.故障期间核心与边缘QPS分布（替代表示） 为什么这份事后总结示例更好？ 这个事后总结符合了好几条写作要求。 明晰 事后总结组织的很好，详细解释了关键术语。例如： 名词解释 一个精心编写的名词解释让事后总结更容易被大众接受和理解。 行动列表 这是一个涉及许多小的行动列表的大事件。按照主题对操作项进行分组可以更加轻松的分配负责人和优先级。 量化指标 事后总结提供了相关事件的有用数据，例如缓存命中率、流量级别和影响持续时间。数据的相关部分将显示原始来源的链接。这种数据透明性消除了歧义并为读者提供了上下文参考。 具体行动列表 没有行动列表的事后总结是无效的。这些行动列表有一些显著特点： 负责人 所以操作项都有负责人和bug跟踪号。 优先级 为所有操作项分配优先级。 可测性 操作项具有可验证的最终状态（例如，“当我们的机器中超过X%的机器退役时添加报警”）。 预防措施 每个操作项“主题”都有预防/缓解操作项，这些操作项有助于避免故障重复发生（例如，“禁止任何单个会影响跨命名空间/类边界的服务器操作。”） 不指责 作者关注的是系统设计中的差距，正是这些差距导致了非预期的故障，例如： 事情进展受挫 没有任何人或团队因此事件受到指责。 根本原因和触发点 关注“什么”出了问题，而不是“谁”造成了这一问题。 行动列表 旨在改善系统而不是改善人。 深度 事后调查不仅仅是调查系统故障的近似区域，也研究了多个团队的影响和系统缺陷。尤其： 影响 本节包含来自不同视角的大量细节，尽可能的做到平衡，客观。 根本原因和触发点 本节对事件进行深入研究，找到根本原因和触发点。 由数据推及结论 提出的所有结论均基于事实和数据。用于得出结论的数据都和文档相关联。 其他资源 以图表的形式进一步呈现有用的信息。向不熟悉系统的读者解释图表帮助其理解上下文。 及时 事件结束后不到一星期就写完并传播了事后总结。快速的事后总结往往更加准确，因为此时任何参与者心理都记着这件事。而受故障影响的人正在等待一个解释，证明你们已经控制了故障。等待的时间越长，他们就越能散发想象，那样对你十分不利。 简明 该事件是全球范围的事件，影响多个系统。事后总结记录了且随后分析了大量数据。冗长的数据源（例如聊天记录和系统日志）被抽象化，未经编辑的版本可从主文档中链接到。总体而言，这份总结在冗长和可读性之间取得了平衡。 组织激励 理想情况下，高级领导应该支持和鼓励有效的事后总结。本节描述了一个组织如何激励健康的事后总结文化。我们着重描述了总结文化失败的征兆，并给出了一些解决方案。同时还提供了工具和模板来简化和自动化事后处理流程。 模型以及对事不对人 为了正确的支持事后总结文化，领导者应始终如一的坚持对事不对人的原则，并在事后讨论中鼓励对事不对人。可以使用一些具体策略来强制组织执行对事不对人这一准则。 使用对事不对人的语言 指责性的语言会影响团队协作。请考虑以下情况： Sandy 错过了服务Foo培训，且不确定如何运行特定的更新命令。因而导致故障时间的延长。 SRE Jesse [对Sandy的leader说]:“你是经理，为什么不确保每个人都完成培训？” 这个交流凸显了一个主要问题，即让收件人处于劣势。更平衡的回应是： SRE Jesse [对Sandy的leader]：“看完事后总结，能够注意到on-call错过了一次重要的培训，导致没有更快的解决故障。因此是否应该要求团队成员加入on-call轮转之前都完成此培训？或者是否可以提醒on-call，如果操作卡住可以尽快升级事件。毕竟，升级不是错误——尤其它有助于降低客户的负担！从长远来看，因为一些细节很容易被忘记，因此我们不能完全依赖培训。” 事后总结的创作要包含所有事件参与者 当事后总结是单人或由单个团队编写时，很容易忽略导致故障的关键因素。 收集反馈 明确的审查过程和事后计划可以帮助防止指责的语言和观点在组织内传播。有关的结构化审核流程请参阅第221页的“事后检查清单”部分。 奖励事后总结 事后总结培训是积极推动组织变革和防止重复故障的有效工具。如果写的够好，采取行动并广泛分享，可以考虑以下策略来激励事后总结文化。 对完成行动列表进行奖励 如果你奖励工程师编写事后总结而不是完成了相关的行动列表，那么可能会出现总结中的行动项目未完成的事情。需要在编写事后总结和成功完成行动计划间平衡奖励措施。 对积极的组织变革进行奖励 你可以将事后总结的重要性作为提高组织影响的依据，通过对标奖金、积极的绩效评估、晋升等作为奖励。来激励并广泛实施事后总结教训。 突出提高可靠性 随着时间的推移，有效的事后总结可以减少故障，让系统更加可靠。因此，团队可以专注于特性功能的开发速度，而不是基础架构的修补上。在总结、演示文稿和绩效评估中强调这些改进在本质上是会提供动力的。 把事后总结的负责人当做领导者 通过电子邮件或会议完成事后总结，或者通过给作者向受众提供经验教训的机会，能够吸引那些喜欢公共赞誉的人。对于寻求同行认可的工程师而言，可以将负责人设置为某种类型的故障的“专家”。例如，你可能会听到有人说：“和Sara说，她现在是专家了。她参与了事后总结的撰写，并且想出了解决问题的方法！” 游戏化 一些人会被成就感和更远大的目标所激励，例如修复系统薄弱点和提高可靠性。对于这些人而言，事后总结行动列表的记录或完成所获得的成就已经是奖励了。在Google，我们每年举办两次“FixIt”周。完成最重要的行动列表项目的SRE会收到小额赞赏和吹牛的权利。图10-3显示了一个事后总结排行榜的示例。 图10-3.事后总结排行榜 公开分享事后总结 为了在组织内保持健康的事后总结文化，要尽可能广泛的分享事后总结。以下策略可以提供一些帮助。 在整个组织内分享总结 在内部沟通渠道、电子邮件、Slack等中宣传事后总结的可用性。如果你负责一个公司，可以分享一个最近的有趣的事后总结。 进行跨团队审核 对事后总结进行跨团队审查。过程中，一个团队过一遍故障，其他团队提出问题并间接学习。在Google，几个办公室都设有非正式的事后总结俱乐部，向所有员工开放。 此外，由开发人员、SRE和组织领导组成的跨职能小组审核整个事后总结流程。他们每个月都会审查事后总结过程和模板的有效性。 进行培训练习 使用“命运之轮”训练新入职工程师：一群工程师重新扮演事后总结的角色，当时的事故总控负责人也参与其中，确保这次演习尽可能的“真实”。 每周总结事件和故障 每周对过去七天内发生的事件和故障的进行总结，并尽可能广泛的进行分享。 响应事后总结文化的失败 事后总结文化的崩溃可能并不明显，以下介绍了常见的故障模式和推荐的解决方案。 逃避 逃避事后总结过程可能是一个组织的事后总结文化失败的征兆，例如，假设SRE 主管 Parker无意中听到以下对话： SWE Sam：哇，你听说了这次的大故障了吗？ SWE Riley：听说了，太可怕了。他们现在得写一个事后总结了。 SWE Sam：不是吧，还好我没有参与进去。 SWE Riley：对啊，我是真的不想参与那个讨论会议。 确保对产生这些抱怨的事件进行高可见度的事后总结可以避免这种逃避。此外，分享高质量的案例并讨论参与的人如何获得奖励将有助于重新团结每个人。。 没有强调文化 当高级管理人员使用责备的语言做出回应可能会使事情更糟糕。假设一个高级领导在会议上对故障做出以下声明： VP Ash：我知道应该对事不对人，但有人事先知道这个操作可能会产生问题，你为什么不听那个人的？ 可以通过更有建设性的话术来减少损害，例如： SRE Dana：我确信每个人的出发点都是好的，所以为了保持对事不对人的准则，我们一般会这样问：是否有任何应该注意到的告警以及我们为何会忽略它们。 对于组织而言，立足正确的出发点，并根据现有的最佳信息做出决策，调查误导性信息的来源比分配责任更有帮助。（如果你知道敏捷原则，那么你应该对这个更加清楚。） 没有时间写事后总结 优质的事后总结撰写是需要时间的。当一个团队负担其他任务时，总结的质量会受到影响。低质量的没有完整的行动列表的事后总结会更容易导致故障复发。事后总结是你写给团队未来成员的信件：以免你不小心教给未来的队友一个错误的教训，保持一致的质量标准十分重要。应该优先考虑事后检查工作，跟踪事后完成情况和审查，并让团队有足够的时间来实施相关的行动计划。我们在第220页的“工具和模板”一节中讨论的工具可以帮助完成此项活动。 重复故障 如果团队在遇到类似故障时采用以前的经验失败了，那么就该深入挖掘了。要考虑以下问题： 行动列表项目是否需要很长时间才能完成？ 故障速度是否超过了可靠性的修复速度？ 最先获得的是正确的行动项目吗？ 重构的故障服务是否过期？ 是否把Band-Aids定位成更严重的问题上了？ 如果你发现了系统性流程或技术问题，应该退一步考虑整体服务运行状况。将每个类似事件的事后总结作者聚集到一起，讨论防止故障重复发生的最佳行动方案。 工具和模板 一组工具和模板可以让编写事后总结和管理相关数据变得更轻松，从而引导事后总结文化。在这一领域，你可以利用Google和其他公司提供的大量资源。 事后总结模板 模板可以使编写和分享完整的事后总结更加轻松。使用标准格式可以使非专业的读者更容易理解事后处理过程。你可以自定义模板。例如，获取特定团队的元数据（如数据中心团队的硬件品牌/型号）或受移动团队影响的Android版本可能更有用。随着团队在这方面的成熟，还可以自定义模板。 Google模板 Google已经通过http://g.co/SiteReliabilityWorkbookMaterials以Google文档格式分享了我们的事后模板。我们在内部主要使用Docs来编写事后总结，可以通过共享编辑权限和注释促进合作。我们的一些内部工具可以使用元数据预填充此模板让事后总结更容易编写。我们利用Google Apps脚本自动化部分创作，并将大量数据捕获到特定的部分和表格中，以便我们的事后总结存储库更容易解析数据。 其他行业模板 其他几家公司和个人分享了他们的事后总结模板： 报警职责 改编原始的Google可靠性站点工程书籍模型 GitHub上托管的四个模板列表 GitHub用户Julian Dunn 服务器故障 事后总结工具 在撰写本文时，Google的事后总结管理工具无法供外部使用（请查看我们的博客获取最新更新）。但是，我们可以解释我们的工具是如何促进事后总结文化的。 事件管理工具 我们的事件管理工具收集并存储大量关于故障的有用数据，并将该数据自动推动到事后总结模板中。我们推送的数据类型包括： 故障指挥人和其他角色 详细的事件时间表和IRC日志 受影响的服务和导致根本原因的服务 事件严重性 事件检测机制 事后总结清单 为了帮助作者确保正确完成事后检查，我们提供了一个事后检查清单，通过关键步骤引导负责人。以下是列表中的一些示例检查： 对事件影响进行全面评估。 进行足够详细的根本原因分析，推动行动列表的规划。 确保行动列表项目通过服务技术主管的审查和批准。 和更多的组织分享事后总结。 完整的清单可在http://g.co/SiteReliabilityWorkbookMaterials找到。 归档事后总结 我们将事后总结归档在一个名为Requiem的工具中，这样任何Google员工都可以轻松找到它们。我们的事件管理工具会自动将所有事后总结推送到Requiem，组织中的任何人都可以发布他们的事后总结给所有人查看。我们有成千上万的总结存档，可以追溯到2009年。Requiem会解析个人事后总结的元数据，使其可以用于搜索、分析和总结。 跟进事后总结 我们的事后总结归档在Requiem的数据库中。任何生成的操作项都会在我们的集中式bug跟踪系统中归档为bug。因此，我们可以监控每个事后总结的行动项目的结束与否。通过这种级别的跟踪，可以确保行动项目不会有漏洞以致服务越来越不稳定。图10-4显示了由我们的工具启用的事后总结操作项监控的模型。 图10-4.事后总结行动项目监控 事后总结分析 我们的事后总结管理工具将信息存储在数据库中以供分析。团队可以使用这些数据编写有关其事后趋势的总结，并确定易受攻击的系统。这有助于我们发现潜在的不稳定因素或可能被忽略的故障管理障碍。例如，图10-5显示了使用我们的分析工具构建的图表。这些图表显示了我们每个组织每月有多少次事后追踪、事件平均持续时间、检测时间、解决时间和故障半径的趋势。 图10-5.事后总结分析 其他行业工具 以下是一些可以帮助你创建、组织和分析事后总结的第三方工具： 报警职责事后调查 Etsy的档案室 VictorOps 尽管完全实现自动化编写事后总结是不可能的，但我们发现事后总结模板和工具会使整个流程更加顺畅。这些工具可以节省时间，让作者能够专注于事后的关键部分，例如根本原因的分析和行动项目计划。 结论 在培养事后总结文化的持续投资中，能够减少故障，为用户提供更好的体验，以及让依赖你的人对你更加信任。这些实践的应用可以使系统设计更完善、宕机时间更短、工程师工作效率更高且工作更快乐。如果最坏的情况确实发生并且故障再次发生，那么你受到的损失会更小并且恢复的更快，而且有更多的数据帮助健壮生产环境。</summary></entry><entry><title type="html">第九章 故障响应</title><link href="http://localhost:4000/sre/2020/01/09/%E4%BA%8B%E4%BB%B6%E5%93%8D%E5%BA%94/" rel="alternate" type="text/html" title="第九章 故障响应" /><published>2020-01-09T00:00:00+08:00</published><updated>2020-01-09T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/09/%E4%BA%8B%E4%BB%B6%E5%93%8D%E5%BA%94</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/09/%E4%BA%8B%E4%BB%B6%E5%93%8D%E5%BA%94/">&lt;!-- more --&gt;
&lt;blockquote&gt;
  &lt;p&gt;写在开头:&lt;/p&gt;

  &lt;p&gt;故障总会发生！如何对故障快速进行有组织的响应？Google方式是一个很好的借鉴。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;每个团队都希望服务没有故障发生，但这个世界是不完美的，像停电这类的事件还是会发生的。当一个紧急故障需要多人或团队协作解决时，响应故障并解决问题是我们要应对的。&lt;/p&gt;

&lt;p&gt;处理故障的目标是减轻影响或将服务恢复到先前的状态。而故障管理意味着以有效的方式协调团队的响应工作，并确保响应者和利益相关者之间的信息流通。包括谷歌在内的许多科技公司都有自己的“最佳实践”来管理应急响应，并且在不断的完善。&lt;/p&gt;

&lt;p&gt;故障管理的基本前提是以结构化的方式响应故障。大规模故障是单人无法解决的；结构化的应急响应可以减少混乱。在灾难发生之前制定沟通和协调规范，使你的团队可以集中精力解决故障，而不用担心沟通和协调问题。&lt;/p&gt;

&lt;p&gt;故障响应的过程并不是困难的事情。大量实践经验总结可以为我们提供一些指导，比如在第一本SRE书中的《故障管理》章节。故障响应的基本原则包括以下内容：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;保持清晰的指挥线&lt;/li&gt;
  &lt;li&gt;明确角色&lt;/li&gt;
  &lt;li&gt;随时掌握调试工作记录&lt;/li&gt;
  &lt;li&gt;响应进度的实时更新&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本章介绍了如何在Google和PagerDuty中进行故障管理。通过一些Case说明我们在其中达到的目标以及没有达到的目标。 “将最佳实践付诸实践”（第191页）中的清单可以帮助你开始创建自己的故障响应实践。&lt;/p&gt;

&lt;h2 id=&quot;google故障管理&quot;&gt;Google故障管理&lt;/h2&gt;

&lt;p&gt;故障响应是用于响应和管理故障的系统。由一个框架和一组定义的程序组成。Google的故障响应系统是基于故障命令系统（ICS）实现的。&lt;/p&gt;

&lt;h3 id=&quot;故障指挥系统&quot;&gt;故障指挥系统&lt;/h3&gt;

&lt;p&gt;ICS成立于1968年，是消防员管理野火的一种方式。该框架提供了在故障响应期间进行通信和明确角色的标准化方法。基于该模型，公司后来采用ICS来应对计算机和系统故障。本章探讨了两个这样的框架：PagerDuty的故障响应流程和Google故障管理（IMAG）。&lt;/p&gt;

&lt;p&gt;故障响应框架有三个共同目标，也称为事件管理的“三个C”（3C）：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;协调响应工作&lt;/li&gt;
  &lt;li&gt;在故障响应者、组织内部和外部之间进行通信&lt;/li&gt;
  &lt;li&gt;保持对故障响应的控制&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当故障响应出现问题时，罪魁祸首可能出现在其中一个方面。掌握3C对于有效的事件响应至关重要。&lt;/p&gt;

&lt;h3 id=&quot;故障响应中的主要角色&quot;&gt;故障响应中的主要角色&lt;/h3&gt;

&lt;p&gt;故障响应中的主要角色是故障指挥官（IC），通信主管（CL）和操作或行动主管（OL）。 IMAG将这些角色组织成一个层次结构：IC引导故障响应，CL和OL报告给IC。&lt;/p&gt;

&lt;p&gt;灾难发生时，声明故障的人通常会进入IC角色并指挥故障的高级别状态。 IC专注于3C，并执行以下操作：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;命令和协调故障响应，根据需要委派角色。默认情况下，IC会假定尚未委派的所有角色。&lt;/li&gt;
  &lt;li&gt;有效沟通。&lt;/li&gt;
  &lt;li&gt;保持对故障响应的控制。&lt;/li&gt;
  &lt;li&gt;与其他相应人员合作解决此事件。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;IC可以将其角色交给其他人并承担OL角色，或将OL角色分配给其他人。 OL通过应用工具来缓解或解决故障，从而对故障做出响应。&lt;/p&gt;

&lt;p&gt;虽然IC和OL致力于减轻和解决故障，但CL是故障响应团队的消息通讯核心。 CL的主要职责包括定期向故障响应团队和利益相关者实时更新故障响应的进展。&lt;/p&gt;

&lt;p&gt;CL和OL都可以领导一个团队来帮助管理他们特定的故障响应区域。这些团队可以根据需要进行扩展或收缩。如果故障变得足够小，那么CL角色就可以被包含到IC角色中。&lt;/p&gt;

&lt;h2 id=&quot;案例研究&quot;&gt;案例研究&lt;/h2&gt;

&lt;p&gt;以下四个大型故障说明故障响应如何在实践中起作用。其中三个案例研究来自Google，最后一个是来自PagerDuty的案例研究，该研究提供了其他组织如何使用ICS派生框架的观点。Google的例子以一个没有有效管理的故障开始，并在有故障管理时对故障响应的改观。&lt;/p&gt;

&lt;h3 id=&quot;案例研究1软件错误灯还开着但没有人google-home&quot;&gt;案例研究1：软件错误—灯还开着，但没有人(Google HOME)&lt;/h3&gt;

&lt;p&gt;这个例子展示了故障发生初期不进行通报，在没有工具快速有效地响应故障时，团队是如何响应故障的。虽然这一故障在没有重大灾难的情况下得到了解决，但是越早组织，就会产生更好的效果。&lt;/p&gt;

&lt;h4 id=&quot;背景&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;Google Home是一款智能扬声器和家庭助手，可响应语音命令。语音命令与Google Home的软件（称为Google智能助理）进行交互。&lt;/p&gt;

&lt;p&gt;当用户说出一个触发Google智能助理的热门词组时，就会启动与Google Home的互动。多个用户可以通过培训助理来监听给定的热门词汇，从而使用相同的Google Home设备。识别扬声器的热词模型是在客户端进行的，但是数据（例如扬声器识别文件）存储在服务器上。服务器处理数据的双向流，为了在繁忙的时候处理超载，服务器对Google助手有一个配额政策。为了保护服务器免受过大请求值的影响，配额限制明显高于在给定设备上的Google助手的基准使用量。&lt;/p&gt;

&lt;p&gt;Google智能助理1.88版中的错误导致扬声器识别文件的获取频率超过预期的50倍，超出此配额。最初，美国中部的GoogleHome用户只有很少的流量损失。然而，随着所有GoogleHome设备的推出逐步增加，用户在2017年6月3日的周末期间丢失了一半的请求。&lt;/p&gt;

&lt;h4 id=&quot;故障&quot;&gt;故障&lt;/h4&gt;

&lt;p&gt;太平洋标准时间5月22日星期一上午11:48，谷歌主页on-call开发人员Jasper正在看着每秒查询（QPS）图表并注意到一些奇怪的事情：谷歌助理每隔30分钟就会收到培训数据，而不是像预期的那样每天一次。他停止了已经推到了25％用户的1.88版本发布。他提了一个漏洞——称之为bug 12345——用谷歌的漏洞追踪系统来探索为什么会发生这种情况。在这一问题上，他指出，谷歌助手每天会对数据进行48次的ping请求，导致其超过了QPS的容量。&lt;/p&gt;

&lt;p&gt;另一位开发人员Melinda将此问题与先前报告的漏洞相关联，我们将其称为错误67890：每当应用程序刷新设备身份验证和注册状态时，语音处理器都会重新启动。该版本将在版本1.88发布后修复，因此该团队要求临时增加模型的配额，以减轻额外查询的过载。&lt;/p&gt;

&lt;p&gt;版本1.88版本再次启动并继续推出，到5月31日星期三达到50％的用户。不幸的是，该团队后来了解到错误67890，虽然负责一些额外的流量，但并不是Jasper所注意到的更频繁的取回的真正根源。&lt;/p&gt;

&lt;p&gt;同一天早上，客户开始向Google支持小组报告问题：每当有人说“OK Google”（或任何其他热门词汇来激活Google Home）时，设备都会回复并显示错误消息，此问题阻止用户向Google智能助理发出命令。该团队开始调查可能导致用户报告的错误的原因。他们怀疑配额问题，所以他们要求增加配额，这似乎可以缓解这个问题。&lt;/p&gt;

&lt;p&gt;与此同时，该团队继续调查bug 12345，看看是什么触发了错误。尽管在调试过程的早期就建立了配额连接，但是客户端和服务器开发人员之间的误解导致了开发人员在故障排除过程中走错了方向，而完整的解决方案仍然无法实现。&lt;/p&gt;

&lt;p&gt;该团队还对为什么Google智能助理的流量不断达到配额限制感到困惑。客户端和服务器开发人员对客户端错误感到困惑，这些错误似乎没有被服务器端的任何问题触发。开发人员将日志记录添加到下一个版本，以帮助团队更好地理解错误，并希望在解决事件方面取得进展。&lt;/p&gt;

&lt;p&gt;截至6月1日星期四，用户报告此问题已得到解决。没有报道任何新问题，因此版本1.88版本继续推出。但是，原始问题的根本原因尚未确定。&lt;/p&gt;

&lt;p&gt;在6月3日的周六早上，版本1.88的发布超过了50%。这一发布是在一个周末进行的，当时开发团队并没有人值班。该团队并没有遵循在工作日期间执行部署的最佳实践，以确保开发人员在场。&lt;/p&gt;

&lt;p&gt;在6月3日星期六，当版本1.88发布的时候达到了100%，客户端又一次达到了Google助理流量的服务器限制。来自客户的错误报告开始出现。谷歌员工报告称，他们的Google HOME设备出现了错误。Google Home支持小组收到了大量关于此问题的客户来电，反馈也包括了推文和Reddit帖子。 Google Home的帮助论坛也出现了正在讨论这个问题的帖子。尽管有大量的用户报告和反馈，bug并没有升级到更高的优先级。&lt;/p&gt;

&lt;p&gt;6月4日星期日，随着客户报告数量的不断增加，支持团队最终将错误优先级提升到最高水平。该团队没有通报故障，而是继续通过“常规”方法解决问题—使用bug跟踪系统进行通信。on-call开发人员注意到一个数据中心集群中的错误率，SRE进行了ping操作，要求他们排除它。与此同时，该团队提出了另一项增加配额的请求。之后，开发团队的一名工程师注意到流量通道已将错误推入其他单元格，这为配额问题提供了额外的证据。下午3：33，开发人员团队经理再次增加了Google智能助理的配额，并停止了对用户的影响。故障结束。该团队此后不久确定了根本原因（参见前面的“环境”部分）。&lt;/p&gt;

&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;

&lt;p&gt;故障处理的某些方面进展顺利，而其他方面则有改进的余地。&lt;/p&gt;

&lt;p&gt;首先，工程师在周末处理故障并为解决问题提供了宝贵的意见，这既好又坏，虽然团队重视这些人在周末所付出的时间和精力，但成功的故障管理不应该依赖于个人的英勇行为。如果无法联系到开发人员怎么办？Google鼓励良好的工作与生活平衡——工程师不应在空闲时间内解决与工作相关的问题。相反，我们应该在工作时间进行部署，或者组织一次在工作时间之外提供付费保障的on-call轮换。&lt;/p&gt;

&lt;p&gt;接下来，该团队致力于缓解这一问题。Google首先要求是阻止故障的影响，然后找到根本原因（除非根本原因只是在早期发现）。一旦问题得到缓解，找到根本原因同样重要，以防止问题再次发生。在这个故障事件中，成功地在三个不同的场合控制了故障影响，但团队只有在发现根本原因后才能防止问题再次发生。在第一次故障得到控制之后，最好是等到根本原因完全确定后再开始实施，避免周末发生大混乱。最后，在问题首次出现时，团队没有通告故障。我们的经验表明，管理故障可以更快地解决。尽早通告故障可确保：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;防止客户端和服务器开发人员之间的沟通错误。&lt;/li&gt;
  &lt;li&gt;根源识别和故障解决发生得更快。&lt;/li&gt;
  &lt;li&gt;相关团队提前进入，使外部沟通更快捷顺畅。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;集中式通信是IMAG协议的重要原则。例如，当灾难发生时，SRE通常聚集在一个“作战室”中。作战室可以是像会议室一样的真实环境，也可以是虚拟的：团队可能聚集在IRC频道。这里的关键是将所有的故障响应者集中在一个地方，并实时沟通以管理并最终解决故障。&lt;/p&gt;

&lt;h3 id=&quot;案例研究2服务故障如果可以请对我进行缓存&quot;&gt;案例研究2：服务故障—如果可以，请对我进行缓存&lt;/h3&gt;

&lt;p&gt;下面的故障说明了当一个“专家级”团队试图调试一个具有复杂交互系统时会发生什么——没有一个人能够掌握所有的细节。&lt;/p&gt;
&lt;h4 id=&quot;背景-1&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;Kubernetes是一个由许多公司和个人贡献者共同建立的开源容器管理系统。 Google Kubernetes Engine（或称GKE）是Google管理的系统，可为用户创建、托管和运行Kubernetes群集。用户以最适合他们的方式上传和管理工作负载。当用户首次创建新群集时，GKE将获取并初始化其群集所需的Docker镜像。理想情况下，这些组件是在内部获取和构建的，因此我们可以验证它们。但Kubernetes是一个开源系统，新的依赖关系有时会让系统出现问题。&lt;/p&gt;

&lt;h4 id=&quot;故障-1&quot;&gt;故障&lt;/h4&gt;

&lt;p&gt;在太平洋标准时间一个星期四上午6点41分，伦敦GKE的on-call人员Zara寻访了多个区域的CreateCluster探查器故障，新的集群没有被成功地创建。 Zara检查了prober仪表板，发现两个区域的故障率都在60％以上。他确认此问题影响了用户创建新群集的功能，但现有群集的流量未受影响。 Zara遵循GKE的文件化程序，于上午7:06对故障进行了通告。最初，有四人参与处理了这一故障：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Zara是第一个注意到这个问题的人，因此被指定为默认故障指挥官。&lt;/li&gt;
  &lt;li&gt;Zara的两名队友。&lt;/li&gt;
  &lt;li&gt;Rohit由故障程序分配的客户支持工程师。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于Rohit总部设在苏黎世，Zara（IC）开设了一个GKE Panic IRC频道，团队可以在那里一起调试。在另外两个SRE深入调查监控和错误信息时，Zara解释了故障及其对Rohit的影响。截至上午7点24分，Rohit向用户发布通知称，创建集群的功能在欧洲西部地区出现故障。这提高了故障的等级。早上7点到8点20分，Zara、Rohit和其他人一直致力于解决这个问题。他们检查了集群启动日志，其中显示了一个错误：无法运行Kubelet；无法创建证书签名——他们需要确定证书创建的哪个部分失败。SRE研究了网络、资源可用性和证书签名过程。所有这些似乎都运作得很好。上午8点22分，Zara向故障管理系统发布了故障调查摘要，并寻找可以帮助他的开发人员。值得庆幸的是，GKE有一位on-call开发人员可以在紧急情况下提供协助。开发人员Victoria加入了该频道。他要求跟踪错误，并要求团队将问题上报给基础架构的on-call团队。现在是上午8点45分，第一个西雅图SRE，II-Seong来到办公室，轻轻地泡了杯咖啡，为这一天做好了准备。 II-Seong是一名资深的SRE，在故障响应方面拥有多年的经验。当他得知正在发生的故障时，他立即进入故障响应频道。首先，II-Seong根据警报的时间检查当天的发布情况，并确定当天的发布不会导致事故障的发生。然后，他开始整理工作文件，收集笔记。他建议Zara将故障升级为基础架构，云网络和计算引擎团队，以尽可能多地消除这些根源。由于Zara升级，其他人加入了故障响应：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GKE节点的开发负责人&lt;/li&gt;
  &lt;li&gt;云网络值班人员&lt;/li&gt;
  &lt;li&gt;计算引擎值班人员&lt;/li&gt;
  &lt;li&gt;Herais，另一个西雅图SRE&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上午9点10分，故障频道有十几个参与者。故障发生2.5小时，没有找到根本原因，也没有控制住影响。沟通成为了一种挑战。通常情况下，从伦敦到西雅图的on-call交接时间是上午10点，但Zara决定在上午10点之前将故障指挥权移交给II-Seong，因为他对IMAG有更多的经验。作为故障指挥官，II-Seong建立了一个正式的响应组织结构来解决这一事件。然后，他指定Zara为Ops Lead，Herais为通信（Comms）负责人。 Rohit仍然是外部沟通负责人。 Herais立即发送了一封“全体人员齐上阵”的电子邮件给GKE，包括所有开发人员的负责人，并要求专家加入故障响应。到目前为止，故障响应者了解到了以下情况：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当节点试图向主机注册时，集群创建失败。&lt;/li&gt;
  &lt;li&gt;错误消息表明证书签名模块是罪魁祸首。&lt;/li&gt;
  &lt;li&gt;欧洲所有集群创建都失败了;其地区都正常。&lt;/li&gt;
  &lt;li&gt;欧洲没有其他GCP服务出现网络或配额问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;之后，GKE安全团队成员Puanani加入了这项工作。他注意到证书签发者没有开始。证书签发者试图从DockerHub中提取图像，图像似乎已损坏。Victoria（GKE on-call开发人员）在两个地理位置运行了Docker的图像拉取命令。它在欧洲的集群上运行失败，在美国的集群上成功。这表明欧洲集群是问题的所在。上午9点56分，该团队确定了一个看似合理的根本原因。&lt;/p&gt;

&lt;p&gt;因为DockerHub有一个外部依赖，故障控制和故障溯源将特别具有挑战性。对于Docker的工作人员来说，第一个控制故障的方法是快速修复。第二个选择是重新配置集群，从不同的位置获取图像，例如Google容器注册表（GCR），Goole的安全图像托管系统。所有其他依赖项，包括对图像的其他引用，都位于GCR中。 II-Seong指派负责人追查这两种可能。然后，他委派了一个小组去调查失败的集群。讨论对于IRC来说过于密集，因此详细的调试转移到共享文档，IRC成为决策的中心。&lt;/p&gt;

&lt;p&gt;对于第二个可能，推送新配置意味着重建二进制文件，这需要大约一个小时。上午10:59，当团队完成90％的重建工作时，他们发现另一个使用错误图像获取路径的位置。作为回应，他们不得不重新启动构建。当IRC的工程师们致力于这两个缓解方案时，SRE中的Tugay有了一个想法。如果他们拦截了Docker的pull请求并使用内部缓存图像替换Docker的响应，那么如何重建配置并将其推出（一个笨重且有风险的过程）呢？ GCR有一个镜像可以做到这一点。 Tugay联系了GCR的SRE团队，他们确认团队可以在Docker配置上设置&amp;lt;–registry-mirror =https://mirror.gcr.io&amp;gt;。Tugay开始设置此功能，并发现镜像已经到位！&lt;/p&gt;

&lt;p&gt;上午11点29分，Tugay向IRC报告说这些图像是从GCR镜像中拉出来的，而不是DockerHub。上午11点37分，故障指挥官在呼叫GCR值班人员。上午11点59分，GCR值班人员在欧洲存储层清除了腐败的图像。截止到下午12点11分，所有欧洲地区的误差都降至0％。&lt;/p&gt;

&lt;p&gt;故障结束了。剩下的只是清理工作，故障文档的整理。在修复之前，CreateCluster在欧洲失败了6小时40分钟。在IRC中，整个事件中出现了41个独立用户，IRC日志扩展到26,000个单词。这项工作在不同的时间分拆了七个IMAG工作组，并且在任何时候都有多达四个同时工作。六支on-call队伍的参与，后期包含28个行动项目。&lt;/p&gt;

&lt;h4 id=&quot;总结-1&quot;&gt;总结&lt;/h4&gt;

&lt;p&gt;无论从哪个角度，GKE CreateCluster中断是一件重大故障。让我们探讨一下哪些事情进行得很顺利，哪些事情本可以处理得更好。&lt;/p&gt;

&lt;p&gt;什么进展顺利？该团队有几个记录在案的升级路径，熟悉故障响应策略。GKE值班人员 Zara很快就证实了这种影响正在影响实际客户。然后，她使用了一个常用的故障管理系统来通知Rohit，Rohit将故障告知客户。&lt;/p&gt;

&lt;p&gt;什么可以更好地处理？该服务本身有一些值得关注的方面。复杂性和对专家的依赖是有问题的。记录对于故障定位来说是不够的，并且团队因为DockerHub上的变化而分散了注意力，而且这不是真正的问题。&lt;/p&gt;

&lt;p&gt;故障发生之初，故障指挥官没有制定一个正式的应急预案。虽然Zara承担了这一角色并将对话转移到IRC，但他可以更积极地协调信息和做出决策。结果，少数应急人员在没有协调的情况下进行自己的调查。II-Seong在第一个警报后两小时就建立了正式的故障响应组织。&lt;/p&gt;

&lt;p&gt;最后，该故障揭示了GKE灾难准备方面的一个不足：该服务没有任何可以减少用户影响的早期通用预案。通用预案是第一响应者采取的降低影响的操作流程，甚至在完全理解根本原因之前都可以使用。例如，当中断与发布周期相关时，响应者可以回滚最近的版本，或者重新配置负载平衡器以避免错误被本地化时出现区域。值得注意的是，通用预案也有弊端，可能会导致其他服务中断。然而，虽然它们可能具有比精确解决方案更广泛的影响，但是当团队发现并解决根本原因时，它们可以快速到位以停止进一步扩大影响。&lt;/p&gt;

&lt;p&gt;让我们再次查看此故障的时间表，看看通用预案可能在哪些方面有效：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;上午7点（评估影响）。 Zara确认用户受到中断的影响。&lt;/li&gt;
  &lt;li&gt;上午9:56（找到可能的原因）。 Puanani和Victoria确定了一个可能的根本原因。&lt;/li&gt;
  &lt;li&gt;上午10点59分（定制缓解措施）。几个团队成员致力于重建二进制文件以推送一个新配置，该配置将从不同位置获取图像。&lt;/li&gt;
  &lt;li&gt;上午11:59（找到根本原因并解决问题）。 Tugay和GCR值班人员取消了GCR缓存，并从其欧洲存储层中清除了损坏的图像。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在步骤2（找到可能的原因）之后的通用预案在这里将非常有用。如果响应者在发现问题后将所有发布回滚到已知的良好状态，则故障将在上午10点之前得到缓解。为了缓解故障，不必完全了解详细信息——你只需要知道根本原因的位置即可。能够在完全理解其中断之前缓解中断对于运行具有高可用性的强大服务至关重要。&lt;/p&gt;

&lt;p&gt;在这种情况下，响应者可以从某种回滚的工具中受益。通用预案工具确实需要时间来开发。创建通用缓解工具的正确时间是在故障发生之前，而不是在应对紧急情况时。浏览postmortems是一种发现缓解故障的好方法，这些缓解故障在回滚过程中非常有用，并将它们构建到服务中，以便在将来更好地管理故障。&lt;/p&gt;

&lt;p&gt;重要的是要记住，第一响应者必须优先考虑通用预案，否则解决问题的时间还会拉的很长。实施通用预案措施（例如回滚）可加快恢复速度并使客户更快乐。最终，客户并不关心你是否完全理解导致中断的原因。他们想要的是停止接收错误。将控制影响作为首要任务，应以下列方式处理积极故障:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;评估故障的影响。&lt;/li&gt;
  &lt;li&gt;减轻影响。&lt;/li&gt;
  &lt;li&gt;对故障进行根本原因分析。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;故障结束后，定位故障的原因并写下事后分析报告。之后，你可以运行故障响应练习演练来修复系统中的漏洞，并且工程师可以在项目中处理这些漏洞。&lt;/p&gt;

&lt;h3 id=&quot;案例研究3断电闪电从未发生过两次直到它发生&quot;&gt;案例研究3：断电—闪电从未发生过两次……直到它发生&lt;/h3&gt;

&lt;p&gt;前面的示例表明，如果没有良好的故障响应策略时，会出现什么问题。下一个示例演示了成功管理的事件。当你遵循一个定义良好且清晰的响应协议时，您甚至可以轻松地处理罕见或异常的事件。&lt;/p&gt;

&lt;h4 id=&quot;背景-2&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;电网事件（例如雷击）导致进入数据中心设施的电力变化很大。影响电网的雷击是罕见的，但并不出人意料。Google可以使用备用发电机和电池来防止突发的意外断电，这些设备经过了充分的测试，并且已知可以在这些情况下正常工作。&lt;/p&gt;

&lt;p&gt;Google的许多服务器都有大量的磁盘连接到它们，这些磁盘位于服务器上方或下方的一个单独的托盘上。这些托盘有自己的不间断电源(UPS)电池。当停电发生时，备用发电机会启动，但启动需要几分钟时间。在此期间，连接到服务器和磁盘托盘上的备用电池提供电力，直到备用发电机完全运行，从而防止电网事件影响数据中心的运行。&lt;/p&gt;

&lt;h4 id=&quot;故障-2&quot;&gt;故障&lt;/h4&gt;

&lt;p&gt;2015年年中，比利时谷歌数据中心附近的电网在两分钟内被闪电击中四次。数据中心的备用发电机被激活，为所有的机器供电。当备份发电机启动时，大多数服务器都使用备用电池运行了几分钟。&lt;/p&gt;

&lt;p&gt;磁盘托盘中的UPS电池并没有在第三次和第四次雷击时将电量用在备用电池上，因为雷击间隔太近了。结果，磁盘托盘失去了电力，直到备用发电机开始工作。这些服务器没有断电，但无法访问那些有电的磁盘。&lt;/p&gt;

&lt;p&gt;在持久磁盘存储中丢失大量磁盘托盘会导致许多在Google Compute Engine(GCE)上运行的虚拟机(VM)实例出现读写错误。持久磁盘SRE在调用时立即发送了这些错误。一旦持久磁盘SRE团队确定了影响，就会向所有受影响的各方通报故障。持久磁盘SRE on-call人员承担了故障指挥官的角色。&lt;/p&gt;

&lt;p&gt;经过利益相关者之间的初步调查和沟通，我们确定:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;由于临时断电而丢失磁盘托盘的每台机器都需要重新启动。&lt;/li&gt;
  &lt;li&gt;在等待重新启动时，一些客户VMs在读取和写入磁盘时出现问题。&lt;/li&gt;
  &lt;li&gt;任何同时具有磁盘托盘和客户vm的主机都不能在不丢失未受影响的客户vm的情况下“重新启动”。持久磁盘SRE请求GCE SRE将未受影响的vm迁移到其他主机。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;持久磁盘SRE的主要值班人员的保留了IC角色，因为该团队对客户影响的可视性最强。&lt;/p&gt;

&lt;p&gt;运维团队成员的任务如下:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;安全恢复电源，使用电网电源而不是备用发电机。&lt;/li&gt;
  &lt;li&gt;重新启动所有非vm主机。&lt;/li&gt;
  &lt;li&gt;协调持久磁盘SRE和GCE SRE，在重新启动之前安全地将vm从受影响的机器移开。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;前两个目标被清楚地定义、很好地理解和记录。数据中心运维值班人员立即开始安全恢复电源，定期向IC提供状态报告。持久磁盘SRE定义了重新启动所有机器而不是虚拟机的程序。一个团队成员开始重新启动这些机器。&lt;/p&gt;

&lt;p&gt;第三个目标更加模糊，不包括任何现有的程序。故障指挥员指派了一个专门的行动小组成员与GCE SRE和持久磁盘SRE进行协调。这些团队合作将VMs安全地从受影响的机器移开，以便重新启动受影响的机器。IC密切关注着他们的进展，并意识到这项工作需要快速编写新的工具。IC组织了更多的工程师向运维团队报告，以便他们能够创建必要的工具。&lt;/p&gt;

&lt;p&gt;沟通负责人观察并询问所有与事件相关的活动，并负责向多个受众报告准确的信息：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;公司领导需要关于问题严重程度的信息，并确保问题得到解决。&lt;/li&gt;
  &lt;li&gt;有存储问题的团队需要知道他们的存储何时可以再次完全可用。&lt;/li&gt;
  &lt;li&gt;需要主动告知外部客户他们的磁盘在这个云区域的问题。&lt;/li&gt;
  &lt;li&gt;提交支持票据的特定客户需要知道他们所看到的问题的更多信息，以及关于解决方案和时间安排的建议。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在我们减轻了最初的客户影响之后，我们需要做一些后续工作，例如:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;诊断为什么磁盘托盘使用的UPS失败，并确保它不会再次发生。&lt;/li&gt;
  &lt;li&gt;更换发生故障的数据中心的电池。&lt;/li&gt;
  &lt;li&gt;手动清除由于同时丢失这么多存储系统而导致的“卡住”操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;事后分析显示，只有一小部分未写入磁盘——在事故发生期间断电的机器上的等待写入操作。由于持久磁盘快照和所有云存储数据都存储在多个数据中心中以实现冗余，因此只有0.000001%的运行GCE计算机的数据丢失，并且只有运行实例的数据存在风险。&lt;/p&gt;

&lt;h4 id=&quot;总结-2&quot;&gt;总结&lt;/h4&gt;

&lt;p&gt;通过及早通报故障，并以明确的领导组织结构有效地处理了这个复杂的故障。&lt;/p&gt;

&lt;p&gt;故障指挥官将恢复电源和重启服务器的正常问题委托给了适当的运维负责人。工程师们致力于解决这个问题，并将他们的进展报告给运维主管。&lt;/p&gt;

&lt;p&gt;要同时满足GCE和持久磁盘的需求，更复杂的问题需要在多个团队之间进行协调决策和交互。事故指挥员确保从两个小组中指派适当的行动小组成员来处理事故，并直接与他们一起工作，朝着解决问题的方向努力。事故指挥官明智地将注意力集中在事故最重要的方面：尽快解决受影响客户的需求。&lt;/p&gt;

&lt;h3 id=&quot;案例研究4pagerduty事件响应&quot;&gt;案例研究4：PagerDuty事件响应&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;作者：PagerDuty的Arup Chakrabarti&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;PagerDuty在几年的时间里开发并完善了内部故障响应实践。最初，配备了一名常设的全公司故障指挥官，并为每个服务配备专门的工程师参与故障响应。随着PagerDuty发展到超过400名员工和几十个工程团队，故障响应过程也发生了变化。每隔几个月都会仔细检查故障响应流程，并更新它们以反映业务需求。所有的经验总结都记录在https://response.pagerduty.com上。我们的故障响应过程并不是一成不变的；它们就像我们的业务一样不断变化和发展。&lt;/p&gt;

&lt;h4 id=&quot;pagerduty重大事件响应&quot;&gt;PagerDuty重大事件响应&lt;/h4&gt;

&lt;p&gt;通常，小的故障只需要一个值班工程师来响应。当涉及到更大的故障时，我们非常重视团队合作。在高压力和高影响的情况下，工程师不应该感到孤独。我们使用以下技巧来促进团队合作：&lt;/p&gt;

&lt;h4 id=&quot;参与模拟演习&quot;&gt;参与模拟演习&lt;/h4&gt;

&lt;p&gt;我们教授团队合作的一个方法是参加“失败星期五”。PagerDuty从Netflix公司的Simian Army(猿人部队)里汲取灵感，制作了这个节目。最初，Failure Friday是一个手动的故障注入练习，目的是了解更多关于我们的系统可能崩溃的方式。今天，我们还使用这个每周练习来重现生产和事件响应场景中的常见问题。&lt;/p&gt;

&lt;p&gt;在“失败星期五”开始之前，我们提名一个故障指挥官(通常是一个训练成为IC的人)。在进行故障注入练习时，他们应该表现得像真正的IC一样。在整个演练过程中，主题专家使用与实际事件相同的过程和术语。这种做法既使新值班工程师熟悉事故障响应语言和流程，又为经验丰富的值班工程师提供了一种复习。&lt;/p&gt;

&lt;h4 id=&quot;玩限时模拟游戏&quot;&gt;玩限时模拟游戏&lt;/h4&gt;

&lt;p&gt;虽然“失败星期五”练习对工程师在不同角色和过程中培训大有帮助，但它们不能完全复制实际重大事故的紧迫性。我们使用具有时限紧迫性的模拟游戏来捕捉事件响应的这一方面。&lt;/p&gt;

&lt;p&gt;“继续说下去，没有人会爆炸”是我们大量使用的一款游戏。它要求玩家在限定时间内共同拆除炸弹。游戏的压力和密集的交流性质迫使玩家有效地合作和有效地协同工作。&lt;/p&gt;

&lt;h4 id=&quot;从以往的故障中吸取教训&quot;&gt;从以往的故障中吸取教训&lt;/h4&gt;

&lt;p&gt;从过去的故障中学习可以帮助我们更好地应对未来的重大故障。为此，我们进行并定期审查故障分析报告。&lt;/p&gt;

&lt;p&gt;PagerDuty事后调查过程包括开放式会议和全面记录。通过使这些信息易于访问和发现，我们的目标是减少未来事件的解决时间，或防止未来故障一起发生。&lt;/p&gt;

&lt;p&gt;我们还会记录所有涉及重大故障的电话，这样我们就可以从实时通信feed中学习。&lt;/p&gt;

&lt;p&gt;让我们看看最近的一个故障，其中PagerDuty不得不利用我们的故障响应流程。故障发生在2017年10月6日，持续时间超过10个小时，但对客户的影响非常小。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;下午7:53 PagerDuty SRE团队的一名成员被告知PagerDuty内部NTP服务器正在显示时钟漂移。值班的SRE验证了所有自动恢复操作已经执行，并完成了相关运行手册中的缓解步骤。这个工作记录在SRE团队的专用Slack频道中。&lt;/li&gt;
  &lt;li&gt;晚上8点20分，PagerDuty软件团队A的一名成员收到了关于他们服务中时钟漂移错误的自动警报。软件团队A和SRE团队致力于解决这个问题。&lt;/li&gt;
  &lt;li&gt;晚上9点17分，PagerDuty软件团队B的一名成员收到了关于他们服务上时钟漂移错误的自动警报。B组的工程师加入了Slack频道，该频道已经对问题进行了测试和调试&lt;/li&gt;
  &lt;li&gt;晚上9点49分，值班SRE宣布发生重大故障，并通知值班故障指挥官。&lt;/li&gt;
  &lt;li&gt;晚上9点55分，IC组建了响应团队，其中包括依赖NTP服务的每个on-call工程师，以及PagerDuty的客户支持。IC让响应小组加入专门的电话会议和Slack频道。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在接下来的8个小时里，响应小组致力于解决和减轻这个问题。当我们运行手册中的程序没有解决问题时，响应团队开始有条理地尝试新的恢复选项。&lt;/p&gt;

&lt;p&gt;在这段时间里，我们每四个小时轮换一次on-call工程师和IC。这样做可以鼓励工程师们休息，并为响应团队带来新的想法。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;上午5：33，值班的SRE对NTP服务器进行了配置更改。&lt;/li&gt;
  &lt;li&gt;上午6点13分，IC与他们各自的值班工程师验证所有的服务都恢复了。验证完成后，IC关闭了电话会议和Slack频道，并宣布故障完成。鉴于NTP服务的广泛影响，有必要进行事后分析。在结束故障之前，IC将事后分析分配给值班的SRE小组。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;用于故障响应的工具&quot;&gt;用于故障响应的工具&lt;/h4&gt;

&lt;p&gt;我们的故障响应流程利用了三个主要工具：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;PagerDuty。我们将所有值班的信息、服务所有权、事后分析、事件元数据等存储在PagerDuty中。这使我们能够在出现问题时迅速组建正确的团队。&lt;/li&gt;
  &lt;li&gt;Slack。我们保持一个专门的频道(#incident-war-room)，作为所有主题专家和故障指挥官的聚会场所。该频道主要用作记录员的信息分类，用于捕获操作、所有者和时间戳。&lt;/li&gt;
  &lt;li&gt;电话会议&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当被要求加入任何故障响应时，on-call的工程师需要拨打一个固定的会议电话号码。我们希望所有的协调决策都在电话会议中做出，而决策结果都记录在Slack中。我们发现这是做出决策的最快方法。我们还会记录每次通话，以确保我们可以重新创建任何时间轴，以防记录员遗漏了重要的细节。&lt;/p&gt;

&lt;p&gt;虽然Slack和电话会议是我们的沟通渠道，但你应该使用最适合贵公司及其工程师的沟通方式。&lt;/p&gt;

&lt;p&gt;在PagerDuty中，我们如何处理响应直接关系到公司的成功。我们不是毫无准备地面对这些故障，而是通过进行模拟练习，回顾以往的故障，选择合适的工具来帮助我们应对可能发生的任何重大事故障，从而有目的地为故障做准备。&lt;/p&gt;

&lt;h2 id=&quot;将最佳实践付诸实践&quot;&gt;将最佳实践付诸实践&lt;/h2&gt;

&lt;p&gt;我们见过一些处理得很好的故障的例子，有些则没有。当警报提醒你一个问题的时候，已经来不及考虑如何处理这个故障了。开始考虑故障管理过程的时间是在故障发生之前。那么，在灾难降临之前，你如何准备并将理论付诸实践呢？本节提供了一些建议。&lt;/p&gt;

&lt;h3 id=&quot;故障响应训练&quot;&gt;故障响应训练&lt;/h3&gt;

&lt;p&gt;我们强烈建议培训应急人员来组织故障，这样他们在真正的紧急情况下就有一个模式可以遵循。知道如何组织一个故障，在整个故障中使用共同的语言，并分享相同的期望，可以减少沟通失误的可能性。&lt;/p&gt;

&lt;p&gt;完整的故障指挥系统方法可能超出了你的需要，但是你可以通过选择故障管理过程中对你的组织非常重要的部分来开发处理故障的框架。例如:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;让接听电话的人知道，他们可以在故障发生时委派和升级。&lt;/li&gt;
  &lt;li&gt;鼓励采取缓解措施。&lt;/li&gt;
  &lt;li&gt;定义故障指挥官、通信主管和运维主管角色。
你可以调整和总结你的故障响应框架，并创建一个PPT展示给新的团队成员。我们了解到，当人们能够将故障反应理论与实际场景和具体行动联系起来时，他们更容易接受故障响应训练。因此，一定要包括亲身实践的练习，分享过去发生的故障，分析哪些进展顺利，哪些进展不太顺利。还可以考虑使用专门从事事件响应课程和培训的外部机构。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;事先做好准备&quot;&gt;事先做好准备&lt;/h3&gt;

&lt;p&gt;除了故障响应训练，它还有助于事先为故障做好准备。使用以下的技巧和策略来做好准备。&lt;/p&gt;

&lt;h3 id=&quot;确定沟通渠道&quot;&gt;确定沟通渠道&lt;/h3&gt;

&lt;p&gt;事先决定并同意一个通信渠道(Slack, a phone bridge, IRC, HipChat，等等)——没有事故指挥官想在事故发生时做出这样的决定。练习使用它，这样就不会有意外了。如果可能的话，选择一个团队已经熟悉的沟通渠道，这样团队中的每个人都能舒服地使用它。&lt;/p&gt;

&lt;h3 id=&quot;让利益相关方知情&quot;&gt;让利益相关方知情&lt;/h3&gt;

&lt;p&gt;除非你承认某个事件正在发生并且正在被积极处理，否则人们会自动地认为没有采取任何措施来解决这个问题。同样，如果你在问题减轻或解决后忘记取消响应，人们会认为事件正在发生。你可以通过定期更新状态，在事件发生的整个过程中不断通知受众，从而抢占这种动态。准备一份联系人列表(请参阅下一条提示)可以节省宝贵的时间，确保你不会错过任何人。&lt;/p&gt;

&lt;p&gt;提前考虑如何起草、审查、批准和发布公共博客文章或新闻稿。在Google，团队寻求公关团队的指导。另外，为共享信息准备两三个现成的模板，确保值班的人知道如何发送它们。没有人愿意在没有指导原则的极端压力下写这些公告。这些模板使得与公众共享信息变得简单，压力最小。&lt;/p&gt;

&lt;h3 id=&quot;准备一份联系人列表&quot;&gt;准备一份联系人列表&lt;/h3&gt;

&lt;p&gt;事先准备好要发邮件或浏览的人的名单可以节省大量的时间和精力。在第180页的“案例研究2：如果可以的话缓存我”中，通讯主管通过发送电子邮件给预先准备好的GKE列表，发出了一个“全体人员待命”的电话。&lt;/p&gt;

&lt;h3 id=&quot;建立故障标准&quot;&gt;建立故障标准&lt;/h3&gt;

&lt;p&gt;有时很明显，告警问题确实是一个故障。其他时候，情况就不那么清楚了。有一个确定的标准列表来确定一个问题是否确实是一个事件是很有帮助的。一个团队可以通过查看过去的停机情况，并考虑到已知的高风险区域，从而得出一个可靠的标准列表。&lt;/p&gt;

&lt;p&gt;综上所述，在应对事件时，建立协调和沟通的共同基础是很重要的。确定沟通事件的方式，你的受众是谁，以及在事件中谁负责。这些指南易于制定，对缩短事件的解决时间有很大的影响。&lt;/p&gt;

&lt;h3 id=&quot;演练&quot;&gt;演练&lt;/h3&gt;

&lt;p&gt;故障管理过程中的最后一步是实践你的故障管理技能。通过在不太危急的情况下进行练习，你的团队会在闪电袭击时养成良好的习惯和行为模式——无论是比喻意义上还是字面意义上。通过培训介绍了事件响应理论之后，实践可以确保事件响应技能保持新鲜。&lt;/p&gt;

&lt;p&gt;有几种方法来进行故障管理演练。Google在全公司范围内运行弹性测试(称为灾难恢复测试，或DiRT；请参阅Kripa Krishnan的文章《抵御意外》(Weathering theUnexpected)。在这篇文章中，我们创建了一个可控的紧急情况，实际上并不影响客户。团队对受控的紧急情况做出反应，就好像这是真正的紧急情况一样。随后，各小组回顾了应急反应程序，并讨论了发生了什么。接受失败作为一种学习的方式，在发现的差距中发现价值，让我们的领导参与进来是成功在Google建立DiRT计划的关键。在较小的范围内，我们使用诸如“不幸之轮”(参见“网络可靠性工程中的灾难角色扮演”)等练习来应对特定事件。&lt;/p&gt;

&lt;p&gt;你还可以通过有意地将次要问题视为需要大规模响应的主要问题来练习事件响应。这可以让你的团队在现实世界中使用低风险的过程和工具进行实践。&lt;/p&gt;

&lt;p&gt;演练是一个尝试新的故障响应技能的友好方式。团队中任何可能深入到故障响应的人——SREs、开发人员、甚至客户支持和营销合作伙伴——都应该对这些策略感到满意。&lt;/p&gt;

&lt;p&gt;要进行演练，可以制造中断并允许你的团队对事件进行响应。你还可以从事后分析中制造中断，其中包含大量事件管理演练的想法。尽可能使用真实的工具来管理事件。考虑破坏你的测试环境，以便团队能够使用现有工具执行真正的故障排除。&lt;/p&gt;

&lt;p&gt;如果这些演习是周期性的，那么它们就会有用得多。你可以通过对每次练习进行跟踪，详细列出哪些做得好，哪些做得不好，以及如何更好地处理事情，来让演练产生影响。进行演练最有价值的部分是检查它们的结果，这可以揭示事件管理中的任何漏洞。一旦你知道了它们是什么，你就可以努力关闭它们。&lt;/p&gt;

&lt;h2 id=&quot;总结-3&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;当灾难来临时做好准备。如果你的团队定期实践和更新故障响应过程，那么当不可避免的故障发生时，便不会感到恐慌。&lt;/p&gt;

&lt;p&gt;在故障发生前，你需要与同事合作的圈子会随着事件的规模而扩大。当你和你不认识的人一起工作的时候，流程会帮助你建立你需要的快速解决方案的结构。我们强烈建议在未报警前提前建立这些流程。定期回顾并重复你的事件管理计划和剧本。&lt;/p&gt;

&lt;p&gt;事故指挥系统是一个简单的概念，很容易理解。它会根据公司的规模和事件的大小进行放大或缩小。虽然理解起来很简单，但实现起来却并不容易，尤其是在突然发生恐慌时。在紧急情况下保持冷静并遵循反应结构需要练习，练习可以建立“肌肉记忆”。这使你对待真正的紧急情况有了信心。&lt;/p&gt;

&lt;p&gt;我们强烈建议在你的团队繁忙的日程中抽出一些时间，定期实践事件管理。确保得到领导的支持，让他们有专门的实践时间，并确保他们了解事件响应如何工作，以防你需要让他们参与到真正的事件中。备灾可以从响应时间中节省宝贵的时间或数小时，并使你具有竞争优势。没有任何一家公司总是能把事情做好——从错误中吸取教训，继续前进，下次做得更好。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">写在开头: 故障总会发生！如何对故障快速进行有组织的响应？Google方式是一个很好的借鉴。 每个团队都希望服务没有故障发生，但这个世界是不完美的，像停电这类的事件还是会发生的。当一个紧急故障需要多人或团队协作解决时，响应故障并解决问题是我们要应对的。 处理故障的目标是减轻影响或将服务恢复到先前的状态。而故障管理意味着以有效的方式协调团队的响应工作，并确保响应者和利益相关者之间的信息流通。包括谷歌在内的许多科技公司都有自己的“最佳实践”来管理应急响应，并且在不断的完善。 故障管理的基本前提是以结构化的方式响应故障。大规模故障是单人无法解决的；结构化的应急响应可以减少混乱。在灾难发生之前制定沟通和协调规范，使你的团队可以集中精力解决故障，而不用担心沟通和协调问题。 故障响应的过程并不是困难的事情。大量实践经验总结可以为我们提供一些指导，比如在第一本SRE书中的《故障管理》章节。故障响应的基本原则包括以下内容： 保持清晰的指挥线 明确角色 随时掌握调试工作记录 响应进度的实时更新 本章介绍了如何在Google和PagerDuty中进行故障管理。通过一些Case说明我们在其中达到的目标以及没有达到的目标。 “将最佳实践付诸实践”（第191页）中的清单可以帮助你开始创建自己的故障响应实践。 Google故障管理 故障响应是用于响应和管理故障的系统。由一个框架和一组定义的程序组成。Google的故障响应系统是基于故障命令系统（ICS）实现的。 故障指挥系统 ICS成立于1968年，是消防员管理野火的一种方式。该框架提供了在故障响应期间进行通信和明确角色的标准化方法。基于该模型，公司后来采用ICS来应对计算机和系统故障。本章探讨了两个这样的框架：PagerDuty的故障响应流程和Google故障管理（IMAG）。 故障响应框架有三个共同目标，也称为事件管理的“三个C”（3C）： 协调响应工作 在故障响应者、组织内部和外部之间进行通信 保持对故障响应的控制 当故障响应出现问题时，罪魁祸首可能出现在其中一个方面。掌握3C对于有效的事件响应至关重要。 故障响应中的主要角色 故障响应中的主要角色是故障指挥官（IC），通信主管（CL）和操作或行动主管（OL）。 IMAG将这些角色组织成一个层次结构：IC引导故障响应，CL和OL报告给IC。 灾难发生时，声明故障的人通常会进入IC角色并指挥故障的高级别状态。 IC专注于3C，并执行以下操作： 命令和协调故障响应，根据需要委派角色。默认情况下，IC会假定尚未委派的所有角色。 有效沟通。 保持对故障响应的控制。 与其他相应人员合作解决此事件。 IC可以将其角色交给其他人并承担OL角色，或将OL角色分配给其他人。 OL通过应用工具来缓解或解决故障，从而对故障做出响应。 虽然IC和OL致力于减轻和解决故障，但CL是故障响应团队的消息通讯核心。 CL的主要职责包括定期向故障响应团队和利益相关者实时更新故障响应的进展。 CL和OL都可以领导一个团队来帮助管理他们特定的故障响应区域。这些团队可以根据需要进行扩展或收缩。如果故障变得足够小，那么CL角色就可以被包含到IC角色中。 案例研究 以下四个大型故障说明故障响应如何在实践中起作用。其中三个案例研究来自Google，最后一个是来自PagerDuty的案例研究，该研究提供了其他组织如何使用ICS派生框架的观点。Google的例子以一个没有有效管理的故障开始，并在有故障管理时对故障响应的改观。 案例研究1：软件错误—灯还开着，但没有人(Google HOME) 这个例子展示了故障发生初期不进行通报，在没有工具快速有效地响应故障时，团队是如何响应故障的。虽然这一故障在没有重大灾难的情况下得到了解决，但是越早组织，就会产生更好的效果。 背景 Google Home是一款智能扬声器和家庭助手，可响应语音命令。语音命令与Google Home的软件（称为Google智能助理）进行交互。 当用户说出一个触发Google智能助理的热门词组时，就会启动与Google Home的互动。多个用户可以通过培训助理来监听给定的热门词汇，从而使用相同的Google Home设备。识别扬声器的热词模型是在客户端进行的，但是数据（例如扬声器识别文件）存储在服务器上。服务器处理数据的双向流，为了在繁忙的时候处理超载，服务器对Google助手有一个配额政策。为了保护服务器免受过大请求值的影响，配额限制明显高于在给定设备上的Google助手的基准使用量。 Google智能助理1.88版中的错误导致扬声器识别文件的获取频率超过预期的50倍，超出此配额。最初，美国中部的GoogleHome用户只有很少的流量损失。然而，随着所有GoogleHome设备的推出逐步增加，用户在2017年6月3日的周末期间丢失了一半的请求。 故障 太平洋标准时间5月22日星期一上午11:48，谷歌主页on-call开发人员Jasper正在看着每秒查询（QPS）图表并注意到一些奇怪的事情：谷歌助理每隔30分钟就会收到培训数据，而不是像预期的那样每天一次。他停止了已经推到了25％用户的1.88版本发布。他提了一个漏洞——称之为bug 12345——用谷歌的漏洞追踪系统来探索为什么会发生这种情况。在这一问题上，他指出，谷歌助手每天会对数据进行48次的ping请求，导致其超过了QPS的容量。 另一位开发人员Melinda将此问题与先前报告的漏洞相关联，我们将其称为错误67890：每当应用程序刷新设备身份验证和注册状态时，语音处理器都会重新启动。该版本将在版本1.88发布后修复，因此该团队要求临时增加模型的配额，以减轻额外查询的过载。 版本1.88版本再次启动并继续推出，到5月31日星期三达到50％的用户。不幸的是，该团队后来了解到错误67890，虽然负责一些额外的流量，但并不是Jasper所注意到的更频繁的取回的真正根源。 同一天早上，客户开始向Google支持小组报告问题：每当有人说“OK Google”（或任何其他热门词汇来激活Google Home）时，设备都会回复并显示错误消息，此问题阻止用户向Google智能助理发出命令。该团队开始调查可能导致用户报告的错误的原因。他们怀疑配额问题，所以他们要求增加配额，这似乎可以缓解这个问题。 与此同时，该团队继续调查bug 12345，看看是什么触发了错误。尽管在调试过程的早期就建立了配额连接，但是客户端和服务器开发人员之间的误解导致了开发人员在故障排除过程中走错了方向，而完整的解决方案仍然无法实现。 该团队还对为什么Google智能助理的流量不断达到配额限制感到困惑。客户端和服务器开发人员对客户端错误感到困惑，这些错误似乎没有被服务器端的任何问题触发。开发人员将日志记录添加到下一个版本，以帮助团队更好地理解错误，并希望在解决事件方面取得进展。 截至6月1日星期四，用户报告此问题已得到解决。没有报道任何新问题，因此版本1.88版本继续推出。但是，原始问题的根本原因尚未确定。 在6月3日的周六早上，版本1.88的发布超过了50%。这一发布是在一个周末进行的，当时开发团队并没有人值班。该团队并没有遵循在工作日期间执行部署的最佳实践，以确保开发人员在场。 在6月3日星期六，当版本1.88发布的时候达到了100%，客户端又一次达到了Google助理流量的服务器限制。来自客户的错误报告开始出现。谷歌员工报告称，他们的Google HOME设备出现了错误。Google Home支持小组收到了大量关于此问题的客户来电，反馈也包括了推文和Reddit帖子。 Google Home的帮助论坛也出现了正在讨论这个问题的帖子。尽管有大量的用户报告和反馈，bug并没有升级到更高的优先级。 6月4日星期日，随着客户报告数量的不断增加，支持团队最终将错误优先级提升到最高水平。该团队没有通报故障，而是继续通过“常规”方法解决问题—使用bug跟踪系统进行通信。on-call开发人员注意到一个数据中心集群中的错误率，SRE进行了ping操作，要求他们排除它。与此同时，该团队提出了另一项增加配额的请求。之后，开发团队的一名工程师注意到流量通道已将错误推入其他单元格，这为配额问题提供了额外的证据。下午3：33，开发人员团队经理再次增加了Google智能助理的配额，并停止了对用户的影响。故障结束。该团队此后不久确定了根本原因（参见前面的“环境”部分）。 总结 故障处理的某些方面进展顺利，而其他方面则有改进的余地。 首先，工程师在周末处理故障并为解决问题提供了宝贵的意见，这既好又坏，虽然团队重视这些人在周末所付出的时间和精力，但成功的故障管理不应该依赖于个人的英勇行为。如果无法联系到开发人员怎么办？Google鼓励良好的工作与生活平衡——工程师不应在空闲时间内解决与工作相关的问题。相反，我们应该在工作时间进行部署，或者组织一次在工作时间之外提供付费保障的on-call轮换。 接下来，该团队致力于缓解这一问题。Google首先要求是阻止故障的影响，然后找到根本原因（除非根本原因只是在早期发现）。一旦问题得到缓解，找到根本原因同样重要，以防止问题再次发生。在这个故障事件中，成功地在三个不同的场合控制了故障影响，但团队只有在发现根本原因后才能防止问题再次发生。在第一次故障得到控制之后，最好是等到根本原因完全确定后再开始实施，避免周末发生大混乱。最后，在问题首次出现时，团队没有通告故障。我们的经验表明，管理故障可以更快地解决。尽早通告故障可确保： 防止客户端和服务器开发人员之间的沟通错误。 根源识别和故障解决发生得更快。 相关团队提前进入，使外部沟通更快捷顺畅。 集中式通信是IMAG协议的重要原则。例如，当灾难发生时，SRE通常聚集在一个“作战室”中。作战室可以是像会议室一样的真实环境，也可以是虚拟的：团队可能聚集在IRC频道。这里的关键是将所有的故障响应者集中在一个地方，并实时沟通以管理并最终解决故障。 案例研究2：服务故障—如果可以，请对我进行缓存 下面的故障说明了当一个“专家级”团队试图调试一个具有复杂交互系统时会发生什么——没有一个人能够掌握所有的细节。 背景 Kubernetes是一个由许多公司和个人贡献者共同建立的开源容器管理系统。 Google Kubernetes Engine（或称GKE）是Google管理的系统，可为用户创建、托管和运行Kubernetes群集。用户以最适合他们的方式上传和管理工作负载。当用户首次创建新群集时，GKE将获取并初始化其群集所需的Docker镜像。理想情况下，这些组件是在内部获取和构建的，因此我们可以验证它们。但Kubernetes是一个开源系统，新的依赖关系有时会让系统出现问题。 故障 在太平洋标准时间一个星期四上午6点41分，伦敦GKE的on-call人员Zara寻访了多个区域的CreateCluster探查器故障，新的集群没有被成功地创建。 Zara检查了prober仪表板，发现两个区域的故障率都在60％以上。他确认此问题影响了用户创建新群集的功能，但现有群集的流量未受影响。 Zara遵循GKE的文件化程序，于上午7:06对故障进行了通告。最初，有四人参与处理了这一故障： Zara是第一个注意到这个问题的人，因此被指定为默认故障指挥官。 Zara的两名队友。 Rohit由故障程序分配的客户支持工程师。 由于Rohit总部设在苏黎世，Zara（IC）开设了一个GKE Panic IRC频道，团队可以在那里一起调试。在另外两个SRE深入调查监控和错误信息时，Zara解释了故障及其对Rohit的影响。截至上午7点24分，Rohit向用户发布通知称，创建集群的功能在欧洲西部地区出现故障。这提高了故障的等级。早上7点到8点20分，Zara、Rohit和其他人一直致力于解决这个问题。他们检查了集群启动日志，其中显示了一个错误：无法运行Kubelet；无法创建证书签名——他们需要确定证书创建的哪个部分失败。SRE研究了网络、资源可用性和证书签名过程。所有这些似乎都运作得很好。上午8点22分，Zara向故障管理系统发布了故障调查摘要，并寻找可以帮助他的开发人员。值得庆幸的是，GKE有一位on-call开发人员可以在紧急情况下提供协助。开发人员Victoria加入了该频道。他要求跟踪错误，并要求团队将问题上报给基础架构的on-call团队。现在是上午8点45分，第一个西雅图SRE，II-Seong来到办公室，轻轻地泡了杯咖啡，为这一天做好了准备。 II-Seong是一名资深的SRE，在故障响应方面拥有多年的经验。当他得知正在发生的故障时，他立即进入故障响应频道。首先，II-Seong根据警报的时间检查当天的发布情况，并确定当天的发布不会导致事故障的发生。然后，他开始整理工作文件，收集笔记。他建议Zara将故障升级为基础架构，云网络和计算引擎团队，以尽可能多地消除这些根源。由于Zara升级，其他人加入了故障响应： GKE节点的开发负责人 云网络值班人员 计算引擎值班人员 Herais，另一个西雅图SRE 上午9点10分，故障频道有十几个参与者。故障发生2.5小时，没有找到根本原因，也没有控制住影响。沟通成为了一种挑战。通常情况下，从伦敦到西雅图的on-call交接时间是上午10点，但Zara决定在上午10点之前将故障指挥权移交给II-Seong，因为他对IMAG有更多的经验。作为故障指挥官，II-Seong建立了一个正式的响应组织结构来解决这一事件。然后，他指定Zara为Ops Lead，Herais为通信（Comms）负责人。 Rohit仍然是外部沟通负责人。 Herais立即发送了一封“全体人员齐上阵”的电子邮件给GKE，包括所有开发人员的负责人，并要求专家加入故障响应。到目前为止，故障响应者了解到了以下情况： 当节点试图向主机注册时，集群创建失败。 错误消息表明证书签名模块是罪魁祸首。 欧洲所有集群创建都失败了;其地区都正常。 欧洲没有其他GCP服务出现网络或配额问题。 之后，GKE安全团队成员Puanani加入了这项工作。他注意到证书签发者没有开始。证书签发者试图从DockerHub中提取图像，图像似乎已损坏。Victoria（GKE on-call开发人员）在两个地理位置运行了Docker的图像拉取命令。它在欧洲的集群上运行失败，在美国的集群上成功。这表明欧洲集群是问题的所在。上午9点56分，该团队确定了一个看似合理的根本原因。 因为DockerHub有一个外部依赖，故障控制和故障溯源将特别具有挑战性。对于Docker的工作人员来说，第一个控制故障的方法是快速修复。第二个选择是重新配置集群，从不同的位置获取图像，例如Google容器注册表（GCR），Goole的安全图像托管系统。所有其他依赖项，包括对图像的其他引用，都位于GCR中。 II-Seong指派负责人追查这两种可能。然后，他委派了一个小组去调查失败的集群。讨论对于IRC来说过于密集，因此详细的调试转移到共享文档，IRC成为决策的中心。 对于第二个可能，推送新配置意味着重建二进制文件，这需要大约一个小时。上午10:59，当团队完成90％的重建工作时，他们发现另一个使用错误图像获取路径的位置。作为回应，他们不得不重新启动构建。当IRC的工程师们致力于这两个缓解方案时，SRE中的Tugay有了一个想法。如果他们拦截了Docker的pull请求并使用内部缓存图像替换Docker的响应，那么如何重建配置并将其推出（一个笨重且有风险的过程）呢？ GCR有一个镜像可以做到这一点。 Tugay联系了GCR的SRE团队，他们确认团队可以在Docker配置上设置&amp;lt;–registry-mirror =https://mirror.gcr.io&amp;gt;。Tugay开始设置此功能，并发现镜像已经到位！ 上午11点29分，Tugay向IRC报告说这些图像是从GCR镜像中拉出来的，而不是DockerHub。上午11点37分，故障指挥官在呼叫GCR值班人员。上午11点59分，GCR值班人员在欧洲存储层清除了腐败的图像。截止到下午12点11分，所有欧洲地区的误差都降至0％。 故障结束了。剩下的只是清理工作，故障文档的整理。在修复之前，CreateCluster在欧洲失败了6小时40分钟。在IRC中，整个事件中出现了41个独立用户，IRC日志扩展到26,000个单词。这项工作在不同的时间分拆了七个IMAG工作组，并且在任何时候都有多达四个同时工作。六支on-call队伍的参与，后期包含28个行动项目。 总结 无论从哪个角度，GKE CreateCluster中断是一件重大故障。让我们探讨一下哪些事情进行得很顺利，哪些事情本可以处理得更好。 什么进展顺利？该团队有几个记录在案的升级路径，熟悉故障响应策略。GKE值班人员 Zara很快就证实了这种影响正在影响实际客户。然后，她使用了一个常用的故障管理系统来通知Rohit，Rohit将故障告知客户。 什么可以更好地处理？该服务本身有一些值得关注的方面。复杂性和对专家的依赖是有问题的。记录对于故障定位来说是不够的，并且团队因为DockerHub上的变化而分散了注意力，而且这不是真正的问题。 故障发生之初，故障指挥官没有制定一个正式的应急预案。虽然Zara承担了这一角色并将对话转移到IRC，但他可以更积极地协调信息和做出决策。结果，少数应急人员在没有协调的情况下进行自己的调查。II-Seong在第一个警报后两小时就建立了正式的故障响应组织。 最后，该故障揭示了GKE灾难准备方面的一个不足：该服务没有任何可以减少用户影响的早期通用预案。通用预案是第一响应者采取的降低影响的操作流程，甚至在完全理解根本原因之前都可以使用。例如，当中断与发布周期相关时，响应者可以回滚最近的版本，或者重新配置负载平衡器以避免错误被本地化时出现区域。值得注意的是，通用预案也有弊端，可能会导致其他服务中断。然而，虽然它们可能具有比精确解决方案更广泛的影响，但是当团队发现并解决根本原因时，它们可以快速到位以停止进一步扩大影响。 让我们再次查看此故障的时间表，看看通用预案可能在哪些方面有效： 上午7点（评估影响）。 Zara确认用户受到中断的影响。 上午9:56（找到可能的原因）。 Puanani和Victoria确定了一个可能的根本原因。 上午10点59分（定制缓解措施）。几个团队成员致力于重建二进制文件以推送一个新配置，该配置将从不同位置获取图像。 上午11:59（找到根本原因并解决问题）。 Tugay和GCR值班人员取消了GCR缓存，并从其欧洲存储层中清除了损坏的图像。 在步骤2（找到可能的原因）之后的通用预案在这里将非常有用。如果响应者在发现问题后将所有发布回滚到已知的良好状态，则故障将在上午10点之前得到缓解。为了缓解故障，不必完全了解详细信息——你只需要知道根本原因的位置即可。能够在完全理解其中断之前缓解中断对于运行具有高可用性的强大服务至关重要。 在这种情况下，响应者可以从某种回滚的工具中受益。通用预案工具确实需要时间来开发。创建通用缓解工具的正确时间是在故障发生之前，而不是在应对紧急情况时。浏览postmortems是一种发现缓解故障的好方法，这些缓解故障在回滚过程中非常有用，并将它们构建到服务中，以便在将来更好地管理故障。 重要的是要记住，第一响应者必须优先考虑通用预案，否则解决问题的时间还会拉的很长。实施通用预案措施（例如回滚）可加快恢复速度并使客户更快乐。最终，客户并不关心你是否完全理解导致中断的原因。他们想要的是停止接收错误。将控制影响作为首要任务，应以下列方式处理积极故障: 评估故障的影响。 减轻影响。 对故障进行根本原因分析。 故障结束后，定位故障的原因并写下事后分析报告。之后，你可以运行故障响应练习演练来修复系统中的漏洞，并且工程师可以在项目中处理这些漏洞。 案例研究3：断电—闪电从未发生过两次……直到它发生 前面的示例表明，如果没有良好的故障响应策略时，会出现什么问题。下一个示例演示了成功管理的事件。当你遵循一个定义良好且清晰的响应协议时，您甚至可以轻松地处理罕见或异常的事件。 背景 电网事件（例如雷击）导致进入数据中心设施的电力变化很大。影响电网的雷击是罕见的，但并不出人意料。Google可以使用备用发电机和电池来防止突发的意外断电，这些设备经过了充分的测试，并且已知可以在这些情况下正常工作。 Google的许多服务器都有大量的磁盘连接到它们，这些磁盘位于服务器上方或下方的一个单独的托盘上。这些托盘有自己的不间断电源(UPS)电池。当停电发生时，备用发电机会启动，但启动需要几分钟时间。在此期间，连接到服务器和磁盘托盘上的备用电池提供电力，直到备用发电机完全运行，从而防止电网事件影响数据中心的运行。 故障 2015年年中，比利时谷歌数据中心附近的电网在两分钟内被闪电击中四次。数据中心的备用发电机被激活，为所有的机器供电。当备份发电机启动时，大多数服务器都使用备用电池运行了几分钟。 磁盘托盘中的UPS电池并没有在第三次和第四次雷击时将电量用在备用电池上，因为雷击间隔太近了。结果，磁盘托盘失去了电力，直到备用发电机开始工作。这些服务器没有断电，但无法访问那些有电的磁盘。 在持久磁盘存储中丢失大量磁盘托盘会导致许多在Google Compute Engine(GCE)上运行的虚拟机(VM)实例出现读写错误。持久磁盘SRE在调用时立即发送了这些错误。一旦持久磁盘SRE团队确定了影响，就会向所有受影响的各方通报故障。持久磁盘SRE on-call人员承担了故障指挥官的角色。 经过利益相关者之间的初步调查和沟通，我们确定: 由于临时断电而丢失磁盘托盘的每台机器都需要重新启动。 在等待重新启动时，一些客户VMs在读取和写入磁盘时出现问题。 任何同时具有磁盘托盘和客户vm的主机都不能在不丢失未受影响的客户vm的情况下“重新启动”。持久磁盘SRE请求GCE SRE将未受影响的vm迁移到其他主机。 持久磁盘SRE的主要值班人员的保留了IC角色，因为该团队对客户影响的可视性最强。 运维团队成员的任务如下: 安全恢复电源，使用电网电源而不是备用发电机。 重新启动所有非vm主机。 协调持久磁盘SRE和GCE SRE，在重新启动之前安全地将vm从受影响的机器移开。 前两个目标被清楚地定义、很好地理解和记录。数据中心运维值班人员立即开始安全恢复电源，定期向IC提供状态报告。持久磁盘SRE定义了重新启动所有机器而不是虚拟机的程序。一个团队成员开始重新启动这些机器。 第三个目标更加模糊，不包括任何现有的程序。故障指挥员指派了一个专门的行动小组成员与GCE SRE和持久磁盘SRE进行协调。这些团队合作将VMs安全地从受影响的机器移开，以便重新启动受影响的机器。IC密切关注着他们的进展，并意识到这项工作需要快速编写新的工具。IC组织了更多的工程师向运维团队报告，以便他们能够创建必要的工具。 沟通负责人观察并询问所有与事件相关的活动，并负责向多个受众报告准确的信息： 公司领导需要关于问题严重程度的信息，并确保问题得到解决。 有存储问题的团队需要知道他们的存储何时可以再次完全可用。 需要主动告知外部客户他们的磁盘在这个云区域的问题。 提交支持票据的特定客户需要知道他们所看到的问题的更多信息，以及关于解决方案和时间安排的建议。 在我们减轻了最初的客户影响之后，我们需要做一些后续工作，例如: 诊断为什么磁盘托盘使用的UPS失败，并确保它不会再次发生。 更换发生故障的数据中心的电池。 手动清除由于同时丢失这么多存储系统而导致的“卡住”操作。 事后分析显示，只有一小部分未写入磁盘——在事故发生期间断电的机器上的等待写入操作。由于持久磁盘快照和所有云存储数据都存储在多个数据中心中以实现冗余，因此只有0.000001%的运行GCE计算机的数据丢失，并且只有运行实例的数据存在风险。 总结 通过及早通报故障，并以明确的领导组织结构有效地处理了这个复杂的故障。 故障指挥官将恢复电源和重启服务器的正常问题委托给了适当的运维负责人。工程师们致力于解决这个问题，并将他们的进展报告给运维主管。 要同时满足GCE和持久磁盘的需求，更复杂的问题需要在多个团队之间进行协调决策和交互。事故指挥员确保从两个小组中指派适当的行动小组成员来处理事故，并直接与他们一起工作，朝着解决问题的方向努力。事故指挥官明智地将注意力集中在事故最重要的方面：尽快解决受影响客户的需求。 案例研究4：PagerDuty事件响应 作者：PagerDuty的Arup Chakrabarti PagerDuty在几年的时间里开发并完善了内部故障响应实践。最初，配备了一名常设的全公司故障指挥官，并为每个服务配备专门的工程师参与故障响应。随着PagerDuty发展到超过400名员工和几十个工程团队，故障响应过程也发生了变化。每隔几个月都会仔细检查故障响应流程，并更新它们以反映业务需求。所有的经验总结都记录在https://response.pagerduty.com上。我们的故障响应过程并不是一成不变的；它们就像我们的业务一样不断变化和发展。 PagerDuty重大事件响应 通常，小的故障只需要一个值班工程师来响应。当涉及到更大的故障时，我们非常重视团队合作。在高压力和高影响的情况下，工程师不应该感到孤独。我们使用以下技巧来促进团队合作： 参与模拟演习 我们教授团队合作的一个方法是参加“失败星期五”。PagerDuty从Netflix公司的Simian Army(猿人部队)里汲取灵感，制作了这个节目。最初，Failure Friday是一个手动的故障注入练习，目的是了解更多关于我们的系统可能崩溃的方式。今天，我们还使用这个每周练习来重现生产和事件响应场景中的常见问题。 在“失败星期五”开始之前，我们提名一个故障指挥官(通常是一个训练成为IC的人)。在进行故障注入练习时，他们应该表现得像真正的IC一样。在整个演练过程中，主题专家使用与实际事件相同的过程和术语。这种做法既使新值班工程师熟悉事故障响应语言和流程，又为经验丰富的值班工程师提供了一种复习。 玩限时模拟游戏 虽然“失败星期五”练习对工程师在不同角色和过程中培训大有帮助，但它们不能完全复制实际重大事故的紧迫性。我们使用具有时限紧迫性的模拟游戏来捕捉事件响应的这一方面。 “继续说下去，没有人会爆炸”是我们大量使用的一款游戏。它要求玩家在限定时间内共同拆除炸弹。游戏的压力和密集的交流性质迫使玩家有效地合作和有效地协同工作。 从以往的故障中吸取教训 从过去的故障中学习可以帮助我们更好地应对未来的重大故障。为此，我们进行并定期审查故障分析报告。 PagerDuty事后调查过程包括开放式会议和全面记录。通过使这些信息易于访问和发现，我们的目标是减少未来事件的解决时间，或防止未来故障一起发生。 我们还会记录所有涉及重大故障的电话，这样我们就可以从实时通信feed中学习。 让我们看看最近的一个故障，其中PagerDuty不得不利用我们的故障响应流程。故障发生在2017年10月6日，持续时间超过10个小时，但对客户的影响非常小。 下午7:53 PagerDuty SRE团队的一名成员被告知PagerDuty内部NTP服务器正在显示时钟漂移。值班的SRE验证了所有自动恢复操作已经执行，并完成了相关运行手册中的缓解步骤。这个工作记录在SRE团队的专用Slack频道中。 晚上8点20分，PagerDuty软件团队A的一名成员收到了关于他们服务中时钟漂移错误的自动警报。软件团队A和SRE团队致力于解决这个问题。 晚上9点17分，PagerDuty软件团队B的一名成员收到了关于他们服务上时钟漂移错误的自动警报。B组的工程师加入了Slack频道，该频道已经对问题进行了测试和调试 晚上9点49分，值班SRE宣布发生重大故障，并通知值班故障指挥官。 晚上9点55分，IC组建了响应团队，其中包括依赖NTP服务的每个on-call工程师，以及PagerDuty的客户支持。IC让响应小组加入专门的电话会议和Slack频道。 在接下来的8个小时里，响应小组致力于解决和减轻这个问题。当我们运行手册中的程序没有解决问题时，响应团队开始有条理地尝试新的恢复选项。 在这段时间里，我们每四个小时轮换一次on-call工程师和IC。这样做可以鼓励工程师们休息，并为响应团队带来新的想法。 上午5：33，值班的SRE对NTP服务器进行了配置更改。 上午6点13分，IC与他们各自的值班工程师验证所有的服务都恢复了。验证完成后，IC关闭了电话会议和Slack频道，并宣布故障完成。鉴于NTP服务的广泛影响，有必要进行事后分析。在结束故障之前，IC将事后分析分配给值班的SRE小组。 用于故障响应的工具 我们的故障响应流程利用了三个主要工具： PagerDuty。我们将所有值班的信息、服务所有权、事后分析、事件元数据等存储在PagerDuty中。这使我们能够在出现问题时迅速组建正确的团队。 Slack。我们保持一个专门的频道(#incident-war-room)，作为所有主题专家和故障指挥官的聚会场所。该频道主要用作记录员的信息分类，用于捕获操作、所有者和时间戳。 电话会议 当被要求加入任何故障响应时，on-call的工程师需要拨打一个固定的会议电话号码。我们希望所有的协调决策都在电话会议中做出，而决策结果都记录在Slack中。我们发现这是做出决策的最快方法。我们还会记录每次通话，以确保我们可以重新创建任何时间轴，以防记录员遗漏了重要的细节。 虽然Slack和电话会议是我们的沟通渠道，但你应该使用最适合贵公司及其工程师的沟通方式。 在PagerDuty中，我们如何处理响应直接关系到公司的成功。我们不是毫无准备地面对这些故障，而是通过进行模拟练习，回顾以往的故障，选择合适的工具来帮助我们应对可能发生的任何重大事故障，从而有目的地为故障做准备。 将最佳实践付诸实践 我们见过一些处理得很好的故障的例子，有些则没有。当警报提醒你一个问题的时候，已经来不及考虑如何处理这个故障了。开始考虑故障管理过程的时间是在故障发生之前。那么，在灾难降临之前，你如何准备并将理论付诸实践呢？本节提供了一些建议。 故障响应训练 我们强烈建议培训应急人员来组织故障，这样他们在真正的紧急情况下就有一个模式可以遵循。知道如何组织一个故障，在整个故障中使用共同的语言，并分享相同的期望，可以减少沟通失误的可能性。 完整的故障指挥系统方法可能超出了你的需要，但是你可以通过选择故障管理过程中对你的组织非常重要的部分来开发处理故障的框架。例如: 让接听电话的人知道，他们可以在故障发生时委派和升级。 鼓励采取缓解措施。 定义故障指挥官、通信主管和运维主管角色。 你可以调整和总结你的故障响应框架，并创建一个PPT展示给新的团队成员。我们了解到，当人们能够将故障反应理论与实际场景和具体行动联系起来时，他们更容易接受故障响应训练。因此，一定要包括亲身实践的练习，分享过去发生的故障，分析哪些进展顺利，哪些进展不太顺利。还可以考虑使用专门从事事件响应课程和培训的外部机构。 事先做好准备 除了故障响应训练，它还有助于事先为故障做好准备。使用以下的技巧和策略来做好准备。 确定沟通渠道 事先决定并同意一个通信渠道(Slack, a phone bridge, IRC, HipChat，等等)——没有事故指挥官想在事故发生时做出这样的决定。练习使用它，这样就不会有意外了。如果可能的话，选择一个团队已经熟悉的沟通渠道，这样团队中的每个人都能舒服地使用它。 让利益相关方知情 除非你承认某个事件正在发生并且正在被积极处理，否则人们会自动地认为没有采取任何措施来解决这个问题。同样，如果你在问题减轻或解决后忘记取消响应，人们会认为事件正在发生。你可以通过定期更新状态，在事件发生的整个过程中不断通知受众，从而抢占这种动态。准备一份联系人列表(请参阅下一条提示)可以节省宝贵的时间，确保你不会错过任何人。 提前考虑如何起草、审查、批准和发布公共博客文章或新闻稿。在Google，团队寻求公关团队的指导。另外，为共享信息准备两三个现成的模板，确保值班的人知道如何发送它们。没有人愿意在没有指导原则的极端压力下写这些公告。这些模板使得与公众共享信息变得简单，压力最小。 准备一份联系人列表 事先准备好要发邮件或浏览的人的名单可以节省大量的时间和精力。在第180页的“案例研究2：如果可以的话缓存我”中，通讯主管通过发送电子邮件给预先准备好的GKE列表，发出了一个“全体人员待命”的电话。 建立故障标准 有时很明显，告警问题确实是一个故障。其他时候，情况就不那么清楚了。有一个确定的标准列表来确定一个问题是否确实是一个事件是很有帮助的。一个团队可以通过查看过去的停机情况，并考虑到已知的高风险区域，从而得出一个可靠的标准列表。 综上所述，在应对事件时，建立协调和沟通的共同基础是很重要的。确定沟通事件的方式，你的受众是谁，以及在事件中谁负责。这些指南易于制定，对缩短事件的解决时间有很大的影响。 演练 故障管理过程中的最后一步是实践你的故障管理技能。通过在不太危急的情况下进行练习，你的团队会在闪电袭击时养成良好的习惯和行为模式——无论是比喻意义上还是字面意义上。通过培训介绍了事件响应理论之后，实践可以确保事件响应技能保持新鲜。 有几种方法来进行故障管理演练。Google在全公司范围内运行弹性测试(称为灾难恢复测试，或DiRT；请参阅Kripa Krishnan的文章《抵御意外》(Weathering theUnexpected)。在这篇文章中，我们创建了一个可控的紧急情况，实际上并不影响客户。团队对受控的紧急情况做出反应，就好像这是真正的紧急情况一样。随后，各小组回顾了应急反应程序，并讨论了发生了什么。接受失败作为一种学习的方式，在发现的差距中发现价值，让我们的领导参与进来是成功在Google建立DiRT计划的关键。在较小的范围内，我们使用诸如“不幸之轮”(参见“网络可靠性工程中的灾难角色扮演”)等练习来应对特定事件。 你还可以通过有意地将次要问题视为需要大规模响应的主要问题来练习事件响应。这可以让你的团队在现实世界中使用低风险的过程和工具进行实践。 演练是一个尝试新的故障响应技能的友好方式。团队中任何可能深入到故障响应的人——SREs、开发人员、甚至客户支持和营销合作伙伴——都应该对这些策略感到满意。 要进行演练，可以制造中断并允许你的团队对事件进行响应。你还可以从事后分析中制造中断，其中包含大量事件管理演练的想法。尽可能使用真实的工具来管理事件。考虑破坏你的测试环境，以便团队能够使用现有工具执行真正的故障排除。 如果这些演习是周期性的，那么它们就会有用得多。你可以通过对每次练习进行跟踪，详细列出哪些做得好，哪些做得不好，以及如何更好地处理事情，来让演练产生影响。进行演练最有价值的部分是检查它们的结果，这可以揭示事件管理中的任何漏洞。一旦你知道了它们是什么，你就可以努力关闭它们。 总结 当灾难来临时做好准备。如果你的团队定期实践和更新故障响应过程，那么当不可避免的故障发生时，便不会感到恐慌。 在故障发生前，你需要与同事合作的圈子会随着事件的规模而扩大。当你和你不认识的人一起工作的时候，流程会帮助你建立你需要的快速解决方案的结构。我们强烈建议在未报警前提前建立这些流程。定期回顾并重复你的事件管理计划和剧本。 事故指挥系统是一个简单的概念，很容易理解。它会根据公司的规模和事件的大小进行放大或缩小。虽然理解起来很简单，但实现起来却并不容易，尤其是在突然发生恐慌时。在紧急情况下保持冷静并遵循反应结构需要练习，练习可以建立“肌肉记忆”。这使你对待真正的紧急情况有了信心。 我们强烈建议在你的团队繁忙的日程中抽出一些时间，定期实践事件管理。确保得到领导的支持，让他们有专门的实践时间，并确保他们了解事件响应如何工作，以防你需要让他们参与到真正的事件中。备灾可以从响应时间中节省宝贵的时间或数小时，并使你具有竞争优势。没有任何一家公司总是能把事情做好——从错误中吸取教训，继续前进，下次做得更好。</summary></entry><entry><title type="html">第八章 On-Call</title><link href="http://localhost:4000/sre/2020/01/08/On-Call/" rel="alternate" type="text/html" title="第八章 On-Call" /><published>2020-01-08T00:00:00+08:00</published><updated>2020-01-08T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/08/On-Call</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/08/On-Call/">&lt;!-- more --&gt;
&lt;p&gt;On-call轮值意味着在某段时间内随叫随到，随时响应紧急问题。站点可靠性工程师（SRE）通常需要参与on-call轮值工作。在on-call轮值期间，SRE需根据需要判断、缓解、修复或升级事件。此外，SRE还需定期响应非紧急生产事件。&lt;/p&gt;

&lt;p&gt;在Google，on-call轮值是SRE的职责之一。SRE团队可以缓解故障，修复生产环境问题且自动执行运维任务。由于大多数SRE团队的运维任务还未完全实现自动化，升级需要人为联系on-call工程师。SRE团队的on-call工作是根据所支持系统的重要程度或系统所处的开发状态而定的。根据我们的经验，大多数SRE团队都需要参与on-call轮值工作。&lt;/p&gt;

&lt;p&gt;On-call是一个庞大而复杂的话题，限制因素很多，但是试错率却很少。我们的第一本书《Site Reliability Engineering》第11章“on-call轮值”已经探讨过这一主题。本章介绍一些我们收到的有关该章的反馈和问题。其中包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“我们不是Google，我们的规模要小得多。我们没有那么多人参与轮值，并且没有位于不同时区的站点。你在第一本书中描述的内容与我无关。”&lt;/li&gt;
  &lt;li&gt;“我们的开发人员和DevOps是混合在一起参与on-call轮值的。如何组织是最佳方案？如何分担责任？”&lt;/li&gt;
  &lt;li&gt;“我们的on-call工程师在24小时轮值中约要处理100个问题。很多问题都被忽略了，而真正重要的问题也淹没其中。我们应该从哪开始处理？”&lt;/li&gt;
  &lt;li&gt;“我们的on-call轮值周转率很高，如何解决团队内部的认知差距？”&lt;/li&gt;
  &lt;li&gt;“我们打算将我们的DevOps团队重组为SRE(注1)。”&lt;/li&gt;
  &lt;li&gt;SRE on-call、DevOps on-call和开发人员on-call的区别是什么？因为DevOps团队非常关注这点，所以请具体说明。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们将为这些问题提供实用的建议。Google是一家拥有成熟SRE组织的大型公司，多年来我们学到的很多东西都可以应用于任何规模和成熟度的公司或者组织中。Google在各种规模的服务上都有数百个on-call轮值人员，从简单到复杂的服务对应着不同的on-call设定。on-call并不专属于SRE：许多开发团队对于他们所负责的服务也需要on-call。每个on-call设定都应满足对应服务的需要。&lt;/p&gt;

&lt;p&gt;本章介绍Google内部以及其他公司的on-call设定。你的设定和情况可能与我们展示的具体示例不同，但我们所涵盖的基本概念是普遍适用的。&lt;/p&gt;

&lt;p&gt;在深入研究和分析手机报警负载的原因后，我们提出优化报警信息设置的策略并最大限度的减少负载。&lt;/p&gt;

&lt;p&gt;最后，我们分享了Google内部实践的两个例子：on-call的灵活性和on-call团队的动态变化。这些实践证明无论on-call设定的数学方法是什么，都不能完全依赖on-call人员的后续处理。需要适当考虑对on-call人员进行激励和人文关怀。&lt;/p&gt;

&lt;h2 id=&quot;回顾第一本sre书中on-call轮值&quot;&gt;回顾第一本SRE书中“On-Call轮值”&lt;/h2&gt;

&lt;p&gt;《站点可靠性工程师》在“On-Call轮值”一章中阐述了Google on-call轮值的基本原则。本节将讨论该章节的重点内容。&lt;/p&gt;

&lt;p&gt;在Google，on-call的目标是确保不牺牲on-call工程师的健康为前提下覆盖到重点服务，保障服务的可靠性。因此，SRE团队的工作应该是个健康的平衡的职责搭配：on-call和日常项目工作。我们要求SRE团队至少花50%的时间进行工程项目开发，以便战略性的解决生产环境中发现的各种问题。团队人员必须确保有时间参与项目开发工作。&lt;/p&gt;

&lt;p&gt;为确保有足够时间跟进，每个on-call轮值班次内最多跟进两个故障（注2）。如果报警信息负载过多，需要采取纠正措施。（我们将在本章后面探讨报警信息负载。）注2：不管同一个“问题”发出了多少报警，都被定义为一个“故障”。一个轮值班次是12个小时。&lt;/p&gt;

&lt;p&gt;安全感（注3）对于on-call的有效轮转至关重要。on-call期间的压力很大，为了减轻on-call人员的压力，调节他们的生活，应该提供一系列清晰的程序和问题升级路线的支持。注3：David Blank-Edelman（O’Reilly）在Seeking SRE一书中有更多关于此主题的内容。&lt;/p&gt;

&lt;p&gt;针对工作时间之外的on-call工作应给予合理的补贴。不同的公司可能有不同的方式进行补贴。Google提供年假或者现金补贴，同时按一定程度的工资比例作为上限。补贴措施激励SRE按需参与on-call工作，且避免因经济原因而过多的参与on-call工作。&lt;/p&gt;

&lt;h2 id=&quot;google内部和其他公司的on-call设定示例&quot;&gt;Google内部和其他公司的On-Call设定示例&lt;/h2&gt;

&lt;p&gt;本节介绍了Google和Evernote的on-call设置，这是一家致力于帮助个人和团队创建、汇总和共享信息的跨平台应用程序的加利福尼亚公司。我们探讨了每家公司on-call设定、理念以及实践背后的原因。&lt;/p&gt;

&lt;h3 id=&quot;google组建新团队&quot;&gt;Google：组建新团队&lt;/h3&gt;

&lt;h4 id=&quot;初始场景&quot;&gt;初始场景&lt;/h4&gt;

&lt;p&gt;几年前，Google Mountain View（地名：山景城）的SER Sara组建了一个新的SRE团队，该团队需要在三个月内胜任on-call工作。Google的大多数SRE团队默认新员工需要三到九个月时间来准备承担on-call工作。新组建的Mountain View SRE团队将支持三个Google Apps服务，这些服务之前是由位于华盛顿州Kirkland（地名：柯克兰）的SRE团队提供支持的（从Mountain View起飞需要两小时才能到达）。Kirkland团队在伦敦有一个姊妹团队，该团队会继续支持这些服务，以及新组建的Mountain View SRE团队和部分产品开发团队（注4）注4：Google的SRE团队通过跨时区协同工作来确保服务的连续性。&lt;/p&gt;

&lt;p&gt;新成立的MountainView SRE团队很快聚集了七个人：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sara，SRE技术主管&lt;/li&gt;
  &lt;li&gt;Mike，来自另一个SRE团队的具有丰富经验的SRE&lt;/li&gt;
  &lt;li&gt;从新产品开发团队转岗过来的SRE&lt;/li&gt;
  &lt;li&gt;四名Nooglers（Nooglers：新员工，特指近期才为Google工作的人）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;面对新服务的on-call工作，即使是个成熟的团队，也是充满挑战的，而新的Mountain View SRE团队是一个相对初级的团队。尽管如此，新团队做到了在不牺牲服务质量或项目速度的前提下提供服务。他们很快着手改进服务，包括将机器成本降低40%，通过灰度发布和其它安全检查完成自动化发布。新团队持续提供可靠性服务，目标为99.98%的可用性，或每季度约26分钟的停机时间。&lt;/p&gt;

&lt;p&gt;新的SRE团队是如何通过自我完善来实现该目标的？答案是通过入门项目，指导和培训。&lt;/p&gt;

&lt;h4 id=&quot;培训方案&quot;&gt;培训方案&lt;/h4&gt;

&lt;p&gt;虽然新的SRE团队对他们的服务对象知之甚少，但Sara和Mike对Google的生产环境和SRE工作非常熟悉，且四位Nooglers也通过了公司的招聘，Sara和Mike整理了一份包含二十多个重点领域的清单供组员在on-call之前进行练习，例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;管理生产作业&lt;/li&gt;
  &lt;li&gt;了解调试信息&lt;/li&gt;
  &lt;li&gt;集群流量切换&lt;/li&gt;
  &lt;li&gt;回滚有问题的版本&lt;/li&gt;
  &lt;li&gt;阻止或限制恶意请求&lt;/li&gt;
  &lt;li&gt;提供额外的服务能力&lt;/li&gt;
  &lt;li&gt;使用监控系统（报警和仪表盘）&lt;/li&gt;
  &lt;li&gt;了解服务的架构，各种组件以及依赖关系&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nooglers(新人)通过研究现有文档和代码（指导，实践编码教程）找到这类信息，并通过研究入门项目来理解相关主题。当团队成员学习到与Nooglers的初学者项目相关的主题时，该成员会召开简短的临时会议，将所学知识分享给其他成员。Sara和Mike会介绍剩余的主题。该团队还进行实践练习，通过执行常见的调试和降损操作帮助成员形成肌肉记忆，增加对自己能力的信心。&lt;/p&gt;

&lt;p&gt;除了练习清单外，这个新团队还进行了一系列“深度潜水”来深入了解他们的服务。团队浏览了监控控制台，确定正在运行的作业，并尝试修复最近的报警。Sara和Mike解释道，要想做到对每项服务都十分熟悉，工程师并非需要多年的专业知识。他们指导团队也是从这一原则出发，鼓励Nooglers熟悉这些服务。他们每个人理解的知识都是有限的，这会教导成员在何时向别人寻求帮助。&lt;/p&gt;

&lt;p&gt;在整个成长过程，新团队并不孤单。Sara和Mike前往其它SRE团队和产品开发团队，向他们取经。新团队通过视频会议、邮件和IRC与Kirkland和伦敦团队进行沟通。此外，该团队还参加每周的生产会议，查看on-call轮值表和事后报告，并浏览现有的服务文档。Kirkland团队派来一名SRE与新团队交流解答问题，伦敦的SRE整理了一套完整的灾难情景，并在Google灾难恢复培训期间进行了运行展示（请参阅“站点可靠性工程”第33章“灾难预案与演习”部分）。&lt;/p&gt;

&lt;p&gt;该小组还通过“幸运之轮”训练演习（见第28章“故障处理分角色演习”一节）如何on-call，扮演各类角色，练习解决生产问题。演习期间，鼓励所有SRE提供解决模拟生产环境故障的建议。在每个人的能力都得到增强之后，团队仍举办这类演习，每个成员轮流担任演习负责人，并且将演习过程记录下来以供未来参考。&lt;/p&gt;

&lt;p&gt;在进行on-call工作之前，团队查看了有关on-call工程师职责的指导原则。例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在每次轮岗开始时，要从上一个on-call工程师那获得轮岗转换邮件。&lt;/li&gt;
  &lt;li&gt;on-call工程师需要先止损，然后确保完全解决问题。&lt;/li&gt;
  &lt;li&gt;在轮岗结束时，on-call工程师向待命的on-call发送轮岗转换邮件通知。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;操作指南规定了问题何时升级到其他人以及如何为大型事件编写事后总结报告。&lt;/p&gt;

&lt;p&gt;最后，该团队阅读并更新了on-call的操作指南。操作指南中有针对报警的详细说明，解释了报警的严重级别和影响，还有针对完全解除报警的操作意见和需要采取的措施。对于SRE，每当产生报警时，都会创建对应的操作指南记录。这些记录可以减小on-call压力，平均恢复时间（MTTR）以及人为犯错误的风险。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;-维护操作指南-&quot;&gt;&lt;center&gt; 维护操作指南 &lt;/center&gt;&lt;/h4&gt;
  &lt;p&gt;操作指南中涉及的细节变化与生产环境的变化保持同步。针对日常发布，指南可能需要随着发布时间进行更新。就像任何一种方式的沟通一样，编写一份好的文档是很难的。因此，如何维护好你的操作指南呢？&lt;/p&gt;

  &lt;p&gt;Google的一些SRE主张操作指南条目要保持通用性，这样迭代的速度就会缓慢些。例如，所有的“RPC Errors High”报警放在一个条目下，经验丰富的on-call工程师可以结合当前报警服务的架构图进行阅读。为了减少人员变更因素影响以及降低MTTR，另有部分SRE主张逐步开放分享操作指南。如果你的团队对指南中的做法另有异议，那么操作指南可能会衍生出多个分支。&lt;/p&gt;

  &lt;p&gt;这个话题颇具争议。不管你想做成什么样的，但至少你和你的团队要明确操作指南的最小粒度和结构化的细节，并时刻关注操作指南的内容是否累计到超出了最初的设定。在这过程中，要学会将实践获取的知识自动化部署到到监控平台。如果你的操作指南是个明确的由命令组成的列表，当对应的报警触发时，on-call工程师会按照列表执行命令的话，我们建议将其转化为自动化执行。&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;两个月后，Sara，Mike和他们的SRE承担了即将卸任的Kirkland SRE团队的on-call备岗工作。在第三个月，他们成了on-call主岗，Kirkland SRE作为备岗。通过这样的方式，Sara和他的SRE团队随时可以替代Kirkland SRE团队。接下来，Nooglers作为更有经验的SRE成员的备岗加入了轮值工作。&lt;/p&gt;

&lt;p&gt;丰富的文档和前文所述的各种策略方法都有助于团队形成坚实的基础并迅速获得提升。虽然on-call意味着压力，但团队已经具备足够的信心让他们在采取行动之前不会怀疑自己。即使他们升级事件，他们的反应也是基于团队的集体知识以及自身心理素质，仍然是个称职的on-call工程师。&lt;/p&gt;

&lt;h4 id=&quot;后续&quot;&gt;后续&lt;/h4&gt;

&lt;p&gt;虽然MountainView SRE仍在成长，但他们了解到他们在伦敦的姊妹团队将转而负责新的项目，并且在苏黎世成立了一个新团队来负责伦敦团队之前的工作。对于第二次工作交接，Mountain View SRE使用了相同的方法，事实证明也是成功的。MountainView SRE之前准备的入职和培训资料帮助新成立的苏黎世SRE团队获得成功。&lt;/p&gt;

&lt;p&gt;当一群SRE组成一个新团队时，Mountain View SRE的方法是有效的，但当团队新加入一个成员时，只需要用更轻量级的方法即可。考虑到将来的轮转，SRE绘制了服务架构图，并将基础培训列表正式化为一系列的练习，这些练习无需导师全程参与，可由成员半自主完成。例如描述存储层，执行扩容以及了解HTTP请求过程。&lt;/p&gt;

&lt;h3 id=&quot;evernote在云中寻找我们的根&quot;&gt;Evernote：在云中寻找我们的根&lt;/h3&gt;

&lt;h4 id=&quot;将我们的本地基础架构迁移到云&quot;&gt;将我们的本地基础架构迁移到云&lt;/h4&gt;

&lt;p&gt;如生活中的大多数事情一样，需求是发明之母，因此我们并没有着手重新设计我们的on-call流程。在2016年12月之前，Evernote仅运行在本地数据中心，支持我们的单体式应用程序。我们的网络和服务器在设计时考虑了特定的架构和数据流，结合其他一些约束，意味着我们缺乏支持水平架构所需的灵活性。Google Cloud Platform（GCP）为我们提供了具体的解决方案。但是，仍有一个障碍需要克服：将我们的所有产品和基础设施迁移到GCP。历时70天，通过艰苦卓绝的努力和无数壮举（例如，移动数千台服务器和3.5PB的数据），我们住进了新家。至此，我们的工作仍未完成：我们如何监控，报警，最重要的——如何在新环境应对问题？&lt;/p&gt;

&lt;h4 id=&quot;调整on-call策略和流程&quot;&gt;调整on-call策略和流程&lt;/h4&gt;

&lt;p&gt;应用迁移到云环境激发了我们基础设施快速增长的潜力，减小了我们的基础设施快速增长的阻力，但我们的on-call策略和流程尚未随着这种增长而调整。迁移完成后，我们着手解决问题。在之前的物理数据中心中，我们几乎在每个组件中都创建了冗余。对我们而言，组件故障很常见，但基本上很少有单个组件对用户体验产生负面影响。要知道任何小的抖动都是源于系统的某处故障，而因为我们可以控制它，所以我们的基础设施非常稳定。我们的报警策略是基于以下思路构建的：一些丢弃的数据包，导致JDBC（Java数据库连接）连接异常，意味着VM（虚拟机）主机即将发生故障，或控制面板某一开关一直处于失常。甚至在我们第一天步入云端之前，我们就意识到这种类型的报警/响应系统在未来是不可行的。在实时迁移和网络延迟的世界中，我们需要采用更全面的监控方法。&lt;/p&gt;

&lt;p&gt;根据第一原则重新构建报警事件，并将这些原则写下来作为明确的SLO（服务级别目标），这有助于团队明确重要信息，从监控基础架构中减少冗余。我们专注更高级别的指标，例如API响应而非类似MySQL中的InnDB行锁等低级别的基础结构，这意味着将更多时间集中在用户在服务中断时遇到的痛点上。对团队而言，也意味着可以减少追踪瞬态问题的时间。相对应的，有了更多的睡眠时间，更高效，工作满意度也更高。&lt;/p&gt;

&lt;h4 id=&quot;重构监控和指标&quot;&gt;重构监控和指标&lt;/h4&gt;

&lt;p&gt;我们的on-call轮值人员是由一个小而充满斗志的工程师团队组成，他们负责生产基础设施和一些业务系统（例如，升级和构建管道基础设施）。每周进行一次轮岗，且在每日的晨会上对之前一天的事故进行复盘。我们的团队规模小，但责任范围大，因此需要努力减轻流程负担，专注于尽快响应报警/报警分类/处理报警/事后分析复盘。我们实现这一目标的方法之一是通过维护简单但有效的报警SLA（服务级别协议）来保持低信噪比。我们将指标或监控基础架构产生的所有故障分成三类：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;P1：立即处理&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;需要立即采取行动&lt;/li&gt;
  &lt;li&gt;呼叫on-call&lt;/li&gt;
  &lt;li&gt;导致事件分类&lt;/li&gt;
  &lt;li&gt;是否影响SLO&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;P2：下个工作日处理&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;通常不面向客户，或范围有限&lt;/li&gt;
  &lt;li&gt;向团队发送电子邮件并通知事件流向&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;P3：故障仅需知晓&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;信息收集在仪表盘，自动发送的邮件中等&lt;/li&gt;
  &lt;li&gt;与容量规划相关的信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所有P1和P2故障都附有故障单，用于描述事件分类，跟踪处理措施，SLO影响，发生的次数和事后报告链接。&lt;/p&gt;

&lt;p&gt;当出现P1级别的事件时，on-call人员需要评估该事件对用户的影响。从1到3对事件的严重性做分级。对于严重性等级为1（Sev 1）的事件，我们有一套标准流程以便响应人员尽可能快速的做出升级决策。故障升级后，我们会组建一个故障团队开始故障处理流程。由记录员和通讯负责人负责故障通报，沟通渠道是完全开放的。故障解决后，我们会主动进行复盘并在公司内部分享结果。对于级别为Sev 2或Sev 3的故障，on-call人员负责处理故障以及故障的事后报告。&lt;/p&gt;

&lt;p&gt;保持流程的轻量化有助于参与项目工作的同事胜任on-call工作，鼓励on-call在遇到故障时立即采取修复行动，并在完成故障复盘后发现工具或流程中的不足之处。通过这种方式，在每次on-call轮岗期间都有持续的改进和灵活的循环方式，能和环境的变化速度保持一致。我们的目标是每个on-call轮次都比上一次更好。&lt;/p&gt;

&lt;h4 id=&quot;追踪观察我们的表现&quot;&gt;追踪观察我们的表现&lt;/h4&gt;

&lt;p&gt;随着SLO的引入，我们希望按照时间维度跟踪性能，并将这些信息分享给公司内部的利益相关者。我们举办了月度级别的服务回顾会议，任何有兴趣的人都可以参加，会议主要回顾和讨论上个月份的服务情况。同时通过该会议判断on-call人员的负担，on-call的负担情况是团队健康的晴雨表，在压力过大时我们需要讨论缓解措施。会议的另一目的是在公司内部宣传SLO的重要性，督促技术团队对我们的服务健康和我们的健康负责。&lt;/p&gt;

&lt;h4 id=&quot;与cre合作&quot;&gt;与CRE合作&lt;/h4&gt;

&lt;p&gt;合理表达我们在SLO方面的目标是与Google客户可靠性工程（CRE）团队合作的奠定了基础。在与CRE讨论我们的SLO以明确它们是否真实可衡量后，两个团队决定针对会影响SLO的故障，都参与报警接收。隐藏在云抽象层背后的故障根本原因是很难被找到的，Google员工的参与在黑盒事件分类方面给予了帮助。更重要的是，这项举措进一步减小了用户最关心的MTTR。&lt;/p&gt;

&lt;h4 id=&quot;保持自我延续的循环&quot;&gt;保持自我延续的循环&lt;/h4&gt;

&lt;p&gt;我们现在有更多时间从团队角度思考如何推进业务发展，而不是将所有时间投入到分类/根因分析/事后分析的事情上。例如，改进我们的微服务平台，为我们的产品开发团队建立生产环境标准等项目。后者包括了我们重组on-call时遵循的很多原则，对于团队第一次上战场“接收报警”十分有帮助。因此，我们也延长改善了on-call的循环时间。&lt;/p&gt;

&lt;h2 id=&quot;实际实施细节&quot;&gt;实际实施细节&lt;/h2&gt;

&lt;p&gt;至此，我们已经讨论了Google和Google以外的on-call设定的细节。但是对于即将参与on-call有哪些具体考虑因素呢？以下部分更深入的讨论了这些实现细节：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;报警负载——它是什么，如何工作，如何管理它&lt;/li&gt;
  &lt;li&gt;如何让on-call时间表更具灵活性，为SRE创造更健康的工作/生活平衡环境&lt;/li&gt;
  &lt;li&gt;对于特定的SRE团队和合作团队要有动态的团队策略&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;手机报警负载剖析&quot;&gt;手机报警负载剖析&lt;/h3&gt;

&lt;p&gt;你的手机一直有报警提醒，多到团队都受到了影响。假定你已经阅读了《》Site Reliability Engineering》的第31章，并和你的团队以及所支持的开发团队定期召开生产会议。现在大家都知道你的on-call工程师因为报警负载而不开心。然后呢？&lt;/p&gt;

&lt;p&gt;手机报警负载的定义是on-call工程师在轮值期间（每天或每周）收到的报警数量。一个故障可能导致多个报警产生。我们将介绍各种影响手机报警负载的因素并提出最小化报警负载的技术。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;-合适的响应时间-&quot;&gt;&lt;center&gt; 合适的响应时间 &lt;/center&gt;&lt;/h3&gt;

  &lt;p&gt;除非有充足的理由，否则工程师无需在收到报警的几分钟内上机器处理问题。虽然面向客户的创收服务故障需要立即响应，但一些不太严重的问题（例如，备份失败），你完全可以在几小时内处理。&lt;/p&gt;

  &lt;p&gt;我们建议你检查当前的报警设置，判断是否应该为当前触发报警的所有事件提供报警服务。你可能试图采用自动修复来解决问题（相对于人为修复，计算机能更好的解决问题）或者采用工单（如果它不是高优先级）。表8-1显示了一些案例和相对的响应。&lt;/p&gt;

&lt;/blockquote&gt;

&lt;center&gt;  表8-1. 实际响应时间案例 &lt;/center&gt;

&lt;style&gt;
table th:nth-of-type(1) { width: 20%; }
table th:nth-of-type(2) { width: 12.5%; }
table th:nth-of-type(3) { width: 67.5%; }
&lt;/style&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;故障描述&lt;/th&gt;
      &lt;th&gt;响应时间&lt;/th&gt;
      &lt;th&gt;对SRE的影响&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;影响盈利的网络中断&lt;/td&gt;
      &lt;td&gt;5分钟&lt;/td&gt;
      &lt;td&gt;SRE需要保证手头的电脑有足够的电量且能联网；不能外出；必须始终与备岗协调保持联系。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;客户订单处理系统挂了&lt;/td&gt;
      &lt;td&gt;30分钟&lt;/td&gt;
      &lt;td&gt;SRE可以出门，短期在外；在此期间，备岗无需保持在线。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;用于预发的数据库备份失败&lt;/td&gt;
      &lt;td&gt;提工单（工作时间处理）&lt;/td&gt;
      &lt;td&gt;无。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;场景超负荷的团队&quot;&gt;场景：超负荷的团队&lt;/h4&gt;
&lt;p&gt;（假设）负责前端负载均衡和终端用户连接的Connection SRE团队发现自身的报警负载超负荷了。他们已经建立了每次轮值报警事件小于2次的目标，但在过去一年中，他们每次轮值平均接收5次报警事件。分析表明，有三分之一的轮值班次内报警数量超出预设值。团队成员已经及时的响应了报警，但仍然无法解决根因；没有足够的时间找到报警根因并妥善处理解决后续问题。一些工程师离开了团队，加入了运维负担较小的团队。由于on-call工程师的时间只够用来缓解眼前的故障，没法跟踪报警的根本原因。&lt;/p&gt;

&lt;p&gt;团队的视野是开阔的：拥有遵循SRE最佳实践的成熟的监控系统，遵循SRE最佳时间。报警阈值设置和SLO保持一致，且报警本质上是基于业务表现特征的，意味着仅在客户受到影响时才会触发。高层管理人员在获知这些信息后，认为该团队已经处于超负荷状态，为了让团队恢复健康状态，他们开始审查项目计划。&lt;/p&gt;

&lt;p&gt;不幸的是，随着时间的推移，Connection团队已经从10多个开发团队中获得了软件组件的所有权，并且对Google面向客户的边缘和骨干网络有着强依赖。群际关系很复杂，也慢慢变得难以管理。&lt;/p&gt;

&lt;p&gt;尽管团队遵循构建监控的最佳实践方法，但所面临的很多报警都超出了他们的直接控制范围。例如，黑盒探测可能会因网络拥塞而失败，导致数据丢包。团队可以采取的唯一措施就是将事件升级到直接负责该网络的团队。&lt;/p&gt;

&lt;p&gt;除了运维负担外，团队还需要为前端系统提供新功能，供所有Google服务使用。更糟糕的是，他们的基础架构正在从一个已有10年历史的遗留框架和集群管理系统中迁移到更高的支持替代品中。该团队的服务受到全所未有的变化速度的影响，这些变化本身也引起了大部分的on-call负担。&lt;/p&gt;

&lt;p&gt;该团队需要各种技术来平衡减少过多的报警负载，团队的技术项目经理和人事经理向高级管理层提交了一份项目建议书，高级管理层审核并通过该建议书。团队全力投入减小报警负载中，在此过程中也获得了宝贵的经验教训。&lt;/p&gt;

&lt;h4 id=&quot;报警负载来源&quot;&gt;报警负载来源&lt;/h4&gt;

&lt;p&gt;解决报警负载的第一步是明确负载出现的原因。报警负载受到三个主要因素的影响：生产环境中的bug （注5）、报警和人为因素。这些因素都有对应的来源，本节将详细讨论其中一部分来源。&lt;/p&gt;

&lt;p&gt;注5：文中的“bug”是由软件或配置错误导致的非预期的系统行为。代码中的逻辑错误，二进制文件的错误配置，错误的容量规划，错误配置的负载平衡或新发现的漏洞都是导致报警负载的“生产 bug”的原因。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;对于生产环境：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;生产环境中存在的bug数量&lt;/li&gt;
  &lt;li&gt;将新bug引入生产环境&lt;/li&gt;
  &lt;li&gt;识别到新引入的bug的速度&lt;/li&gt;
  &lt;li&gt;缓解bug并从生产环境中删除之的速度&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;对于报警：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;触发报警的阈值&lt;/li&gt;
  &lt;li&gt;引入新的报警规则&lt;/li&gt;
  &lt;li&gt;将服务的SLO与其所依赖的服务的SLO关联对齐&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;对于人为因素：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;严格的修复和追踪bug&lt;/li&gt;
  &lt;li&gt;收集报警的数据质量&lt;/li&gt;
  &lt;li&gt;注意报警负载变化&lt;/li&gt;
  &lt;li&gt;人为驱动的生产环境变化&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;已经存在的bug&lt;/strong&gt;。不存在完美的系统。无论是在你的代码里，还是在你依赖的软件和库中，或者是接口之间，产品总会存在bug。虽然这些bug可能并不会立即触发报警，但它们却是客观存在的。你可以利用一些技术来识别或防止尚未导致报警的bug：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;确保系统的复杂度和实际相符，并不是越复杂越好。（见第7章）。&lt;/li&gt;
  &lt;li&gt;利用修复bug的机会，定期更新系统所依赖的软件或库（请参阅下一节有关新bug的部分）。&lt;/li&gt;
  &lt;li&gt;定期执行破坏性测试或模糊测试（例如，使用Netflix的Chaos Monkey）。&lt;/li&gt;
  &lt;li&gt;除集成和单元测试外，还执行常规负载测试。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;新bug&lt;/strong&gt;。理想情况下，SRE团队及其合作的开发团队应该在新bug进入生产环境之前检测到。事实上自动化测试漏测了很多bug，这些bug最终进入了生产环境。&lt;/p&gt;

&lt;p&gt;软件测试是个覆盖面很广的主题（例如，Martin Fowler on Testing）。这项技术在减少进入生产环境的bug数量以及减少bug在生产环境停留的时间方面很有帮助：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;随着时间推移不断改进测试（方法、技术）。尤其是你在生产环境中每发现一个bug，都要自问“如何才能在预发环境检测到这个bug？”确保有必要的工程技术跟进解决此问题。（请参阅“严谨跟踪”，第164页）。&lt;/li&gt;
  &lt;li&gt;不要忽略负载测试，虽然负载测试的优先级常被视为低于功能测试的。但许多bug仅在特定的负载条件下或特定的请求组合中才会显露出来。&lt;/li&gt;
  &lt;li&gt;在生产环境中集成（使用类似生产环境但是是合成的流量进行测试）。我们将在本书的第5章简要讨论生成合成流量。&lt;/li&gt;
  &lt;li&gt;在生产环境执行canarying（第16章）。&lt;/li&gt;
  &lt;li&gt;对新bug保持较低的容忍度。遵循“检测，回滚，修复和发布”策略，而不是“检测，虽然找到bug，但继续发布，修复并再次发布”策略。（相关详细信息，请参阅第162页的“减少延迟”。）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这种回滚策略需要可预测且频繁发布，因此回滚任何版本的成本都很小。我们在《Site Reliability Engineering》一书“发布工程”章节中讨论了相关主题。&lt;/p&gt;

&lt;p&gt;一些bug可能仅仅是由于改变客户端行为导致的。例如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;仅在特定负载水平下出现的bug——例如，9月返校流量，黑色星期五，网络星期一，或一年中夏令时即欧洲和北美时差一小时的那一周，意味着更多用户同时保持清醒和在线状态。&lt;/li&gt;
  &lt;li&gt;只有特定混合请求才显示的bug——例如，用于亚洲字符集的语言编码，更接近亚洲的服务器的流量消耗更大。&lt;/li&gt;
  &lt;li&gt;仅在用户以意想不到的方式运行系统时才会显示的bug——例如，（在）航空公司订票系统使用的日期（下运行系统）！因此，为了测试能够覆盖到不常发生的行为（导致的bug），扩展您的测试方案是十分必要的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当生产系统受到多个并发错误的影响时，判断报警是由于现有bug还是新bug引起的是很困难的。最大限度的减少生产环境中的bug不仅可以减少报警负载，还对新bug的识别和分类很有帮助。因此。尽快从系统中删除生产环境的bug至关重要，修复现有bug的优先级应该在开发新功能之上；如果过程中需要跨团队合作，请参阅第18章。&lt;/p&gt;

&lt;p&gt;架构或程序问题，例如自动健康检查，自我修复和减小负载，可能需要大量的工程工作来解决。为简单起见，我们将这些问题视为“bug”，即使它们的规模、复杂度或解决它们需要的工作量很大。&lt;/p&gt;

&lt;p&gt;《SiteReliability Engineering》中第3章描述了错误预算如何控制新bug发布到生产环境的方法。例如，当服务的SLO超过其总季度错误预算的某一部分时——事先在开发人员和SRE团队间达成一致意见——可以暂停新功能开发以及和功能相关的部署，以专注于系统稳定，减少报警的频率。&lt;/p&gt;

&lt;p&gt;示例中的Connection团队采用严格的策略，要求每次故障都需要追踪bug。该举措能让团队的技术项目经理知道产生新bug的根本原因在哪。数据显示，人为错误是生产环境中新bug产生的第二大常见原因。&lt;/p&gt;

&lt;p&gt;由于人类容易出错，如果对生产系统所做的所有变更都是通过（人为开发的）配置自动生成的，那么效果会更好。在对生产环境进行变更之前，自动化手段可以执行人类无法进行的测试。Connection团队是半手工的对生产环境进行复杂的变更的。毫无疑问，团队的手动变更有时会出错；该团队引入了触发报警的新bug。在新bug进入生产系统并触发报警前，将要做出类似变更的自动化系统就会判断出这种变更是不安全。技术项目经理将这些数据提供给团队，说服他们优先考虑进行自动化项目。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;识别延迟&lt;/strong&gt;。及时识别报警的原因十分重要，这个识别的时间越长，意味着报警再次产生的几率越大。例如，有一个仅在高负载情况下才会产生的报警，如果在下一个峰值之前未识别有问题的代码或配置，那么问题可能会再次发生。你可以用这些技术来减少报警识别时间：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;使用合理有效的报警和控制台&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;确保报警页面链接到相关的监控控制台，且该控制台突出显示系统运行超出规范的位置。在控制台中，将黑盒和白盒监控报警相关联，并对关联的图表执行相同的操作。确保操作指南是最新的，提供相应每种报警类型的行动建议。on-call工程师应在相应的报警触发时用最新信息更新操作指南。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;实践应急响应&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;进行“幸运之轮”练习（在《Site Reliability Engineering》中有描述），和同事共享常用的和针对特定服务的调试技术。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;执行小变更&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;如果您频繁执行局部（部分功能、部分模块）的变更而不是偶尔的整体（所有功能、所有模块）变更，那么能很容易的将bug与引入它们对应的变更相关联。第16章中描述的Canarying版本给出了一个判断，表明新bug是否是由于新版本引起的。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;日志变更&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;将变更信息聚合到可搜索的时间线中可以更简单（且更快）的将新bug与引入它们的变更相关联。Jenkins的Slack插件可能会有所帮助。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;寻求帮助&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;在《SiteReliability Engineering》“故障管理”中，我们讨论了共同管理大型故障的问题。on-call工程师从来不会只是一个人；要让你的团队在寻求帮助时有安全感。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;减少延误&lt;/strong&gt;。一旦找到bug，修复bug所需的时间越长，就越可能再次发生问题并产生报警。可以考虑这些减少延误的技术：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;回滚变更&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果bug是在一次最近的代码、配置变更中引入的，在安全和恰当的情况下（单独回滚代码、配置可能是必要的，但如果bug是因为数据损坏导致的，那只回滚代码、配置就不能解决问题了）我们可以通过立即回滚生产环境的变更消除bug。谨记，即使是“快速修复”也需要时间进行验证，构建和发布。验证是至关重要的，要确保“快速修复”确实可以修复bug，并且不会引入额外的bug或其他非预期的影响。通常，采取“回滚，修复和发布”要优于“发布，修复和再发布”操作。&lt;/li&gt;
  &lt;li&gt;如果你的目标是99.99%的可用性，那么每季度约有15分钟的错误预算时间。上线发布的构建步骤可能需要15分钟以上，因此回滚对用户的影响更小。（99.999%的可用性对应每季度80秒的错误预算，这样的系统可能需要自我修复的属性，超出了本章的讨论范围。）&lt;/li&gt;
  &lt;li&gt;如果可能，避免接入无法回滚的变更，例如API不兼容的变更和锁步版本。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;使用功能隔离&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;设计你的系统，以便在功能X出错时，可以通过一个功能标志禁用它，而不影响功能Y。该策略还能提高发布速率让禁用X功能变得简单——且不需知道产品经理是否习惯于禁用功能。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;切走请求流量&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;把请求流量从出现bug的系统组件中切走（即重定向客户请求）。例如，如果bug是代码或配置上线导致的，并且是逐步发布到生产环境中的，那么你还有机会通过把流量从已发生变更的基础架构的元素切走（达到快速止损的目的）。这样你可以在几秒钟内降低对客户的影响，但回滚可能需要几分钟或更长时间。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;报警&lt;/strong&gt;。Google SRE每次轮值时间即12小时最多发生两次不同的报警事件，因此我们对如何配置报警以及如何引入新的报警是经过深思熟虑的。网站可靠性工程“监控分布式系统”描述了Google定义报警阈值的方法。严格遵守这些准则有助于健康的on-call轮转。&lt;/p&gt;

&lt;p&gt;需要强调一下，这章讨论了一些关键元素：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;收到的所有报警都应该立即去操作。我们希望团队在收到系统无法自愈的报警后立刻采取行动。信噪比要高，确保较低的误报率；低信噪比会增加on-call工程师产生“狼来了”的感觉的几率。&lt;/li&gt;
  &lt;li&gt;如果团队的报警规则是基于SLO，或错误上限（请参阅站点可靠性工程中的“黑盒监控与白盒监控”部分），那么所有参与开发和维护站点可靠性的团队都需要认同SLO的重要性并明确他们的工作优先级。&lt;/li&gt;
  &lt;li&gt;如果团队完全基于SLO和现象制订报警策略，那么放宽报警阈值是对报警的合理调整。&lt;/li&gt;
  &lt;li&gt;就像新的代码，新的报警策略也应该经过彻底和周密的审查，每条报警都应该有对应的操作指南条目。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接收报警会对人产生负面的心理影响。为了最大限度的减少这类影响，最好只有在真正需要时才引入新的报警规则。团队中任何人都可以编写新的报警策略，但新的策略需要经过整个团队的审核建议以及提出替代方案。在将新策略发布到线上之前，要彻底测试生产中的新策略来审查是否有误报。例如，可以在报警触发时给作者发送电子邮件，而不是直接呼叫on-call工程师。&lt;/p&gt;

&lt;p&gt;新的报警信息会帮你发现之前并不知道的生产环境问题。在解决了这些bug之后，报警将仅被新bug触发，也起到了回归测试的作用。&lt;/p&gt;

&lt;p&gt;确保新报警在测试环境下运行的时间足够长，能适应典型的生产环境，例如常规软件部署，云提供商的维护需求，每周负载峰值等。通常一周的测试时间是足够的，但具体时间窗口仍取决于报警和系统。&lt;/p&gt;

&lt;p&gt;最后，利用测试期间报警的触发率预测新报警可能会产生的报警负担。对新报警配置的批准或禁止要以团队为单位。如果引入新报警会导致你的服务超出报警阈值，那么需要额外注意系统的稳定性。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;事后密切跟进&lt;/strong&gt;。目的是确定每个报警的根本原因。查找“根因”的范围要从机器层面延伸到团队流程层面。服务中断是由一个本可以通过单元测试发现的bug导致的吗？根因可能不是代码中的bug，而是代码审查中团队流程的bug。&lt;/p&gt;

&lt;p&gt;如果你知道根因，你就可以修复它防止再次困扰你或你的同事。如果你的团队无法确定根本原因，可以添加监控或日志记录，帮助在下次发生这种情况时找到报警的根本原因。如果没有足够的信息来识别bug，你可以做一些事情来帮助进一步调试bug。或者至少可以得出结论，即报警是由“未知原因”触发的。请牢记，身为on-call工程师，你永远不是孤军奋战，所以可以请同事帮忙检查你的发现，看看是否有遗漏的地方。通常，报警触发后有新的证据可用时，很快能找到报警的根本原因。&lt;/p&gt;

&lt;p&gt;将一个报警解释为“瞬态的”或由于系统“自行修复”或莫名其妙“消失”而不采取任何行动时，这个报警很可能会再次发生并导致另一个报警，会给下一下on-call工程师带来麻烦。&lt;/p&gt;

&lt;p&gt;简单修复眼前的bug（或进行一个“点”修复）错过了一个避免将来出现类似报警的黄金机会。把报警信息看作一个带来工程工作的机会，这种工程工作可以改进系统并且消除可能出现的一类bug。可以在你的团队的生产组件中归档项目bug来做到这一点，我们提倡通过收集这个项目会消除的bug以及报警数量，按轻重缓急进行bug修复。如果你的提案需要3个工作周或120个工作时来实施，并且报警平均需要4个工作时才能正确处理，那么30个报警产生后会有一个明确的盈亏平衡点。&lt;/p&gt;

&lt;p&gt;举个例子，假设有这样一种情况，在同一故障域上存在很多服务器，例如这些机器在数据中心中的同一个交换机下，会导致定期同时发生多个机器的故障。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;点修复&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;在众多故障域中重新平衡当前的覆盖区。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;系统修复&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;使用自动化手段确保此类服务器和所有其他类似服务器始终分布在足够的故障域中，并在必要时自动重新平衡。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;监控（或预防）修复&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;当故障域多样性低于预期水平但尚未影响服务时，预先发出警告。理想情况下，警报将是故障工单警报，而不是呼叫报警，因为不需要立即响应。尽管处于较低的冗余水平，该系统仍可以进行服务。&lt;/p&gt;

&lt;p&gt;为确保您对寻呼警报的后续工作有所了解，请考虑以下问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如何防止此特定bug再次发生？&lt;/li&gt;
  &lt;li&gt;对于此系统和我负责的其他系统，如何防止此类bug再次发生？&lt;/li&gt;
  &lt;li&gt;哪些测试可以防止此bug被发布到生产环境中？&lt;/li&gt;
  &lt;li&gt;哪些故障工单警报会触发操作以防止bug在被报警前变的严重？&lt;/li&gt;
  &lt;li&gt;在变得严重之前，哪些报警信息会出现在控制台上？&lt;/li&gt;
  &lt;li&gt;我是否最大化了修复bug带来的收益？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当然，对于on-call工程师，仅仅提交值班期间发生的呼叫报警相关的bug是不够的。重要的是，SRE团队会迅速处理他们确定的bug以减少它们再次发生的可能性。要确保SRE和开发人员团队的资源规划考虑了响应bug所需的工作量。&lt;/p&gt;

&lt;p&gt;我们建议保留一小部分SRE和开发人员团队的时间来响应出现的生产bug。例如，Google on-call工程师通常不会在轮值期间处理项目工作。相反，他们处理可以改善系统健康状况的bug。确保你的团队常规下处理生产环境bug的优先级高于其他项目工作。SRE经理和技术主管应确保及时处理生产环境bug，必要时要升级到开发人员团队决策者。&lt;/p&gt;

&lt;p&gt;当电话报警严重到需要事后调查时，遵循此方法来安排和跟踪后续行动更为重要。（有关详细信息，请参阅第10章。）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;数据质量&lt;/strong&gt;。一旦识别出系统中导致报警的bug，就会出现一些问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如何明确先修复那个bug？&lt;/li&gt;
  &lt;li&gt;如何得知系统中哪个组件导致大多数报警？&lt;/li&gt;
  &lt;li&gt;如何确定on-call工程师为解决这些报警而采取的重复性手动操作？&lt;/li&gt;
  &lt;li&gt;如何判断有多少报警仍有未识别的根本原因？&lt;/li&gt;
  &lt;li&gt;如何得知哪些bug是真实存在的、最严重的，而不是未确定的？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;答案很简单：收集数据！&lt;/p&gt;

&lt;p&gt;你可能会通过跟踪和收集on-call负载的方式来收集数据，但这种方法是有局限性的。更加可持续的做法是，为你的bug跟踪系统（例如，Jira，Issue-Tracker）中的每个电话报警提交一个bug，当on-call工程师意识到每个报警都是已存在的bug的表征时，需要在监控系统的相关报警和相关bug间建立链接。你将在一列中找到尚未解决的bug列表，以及每个相关联bug的页面列表。&lt;/p&gt;

&lt;p&gt;当你拥有有关报警原因的结构化数据，就可以着手分析数据生成报告，这些报告能够回答以下问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;哪些bug导致大多数的报警？理想情况下，我们会立即回滚并修复bug，但有时候，查找根本原因并部署修复程序需要很长时间，有时忽略关键报警并不是一个合理的选择。例如，上述Connection SRE团队可能会遇到持续的网络拥塞，这种拥塞无法立即解决，但仍需要跟踪问题。为团队收集导致了最多的报警和压力的生产环境问题的数据，支持进行数据驱动的有系统的、有优先级的对话。&lt;/li&gt;
  &lt;li&gt;系统的哪个组成部分是大多数报警的原因（支付网关，身份验证微服务等）？&lt;/li&gt;
  &lt;li&gt;与其他监控数据相关联时，特定报警是否与其他信号相对应（请求量高峰，并发客户会话数，注册次数，提款次数等）？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将bug数据和报警根本原因数据结构化还有其他好处：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;你可以自动填充现有bug列表（即已知bug），这对你所支持的团队可能有益处。&lt;/li&gt;
  &lt;li&gt;你可以根据每个bug导致的报警数确定bug的优先级。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;你所收集的数据质量将决定人工或机器可以做出的决策质量。为了确保高质量的数据，请考虑以下技术：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;定义并记录你的团队对报警数据收集的策略和预期。&lt;/li&gt;
  &lt;li&gt;设置来自监控系统的非呼叫报警，突出显示未处理报警的位置。经理和技术主管应确保达到预期。&lt;/li&gt;
  &lt;li&gt;当轮岗交接不符合预期时，队友应该相互帮助跟进。积极的评论有“也许跟bug123有关”，“我已经根据你的调查结果提交了bug报告，所以我们可以进一步跟进了”，或“这看起来像我上周三轮岗发生的事情：&amp;lt;报警，bug的链接&amp;gt;”强化预期行为，确保最大化的改进。没人愿意为上一轮岗就已发生的报警再接收一次报警。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;警觉&lt;/strong&gt;。很多时候，团队会因为多次减员而陷入运维过载中。为了避免温水煮青蛙，要注意on-call工程师的健康状况，确保SRE和开发团队始终优先考虑生产环境健康状况。&lt;/p&gt;

&lt;p&gt;以下技术可以帮助团队密切关注报警呼叫负载：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在生产会议上（参见“站点可靠性工程”中的“沟通：生产会议”一节，第31章），定期根据收集的结构化数据分析报警呼叫负载的趋势。追踪21天的平均值非常有用。&lt;/li&gt;
  &lt;li&gt;当呼叫报警负载超过你的团队事先明确的“告警”阈值时，设置针对技术主管或经理的故障单报警。&lt;/li&gt;
  &lt;li&gt;在SRE团队和开发团队之间定期召开会议，讨论当前的生产状况以及当前SRE为解决的生产环境bug。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;on-call灵活性&quot;&gt;on-call灵活性&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;h3 id=&quot;-值班时长-&quot;&gt;&lt;center&gt; 值班时长 &lt;/center&gt;&lt;/h3&gt;

  &lt;p&gt;on-call值班期间每天需要处理一个或几个报警，因此值班安排必须是合理可持续的：我们建议将时长限制为12小时。较短的值班时长对on-call工程师的健康是有利的。当在岗时间太长时，团队成员大概率会觉得疲惫，随之而来的是他们在工作中可能会犯错误。如果一直进行on-call工作，大多数人无法保持高质量的产出。许多国家都有关于最长工作时间，休息时间和工作条件的法律。&lt;/p&gt;

  &lt;p&gt;虽然理想情况下是一直在白天值班，但12小时轮岗制也并不需要全球分布的团队。整夜12小时处于on-call中比on-call24小时或更长时间更好。即使工作在一个地方，你也可以进行12小时轮岗值班。例如，在为期一周的班次中，不是让一名工程师每天24小时on-call，而是两名工程师一人在白天on-call，一人在夜间on-call。&lt;/p&gt;

  &lt;p&gt;根据我们的经验，如果没有缓解机制，24小时on-call是不可持续的。虽然不理想，但偶尔on-call一整夜至少可以确保你的工程师的休息时间。另一个选择是缩短值班时间——比如3天值班，4天休息。&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h4 id=&quot;情景个人情况的变化&quot;&gt;情景：个人情况的变化&lt;/h4&gt;

&lt;p&gt;想象一下，你是一个大型服务的on-call团队成员，该服务具有跨越两个站点的24/7跟随太阳模型。为了在提高服务可靠性的同时保持运维负载的可控性，虽然你并不乐意在上午6点可能会接到报警，但你对你和团队正在进行的工作感到满意。&lt;/p&gt;

&lt;p&gt;一切都很好…直到某天你才意识到on-call的时间表和你个人生活的需求开始发生冲突。有许多潜在的原因——例如，成为父母，需要短期旅行，休假或生病。&lt;/p&gt;

&lt;p&gt;你需要on-call的职责和新的个人日程表能够共存。&lt;/p&gt;

&lt;p&gt;许多团队和组织在成熟时都面临这一挑战。随着时间推移，人们的需求发生变化，为了保持多元化团队成员的健康平衡，on-call轮值的需求变的多样化。保持健康，公平以及on-call工作和个人生活健康的平衡的关键在于灵活性。&lt;/p&gt;

&lt;p&gt;为满足团队成员的需求，确保覆盖到你的服务或产品，你可以通过多种方式灵活的进行on-call轮转。指定一套全面的，一刀切的指导方针是不可能的。我们鼓励将灵活性作为一项原则，而不是简单的采用此处列举的实例。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动化on-call时间安排&lt;/strong&gt;。随着团队的发展，时间表的安排受到以下约束——休假计划，on-call工作日与周末的分布，个人偏好，宗教要求等，时间表的安排变得越来越困难。你无法手动管理此任务，很难找到任何解决方案，更不用说公平的解决方案了。&lt;/p&gt;

&lt;p&gt;“公平”并不意味着跨团队成员的每个变化都是一致的。不同的人有不同的需求和不同的偏好。因此，团队应该分享这些偏好并尝试以智能的方式满足这些偏好。团队组成和首选项决定了你的团队是更喜欢统一分发，还是以自定义的方式来满足日程安排首选项。&lt;/p&gt;

&lt;p&gt;使用自动化工具来安排on-call班次会更容易。这个自动化工具应该有这些基本特征：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;它应该重新安排on-call班次以适应团队成员不断变化的需求。&lt;/li&gt;
  &lt;li&gt;它应该自动重新平衡报警负载以响应任何更改。&lt;/li&gt;
  &lt;li&gt;应该尽量通过考虑个人偏好来确保公平，例如“4月份周末不用上学”，以及历史信息，例如最近每位on-call的值班负载。&lt;/li&gt;
  &lt;li&gt;因此，on-call工程师可以依据on-call班次进行计划，但绝不能改变已经生成的时间表。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;时间表既可以是完全自动化的，也可以是团队人员安排。同时，一些团队更愿意让成员明确遵守时间表，而其他团队则对完全自动化的流程感到满意。如果你的需求很复杂，可以选择在内部开发自己的工具，此外也有许多商业和开源软件包可以帮助自动化生产on-call时间表。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;短期互换的计划&lt;/strong&gt;。on-call时间表通常会收到的短期变化请求。没人能在周一就承诺周四肯定不会感冒。或者你可能需要在on-call期间处理无法预料的紧急事务。&lt;/p&gt;

&lt;p&gt;你可能还希望因为非常规原因能够在on-call中换岗——例如，允许on-call人员参加运动训练课程。在这种情况下，团队成员可以交换一天的on-call日（例如，周日的一半）。非竞争性的互换通常是更好的选择。&lt;/p&gt;

&lt;p&gt;具有严格报警响应SLO的团队需要考虑通勤时间。如果你的报警响应SLO为5分组，而你的通勤时间为30，那么要确保其他人在你上班途中能处理紧急情况。&lt;/p&gt;

&lt;p&gt;为了在灵活性方面实现这些目标，我们建议给予团队成员权利更新on-call轮值表。此外，有一个记录下来的策略描述转换如何操作。权利下放的策略包括只有经理可以改变的完全集中的政策，到任何成员都可以改变的完全分散的政策。根据我们的经验，对变更进行同行评审可以在安全性和灵活性之间进行良好的权衡。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;长期休息的计划&lt;/strong&gt;。由于个人情况或职业倦怠的变化，有时团队成员需要停止on-call工作。团队的结构应该能够允许on-call人员暂时不参与值班。&lt;/p&gt;

&lt;p&gt;理想情况下，团队规模应该满足在（临时）员工减少时其他成员能够承受增加的运维负担。根据我们的经验，每个站点至少需要五个人进行多站点全天候的on-call，至少需要8个人进行单站点全天候的on-call。因此，假设每个站点需要一名额外的工程师来防止人员减少，每个站点（多站点）最多需要6名工程师，每个站点（单站点）为9名。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;兼职工作的时间表计划&lt;/strong&gt;。on-call工作的时间表看起来是不兼容的，但我们发现如果你采取某些预防措施，on-call工作和兼职工作是能够做到兼容的。以下讨论假设你的on-call成员是兼职工作，他们无法在兼职工作周之外完成值班工作。&lt;/p&gt;

&lt;p&gt;兼职工作主要有两种模式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;每周工作减少一天——例如，每周工作4天，而非5天&lt;/li&gt;
  &lt;li&gt;每天减少工作时间——例如，每天工作6小时，而非8小时
两种模式都可以兼容on-call工作，但需要对on-call时间进行调整。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果非工作日是一直不变的，那么第一个模式很容易和on-call工作兼容。对应的，你可以采用每周少于7天的on-call时间（例如，周一至周四，或周五至周日），并自动调整时间表以便在兼职工程师非工作时间不会参与on-call工作。&lt;/p&gt;

&lt;p&gt;第二种模式可以通过以下几种方式实现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;与另一名工程师分担on-call时间，这样当兼职工程师不在时，仍然有人值班。例如，如果on-call工程师需要从上午9点工作到下午4点，你可以将值班的前半部分（上午9点到下午3点）分配给他们，后半部分（下午3点到晚上9点）可以以相同的方式分配给其他on-call成员。&lt;/li&gt;
  &lt;li&gt;如果on-call频率不是太高，兼职工程师可以在on-call日工作整整几个小时也是可行的。
如站点可靠性工程的第11章所述，根据当地劳动法和法规，Google SRE会在正常工作时间之外补偿小时工资或休假时间。在确定on-call补偿时，要考虑兼职工程师的时间表。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了平衡项目时间和on-call时间，工作时间较少的工程师对应的工作内容应该少点。与小型团队相比，较大的团队更容易吸收额外的on-call负载。&lt;/p&gt;

&lt;h3 id=&quot;on-call团队动态&quot;&gt;on-call团队动态&lt;/h3&gt;

&lt;p&gt;我们的第一本书谈到了高报警负载和时间压力等压力因素是如何迫使on-call工程师采用基于直觉的未经详细考虑而非基于理性和数据的决策策略（参加该书第11章“安全感”一节）。基于团队心理学的讨论，你如何建立一个积极的动态团队？考虑一个on-call团队，其中包含以下一组假设问题。&lt;/p&gt;

&lt;h4 id=&quot;情景一周生存的文化&quot;&gt;情景：“一周生存”的文化&lt;/h4&gt;

&lt;p&gt;一家公司从几位创始人和少数员工开始，他们都是开发人员，每个人都相互了解，每个人都需要接收报警。&lt;/p&gt;

&lt;p&gt;公司规模开始变大。on-call的职责仅限于一小部分更有经验的功能开发人员，因为他们更了解系统。&lt;/p&gt;

&lt;p&gt;公司变得更大。他们增加了ops角色来解决可靠性问题。该团队负责生产环境监控，成员主要集中在运维，而非编码。功能开发人员和ops人员轮流进行on-call工作。功能开发人员在维护服务方面有最终决定权，而ops仅限于运维任务。到目前为止，有30名工程师参与on-call工作：25名功能开发人员和5名ops，都位于同一站点。&lt;/p&gt;

&lt;p&gt;团队被高报警量所困扰，尽管遵循了本章前面所述的建议，尽量减少报警负载，但团队的士气仍然很低落。由于功能开发人员优先考虑开发新功能，因此on-call的后续工作需要很长时间才能实现。&lt;/p&gt;

&lt;p&gt;更糟糕的是，由于功能开发人员关注的是自己子系统的健康状况，尽管团队中其他人提出了投诉，但有位功能开发人员坚持按错误率而非关键模块错误比率来进行报警。这些报警很嘈杂，会有很多误报或者不可执行的报警。&lt;/p&gt;

&lt;p&gt;高报警负载对on-call岗的其他成员的影响不会特别大，确实有许多报警，但大多数报警都没有花太多时间来解决。正如一名on-call工程师所说：“我快速浏览一下报警主题，知道它们是重复的。所以我要做的就是忽略它们。”&lt;/p&gt;

&lt;p&gt;听起来很熟悉？&lt;/p&gt;

&lt;p&gt;Google的一些团队在成熟的早期阶段遇到过类似问题。如果不小心处理，这些问题可能扰乱功能开发团队和运维团队，并阻碍on-call的操作。没有灵丹妙药能解决这些问题，但我们发现了一些特别有用的方法。虽然你的方法可能有所不同，但总体目标应该是相同的：建立积极的团队氛围，避免混乱。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;建议一：给你的ops工程师授权&lt;/strong&gt;。你可以根据本书和站点可靠性工程中列出的指南对运维组织进行重新构建，甚至可以更改名称（SRE或类似名称）来表示角色的更改。重新命名你的运维组织并非灵丹妙药，但它有助于体现别于旧的以操作为中心的模型的新的责任变化。向团队和整个公司明确说明SRE拥有站点操作权限，包括定义可靠性的共享路线图，推动问题的全面解决，维护监控策略等。功能开发人员是必要的协作者，但没有这些权限。&lt;/p&gt;

&lt;p&gt;回到我们之前假设的团队，本公告引入了以下运维变化：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;操作项仅分配给5个DevOps工程师——即SRE。SRE与项目专家合作——大多为开发人员——来完成这些任务。SRE就前面提到的：“错误率与错误比例”的报警策略与功能开发人员进行协商。&lt;/li&gt;
  &lt;li&gt;如果可能，鼓励SRE深入研究代码以自行进行更改。他们将代码审查发送给项目专家。这样有利于在SRE之间建立主人翁意识，并在未来的场合提升他们的技能和权威。
通过这种安排，功能开发人员是可靠性功能的明确协作者，且SRE有权利拥有站点以及改进站点的责任。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;建议二：改善团队关系&lt;/strong&gt;。另一种可能的解决方案是建立更强有力的团队关系。Google设置了一个“有趣的预算”，专门用于组织异地活动来加强团队合作。&lt;/p&gt;

&lt;p&gt;我们发现，强大的团队关系可以增强团队成员之间的理解和协作精神。因此，工程师修复bug，完成操作项目并且帮助同事的几率更高。例如，假设你关闭了夜间管道工作，但忘记关闭检查管道是否成功运行的监控。结果，同事在凌晨3点收到了报警。如果你和那位同事为处理报警花了点时间，你对这件事感到很抱歉，并在将来对此类操作更加小心。“我要保护我的同事”这一心态会转化成为更富有成效的工作氛围。&lt;/p&gt;

&lt;p&gt;我们还发现，无论职称和职能如何，让on-call的所有成员坐在一起，有助于改善团队关系。还可以鼓励团队一起吃午饭，不要低估这些相对简单的变化，它会直接影响团队动力。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;p&gt;SRE on-call与传统的ops角色不同。SRE不仅专注于日常运维，且拥有生产环境权限，并通过定义适当的可靠性阈值，开发自动化工具以及开展战略工程项目来获得更好的生产环境。on-call的站点操作十分重要，公司必须正确处理。&lt;/p&gt;

&lt;p&gt;on-call是个人和集体压力的根源。但如果你盯着怪物的眼睛看久了，就会发现智慧。本章阐述了一些关于on-call的案例；希望我们的经验可以帮助他人避免或解决类似的问题。&lt;/p&gt;

&lt;p&gt;如果你的on-call团队淹没在无休止的报警中，我们建议你退一步观察更顶层的情况，和其他SRE和合作伙伴团队对比讨论，一旦收集了必要的信息，就要系统的解决问题。对on-call工程师，on-call团队以及整个公司来说，构建合理的on-call机制是值得投入时间的。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">On-call轮值意味着在某段时间内随叫随到，随时响应紧急问题。站点可靠性工程师（SRE）通常需要参与on-call轮值工作。在on-call轮值期间，SRE需根据需要判断、缓解、修复或升级事件。此外，SRE还需定期响应非紧急生产事件。 在Google，on-call轮值是SRE的职责之一。SRE团队可以缓解故障，修复生产环境问题且自动执行运维任务。由于大多数SRE团队的运维任务还未完全实现自动化，升级需要人为联系on-call工程师。SRE团队的on-call工作是根据所支持系统的重要程度或系统所处的开发状态而定的。根据我们的经验，大多数SRE团队都需要参与on-call轮值工作。 On-call是一个庞大而复杂的话题，限制因素很多，但是试错率却很少。我们的第一本书《Site Reliability Engineering》第11章“on-call轮值”已经探讨过这一主题。本章介绍一些我们收到的有关该章的反馈和问题。其中包括： “我们不是Google，我们的规模要小得多。我们没有那么多人参与轮值，并且没有位于不同时区的站点。你在第一本书中描述的内容与我无关。” “我们的开发人员和DevOps是混合在一起参与on-call轮值的。如何组织是最佳方案？如何分担责任？” “我们的on-call工程师在24小时轮值中约要处理100个问题。很多问题都被忽略了，而真正重要的问题也淹没其中。我们应该从哪开始处理？” “我们的on-call轮值周转率很高，如何解决团队内部的认知差距？” “我们打算将我们的DevOps团队重组为SRE(注1)。” SRE on-call、DevOps on-call和开发人员on-call的区别是什么？因为DevOps团队非常关注这点，所以请具体说明。 我们将为这些问题提供实用的建议。Google是一家拥有成熟SRE组织的大型公司，多年来我们学到的很多东西都可以应用于任何规模和成熟度的公司或者组织中。Google在各种规模的服务上都有数百个on-call轮值人员，从简单到复杂的服务对应着不同的on-call设定。on-call并不专属于SRE：许多开发团队对于他们所负责的服务也需要on-call。每个on-call设定都应满足对应服务的需要。 本章介绍Google内部以及其他公司的on-call设定。你的设定和情况可能与我们展示的具体示例不同，但我们所涵盖的基本概念是普遍适用的。 在深入研究和分析手机报警负载的原因后，我们提出优化报警信息设置的策略并最大限度的减少负载。 最后，我们分享了Google内部实践的两个例子：on-call的灵活性和on-call团队的动态变化。这些实践证明无论on-call设定的数学方法是什么，都不能完全依赖on-call人员的后续处理。需要适当考虑对on-call人员进行激励和人文关怀。 回顾第一本SRE书中“On-Call轮值” 《站点可靠性工程师》在“On-Call轮值”一章中阐述了Google on-call轮值的基本原则。本节将讨论该章节的重点内容。 在Google，on-call的目标是确保不牺牲on-call工程师的健康为前提下覆盖到重点服务，保障服务的可靠性。因此，SRE团队的工作应该是个健康的平衡的职责搭配：on-call和日常项目工作。我们要求SRE团队至少花50%的时间进行工程项目开发，以便战略性的解决生产环境中发现的各种问题。团队人员必须确保有时间参与项目开发工作。 为确保有足够时间跟进，每个on-call轮值班次内最多跟进两个故障（注2）。如果报警信息负载过多，需要采取纠正措施。（我们将在本章后面探讨报警信息负载。）注2：不管同一个“问题”发出了多少报警，都被定义为一个“故障”。一个轮值班次是12个小时。 安全感（注3）对于on-call的有效轮转至关重要。on-call期间的压力很大，为了减轻on-call人员的压力，调节他们的生活，应该提供一系列清晰的程序和问题升级路线的支持。注3：David Blank-Edelman（O’Reilly）在Seeking SRE一书中有更多关于此主题的内容。 针对工作时间之外的on-call工作应给予合理的补贴。不同的公司可能有不同的方式进行补贴。Google提供年假或者现金补贴，同时按一定程度的工资比例作为上限。补贴措施激励SRE按需参与on-call工作，且避免因经济原因而过多的参与on-call工作。 Google内部和其他公司的On-Call设定示例 本节介绍了Google和Evernote的on-call设置，这是一家致力于帮助个人和团队创建、汇总和共享信息的跨平台应用程序的加利福尼亚公司。我们探讨了每家公司on-call设定、理念以及实践背后的原因。 Google：组建新团队 初始场景 几年前，Google Mountain View（地名：山景城）的SER Sara组建了一个新的SRE团队，该团队需要在三个月内胜任on-call工作。Google的大多数SRE团队默认新员工需要三到九个月时间来准备承担on-call工作。新组建的Mountain View SRE团队将支持三个Google Apps服务，这些服务之前是由位于华盛顿州Kirkland（地名：柯克兰）的SRE团队提供支持的（从Mountain View起飞需要两小时才能到达）。Kirkland团队在伦敦有一个姊妹团队，该团队会继续支持这些服务，以及新组建的Mountain View SRE团队和部分产品开发团队（注4）注4：Google的SRE团队通过跨时区协同工作来确保服务的连续性。 新成立的MountainView SRE团队很快聚集了七个人： Sara，SRE技术主管 Mike，来自另一个SRE团队的具有丰富经验的SRE 从新产品开发团队转岗过来的SRE 四名Nooglers（Nooglers：新员工，特指近期才为Google工作的人） 面对新服务的on-call工作，即使是个成熟的团队，也是充满挑战的，而新的Mountain View SRE团队是一个相对初级的团队。尽管如此，新团队做到了在不牺牲服务质量或项目速度的前提下提供服务。他们很快着手改进服务，包括将机器成本降低40%，通过灰度发布和其它安全检查完成自动化发布。新团队持续提供可靠性服务，目标为99.98%的可用性，或每季度约26分钟的停机时间。 新的SRE团队是如何通过自我完善来实现该目标的？答案是通过入门项目，指导和培训。 培训方案 虽然新的SRE团队对他们的服务对象知之甚少，但Sara和Mike对Google的生产环境和SRE工作非常熟悉，且四位Nooglers也通过了公司的招聘，Sara和Mike整理了一份包含二十多个重点领域的清单供组员在on-call之前进行练习，例如： 管理生产作业 了解调试信息 集群流量切换 回滚有问题的版本 阻止或限制恶意请求 提供额外的服务能力 使用监控系统（报警和仪表盘） 了解服务的架构，各种组件以及依赖关系 Nooglers(新人)通过研究现有文档和代码（指导，实践编码教程）找到这类信息，并通过研究入门项目来理解相关主题。当团队成员学习到与Nooglers的初学者项目相关的主题时，该成员会召开简短的临时会议，将所学知识分享给其他成员。Sara和Mike会介绍剩余的主题。该团队还进行实践练习，通过执行常见的调试和降损操作帮助成员形成肌肉记忆，增加对自己能力的信心。 除了练习清单外，这个新团队还进行了一系列“深度潜水”来深入了解他们的服务。团队浏览了监控控制台，确定正在运行的作业，并尝试修复最近的报警。Sara和Mike解释道，要想做到对每项服务都十分熟悉，工程师并非需要多年的专业知识。他们指导团队也是从这一原则出发，鼓励Nooglers熟悉这些服务。他们每个人理解的知识都是有限的，这会教导成员在何时向别人寻求帮助。 在整个成长过程，新团队并不孤单。Sara和Mike前往其它SRE团队和产品开发团队，向他们取经。新团队通过视频会议、邮件和IRC与Kirkland和伦敦团队进行沟通。此外，该团队还参加每周的生产会议，查看on-call轮值表和事后报告，并浏览现有的服务文档。Kirkland团队派来一名SRE与新团队交流解答问题，伦敦的SRE整理了一套完整的灾难情景，并在Google灾难恢复培训期间进行了运行展示（请参阅“站点可靠性工程”第33章“灾难预案与演习”部分）。 该小组还通过“幸运之轮”训练演习（见第28章“故障处理分角色演习”一节）如何on-call，扮演各类角色，练习解决生产问题。演习期间，鼓励所有SRE提供解决模拟生产环境故障的建议。在每个人的能力都得到增强之后，团队仍举办这类演习，每个成员轮流担任演习负责人，并且将演习过程记录下来以供未来参考。 在进行on-call工作之前，团队查看了有关on-call工程师职责的指导原则。例如： 在每次轮岗开始时，要从上一个on-call工程师那获得轮岗转换邮件。 on-call工程师需要先止损，然后确保完全解决问题。 在轮岗结束时，on-call工程师向待命的on-call发送轮岗转换邮件通知。 操作指南规定了问题何时升级到其他人以及如何为大型事件编写事后总结报告。 最后，该团队阅读并更新了on-call的操作指南。操作指南中有针对报警的详细说明，解释了报警的严重级别和影响，还有针对完全解除报警的操作意见和需要采取的措施。对于SRE，每当产生报警时，都会创建对应的操作指南记录。这些记录可以减小on-call压力，平均恢复时间（MTTR）以及人为犯错误的风险。 维护操作指南 操作指南中涉及的细节变化与生产环境的变化保持同步。针对日常发布，指南可能需要随着发布时间进行更新。就像任何一种方式的沟通一样，编写一份好的文档是很难的。因此，如何维护好你的操作指南呢？ Google的一些SRE主张操作指南条目要保持通用性，这样迭代的速度就会缓慢些。例如，所有的“RPC Errors High”报警放在一个条目下，经验丰富的on-call工程师可以结合当前报警服务的架构图进行阅读。为了减少人员变更因素影响以及降低MTTR，另有部分SRE主张逐步开放分享操作指南。如果你的团队对指南中的做法另有异议，那么操作指南可能会衍生出多个分支。 这个话题颇具争议。不管你想做成什么样的，但至少你和你的团队要明确操作指南的最小粒度和结构化的细节，并时刻关注操作指南的内容是否累计到超出了最初的设定。在这过程中，要学会将实践获取的知识自动化部署到到监控平台。如果你的操作指南是个明确的由命令组成的列表，当对应的报警触发时，on-call工程师会按照列表执行命令的话，我们建议将其转化为自动化执行。 两个月后，Sara，Mike和他们的SRE承担了即将卸任的Kirkland SRE团队的on-call备岗工作。在第三个月，他们成了on-call主岗，Kirkland SRE作为备岗。通过这样的方式，Sara和他的SRE团队随时可以替代Kirkland SRE团队。接下来，Nooglers作为更有经验的SRE成员的备岗加入了轮值工作。 丰富的文档和前文所述的各种策略方法都有助于团队形成坚实的基础并迅速获得提升。虽然on-call意味着压力，但团队已经具备足够的信心让他们在采取行动之前不会怀疑自己。即使他们升级事件，他们的反应也是基于团队的集体知识以及自身心理素质，仍然是个称职的on-call工程师。 后续 虽然MountainView SRE仍在成长，但他们了解到他们在伦敦的姊妹团队将转而负责新的项目，并且在苏黎世成立了一个新团队来负责伦敦团队之前的工作。对于第二次工作交接，Mountain View SRE使用了相同的方法，事实证明也是成功的。MountainView SRE之前准备的入职和培训资料帮助新成立的苏黎世SRE团队获得成功。 当一群SRE组成一个新团队时，Mountain View SRE的方法是有效的，但当团队新加入一个成员时，只需要用更轻量级的方法即可。考虑到将来的轮转，SRE绘制了服务架构图，并将基础培训列表正式化为一系列的练习，这些练习无需导师全程参与，可由成员半自主完成。例如描述存储层，执行扩容以及了解HTTP请求过程。 Evernote：在云中寻找我们的根 将我们的本地基础架构迁移到云 如生活中的大多数事情一样，需求是发明之母，因此我们并没有着手重新设计我们的on-call流程。在2016年12月之前，Evernote仅运行在本地数据中心，支持我们的单体式应用程序。我们的网络和服务器在设计时考虑了特定的架构和数据流，结合其他一些约束，意味着我们缺乏支持水平架构所需的灵活性。Google Cloud Platform（GCP）为我们提供了具体的解决方案。但是，仍有一个障碍需要克服：将我们的所有产品和基础设施迁移到GCP。历时70天，通过艰苦卓绝的努力和无数壮举（例如，移动数千台服务器和3.5PB的数据），我们住进了新家。至此，我们的工作仍未完成：我们如何监控，报警，最重要的——如何在新环境应对问题？ 调整on-call策略和流程 应用迁移到云环境激发了我们基础设施快速增长的潜力，减小了我们的基础设施快速增长的阻力，但我们的on-call策略和流程尚未随着这种增长而调整。迁移完成后，我们着手解决问题。在之前的物理数据中心中，我们几乎在每个组件中都创建了冗余。对我们而言，组件故障很常见，但基本上很少有单个组件对用户体验产生负面影响。要知道任何小的抖动都是源于系统的某处故障，而因为我们可以控制它，所以我们的基础设施非常稳定。我们的报警策略是基于以下思路构建的：一些丢弃的数据包，导致JDBC（Java数据库连接）连接异常，意味着VM（虚拟机）主机即将发生故障，或控制面板某一开关一直处于失常。甚至在我们第一天步入云端之前，我们就意识到这种类型的报警/响应系统在未来是不可行的。在实时迁移和网络延迟的世界中，我们需要采用更全面的监控方法。 根据第一原则重新构建报警事件，并将这些原则写下来作为明确的SLO（服务级别目标），这有助于团队明确重要信息，从监控基础架构中减少冗余。我们专注更高级别的指标，例如API响应而非类似MySQL中的InnDB行锁等低级别的基础结构，这意味着将更多时间集中在用户在服务中断时遇到的痛点上。对团队而言，也意味着可以减少追踪瞬态问题的时间。相对应的，有了更多的睡眠时间，更高效，工作满意度也更高。 重构监控和指标 我们的on-call轮值人员是由一个小而充满斗志的工程师团队组成，他们负责生产基础设施和一些业务系统（例如，升级和构建管道基础设施）。每周进行一次轮岗，且在每日的晨会上对之前一天的事故进行复盘。我们的团队规模小，但责任范围大，因此需要努力减轻流程负担，专注于尽快响应报警/报警分类/处理报警/事后分析复盘。我们实现这一目标的方法之一是通过维护简单但有效的报警SLA（服务级别协议）来保持低信噪比。我们将指标或监控基础架构产生的所有故障分成三类： P1：立即处理 需要立即采取行动 呼叫on-call 导致事件分类 是否影响SLO P2：下个工作日处理 通常不面向客户，或范围有限 向团队发送电子邮件并通知事件流向 P3：故障仅需知晓 信息收集在仪表盘，自动发送的邮件中等 与容量规划相关的信息 所有P1和P2故障都附有故障单，用于描述事件分类，跟踪处理措施，SLO影响，发生的次数和事后报告链接。 当出现P1级别的事件时，on-call人员需要评估该事件对用户的影响。从1到3对事件的严重性做分级。对于严重性等级为1（Sev 1）的事件，我们有一套标准流程以便响应人员尽可能快速的做出升级决策。故障升级后，我们会组建一个故障团队开始故障处理流程。由记录员和通讯负责人负责故障通报，沟通渠道是完全开放的。故障解决后，我们会主动进行复盘并在公司内部分享结果。对于级别为Sev 2或Sev 3的故障，on-call人员负责处理故障以及故障的事后报告。 保持流程的轻量化有助于参与项目工作的同事胜任on-call工作，鼓励on-call在遇到故障时立即采取修复行动，并在完成故障复盘后发现工具或流程中的不足之处。通过这种方式，在每次on-call轮岗期间都有持续的改进和灵活的循环方式，能和环境的变化速度保持一致。我们的目标是每个on-call轮次都比上一次更好。 追踪观察我们的表现 随着SLO的引入，我们希望按照时间维度跟踪性能，并将这些信息分享给公司内部的利益相关者。我们举办了月度级别的服务回顾会议，任何有兴趣的人都可以参加，会议主要回顾和讨论上个月份的服务情况。同时通过该会议判断on-call人员的负担，on-call的负担情况是团队健康的晴雨表，在压力过大时我们需要讨论缓解措施。会议的另一目的是在公司内部宣传SLO的重要性，督促技术团队对我们的服务健康和我们的健康负责。 与CRE合作 合理表达我们在SLO方面的目标是与Google客户可靠性工程（CRE）团队合作的奠定了基础。在与CRE讨论我们的SLO以明确它们是否真实可衡量后，两个团队决定针对会影响SLO的故障，都参与报警接收。隐藏在云抽象层背后的故障根本原因是很难被找到的，Google员工的参与在黑盒事件分类方面给予了帮助。更重要的是，这项举措进一步减小了用户最关心的MTTR。 保持自我延续的循环 我们现在有更多时间从团队角度思考如何推进业务发展，而不是将所有时间投入到分类/根因分析/事后分析的事情上。例如，改进我们的微服务平台，为我们的产品开发团队建立生产环境标准等项目。后者包括了我们重组on-call时遵循的很多原则，对于团队第一次上战场“接收报警”十分有帮助。因此，我们也延长改善了on-call的循环时间。 实际实施细节 至此，我们已经讨论了Google和Google以外的on-call设定的细节。但是对于即将参与on-call有哪些具体考虑因素呢？以下部分更深入的讨论了这些实现细节： 报警负载——它是什么，如何工作，如何管理它 如何让on-call时间表更具灵活性，为SRE创造更健康的工作/生活平衡环境 对于特定的SRE团队和合作团队要有动态的团队策略 手机报警负载剖析 你的手机一直有报警提醒，多到团队都受到了影响。假定你已经阅读了《》Site Reliability Engineering》的第31章，并和你的团队以及所支持的开发团队定期召开生产会议。现在大家都知道你的on-call工程师因为报警负载而不开心。然后呢？ 手机报警负载的定义是on-call工程师在轮值期间（每天或每周）收到的报警数量。一个故障可能导致多个报警产生。我们将介绍各种影响手机报警负载的因素并提出最小化报警负载的技术。 合适的响应时间 除非有充足的理由，否则工程师无需在收到报警的几分钟内上机器处理问题。虽然面向客户的创收服务故障需要立即响应，但一些不太严重的问题（例如，备份失败），你完全可以在几小时内处理。 我们建议你检查当前的报警设置，判断是否应该为当前触发报警的所有事件提供报警服务。你可能试图采用自动修复来解决问题（相对于人为修复，计算机能更好的解决问题）或者采用工单（如果它不是高优先级）。表8-1显示了一些案例和相对的响应。 表8-1. 实际响应时间案例 故障描述 响应时间 对SRE的影响 影响盈利的网络中断 5分钟 SRE需要保证手头的电脑有足够的电量且能联网；不能外出；必须始终与备岗协调保持联系。 客户订单处理系统挂了 30分钟 SRE可以出门，短期在外；在此期间，备岗无需保持在线。 用于预发的数据库备份失败 提工单（工作时间处理） 无。 场景：超负荷的团队 （假设）负责前端负载均衡和终端用户连接的Connection SRE团队发现自身的报警负载超负荷了。他们已经建立了每次轮值报警事件小于2次的目标，但在过去一年中，他们每次轮值平均接收5次报警事件。分析表明，有三分之一的轮值班次内报警数量超出预设值。团队成员已经及时的响应了报警，但仍然无法解决根因；没有足够的时间找到报警根因并妥善处理解决后续问题。一些工程师离开了团队，加入了运维负担较小的团队。由于on-call工程师的时间只够用来缓解眼前的故障，没法跟踪报警的根本原因。 团队的视野是开阔的：拥有遵循SRE最佳实践的成熟的监控系统，遵循SRE最佳时间。报警阈值设置和SLO保持一致，且报警本质上是基于业务表现特征的，意味着仅在客户受到影响时才会触发。高层管理人员在获知这些信息后，认为该团队已经处于超负荷状态，为了让团队恢复健康状态，他们开始审查项目计划。 不幸的是，随着时间的推移，Connection团队已经从10多个开发团队中获得了软件组件的所有权，并且对Google面向客户的边缘和骨干网络有着强依赖。群际关系很复杂，也慢慢变得难以管理。 尽管团队遵循构建监控的最佳实践方法，但所面临的很多报警都超出了他们的直接控制范围。例如，黑盒探测可能会因网络拥塞而失败，导致数据丢包。团队可以采取的唯一措施就是将事件升级到直接负责该网络的团队。 除了运维负担外，团队还需要为前端系统提供新功能，供所有Google服务使用。更糟糕的是，他们的基础架构正在从一个已有10年历史的遗留框架和集群管理系统中迁移到更高的支持替代品中。该团队的服务受到全所未有的变化速度的影响，这些变化本身也引起了大部分的on-call负担。 该团队需要各种技术来平衡减少过多的报警负载，团队的技术项目经理和人事经理向高级管理层提交了一份项目建议书，高级管理层审核并通过该建议书。团队全力投入减小报警负载中，在此过程中也获得了宝贵的经验教训。 报警负载来源 解决报警负载的第一步是明确负载出现的原因。报警负载受到三个主要因素的影响：生产环境中的bug （注5）、报警和人为因素。这些因素都有对应的来源，本节将详细讨论其中一部分来源。 注5：文中的“bug”是由软件或配置错误导致的非预期的系统行为。代码中的逻辑错误，二进制文件的错误配置，错误的容量规划，错误配置的负载平衡或新发现的漏洞都是导致报警负载的“生产 bug”的原因。 对于生产环境： 生产环境中存在的bug数量 将新bug引入生产环境 识别到新引入的bug的速度 缓解bug并从生产环境中删除之的速度 对于报警： 触发报警的阈值 引入新的报警规则 将服务的SLO与其所依赖的服务的SLO关联对齐 对于人为因素： 严格的修复和追踪bug 收集报警的数据质量 注意报警负载变化 人为驱动的生产环境变化 已经存在的bug。不存在完美的系统。无论是在你的代码里，还是在你依赖的软件和库中，或者是接口之间，产品总会存在bug。虽然这些bug可能并不会立即触发报警，但它们却是客观存在的。你可以利用一些技术来识别或防止尚未导致报警的bug： 确保系统的复杂度和实际相符，并不是越复杂越好。（见第7章）。 利用修复bug的机会，定期更新系统所依赖的软件或库（请参阅下一节有关新bug的部分）。 定期执行破坏性测试或模糊测试（例如，使用Netflix的Chaos Monkey）。 除集成和单元测试外，还执行常规负载测试。 新bug。理想情况下，SRE团队及其合作的开发团队应该在新bug进入生产环境之前检测到。事实上自动化测试漏测了很多bug，这些bug最终进入了生产环境。 软件测试是个覆盖面很广的主题（例如，Martin Fowler on Testing）。这项技术在减少进入生产环境的bug数量以及减少bug在生产环境停留的时间方面很有帮助： 随着时间推移不断改进测试（方法、技术）。尤其是你在生产环境中每发现一个bug，都要自问“如何才能在预发环境检测到这个bug？”确保有必要的工程技术跟进解决此问题。（请参阅“严谨跟踪”，第164页）。 不要忽略负载测试，虽然负载测试的优先级常被视为低于功能测试的。但许多bug仅在特定的负载条件下或特定的请求组合中才会显露出来。 在生产环境中集成（使用类似生产环境但是是合成的流量进行测试）。我们将在本书的第5章简要讨论生成合成流量。 在生产环境执行canarying（第16章）。 对新bug保持较低的容忍度。遵循“检测，回滚，修复和发布”策略，而不是“检测，虽然找到bug，但继续发布，修复并再次发布”策略。（相关详细信息，请参阅第162页的“减少延迟”。） 这种回滚策略需要可预测且频繁发布，因此回滚任何版本的成本都很小。我们在《Site Reliability Engineering》一书“发布工程”章节中讨论了相关主题。 一些bug可能仅仅是由于改变客户端行为导致的。例如： 仅在特定负载水平下出现的bug——例如，9月返校流量，黑色星期五，网络星期一，或一年中夏令时即欧洲和北美时差一小时的那一周，意味着更多用户同时保持清醒和在线状态。 只有特定混合请求才显示的bug——例如，用于亚洲字符集的语言编码，更接近亚洲的服务器的流量消耗更大。 仅在用户以意想不到的方式运行系统时才会显示的bug——例如，（在）航空公司订票系统使用的日期（下运行系统）！因此，为了测试能够覆盖到不常发生的行为（导致的bug），扩展您的测试方案是十分必要的。 当生产系统受到多个并发错误的影响时，判断报警是由于现有bug还是新bug引起的是很困难的。最大限度的减少生产环境中的bug不仅可以减少报警负载，还对新bug的识别和分类很有帮助。因此。尽快从系统中删除生产环境的bug至关重要，修复现有bug的优先级应该在开发新功能之上；如果过程中需要跨团队合作，请参阅第18章。 架构或程序问题，例如自动健康检查，自我修复和减小负载，可能需要大量的工程工作来解决。为简单起见，我们将这些问题视为“bug”，即使它们的规模、复杂度或解决它们需要的工作量很大。 《SiteReliability Engineering》中第3章描述了错误预算如何控制新bug发布到生产环境的方法。例如，当服务的SLO超过其总季度错误预算的某一部分时——事先在开发人员和SRE团队间达成一致意见——可以暂停新功能开发以及和功能相关的部署，以专注于系统稳定，减少报警的频率。 示例中的Connection团队采用严格的策略，要求每次故障都需要追踪bug。该举措能让团队的技术项目经理知道产生新bug的根本原因在哪。数据显示，人为错误是生产环境中新bug产生的第二大常见原因。 由于人类容易出错，如果对生产系统所做的所有变更都是通过（人为开发的）配置自动生成的，那么效果会更好。在对生产环境进行变更之前，自动化手段可以执行人类无法进行的测试。Connection团队是半手工的对生产环境进行复杂的变更的。毫无疑问，团队的手动变更有时会出错；该团队引入了触发报警的新bug。在新bug进入生产系统并触发报警前，将要做出类似变更的自动化系统就会判断出这种变更是不安全。技术项目经理将这些数据提供给团队，说服他们优先考虑进行自动化项目。 识别延迟。及时识别报警的原因十分重要，这个识别的时间越长，意味着报警再次产生的几率越大。例如，有一个仅在高负载情况下才会产生的报警，如果在下一个峰值之前未识别有问题的代码或配置，那么问题可能会再次发生。你可以用这些技术来减少报警识别时间： 使用合理有效的报警和控制台 确保报警页面链接到相关的监控控制台，且该控制台突出显示系统运行超出规范的位置。在控制台中，将黑盒和白盒监控报警相关联，并对关联的图表执行相同的操作。确保操作指南是最新的，提供相应每种报警类型的行动建议。on-call工程师应在相应的报警触发时用最新信息更新操作指南。 实践应急响应 进行“幸运之轮”练习（在《Site Reliability Engineering》中有描述），和同事共享常用的和针对特定服务的调试技术。 执行小变更 如果您频繁执行局部（部分功能、部分模块）的变更而不是偶尔的整体（所有功能、所有模块）变更，那么能很容易的将bug与引入它们对应的变更相关联。第16章中描述的Canarying版本给出了一个判断，表明新bug是否是由于新版本引起的。 日志变更 将变更信息聚合到可搜索的时间线中可以更简单（且更快）的将新bug与引入它们的变更相关联。Jenkins的Slack插件可能会有所帮助。 寻求帮助 在《SiteReliability Engineering》“故障管理”中，我们讨论了共同管理大型故障的问题。on-call工程师从来不会只是一个人；要让你的团队在寻求帮助时有安全感。 减少延误。一旦找到bug，修复bug所需的时间越长，就越可能再次发生问题并产生报警。可以考虑这些减少延误的技术： 回滚变更 如果bug是在一次最近的代码、配置变更中引入的，在安全和恰当的情况下（单独回滚代码、配置可能是必要的，但如果bug是因为数据损坏导致的，那只回滚代码、配置就不能解决问题了）我们可以通过立即回滚生产环境的变更消除bug。谨记，即使是“快速修复”也需要时间进行验证，构建和发布。验证是至关重要的，要确保“快速修复”确实可以修复bug，并且不会引入额外的bug或其他非预期的影响。通常，采取“回滚，修复和发布”要优于“发布，修复和再发布”操作。 如果你的目标是99.99%的可用性，那么每季度约有15分钟的错误预算时间。上线发布的构建步骤可能需要15分钟以上，因此回滚对用户的影响更小。（99.999%的可用性对应每季度80秒的错误预算，这样的系统可能需要自我修复的属性，超出了本章的讨论范围。） 如果可能，避免接入无法回滚的变更，例如API不兼容的变更和锁步版本。 使用功能隔离 设计你的系统，以便在功能X出错时，可以通过一个功能标志禁用它，而不影响功能Y。该策略还能提高发布速率让禁用X功能变得简单——且不需知道产品经理是否习惯于禁用功能。 切走请求流量 把请求流量从出现bug的系统组件中切走（即重定向客户请求）。例如，如果bug是代码或配置上线导致的，并且是逐步发布到生产环境中的，那么你还有机会通过把流量从已发生变更的基础架构的元素切走（达到快速止损的目的）。这样你可以在几秒钟内降低对客户的影响，但回滚可能需要几分钟或更长时间。 报警。Google SRE每次轮值时间即12小时最多发生两次不同的报警事件，因此我们对如何配置报警以及如何引入新的报警是经过深思熟虑的。网站可靠性工程“监控分布式系统”描述了Google定义报警阈值的方法。严格遵守这些准则有助于健康的on-call轮转。 需要强调一下，这章讨论了一些关键元素： 收到的所有报警都应该立即去操作。我们希望团队在收到系统无法自愈的报警后立刻采取行动。信噪比要高，确保较低的误报率；低信噪比会增加on-call工程师产生“狼来了”的感觉的几率。 如果团队的报警规则是基于SLO，或错误上限（请参阅站点可靠性工程中的“黑盒监控与白盒监控”部分），那么所有参与开发和维护站点可靠性的团队都需要认同SLO的重要性并明确他们的工作优先级。 如果团队完全基于SLO和现象制订报警策略，那么放宽报警阈值是对报警的合理调整。 就像新的代码，新的报警策略也应该经过彻底和周密的审查，每条报警都应该有对应的操作指南条目。 接收报警会对人产生负面的心理影响。为了最大限度的减少这类影响，最好只有在真正需要时才引入新的报警规则。团队中任何人都可以编写新的报警策略，但新的策略需要经过整个团队的审核建议以及提出替代方案。在将新策略发布到线上之前，要彻底测试生产中的新策略来审查是否有误报。例如，可以在报警触发时给作者发送电子邮件，而不是直接呼叫on-call工程师。 新的报警信息会帮你发现之前并不知道的生产环境问题。在解决了这些bug之后，报警将仅被新bug触发，也起到了回归测试的作用。 确保新报警在测试环境下运行的时间足够长，能适应典型的生产环境，例如常规软件部署，云提供商的维护需求，每周负载峰值等。通常一周的测试时间是足够的，但具体时间窗口仍取决于报警和系统。 最后，利用测试期间报警的触发率预测新报警可能会产生的报警负担。对新报警配置的批准或禁止要以团队为单位。如果引入新报警会导致你的服务超出报警阈值，那么需要额外注意系统的稳定性。 事后密切跟进。目的是确定每个报警的根本原因。查找“根因”的范围要从机器层面延伸到团队流程层面。服务中断是由一个本可以通过单元测试发现的bug导致的吗？根因可能不是代码中的bug，而是代码审查中团队流程的bug。 如果你知道根因，你就可以修复它防止再次困扰你或你的同事。如果你的团队无法确定根本原因，可以添加监控或日志记录，帮助在下次发生这种情况时找到报警的根本原因。如果没有足够的信息来识别bug，你可以做一些事情来帮助进一步调试bug。或者至少可以得出结论，即报警是由“未知原因”触发的。请牢记，身为on-call工程师，你永远不是孤军奋战，所以可以请同事帮忙检查你的发现，看看是否有遗漏的地方。通常，报警触发后有新的证据可用时，很快能找到报警的根本原因。 将一个报警解释为“瞬态的”或由于系统“自行修复”或莫名其妙“消失”而不采取任何行动时，这个报警很可能会再次发生并导致另一个报警，会给下一下on-call工程师带来麻烦。 简单修复眼前的bug（或进行一个“点”修复）错过了一个避免将来出现类似报警的黄金机会。把报警信息看作一个带来工程工作的机会，这种工程工作可以改进系统并且消除可能出现的一类bug。可以在你的团队的生产组件中归档项目bug来做到这一点，我们提倡通过收集这个项目会消除的bug以及报警数量，按轻重缓急进行bug修复。如果你的提案需要3个工作周或120个工作时来实施，并且报警平均需要4个工作时才能正确处理，那么30个报警产生后会有一个明确的盈亏平衡点。 举个例子，假设有这样一种情况，在同一故障域上存在很多服务器，例如这些机器在数据中心中的同一个交换机下，会导致定期同时发生多个机器的故障。 点修复 在众多故障域中重新平衡当前的覆盖区。 系统修复 使用自动化手段确保此类服务器和所有其他类似服务器始终分布在足够的故障域中，并在必要时自动重新平衡。 监控（或预防）修复 当故障域多样性低于预期水平但尚未影响服务时，预先发出警告。理想情况下，警报将是故障工单警报，而不是呼叫报警，因为不需要立即响应。尽管处于较低的冗余水平，该系统仍可以进行服务。 为确保您对寻呼警报的后续工作有所了解，请考虑以下问题： 如何防止此特定bug再次发生？ 对于此系统和我负责的其他系统，如何防止此类bug再次发生？ 哪些测试可以防止此bug被发布到生产环境中？ 哪些故障工单警报会触发操作以防止bug在被报警前变的严重？ 在变得严重之前，哪些报警信息会出现在控制台上？ 我是否最大化了修复bug带来的收益？ 当然，对于on-call工程师，仅仅提交值班期间发生的呼叫报警相关的bug是不够的。重要的是，SRE团队会迅速处理他们确定的bug以减少它们再次发生的可能性。要确保SRE和开发人员团队的资源规划考虑了响应bug所需的工作量。 我们建议保留一小部分SRE和开发人员团队的时间来响应出现的生产bug。例如，Google on-call工程师通常不会在轮值期间处理项目工作。相反，他们处理可以改善系统健康状况的bug。确保你的团队常规下处理生产环境bug的优先级高于其他项目工作。SRE经理和技术主管应确保及时处理生产环境bug，必要时要升级到开发人员团队决策者。 当电话报警严重到需要事后调查时，遵循此方法来安排和跟踪后续行动更为重要。（有关详细信息，请参阅第10章。） 数据质量。一旦识别出系统中导致报警的bug，就会出现一些问题： 如何明确先修复那个bug？ 如何得知系统中哪个组件导致大多数报警？ 如何确定on-call工程师为解决这些报警而采取的重复性手动操作？ 如何判断有多少报警仍有未识别的根本原因？ 如何得知哪些bug是真实存在的、最严重的，而不是未确定的？ 答案很简单：收集数据！ 你可能会通过跟踪和收集on-call负载的方式来收集数据，但这种方法是有局限性的。更加可持续的做法是，为你的bug跟踪系统（例如，Jira，Issue-Tracker）中的每个电话报警提交一个bug，当on-call工程师意识到每个报警都是已存在的bug的表征时，需要在监控系统的相关报警和相关bug间建立链接。你将在一列中找到尚未解决的bug列表，以及每个相关联bug的页面列表。 当你拥有有关报警原因的结构化数据，就可以着手分析数据生成报告，这些报告能够回答以下问题： 哪些bug导致大多数的报警？理想情况下，我们会立即回滚并修复bug，但有时候，查找根本原因并部署修复程序需要很长时间，有时忽略关键报警并不是一个合理的选择。例如，上述Connection SRE团队可能会遇到持续的网络拥塞，这种拥塞无法立即解决，但仍需要跟踪问题。为团队收集导致了最多的报警和压力的生产环境问题的数据，支持进行数据驱动的有系统的、有优先级的对话。 系统的哪个组成部分是大多数报警的原因（支付网关，身份验证微服务等）？ 与其他监控数据相关联时，特定报警是否与其他信号相对应（请求量高峰，并发客户会话数，注册次数，提款次数等）？ 将bug数据和报警根本原因数据结构化还有其他好处： 你可以自动填充现有bug列表（即已知bug），这对你所支持的团队可能有益处。 你可以根据每个bug导致的报警数确定bug的优先级。 你所收集的数据质量将决定人工或机器可以做出的决策质量。为了确保高质量的数据，请考虑以下技术： 定义并记录你的团队对报警数据收集的策略和预期。 设置来自监控系统的非呼叫报警，突出显示未处理报警的位置。经理和技术主管应确保达到预期。 当轮岗交接不符合预期时，队友应该相互帮助跟进。积极的评论有“也许跟bug123有关”，“我已经根据你的调查结果提交了bug报告，所以我们可以进一步跟进了”，或“这看起来像我上周三轮岗发生的事情：&amp;lt;报警，bug的链接&amp;gt;”强化预期行为，确保最大化的改进。没人愿意为上一轮岗就已发生的报警再接收一次报警。 警觉。很多时候，团队会因为多次减员而陷入运维过载中。为了避免温水煮青蛙，要注意on-call工程师的健康状况，确保SRE和开发团队始终优先考虑生产环境健康状况。 以下技术可以帮助团队密切关注报警呼叫负载： 在生产会议上（参见“站点可靠性工程”中的“沟通：生产会议”一节，第31章），定期根据收集的结构化数据分析报警呼叫负载的趋势。追踪21天的平均值非常有用。 当呼叫报警负载超过你的团队事先明确的“告警”阈值时，设置针对技术主管或经理的故障单报警。 在SRE团队和开发团队之间定期召开会议，讨论当前的生产状况以及当前SRE为解决的生产环境bug。 on-call灵活性 值班时长 on-call值班期间每天需要处理一个或几个报警，因此值班安排必须是合理可持续的：我们建议将时长限制为12小时。较短的值班时长对on-call工程师的健康是有利的。当在岗时间太长时，团队成员大概率会觉得疲惫，随之而来的是他们在工作中可能会犯错误。如果一直进行on-call工作，大多数人无法保持高质量的产出。许多国家都有关于最长工作时间，休息时间和工作条件的法律。 虽然理想情况下是一直在白天值班，但12小时轮岗制也并不需要全球分布的团队。整夜12小时处于on-call中比on-call24小时或更长时间更好。即使工作在一个地方，你也可以进行12小时轮岗值班。例如，在为期一周的班次中，不是让一名工程师每天24小时on-call，而是两名工程师一人在白天on-call，一人在夜间on-call。 根据我们的经验，如果没有缓解机制，24小时on-call是不可持续的。虽然不理想，但偶尔on-call一整夜至少可以确保你的工程师的休息时间。另一个选择是缩短值班时间——比如3天值班，4天休息。 情景：个人情况的变化 想象一下，你是一个大型服务的on-call团队成员，该服务具有跨越两个站点的24/7跟随太阳模型。为了在提高服务可靠性的同时保持运维负载的可控性，虽然你并不乐意在上午6点可能会接到报警，但你对你和团队正在进行的工作感到满意。 一切都很好…直到某天你才意识到on-call的时间表和你个人生活的需求开始发生冲突。有许多潜在的原因——例如，成为父母，需要短期旅行，休假或生病。 你需要on-call的职责和新的个人日程表能够共存。 许多团队和组织在成熟时都面临这一挑战。随着时间推移，人们的需求发生变化，为了保持多元化团队成员的健康平衡，on-call轮值的需求变的多样化。保持健康，公平以及on-call工作和个人生活健康的平衡的关键在于灵活性。 为满足团队成员的需求，确保覆盖到你的服务或产品，你可以通过多种方式灵活的进行on-call轮转。指定一套全面的，一刀切的指导方针是不可能的。我们鼓励将灵活性作为一项原则，而不是简单的采用此处列举的实例。 自动化on-call时间安排。随着团队的发展，时间表的安排受到以下约束——休假计划，on-call工作日与周末的分布，个人偏好，宗教要求等，时间表的安排变得越来越困难。你无法手动管理此任务，很难找到任何解决方案，更不用说公平的解决方案了。 “公平”并不意味着跨团队成员的每个变化都是一致的。不同的人有不同的需求和不同的偏好。因此，团队应该分享这些偏好并尝试以智能的方式满足这些偏好。团队组成和首选项决定了你的团队是更喜欢统一分发，还是以自定义的方式来满足日程安排首选项。 使用自动化工具来安排on-call班次会更容易。这个自动化工具应该有这些基本特征： 它应该重新安排on-call班次以适应团队成员不断变化的需求。 它应该自动重新平衡报警负载以响应任何更改。 应该尽量通过考虑个人偏好来确保公平，例如“4月份周末不用上学”，以及历史信息，例如最近每位on-call的值班负载。 因此，on-call工程师可以依据on-call班次进行计划，但绝不能改变已经生成的时间表。 时间表既可以是完全自动化的，也可以是团队人员安排。同时，一些团队更愿意让成员明确遵守时间表，而其他团队则对完全自动化的流程感到满意。如果你的需求很复杂，可以选择在内部开发自己的工具，此外也有许多商业和开源软件包可以帮助自动化生产on-call时间表。 短期互换的计划。on-call时间表通常会收到的短期变化请求。没人能在周一就承诺周四肯定不会感冒。或者你可能需要在on-call期间处理无法预料的紧急事务。 你可能还希望因为非常规原因能够在on-call中换岗——例如，允许on-call人员参加运动训练课程。在这种情况下，团队成员可以交换一天的on-call日（例如，周日的一半）。非竞争性的互换通常是更好的选择。 具有严格报警响应SLO的团队需要考虑通勤时间。如果你的报警响应SLO为5分组，而你的通勤时间为30，那么要确保其他人在你上班途中能处理紧急情况。 为了在灵活性方面实现这些目标，我们建议给予团队成员权利更新on-call轮值表。此外，有一个记录下来的策略描述转换如何操作。权利下放的策略包括只有经理可以改变的完全集中的政策，到任何成员都可以改变的完全分散的政策。根据我们的经验，对变更进行同行评审可以在安全性和灵活性之间进行良好的权衡。 长期休息的计划。由于个人情况或职业倦怠的变化，有时团队成员需要停止on-call工作。团队的结构应该能够允许on-call人员暂时不参与值班。 理想情况下，团队规模应该满足在（临时）员工减少时其他成员能够承受增加的运维负担。根据我们的经验，每个站点至少需要五个人进行多站点全天候的on-call，至少需要8个人进行单站点全天候的on-call。因此，假设每个站点需要一名额外的工程师来防止人员减少，每个站点（多站点）最多需要6名工程师，每个站点（单站点）为9名。 兼职工作的时间表计划。on-call工作的时间表看起来是不兼容的，但我们发现如果你采取某些预防措施，on-call工作和兼职工作是能够做到兼容的。以下讨论假设你的on-call成员是兼职工作，他们无法在兼职工作周之外完成值班工作。 兼职工作主要有两种模式： 每周工作减少一天——例如，每周工作4天，而非5天 每天减少工作时间——例如，每天工作6小时，而非8小时 两种模式都可以兼容on-call工作，但需要对on-call时间进行调整。 如果非工作日是一直不变的，那么第一个模式很容易和on-call工作兼容。对应的，你可以采用每周少于7天的on-call时间（例如，周一至周四，或周五至周日），并自动调整时间表以便在兼职工程师非工作时间不会参与on-call工作。 第二种模式可以通过以下几种方式实现： 与另一名工程师分担on-call时间，这样当兼职工程师不在时，仍然有人值班。例如，如果on-call工程师需要从上午9点工作到下午4点，你可以将值班的前半部分（上午9点到下午3点）分配给他们，后半部分（下午3点到晚上9点）可以以相同的方式分配给其他on-call成员。 如果on-call频率不是太高，兼职工程师可以在on-call日工作整整几个小时也是可行的。 如站点可靠性工程的第11章所述，根据当地劳动法和法规，Google SRE会在正常工作时间之外补偿小时工资或休假时间。在确定on-call补偿时，要考虑兼职工程师的时间表。 为了平衡项目时间和on-call时间，工作时间较少的工程师对应的工作内容应该少点。与小型团队相比，较大的团队更容易吸收额外的on-call负载。 on-call团队动态 我们的第一本书谈到了高报警负载和时间压力等压力因素是如何迫使on-call工程师采用基于直觉的未经详细考虑而非基于理性和数据的决策策略（参加该书第11章“安全感”一节）。基于团队心理学的讨论，你如何建立一个积极的动态团队？考虑一个on-call团队，其中包含以下一组假设问题。 情景：“一周生存”的文化 一家公司从几位创始人和少数员工开始，他们都是开发人员，每个人都相互了解，每个人都需要接收报警。 公司规模开始变大。on-call的职责仅限于一小部分更有经验的功能开发人员，因为他们更了解系统。 公司变得更大。他们增加了ops角色来解决可靠性问题。该团队负责生产环境监控，成员主要集中在运维，而非编码。功能开发人员和ops人员轮流进行on-call工作。功能开发人员在维护服务方面有最终决定权，而ops仅限于运维任务。到目前为止，有30名工程师参与on-call工作：25名功能开发人员和5名ops，都位于同一站点。 团队被高报警量所困扰，尽管遵循了本章前面所述的建议，尽量减少报警负载，但团队的士气仍然很低落。由于功能开发人员优先考虑开发新功能，因此on-call的后续工作需要很长时间才能实现。 更糟糕的是，由于功能开发人员关注的是自己子系统的健康状况，尽管团队中其他人提出了投诉，但有位功能开发人员坚持按错误率而非关键模块错误比率来进行报警。这些报警很嘈杂，会有很多误报或者不可执行的报警。 高报警负载对on-call岗的其他成员的影响不会特别大，确实有许多报警，但大多数报警都没有花太多时间来解决。正如一名on-call工程师所说：“我快速浏览一下报警主题，知道它们是重复的。所以我要做的就是忽略它们。” 听起来很熟悉？ Google的一些团队在成熟的早期阶段遇到过类似问题。如果不小心处理，这些问题可能扰乱功能开发团队和运维团队，并阻碍on-call的操作。没有灵丹妙药能解决这些问题，但我们发现了一些特别有用的方法。虽然你的方法可能有所不同，但总体目标应该是相同的：建立积极的团队氛围，避免混乱。 建议一：给你的ops工程师授权。你可以根据本书和站点可靠性工程中列出的指南对运维组织进行重新构建，甚至可以更改名称（SRE或类似名称）来表示角色的更改。重新命名你的运维组织并非灵丹妙药，但它有助于体现别于旧的以操作为中心的模型的新的责任变化。向团队和整个公司明确说明SRE拥有站点操作权限，包括定义可靠性的共享路线图，推动问题的全面解决，维护监控策略等。功能开发人员是必要的协作者，但没有这些权限。 回到我们之前假设的团队，本公告引入了以下运维变化： 操作项仅分配给5个DevOps工程师——即SRE。SRE与项目专家合作——大多为开发人员——来完成这些任务。SRE就前面提到的：“错误率与错误比例”的报警策略与功能开发人员进行协商。 如果可能，鼓励SRE深入研究代码以自行进行更改。他们将代码审查发送给项目专家。这样有利于在SRE之间建立主人翁意识，并在未来的场合提升他们的技能和权威。 通过这种安排，功能开发人员是可靠性功能的明确协作者，且SRE有权利拥有站点以及改进站点的责任。 建议二：改善团队关系。另一种可能的解决方案是建立更强有力的团队关系。Google设置了一个“有趣的预算”，专门用于组织异地活动来加强团队合作。 我们发现，强大的团队关系可以增强团队成员之间的理解和协作精神。因此，工程师修复bug，完成操作项目并且帮助同事的几率更高。例如，假设你关闭了夜间管道工作，但忘记关闭检查管道是否成功运行的监控。结果，同事在凌晨3点收到了报警。如果你和那位同事为处理报警花了点时间，你对这件事感到很抱歉，并在将来对此类操作更加小心。“我要保护我的同事”这一心态会转化成为更富有成效的工作氛围。 我们还发现，无论职称和职能如何，让on-call的所有成员坐在一起，有助于改善团队关系。还可以鼓励团队一起吃午饭，不要低估这些相对简单的变化，它会直接影响团队动力。 结论 SRE on-call与传统的ops角色不同。SRE不仅专注于日常运维，且拥有生产环境权限，并通过定义适当的可靠性阈值，开发自动化工具以及开展战略工程项目来获得更好的生产环境。on-call的站点操作十分重要，公司必须正确处理。 on-call是个人和集体压力的根源。但如果你盯着怪物的眼睛看久了，就会发现智慧。本章阐述了一些关于on-call的案例；希望我们的经验可以帮助他人避免或解决类似的问题。 如果你的on-call团队淹没在无休止的报警中，我们建议你退一步观察更顶层的情况，和其他SRE和合作伙伴团队对比讨论，一旦收集了必要的信息，就要系统的解决问题。对on-call工程师，on-call团队以及整个公司来说，构建合理的on-call机制是值得投入时间的。</summary></entry><entry><title type="html">第七章 简单化</title><link href="http://localhost:4000/sre/2020/01/07/%E7%AE%80%E5%8D%95%E5%8C%96/" rel="alternate" type="text/html" title="第七章 简单化" /><published>2020-01-07T00:00:00+08:00</published><updated>2020-01-07T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/07/%E7%AE%80%E5%8D%95%E5%8C%96</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/07/%E7%AE%80%E5%8D%95%E5%8C%96/">&lt;!-- more --&gt;

&lt;p&gt;简单化是SRE的重要目标，因为它与可靠性密切相关：简单的软件很少出现故障，在故障发生时更容易且迅速地修复。简单的系统更易于理解、维护以及测试。&lt;/p&gt;

&lt;p&gt;对于SRE而言，简单化是一个端到端的目标：它应该超越代码本身，延伸到系统架构以及用于管理软件生命周期的工具和流程中。本章探讨了一些样例，这些样例展示了SRE是如何衡量、思考和鼓励简单化的。&lt;/p&gt;

&lt;h2 id=&quot;衡量复杂度&quot;&gt;衡量复杂度&lt;/h2&gt;

&lt;p&gt;衡量软件系统的复杂度并不是一门绝对的科学。有许多方法可以衡量软件代码的复杂性，大多数是非常客观的。&lt;/p&gt;

&lt;p&gt;最著名且使用最广泛的衡量标准应该是代码圈复杂度，它通过一组特定的语句来衡量不同代码路径的数量。例如，没有循环或条件语句的代码块的圈复杂度（CCN）为1。其实软件社区很擅长测量代码复杂度，并且有许多用于集成开发环境的测量工具（包括Visual Studio，Eclipse和IntelliJ）。我们无法判断所得到的测量复杂度是必然还是偶然的，一种方法的复杂度是如何影响到系统的，以及哪种方法更适合重构。&lt;/p&gt;

&lt;p&gt;另一方面，衡量系统复杂性的正式的方法很少见。&lt;/p&gt;

&lt;p&gt;你可能尝试使用类似CCN的方法来计算不同实体（例如，微服务）的数量以及它们之间可能存在的通信路径。但是，对于大多数较大规模的系统而言，这个数字的增幅十分迅速。&lt;/p&gt;

&lt;p&gt;针对系统级复杂度，有一些更实用的替代度量方法：&lt;/p&gt;

&lt;h3 id=&quot;训练时长&quot;&gt;训练时长&lt;/h3&gt;

&lt;p&gt;新成员多久能参与on-call工作？糟糕的或缺失的文档可能是主观复杂性的重要来源。&lt;/p&gt;

&lt;h3 id=&quot;解释时长&quot;&gt;解释时长&lt;/h3&gt;

&lt;p&gt;向团队新成员解释服务的全面高级视图需要多久（例如，在白板上绘制系统架构图并解释各个组件的功能和依赖关系）？&lt;/p&gt;

&lt;h3 id=&quot;管理多样性&quot;&gt;管理多样性&lt;/h3&gt;

&lt;p&gt;有多少种方法可以在系统的不同部分配置类似的设置？配置是集中存储在一个位置还是存储在多个位置？&lt;/p&gt;

&lt;h3 id=&quot;部署配置的多样性&quot;&gt;部署配置的多样性&lt;/h3&gt;

&lt;p&gt;生产过程中部署了多少唯一的配置（包括二进制文件、二进制版本、标志和环境）？&lt;/p&gt;

&lt;h3 id=&quot;年龄&quot;&gt;年龄&lt;/h3&gt;

&lt;p&gt;系统使用多久了？Hyrum定律指出，随着时间的推移，API的用户依赖于它实现的每个方面，导致了脆弱和不可预测的行为。&lt;/p&gt;

&lt;p&gt;虽然测量复杂度有时是有价值的，但过程很困难。然而以下这些结论是没有争议的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一般而言，除非付出努力补偿，否则现存的软件系统的复杂度将会随时间增加。&lt;/li&gt;
  &lt;li&gt;付出这样的努力是值得的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;简单化是端到端的并且sre会获益于此&quot;&gt;简单化是端到端的，并且SRE会获益于此&lt;/h2&gt;

&lt;p&gt;通常，生产系统不是通过整体的方式设计的；相反，它们是有机地生长。随着团队添加新特性和推出新产品，它们会逐渐积累组件和连接。虽然单个变更可能相对简单，但每个变更都会影响周围的组件。因此，整体复杂度很快就会超出控制。例如，在一个组件中添加重试可能会使数据库过载并使整个系统不稳定，或者使对给定查询在系统中遵循的路径进行推理变得更加困难。&lt;/p&gt;

&lt;p&gt;一般而言，复杂度带来的成本并不直接影响引入它的个人、团队或从经济角度来看的任何角色，复杂度是一种外部特性。相反，复杂性会影响那些继续在其中和周围工作的人。因此，有个拥护端到端系统简单化的支持者十分重要。&lt;/p&gt;

&lt;p&gt;SRE非常适合这个角色，因为他们的工作需要他们将系统作为一个整体来对待。除了维护自己的服务，SRE还必须深入了解与服务有交互的系统。Google的产品开发团队通常无法查看生产范围内的问题，因此他们可以通过咨询SRE来获取系统设计和运营的相关建议。&lt;/p&gt;

&lt;p&gt;（一般说明）读者操作：在工程师第一次加入on-call工作之前，鼓励他们绘制（或重绘）系统架构图。可以在你的文档中保留一组规范的图表：不仅对新加入的工程师非常有帮助，还可以帮助更多有经验的工程师随时跟上系统的变更。&lt;/p&gt;

&lt;p&gt;根据我们的经验，通常产品开发人员的工作局限在子系统或组件中。因此，他们没有形成针对整个系统的思维模式，所在的团队也没有制作系统级别的架构图。系统架构图的价值在于可以将系统交互可视化地呈现给成员，并且帮助成员使用常用词汇来阐明问题。通常，SRE团队都绘制了所有服务的系统级架构图。&lt;/p&gt;

&lt;p&gt;（一般说明）读者操作：SRE要检查所有重要的设计文档，且团队文档中需要说明新设计会如何影响系统结构。如果一个设计会增加系统复杂度，SRE可能会建议选择降低系统复杂度的替代方案。&lt;/p&gt;

&lt;h3 id=&quot;案例学习1端到端api简单化&quot;&gt;案例学习1：端到端API简单化&lt;/h3&gt;

&lt;h4 id=&quot;背景&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;之前章节的一位作者在一家使用键/值包数据结构的核心库的初创公司工作。RPCs（远程过程调用）取一个包并返回一个包；实际参数作为键/值对存储在包中。核心库支持包的常见操作，比如序列化、加密和日志记录。看起来所有的核心库和API都非常简单灵活，对吧？&lt;/p&gt;

&lt;p&gt;遗憾的是，答案是否定的：核心库的客户最终为核心API的抽象化付出了代价。每个服务都需要仔细记录键和值（和值类型）的集合，但通常做法并非如此。此外，随着时间的推移、参数的添加、删除或更改，维护向后/向前的兼容性变得很困难。&lt;/p&gt;

&lt;h4 id=&quot;经验教训&quot;&gt;经验教训&lt;/h4&gt;

&lt;p&gt;类似Google Protocol Buffers或Apache Thrift这样的结构化数据类型看起来可能比它们抽象的通用替代方案更复杂。但是由于它们强制预先设计方案和准备文档，获得了更简单的端到端解决方案。&lt;/p&gt;

&lt;h3 id=&quot;案例学习2项目生命周期复杂度&quot;&gt;案例学习2：项目生命周期复杂度&lt;/h3&gt;

&lt;p&gt;当您查看现有系统，发现它像一团乱麻，您可能希望用一个新的、干净的、简单的系统取而代之，且这个简单的系统能解决相同的问题。不幸的是，在保持现有系统的同时创建新系统的成本可能超乎您的预期。&lt;/p&gt;

&lt;h4 id=&quot;背景-1&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;Borg是Google的内部容器管理系统。运行了大量Linux容器且具有多种使用模式：批处理与生产，管道与服务器等。多年来，随着硬件的变化，功能的增加以及规模的不断扩大，Borg及其周边生态系统在不断的发展壮大。&lt;/p&gt;

&lt;p&gt;Omega旨在成为一个更合理，更清爽的Borg版本，且能支持相同的功能。然而，从Borg到Omega的转变过程产生了一些严重的问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Omega发展的同时，Borg的发展也没有停滞，因此Omega一直在追逐一个变化的目标。&lt;/li&gt;
  &lt;li&gt;事实证明，前期对改善Borg难度的估计太过悲观，而对Omega的期望太过乐观（实际上，外国的月亮未必更圆）。&lt;/li&gt;
  &lt;li&gt;我们对从Borg迁移到Omega的困难没有了然于胸。数百万行配置代码跨越数千个服务和多个SRE团队，这意味着迁移工作在工程和时间维度上成本都是极高的。可能需要数年时间完成迁移，在这期间，我们必须同时支持和维护这两个系统。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;我们决定做什么&quot;&gt;我们决定做什么&lt;/h4&gt;

&lt;p&gt;最后，我们提供了一些在设计Omega回归Borg时出现的想法。我们还使用了很多Omega的概念来启动Kubernetes，一个开源的容器管理系统。&lt;/p&gt;

&lt;h4 id=&quot;经验教训-1&quot;&gt;经验教训&lt;/h4&gt;

&lt;p&gt;在考虑重写时，要考虑整个项目生命周期，包括对移动的目标的开发，完整的迁移计划以及在迁移时间窗口内可能产生的额外成本。具有大量用户的APIs很难迁移。在您投入了相应的努力之前，不要想当然的将预期结果与当前系统进行比较。在确保已经衡量了成本和收益以及没有低估成本的前提下，有时重写是最好的前进方式。&lt;/p&gt;

&lt;h2 id=&quot;重获简单化&quot;&gt;重获简单化&lt;/h2&gt;

&lt;p&gt;大多数的简化工作是从系统中删除元素。简化工作有时很直接（例如，消除对从远程系统获取的未使用数据的依赖）。简化工作有时需要重新设计。例如，系统的两个部分需要访问相同的远程数据。一个更简单的系统可能只需要获取一次数据并转发结果而非获取两次。&lt;/p&gt;

&lt;p&gt;无论什么工作，领导层必须确保优先考虑简化工作。这里的简化指的是效率-而不是节省计算或网络资源，它节省了工程时间和认知负荷。项目成功的简化就如同成功启用了一个有价值的功能，就如同成功的度量并对代码进行了增删。例如，Google的内部网络会为删除大量代码的工程师显示“Zombie Code Slayer”徽章。&lt;/p&gt;

&lt;p&gt;简化是一项功能。您需要明确优先级并给出待简化的项目，同时为SRE预留时间。如果产品开发和SRE人员发现待简化项目对他们的工作没有益处，他们就不会承担这些项目。对于特别复杂的系统或过载的团队而言，可以将简单化作为明确的目标，安排一个独立的时间来完成这项工作。例如，为“简单化”项目保留10%的工程项目时间。&lt;/p&gt;

&lt;p&gt;（一般说明）读者行动：让工程师集体讨论系统中已知的复杂度，并讨论如何简化。&lt;/p&gt;

&lt;p&gt;随着系统复杂度的增加，SRE团队存在分裂的趋势，每个新的团队分别集中运维系统的某一部分。这样的操作有时是必要的，但新团队规模的缩小可能会降低他们推动较大简化项目的动力或能力。可以考虑指定一个小的轮转的SRE团队来维护整个堆栈的工作信息（可能比较浅显），推动整个堆栈的整合和简化。&lt;/p&gt;

&lt;p&gt;如前所述，绘制系统图表的行为可以帮助您理解系统并预测其行为。例如，在绘制系统图表的过程中，你可能需要查找以下内容：&lt;/p&gt;

&lt;h4 id=&quot;放大&quot;&gt;放大&lt;/h4&gt;

&lt;p&gt;当一个调用操作返回一个错误或超时，且在几个级别上进行重试时，会导致RPC的总数相乘。&lt;/p&gt;

&lt;h4 id=&quot;循环依赖&quot;&gt;循环依赖&lt;/h4&gt;

&lt;p&gt;当组件依赖于自身（通常是间接的）时，系统完整性可能会严重受损-整个系统可能无法进行冷启动。&lt;/p&gt;

&lt;h3 id=&quot;案例学习3简化广告网络的展示&quot;&gt;案例学习3：简化广告网络的展示&lt;/h3&gt;

&lt;h4 id=&quot;背景-2&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;Google的广告展示业务有许多关联产品，其中包括一些收购于DoubleClick，AdMob，Invite Media等公司的产品。这些产品必须适用于Google基础架构和现有产品。例如，我们希望使用DFP广告管理系统的网站展示Google AdSense筛选的广告，也希望使用Double Click Bid Manager进行投标时可以通过访问Google Ad Exchange进行实时竞价。&lt;/p&gt;

&lt;p&gt;独立开发的产品形成了难以推理的互连后端系统，很难观察流量在各组件的流通情况，因此不便且无法精确的为每个产品配置合适的容量。为了确保删除了查询流量中的所有无限循环，我们在其中添加了测试。&lt;/p&gt;

&lt;h4 id=&quot;我们决定做什么-1&quot;&gt;我们决定做什么&lt;/h4&gt;

&lt;p&gt;Ads的运维团队自然而然会推动标准化：虽然产品的每个组件都有特定的开发团队，但SRE是服务于整个系统的。我们的首要任务是制订统一的标准，与开发团队合作逐步采用这个标准。这些标准是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;建立一种复制大规模数据集的方法&lt;/li&gt;
  &lt;li&gt;建立一种执行外部数据查找的方法&lt;/li&gt;
  &lt;li&gt;提供用于监控、配置、组态的通用模板&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在此之前，需要为每个产品单独提供前端和拍卖功能。如图7-1所示，当广告请求可能到达两个系统时，需要重写请求以符合第二个系统的要求。过程中，增加了额外的代码和处理，还加大了非预期循环的可能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/7-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图7-1：之前，广告请求可能会同时触及AdMob和AdSense系统 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;为了简化系统，我们为满足所有用例的常用程序增加了逻辑，并且添加了用于保护程序的标志。随着时间的推移，我们删除了标志，将功能整合到较少的的服务器后端中。&lt;/p&gt;

&lt;p&gt;当服务器统一时，拍卖服务器可直接与两个目标服务器通信。如图7-2所示，当多个目标服务器需要查找数据时，查询只需统一在拍卖服务器中进行一次。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/7-2.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图7-2：统一后的拍卖服务器只需执行一次数据查询 &lt;/center&gt;&lt;/p&gt;

&lt;h4 id=&quot;经验教训-2&quot;&gt;经验教训&lt;/h4&gt;

&lt;p&gt;最好将已经在运行的系统逐步集成到你的基础架构中。&lt;/p&gt;

&lt;p&gt;正如在单个程序中存在相似的函数表示“代码气味”来反应更深层次的设计问题一样，单个请求的冗余查询表示“系统气味”。&lt;/p&gt;

&lt;p&gt;当你通过SRE和开发人员的支持建立了有明确定义的标准时，你可以提供更清晰的蓝图以便管理者对高复杂度的系统更认可和鼓励。&lt;/p&gt;

&lt;h3 id=&quot;案例学习4在共享平台上运行数百个微服务&quot;&gt;案例学习4：在共享平台上运行数百个微服务&lt;/h3&gt;

&lt;h4 id=&quot;背景-3&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;在过去15年，Google成功开发了多个垂直类产品（搜索、广告和Gmail，仅举几例），并源源不断的产生了新的重构的系统。其中很多系统都有专门的SRE团队和与之对应的特定领域生产堆栈，包括定制化开发工作流程，持续集成和持续交付（CI/CD）软件周期以及监控。生产堆栈的定制化带来了巨大的维护、开发以及新的SRE成员工作的成本。此外也为团队之间轮转服务（或工程师！）以及新增服务带来困难。&lt;/p&gt;

&lt;h4 id=&quot;我们决定做什么-2&quot;&gt;我们决定做什么&lt;/h4&gt;

&lt;p&gt;负责社交网络领域的一组SRE团队致力于将其服务的生产堆栈融合到一个托管的微服务平台中，由一个SRE团队管理。共享平台是到目前为止的最佳实践，平台会绑定并自动配置一些之前并未充分利用的功能，这些功能可以提高可靠性且便于调试。无论该SRE团队有多熟悉所负责的服务，新增的服务都必须使用通用平台，而旧式服务必须迁移到新平台或逐步被淘汰。&lt;/p&gt;

&lt;p&gt;共享平台在社交网络领域取得成功后，谷歌的其他SRE团队和非SRE团队也开始使用它。&lt;/p&gt;

&lt;h4 id=&quot;设计&quot;&gt;设计&lt;/h4&gt;

&lt;p&gt;要知道，单个整体服务的变化是缓慢的，使用微服务可以迅速更新和部署功能。微服务实现自我管理，而非托管：团队可以有效的管理他们所负责的服务，无需委托个别团队管理和负责。微服务为每个团队提供工作流程工具用于发布、监控等功能。&lt;/p&gt;

&lt;p&gt;微服务提供的工具包括UI，API和SRE以及开发人员常用的命令行交互界面。即使这些工具可能涉及许多底层系统，开发人员的体验感却是统一的。&lt;/p&gt;

&lt;h4 id=&quot;成果&quot;&gt;成果&lt;/h4&gt;

&lt;p&gt;微服务平台的高质量和功能集成带来了意想不到的好处：开发人员团队可以运行数百项服务，而无需任何SRE的深入参与。&lt;/p&gt;

&lt;p&gt;通用平台还改变了SRE和开发人员的关系。Google的SRE团队开始分层的参与到工作中，从咨询和设计审查到深度参与（即SRE承担on-call职责）。&lt;/p&gt;

&lt;h4 id=&quot;经验教训-3&quot;&gt;经验教训&lt;/h4&gt;

&lt;p&gt;从稀疏的或不明确的标准转变为高度标准化的平台是一个长期项目。每个步骤可能都让人觉得是增量式的，但最终，这些步骤可以减少开销并使大规模运行服务成为可能。&lt;/p&gt;

&lt;p&gt;这种转变可以让开发人员看到价值所在。即不要尝试说服人们执行一个只在全部完成后才得到回报的巨大重构工程，而是在每个开发阶段解锁增量生产力。&lt;/p&gt;

&lt;h3 id=&quot;案例研究5pdns不再取决于自身&quot;&gt;案例研究5：pDNS不再取决于自身&lt;/h3&gt;

&lt;h4 id=&quot;背景-4&quot;&gt;背景&lt;/h4&gt;

&lt;p&gt;当Google生产的客户要查询服务的IP地址时，通常使用名为Svelte的查找服务。过去，为了找到Svelte的IP地址，客户端使用了名为pDNS（生产DNS）的Google命名服务。通过负载均衡访问pDNS服务，负载均衡使用Svelte查找实际pDNS服务器的IP地址。&lt;/p&gt;

&lt;h4 id=&quot;问题描述&quot;&gt;问题描述&lt;/h4&gt;

&lt;p&gt;pDNS对自身具有传递依赖性，某种程度上说这是无意中引入的，后来被明确为是可靠性问题。由于pDNS服务可复制，且在生产中的始终可以获得打破依赖关系循环所需的数据，因此查找通常不会遇到问题。然而，冷启动是无法做到的。借用一位SRE的话来说，“我们就像穴居人，只能依赖现有篝火来点火。”&lt;/p&gt;

&lt;h4 id=&quot;我们决定做什么-3&quot;&gt;我们决定做什么&lt;/h4&gt;

&lt;p&gt;我们修改了Google生产中的低级组件以便为所有Google生产机器的本地存储附近的Svelte服务器维护当前IP地址列表。除了打破前文所述的循环依赖之外，此举还消除了对大多数其他Google服务的pDNS的隐式依赖。&lt;/p&gt;

&lt;p&gt;为了避免此类问题，我们还引入了一种方法，将允许与pDNS通信的服务集列入白名单，并慢慢减少该集合。因此，生产中每个服务的查找都通过系统且具有更简单更可靠的路径。&lt;/p&gt;

&lt;h4 id=&quot;经验教训-4&quot;&gt;经验教训&lt;/h4&gt;

&lt;p&gt;注意服务的依赖关系 - 使用明确的白名单以防止意外添加。另外，需要注意循环依赖。&lt;/p&gt;

&lt;h3 id=&quot;结论&quot;&gt;结论&lt;/h3&gt;

&lt;p&gt;通常简单的系统往往是可靠的且易于运行的，因此简单化自然而然就是SRE的目标。很难定量衡量分布式系统的简单性（或取逆，即复杂度），但可以挑选和改进合理的替代测量方案。&lt;/p&gt;

&lt;p&gt;SRE对系统有着端到端的理解，在识别，预防和修复复杂度来源方具有优势，在软件设计，系统架构，配置，部署过程或是其他地方，SRE都应该参与设计讨论，提供对成本和效益的独特见解，尤其是简单化。SRE还可以主动制订标准来使生产统一化。&lt;/p&gt;

&lt;p&gt;作为SRE，追求简单化应该是工作的重点内容。我们强烈建议SRE领导层授予SRE团队权利和奖励来推动简单化。系统在不断发展的过程中会不可避免的越来越复杂，因此追求简单化的斗争道路需要持久的关注和付出-但这份追求是值得的。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">简单化是SRE的重要目标，因为它与可靠性密切相关：简单的软件很少出现故障，在故障发生时更容易且迅速地修复。简单的系统更易于理解、维护以及测试。 对于SRE而言，简单化是一个端到端的目标：它应该超越代码本身，延伸到系统架构以及用于管理软件生命周期的工具和流程中。本章探讨了一些样例，这些样例展示了SRE是如何衡量、思考和鼓励简单化的。 衡量复杂度 衡量软件系统的复杂度并不是一门绝对的科学。有许多方法可以衡量软件代码的复杂性，大多数是非常客观的。 最著名且使用最广泛的衡量标准应该是代码圈复杂度，它通过一组特定的语句来衡量不同代码路径的数量。例如，没有循环或条件语句的代码块的圈复杂度（CCN）为1。其实软件社区很擅长测量代码复杂度，并且有许多用于集成开发环境的测量工具（包括Visual Studio，Eclipse和IntelliJ）。我们无法判断所得到的测量复杂度是必然还是偶然的，一种方法的复杂度是如何影响到系统的，以及哪种方法更适合重构。 另一方面，衡量系统复杂性的正式的方法很少见。 你可能尝试使用类似CCN的方法来计算不同实体（例如，微服务）的数量以及它们之间可能存在的通信路径。但是，对于大多数较大规模的系统而言，这个数字的增幅十分迅速。 针对系统级复杂度，有一些更实用的替代度量方法： 训练时长 新成员多久能参与on-call工作？糟糕的或缺失的文档可能是主观复杂性的重要来源。 解释时长 向团队新成员解释服务的全面高级视图需要多久（例如，在白板上绘制系统架构图并解释各个组件的功能和依赖关系）？ 管理多样性 有多少种方法可以在系统的不同部分配置类似的设置？配置是集中存储在一个位置还是存储在多个位置？ 部署配置的多样性 生产过程中部署了多少唯一的配置（包括二进制文件、二进制版本、标志和环境）？ 年龄 系统使用多久了？Hyrum定律指出，随着时间的推移，API的用户依赖于它实现的每个方面，导致了脆弱和不可预测的行为。 虽然测量复杂度有时是有价值的，但过程很困难。然而以下这些结论是没有争议的： 一般而言，除非付出努力补偿，否则现存的软件系统的复杂度将会随时间增加。 付出这样的努力是值得的。 简单化是端到端的，并且SRE会获益于此 通常，生产系统不是通过整体的方式设计的；相反，它们是有机地生长。随着团队添加新特性和推出新产品，它们会逐渐积累组件和连接。虽然单个变更可能相对简单，但每个变更都会影响周围的组件。因此，整体复杂度很快就会超出控制。例如，在一个组件中添加重试可能会使数据库过载并使整个系统不稳定，或者使对给定查询在系统中遵循的路径进行推理变得更加困难。 一般而言，复杂度带来的成本并不直接影响引入它的个人、团队或从经济角度来看的任何角色，复杂度是一种外部特性。相反，复杂性会影响那些继续在其中和周围工作的人。因此，有个拥护端到端系统简单化的支持者十分重要。 SRE非常适合这个角色，因为他们的工作需要他们将系统作为一个整体来对待。除了维护自己的服务，SRE还必须深入了解与服务有交互的系统。Google的产品开发团队通常无法查看生产范围内的问题，因此他们可以通过咨询SRE来获取系统设计和运营的相关建议。 （一般说明）读者操作：在工程师第一次加入on-call工作之前，鼓励他们绘制（或重绘）系统架构图。可以在你的文档中保留一组规范的图表：不仅对新加入的工程师非常有帮助，还可以帮助更多有经验的工程师随时跟上系统的变更。 根据我们的经验，通常产品开发人员的工作局限在子系统或组件中。因此，他们没有形成针对整个系统的思维模式，所在的团队也没有制作系统级别的架构图。系统架构图的价值在于可以将系统交互可视化地呈现给成员，并且帮助成员使用常用词汇来阐明问题。通常，SRE团队都绘制了所有服务的系统级架构图。 （一般说明）读者操作：SRE要检查所有重要的设计文档，且团队文档中需要说明新设计会如何影响系统结构。如果一个设计会增加系统复杂度，SRE可能会建议选择降低系统复杂度的替代方案。 案例学习1：端到端API简单化 背景 之前章节的一位作者在一家使用键/值包数据结构的核心库的初创公司工作。RPCs（远程过程调用）取一个包并返回一个包；实际参数作为键/值对存储在包中。核心库支持包的常见操作，比如序列化、加密和日志记录。看起来所有的核心库和API都非常简单灵活，对吧？ 遗憾的是，答案是否定的：核心库的客户最终为核心API的抽象化付出了代价。每个服务都需要仔细记录键和值（和值类型）的集合，但通常做法并非如此。此外，随着时间的推移、参数的添加、删除或更改，维护向后/向前的兼容性变得很困难。 经验教训 类似Google Protocol Buffers或Apache Thrift这样的结构化数据类型看起来可能比它们抽象的通用替代方案更复杂。但是由于它们强制预先设计方案和准备文档，获得了更简单的端到端解决方案。 案例学习2：项目生命周期复杂度 当您查看现有系统，发现它像一团乱麻，您可能希望用一个新的、干净的、简单的系统取而代之，且这个简单的系统能解决相同的问题。不幸的是，在保持现有系统的同时创建新系统的成本可能超乎您的预期。 背景 Borg是Google的内部容器管理系统。运行了大量Linux容器且具有多种使用模式：批处理与生产，管道与服务器等。多年来，随着硬件的变化，功能的增加以及规模的不断扩大，Borg及其周边生态系统在不断的发展壮大。 Omega旨在成为一个更合理，更清爽的Borg版本，且能支持相同的功能。然而，从Borg到Omega的转变过程产生了一些严重的问题： Omega发展的同时，Borg的发展也没有停滞，因此Omega一直在追逐一个变化的目标。 事实证明，前期对改善Borg难度的估计太过悲观，而对Omega的期望太过乐观（实际上，外国的月亮未必更圆）。 我们对从Borg迁移到Omega的困难没有了然于胸。数百万行配置代码跨越数千个服务和多个SRE团队，这意味着迁移工作在工程和时间维度上成本都是极高的。可能需要数年时间完成迁移，在这期间，我们必须同时支持和维护这两个系统。 我们决定做什么 最后，我们提供了一些在设计Omega回归Borg时出现的想法。我们还使用了很多Omega的概念来启动Kubernetes，一个开源的容器管理系统。 经验教训 在考虑重写时，要考虑整个项目生命周期，包括对移动的目标的开发，完整的迁移计划以及在迁移时间窗口内可能产生的额外成本。具有大量用户的APIs很难迁移。在您投入了相应的努力之前，不要想当然的将预期结果与当前系统进行比较。在确保已经衡量了成本和收益以及没有低估成本的前提下，有时重写是最好的前进方式。 重获简单化 大多数的简化工作是从系统中删除元素。简化工作有时很直接（例如，消除对从远程系统获取的未使用数据的依赖）。简化工作有时需要重新设计。例如，系统的两个部分需要访问相同的远程数据。一个更简单的系统可能只需要获取一次数据并转发结果而非获取两次。 无论什么工作，领导层必须确保优先考虑简化工作。这里的简化指的是效率-而不是节省计算或网络资源，它节省了工程时间和认知负荷。项目成功的简化就如同成功启用了一个有价值的功能，就如同成功的度量并对代码进行了增删。例如，Google的内部网络会为删除大量代码的工程师显示“Zombie Code Slayer”徽章。 简化是一项功能。您需要明确优先级并给出待简化的项目，同时为SRE预留时间。如果产品开发和SRE人员发现待简化项目对他们的工作没有益处，他们就不会承担这些项目。对于特别复杂的系统或过载的团队而言，可以将简单化作为明确的目标，安排一个独立的时间来完成这项工作。例如，为“简单化”项目保留10%的工程项目时间。 （一般说明）读者行动：让工程师集体讨论系统中已知的复杂度，并讨论如何简化。 随着系统复杂度的增加，SRE团队存在分裂的趋势，每个新的团队分别集中运维系统的某一部分。这样的操作有时是必要的，但新团队规模的缩小可能会降低他们推动较大简化项目的动力或能力。可以考虑指定一个小的轮转的SRE团队来维护整个堆栈的工作信息（可能比较浅显），推动整个堆栈的整合和简化。 如前所述，绘制系统图表的行为可以帮助您理解系统并预测其行为。例如，在绘制系统图表的过程中，你可能需要查找以下内容： 放大 当一个调用操作返回一个错误或超时，且在几个级别上进行重试时，会导致RPC的总数相乘。 循环依赖 当组件依赖于自身（通常是间接的）时，系统完整性可能会严重受损-整个系统可能无法进行冷启动。 案例学习3：简化广告网络的展示 背景 Google的广告展示业务有许多关联产品，其中包括一些收购于DoubleClick，AdMob，Invite Media等公司的产品。这些产品必须适用于Google基础架构和现有产品。例如，我们希望使用DFP广告管理系统的网站展示Google AdSense筛选的广告，也希望使用Double Click Bid Manager进行投标时可以通过访问Google Ad Exchange进行实时竞价。 独立开发的产品形成了难以推理的互连后端系统，很难观察流量在各组件的流通情况，因此不便且无法精确的为每个产品配置合适的容量。为了确保删除了查询流量中的所有无限循环，我们在其中添加了测试。 我们决定做什么 Ads的运维团队自然而然会推动标准化：虽然产品的每个组件都有特定的开发团队，但SRE是服务于整个系统的。我们的首要任务是制订统一的标准，与开发团队合作逐步采用这个标准。这些标准是： 建立一种复制大规模数据集的方法 建立一种执行外部数据查找的方法 提供用于监控、配置、组态的通用模板 在此之前，需要为每个产品单独提供前端和拍卖功能。如图7-1所示，当广告请求可能到达两个系统时，需要重写请求以符合第二个系统的要求。过程中，增加了额外的代码和处理，还加大了非预期循环的可能。 图7-1：之前，广告请求可能会同时触及AdMob和AdSense系统 为了简化系统，我们为满足所有用例的常用程序增加了逻辑，并且添加了用于保护程序的标志。随着时间的推移，我们删除了标志，将功能整合到较少的的服务器后端中。 当服务器统一时，拍卖服务器可直接与两个目标服务器通信。如图7-2所示，当多个目标服务器需要查找数据时，查询只需统一在拍卖服务器中进行一次。 图7-2：统一后的拍卖服务器只需执行一次数据查询 经验教训 最好将已经在运行的系统逐步集成到你的基础架构中。 正如在单个程序中存在相似的函数表示“代码气味”来反应更深层次的设计问题一样，单个请求的冗余查询表示“系统气味”。 当你通过SRE和开发人员的支持建立了有明确定义的标准时，你可以提供更清晰的蓝图以便管理者对高复杂度的系统更认可和鼓励。 案例学习4：在共享平台上运行数百个微服务 背景 在过去15年，Google成功开发了多个垂直类产品（搜索、广告和Gmail，仅举几例），并源源不断的产生了新的重构的系统。其中很多系统都有专门的SRE团队和与之对应的特定领域生产堆栈，包括定制化开发工作流程，持续集成和持续交付（CI/CD）软件周期以及监控。生产堆栈的定制化带来了巨大的维护、开发以及新的SRE成员工作的成本。此外也为团队之间轮转服务（或工程师！）以及新增服务带来困难。 我们决定做什么 负责社交网络领域的一组SRE团队致力于将其服务的生产堆栈融合到一个托管的微服务平台中，由一个SRE团队管理。共享平台是到目前为止的最佳实践，平台会绑定并自动配置一些之前并未充分利用的功能，这些功能可以提高可靠性且便于调试。无论该SRE团队有多熟悉所负责的服务，新增的服务都必须使用通用平台，而旧式服务必须迁移到新平台或逐步被淘汰。 共享平台在社交网络领域取得成功后，谷歌的其他SRE团队和非SRE团队也开始使用它。 设计 要知道，单个整体服务的变化是缓慢的，使用微服务可以迅速更新和部署功能。微服务实现自我管理，而非托管：团队可以有效的管理他们所负责的服务，无需委托个别团队管理和负责。微服务为每个团队提供工作流程工具用于发布、监控等功能。 微服务提供的工具包括UI，API和SRE以及开发人员常用的命令行交互界面。即使这些工具可能涉及许多底层系统，开发人员的体验感却是统一的。 成果 微服务平台的高质量和功能集成带来了意想不到的好处：开发人员团队可以运行数百项服务，而无需任何SRE的深入参与。 通用平台还改变了SRE和开发人员的关系。Google的SRE团队开始分层的参与到工作中，从咨询和设计审查到深度参与（即SRE承担on-call职责）。 经验教训 从稀疏的或不明确的标准转变为高度标准化的平台是一个长期项目。每个步骤可能都让人觉得是增量式的，但最终，这些步骤可以减少开销并使大规模运行服务成为可能。 这种转变可以让开发人员看到价值所在。即不要尝试说服人们执行一个只在全部完成后才得到回报的巨大重构工程，而是在每个开发阶段解锁增量生产力。 案例研究5：pDNS不再取决于自身 背景 当Google生产的客户要查询服务的IP地址时，通常使用名为Svelte的查找服务。过去，为了找到Svelte的IP地址，客户端使用了名为pDNS（生产DNS）的Google命名服务。通过负载均衡访问pDNS服务，负载均衡使用Svelte查找实际pDNS服务器的IP地址。 问题描述 pDNS对自身具有传递依赖性，某种程度上说这是无意中引入的，后来被明确为是可靠性问题。由于pDNS服务可复制，且在生产中的始终可以获得打破依赖关系循环所需的数据，因此查找通常不会遇到问题。然而，冷启动是无法做到的。借用一位SRE的话来说，“我们就像穴居人，只能依赖现有篝火来点火。” 我们决定做什么 我们修改了Google生产中的低级组件以便为所有Google生产机器的本地存储附近的Svelte服务器维护当前IP地址列表。除了打破前文所述的循环依赖之外，此举还消除了对大多数其他Google服务的pDNS的隐式依赖。 为了避免此类问题，我们还引入了一种方法，将允许与pDNS通信的服务集列入白名单，并慢慢减少该集合。因此，生产中每个服务的查找都通过系统且具有更简单更可靠的路径。 经验教训 注意服务的依赖关系 - 使用明确的白名单以防止意外添加。另外，需要注意循环依赖。 结论 通常简单的系统往往是可靠的且易于运行的，因此简单化自然而然就是SRE的目标。很难定量衡量分布式系统的简单性（或取逆，即复杂度），但可以挑选和改进合理的替代测量方案。 SRE对系统有着端到端的理解，在识别，预防和修复复杂度来源方具有优势，在软件设计，系统架构，配置，部署过程或是其他地方，SRE都应该参与设计讨论，提供对成本和效益的独特见解，尤其是简单化。SRE还可以主动制订标准来使生产统一化。 作为SRE，追求简单化应该是工作的重点内容。我们强烈建议SRE领导层授予SRE团队权利和奖励来推动简单化。系统在不断发展的过程中会不可避免的越来越复杂，因此追求简单化的斗争道路需要持久的关注和付出-但这份追求是值得的。</summary></entry><entry><title type="html">第六章 减少琐事</title><link href="http://localhost:4000/sre/2020/01/06/%E5%87%8F%E5%B0%91%E7%90%90%E4%BA%8B/" rel="alternate" type="text/html" title="第六章 减少琐事" /><published>2020-01-06T00:00:00+08:00</published><updated>2020-01-06T00:00:00+08:00</updated><id>http://localhost:4000/sre/2020/01/06/%E5%87%8F%E5%B0%91%E7%90%90%E4%BA%8B</id><content type="html" xml:base="http://localhost:4000/sre/2020/01/06/%E5%87%8F%E5%B0%91%E7%90%90%E4%BA%8B/">&lt;!-- more --&gt;
&lt;p&gt;Google SRE的大量时间用于系统优化，通过工程化的方法，与开发一起协同努力，追求卓越。哪怕是很少的性能收益也是值得的。但优化范围不仅局限于服务器资源，SRE的工作耗时也是优化的范畴。首先，SRE的工作不是琐事（关于琐事请参阅《SRE：Google运维解密》第5章内容）。本章我们将琐事定义为：与维护服务相关的，重复的、可预测的、持续的任务流。&lt;/p&gt;

&lt;p&gt;对于产品运维团队来说，琐事不可避免。运维不可避免地需要处理部署、升级、重启、告警等工作。如果没有系统的方法，这些工作很快将耗尽整个团队精力。Google将SRE团队日常操作的耗时占比限制在50%以内（包括琐事和非劳动密集型工作。这样做的原因，请参阅《SRE：Google运维解密》书中第5章内容）。虽然这个目标可能不适合所有团队，但花费在琐事上的时间上限仍然很重要，因为识别和量化琐事是团队时间优化的第一步。&lt;/p&gt;

&lt;h2 id=&quot;琐事的定义&quot;&gt;琐事的定义&lt;/h2&gt;
&lt;p&gt;琐事往往具有如下特征：这在我们的上一本书中有所阐述（《SRE：Google运维解密》译者注）。在这里，我们列举出琐事的特征，并给出了一个具体的例子加以解释：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;手动性&lt;/code&gt;: 当web服务器上的/tmp目录磁盘占用率达到95%时，工程师Anne登录到服务器，在文件系统中查找并删除了无用的日志文件。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;重复性&lt;/code&gt;: 写满/tmp目录的事情不太可能只发生一次，因此我们需要反复处理。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;可以被自动化&lt;/code&gt;: 假设修复文件的工作包括如下几个步骤：“X登录，执行此命令，检查输出，执行命令，并通过命令的输出来判断是否需要重启Y”。这些指令流本质上就是伪代码！在上面的例子中，解决方案实际上已经可以部分自动化了。如果不需要人来运行脚本，可以自动化的检测故障并修复是再好不过了。更进一步，我们可以提交一个补丁使软件不再因为文档损坏的问题而中断。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;非技术性&lt;/code&gt;: “磁盘写满”和“服务宕机”之类的告警会分散工程师的注意力，从而忽略高价值的事情，并可能掩盖其他更严重的告警。大量类似的告警造成的后果会波及到服务的健康状况。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;没有持续的价值&lt;/code&gt;: 完成一项任务会带来一种令人满意的成就感。但长远来看，这种重复的满足感不能给工程师带来持续的价值。比如，处理告警能够确保用户查询持续进行；确保HTTP请求状态码小于400，以便可以让应用提供持续的服务，这些固然很好。然而，今天解决的问题并不能防止将来不再出现类似的问题，所以这样做的回报只是短期的。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;与服务同步增长&lt;/code&gt;: 许多业务工作量的增长速度与基础设施规模的增长速度一样快(或许更快)。例如，你花费在修复硬件故障的时间会随着服务器集群规模的增加而增加。但请注意，相关的辅助任务(例如，软件/配置更改)不一定是这个趋势。
我们并不能将带来琐事的原因规范化和标准化，但是我们需要知道琐事的一些的特征。除上述特征外，还要考虑某项工作对团队士气的影响。人们是乐于完成一项觉得会有回报的任务？还是会处理无益的琐碎和无聊的任务？答案显而易见，琐事会慢慢地降低团队士气——时间往往花在琐事上而不是花在批判性思考或者是表达创造力上了；只有减少琐事，工程师才能更好地将时间用于思考和进行创造的领域。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;案例：人工处理琐事&lt;br /&gt;
作者：John Looney，Facebook资深 SRE&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;哪些工作内容是琐事，通常是模糊的。一个“创造性”的解决方案，可能使问题得到最优解决，因此，SRE团队应奖励那些分析根因并解决问题的人，而不是那些&lt;code class=&quot;highlighter-rouge&quot;&gt;掩盖&lt;/code&gt;问题的人。&lt;/p&gt;

  &lt;p&gt;我加入Google后的第一个任务（2005年4月）是追查一批机器死机原因并修复。如果确认是硬件原因，则转交给硬件技术人员维修。这个任务并没有看似那样的简单，因为我需在截止日期前处理超过20,000台机器。&lt;/p&gt;

  &lt;p&gt;第一台机器死机原因是：Google网络驱动补丁不断打印毫无意义的日志，导致文件系统的根目录写满，类似的一千台机器都是同样的问题。&lt;/p&gt;

  &lt;p&gt;我和同事沟通了解决这个问题的方案：编写一个脚本，ssh到所有异常机器，如果根目录已满，则清空/var/log中大文件日志，并重启syslog。我的同事对此方案不认可，他说最好找到根因并修复。如果&lt;code class=&quot;highlighter-rouge&quot;&gt;掩盖&lt;/code&gt;了问题，在后续一段时间内，可能会引起更多严重性问题。&lt;/p&gt;

  &lt;p&gt;理论上，每台机器每小时的成本约为1美元。我的想法是，成本是运维工作很重要的衡量指标，应该高优让机器提供服务，利用起来。但我没有考虑的是：如果只是解决了这个表象，就没有机会去追查根因。&lt;/p&gt;

  &lt;p&gt;在高级工程师指导下，我翻阅了内核源码，找到导致此问题的可疑代码，并且记录了bug，帮助内核团队完善了他们的测试用例。从成本来看，解决这个网络补丁问题，每花费一小时，Google将为此付出1,000美元。&lt;/p&gt;

  &lt;p&gt;那天晚上就发布了新的内核版本，第二天我就把它升级到所有受影响的机器，内核团队在第二周更新了他们的测试用例。这个问题的处理，我很满意，因为找到了根因并成功修复，而不是每天上班后清理日志。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;琐事的度量&quot;&gt;琐事的度量&lt;/h2&gt;

&lt;p&gt;运维工作是辛苦的。如果你做了一些工作减少了琐事，如何知道你的努力是成功的？许多SRE团队是结合经验和直觉来回答这个问题。经验和直觉会产生好的效果，但是我们还可以将方法上升到一个理论的维度。&lt;/p&gt;

&lt;p&gt;经验和直觉是因人而异、非客观的。根据场景的不同，琐事的定义也不同。比如，同一团队的不同成员会根据工作的投入产出比来判断一件事情是否可以定义为琐事。此外，为了减少琐事所做的工作可能会持续几个季度甚至几年的时间(本章的一些案例研究就证明了这一点)，在此期间团队的人员主要任务可能会发生改变。所以，为了保证减少琐事的工作能够长期进行，一般的，团队必须从几个确定的琐事中选择一个琐事来消灭它。我们应当将这件事上升为一个项目，并且需要建立起这个项目的长期的客观的度量机制以保证投入得到回报。&lt;/p&gt;

&lt;p&gt;在启动项目之前，重要的是分析成本与收益，并确认通过减少琐事所节省的时间(至少)与第一次开发和维护自动化解决方案所投入的时间成正比(图6-1)。从节省的时间与投入的时间的简单比较来看，那些看起来“无利可图”的项目可能仍然值得进行，因为自动化有许多间接或无形的好处。潜在的好处包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;随着业务规模扩大，收益越明显&lt;/li&gt;
  &lt;li&gt;提高团队士气，减少团队流失和成员的厌倦情绪&lt;/li&gt;
  &lt;li&gt;更少的中断性工作，从而提高团队工作效率&lt;/li&gt;
  &lt;li&gt;提高流程清晰度和标准化&lt;/li&gt;
  &lt;li&gt;增强团队成员的技术技能和拥有更全面的职业发展&lt;/li&gt;
  &lt;li&gt;缩短新成员的培训时间&lt;/li&gt;
  &lt;li&gt;减少人为错误导致的问题&lt;/li&gt;
  &lt;li&gt;提高安全性&lt;/li&gt;
  &lt;li&gt;缩短用户投诉的响应时间
&lt;img src=&quot;/blog/something/images/SRE/6-1.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图 6-1 预测在减少琐事工作上花费的时间，并确保其收益大于投入 &lt;/center&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;琐事的度量方法&quot;&gt;琐事的度量方法&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;识别它。第一本SRE书的第5章提供了如何识别琐事。最能够识别琐事的人取决于团队本身。理想情况下，SRE团队既是利益相关方，也是实际操作方。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;选择适当的计量单位来量化人力成本。我们可以选择“分钟”或者“小时”这么一个客观和普遍能够理解的计量单位。务必还要考虑琐事转自动化的成本。有些人力成本具有分散性和碎片化的特征，所以我们从成员工作的内容来衡量更为合适。度量单位应该要能够很好的度量如下工作：为应用增加的补丁，完成的票证，手动生产环境的变更，电子邮件交换或者是一些对硬件的操作。总的来说，只要度量单位客观，一致且易于理解，它就可以作为工作的衡量标准。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在项目的整个周期内我们需要连续跟踪并记录度量的指标。我们可以使用工具或脚本来简化度量指标的测量过程，使得收集这些测量值不会产生额外的工作。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;琐事分类法&quot;&gt;琐事分类法&lt;/h2&gt;

&lt;p&gt;琐事，就像一座摇摇欲坠的桥梁或一座漏水的大坝，日复一日地隐藏在广阔无垠的大地之中。本节中的分类并不能够详尽无遗，但代表了一些常见的琐事类别。这些类别中有许多类似“正常”的工作，但是它们实际上就属于琐事。&lt;/p&gt;
&lt;h3 id=&quot;商业流程&quot;&gt;商业流程&lt;/h3&gt;

&lt;p&gt;这可能是最常见的琐事来源。也许你的团队管理一些计算机资源——计算、存储、网络、负载平衡器、数据库等，以及为该资源提供支持的硬件资源。你需要处理用户登录、配置修改和计算机安全维护、软件更新以及扩缩容。你还需要最大限度地降低成本避免计算机资源的浪费。你的团队是计算机的人机界面，通常与为其需求提交票证的内部客户进行交互。你的组织甚至可能拥有多个票务系统和工作系统。
票务系统属于“隐藏”一类的琐事，因为其驱动的业务流程通常是我们需要完成的目标。用户得到了他们想要的东西，并且因为琐事往往分散在整个团队中，所以琐事并不能明显地显现出来。在以票据驱动的任何地方，都有可能悄悄地积累这琐事。即使你没有明确的自动化流程，仍然需要执行流程的改进工作，例如简化流程，使其未来更容易做到自动化，同时更加容易管理。&lt;/p&gt;
&lt;h3 id=&quot;工作中断&quot;&gt;工作中断&lt;/h3&gt;

&lt;p&gt;中断是一类为了保证系统运行的时间敏感类任务，简单理解为被其他紧急事情打断。例如，你可能需要通过手动释放磁盘空间或重新启动泄漏内存的应用程序来解决某些资源（磁盘，内存，I/O）的严重短缺。你可能正在提交更换硬盘驱动器，“踢”出无响应的系统或手动调整容量以满足当前或预期的负载请求。通常，中断会将注意力从更重要的工作上移开。&lt;/p&gt;
&lt;h3 id=&quot;流程监督&quot;&gt;流程监督&lt;/h3&gt;

&lt;p&gt;在许多组织中，部署工具从发布到生产需要SRE进行监督。即使有自动化，全面的代码覆盖，代码审查和多种形式的自动化测试，这个过程并不总是顺利进行。根据工具和发布节奏，发布请求、回滚、紧急补丁以及重复或手动配置更改，发布仍产生琐事。&lt;/p&gt;
&lt;h3 id=&quot;服务迁移&quot;&gt;服务迁移&lt;/h3&gt;

&lt;p&gt;服务迁移也是我们经常要处理的一类事情。你可以手动或使用有限的脚本来执行此工作，而且希望只迁移一次。迁移有多种形式，包括有数据存储、云供应商、源代码控制系统、应用程序库和工具的更改。如果你手动迁移大规模的工程，迁移很可能涉及到“琐事”。对于大规模的迁移，你可能倾向于手动执行迁移，因为这是一次性的工作。并且我们甚至会将其视为“项目”的一部分而非“琐事”，但迁移工作的很多特征与“琐事”的特征是吻合的。从技术上讲，修改一个数据库的备份工具以便与另一个数据库可以协同工作是软件开发的范畴，但这项工作本质上只是重构代码，用一个接口替换另一个接口。这项工作是重复的，并且在很大程度上，备份工具的业务价值与之前是相同的。&lt;/p&gt;
&lt;h3 id=&quot;压缩成本和容量规划&quot;&gt;压缩成本和容量规划&lt;/h3&gt;

&lt;p&gt;无论是拥有硬件还是使用基础架构提供商（云），压缩成本和容量规划通常是一些劳动密集型的工作。例如：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在计算、内存或IOPS（每秒输入/输出操作）等资源的未来规划中要确保成本效益和突发情况的扩容能力。这可能转化为采购订单，AWS预留实例或云/基础设施即服务合同协商。&lt;/li&gt;
  &lt;li&gt;应对（并从中恢复）关键的高流量事件，如产品发布或者遇到假期。&lt;/li&gt;
  &lt;li&gt;排查下游和上游服务水平和容量情况。&lt;/li&gt;
  &lt;li&gt;根据专有云服务产品的计费细节优化应用程序（适用于AWS的DynamoDB或适用于GCP的Cloud Datastore）。&lt;/li&gt;
  &lt;li&gt;重构工具以便更好地利用现有资源。&lt;/li&gt;
  &lt;li&gt;处理超预算的资源，无论是基础设施提供商的上游还是与下游客户之间。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;黑盒系统故障排除&quot;&gt;黑盒系统故障排除&lt;/h3&gt;

&lt;p&gt;分布式微服务架构现在很常见。随着系统更加分散，出现了新的故障模式。团队可能没有能力来构建复杂的分布式跟踪，高可靠监控或详细的仪表盘。即使企业确实拥有这些工具，它们也可能不适用于所有系统。故障排除甚至可能需要登录到各个系统并使用脚本工具来对日志进行实时地查询分析。&lt;/p&gt;

&lt;p&gt;故障排除本身并不是坏事，但你应该把精力集中在新的故障模式上，而不是每周都发生的由脆弱系统架构导致的故障。随着可用度为“P”的新关键上游依赖性服务的上线，系统可用性将下降（1-P）倍。一个可用度为4个9的服务增加了9个关键的4个9的核心组件，现在就变为了是一个三个9的服务。&lt;/p&gt;
&lt;h2 id=&quot;琐事管理战略&quot;&gt;琐事管理战略&lt;/h2&gt;

&lt;p&gt;任何规模的生产系统，琐事管理都是至关重要的。一旦确定并量化了琐事，消除琐事的计划就要提上日程。这个工作可能需要数周才能完成，因此制定一个完善的计划是至关重要。首先，从源头上消除琐事是最佳的解决方案，但是对于源头上无法消除的琐事，则需要通过其他方式来消除。在我们深入研究两个案例之前，本节提供了此方面工作的通用性准则。正如下文的两个案例中提到的，琐事的细微差别是因团队而异。但无论如何，一些常见的准则是适用于任何规模或风格的组织。在后续案例中将以具体方式诠释每种策略。&lt;/p&gt;
&lt;h3 id=&quot;琐事的识别与度量&quot;&gt;琐事的识别与度量&lt;/h3&gt;

&lt;p&gt;采用数据驱动的方法来识别琐事，并配合客观的成本控制策略，获得此类项目最优的投入产出比。如果你的团队正在被琐事缠身，并将减少琐事作为了一个长期的项目。Google SRE团队根据多年的经验，在控制项目投入产出比方面是一个不错的借鉴。有关技术和指导，请参见第96页的“量化琐事”一节。&lt;/p&gt;
&lt;h3 id=&quot;让sre从琐事中解脱出来&quot;&gt;让SRE从琐事中解脱出来&lt;/h3&gt;

&lt;p&gt;减少琐事的最佳策略是从源头杜绝琐事。在进行系统设计和为生产环境制定流程之前，工程师要优化产品和系统来减少甚至消除琐事。&lt;/p&gt;

&lt;p&gt;真正了解生产环境痛点和知道导致系统出现琐事原因的那部分人正是SRE，因为只有他们和生产环境紧密联系。SRE应该在与产品开发团队合作的过程中，将自己的运维经验与产品开发团队共享从而开发出人机交互友好型的软件，从源头减少琐事，并且使产品具有更好的扩展性和弹性。&lt;/p&gt;
&lt;h3 id=&quot;拒绝琐事&quot;&gt;拒绝琐事&lt;/h3&gt;

&lt;p&gt;一个被琐事缠身的团队应该尽早的做出“消除琐事”决策。第一种策略是对琐事说“不”！对于每个琐事，量化它并以此为原则决定是否要做，但是根据Google的经验，这一种策略可能会适得其反。另一种策略是故意拖延这些琐事，直到我们可以通过批处理或并行处理来解决它。将琐事集中在一起一并处理它们，这种方式可以减少工作中的中断，并帮助你们识别琐事的特征，并将它们作为下一个消除目标。&lt;/p&gt;
&lt;h3 id=&quot;使用slo减少琐事&quot;&gt;使用SLO减少琐事&lt;/h3&gt;

&lt;p&gt;如第2章所述，服务系统应具有文档化的SLO。明确定义SLO才能使工程师做出明智的决策。例如，如果某项工作即使做也不会减少服务的错误预算，你就可以考虑忽略某项工作。随着服务的增长，专注于整体服务的可用性而不是单个设备的SLO，这样做是非常有利的，也是可持续的。有关编写有效SLO的指导，请参阅第2章。&lt;/p&gt;
&lt;h3 id=&quot;从部分自动化开始&quot;&gt;从部分自动化开始&lt;/h3&gt;

&lt;p&gt;如果你的业务特别复杂，请将“部分自动化”方法视为实现“完全自动化”的临时步骤。在这种方法中，你的服务通常可以通过定义的API接收结构化数据。工程师也可以进行一些操作从而得到想要的结果。虽然这样做需要一些手动的操作，但是这种“幕后工程师”方法是逐步实现全自动化的前提。使用“客户端输入”来统一收集数据；通过确定的请求格式，你可以更容易的以编程的方式对请求进行处理。这种方法让客户也能够明白你需要的信息和指标，并在你完全理解系统服务之前避免使用大型的解决方案而产生的未知问题。&lt;/p&gt;
&lt;h3 id=&quot;提供一种自助的服务方法&quot;&gt;提供一种自助的服务方法&lt;/h3&gt;

&lt;p&gt;一旦你们提供了交互型界面的服务产品，请进一步的为用户提供自助式的服务方法。你可以提供Web表单、二进制、脚本、API，甚至只是告诉用户如何向服务的配置文件发出拉取请求的文档。例如，软件开发工程师要求SRE工程师为其开发工作配置新虚拟机，我们为他们提供一个简单的Web表单或脚本来触发配置，而不是让他们提交相关票证来进行这件事。如果发生了特殊的情况，我们也允许使用“票证”的方式替代自助的服务，这是可接受的。部分自动化是一个良好的开端，但服务SRE工程师应该始终要致力于尽可能让服务自动化起来。&lt;/p&gt;
&lt;h3 id=&quot;获得管理层和同事的支持&quot;&gt;获得管理层和同事的支持&lt;/h3&gt;

&lt;p&gt;在短期内，减少琐事的项目需要投入人力成本，反之会减少处理其他日常任务的人员数量。但长远来看，如果项目达到了减少琐事的目标，团队将更加健康，并有更多的时间进行更重要的工程改进。对于团队中的每个人来说，“减少琐事”作为一个共同的价值目标是很重要的。管理层的支持对于减少工程师的干扰是至关重要。制定琐事评估的客观指标来说明项目的推进情况可以让管理层更加支持项目的进行。&lt;/p&gt;
&lt;h3 id=&quot;减少琐事作为提高服务稳定性一部分&quot;&gt;减少琐事作为提高服务稳定性一部分&lt;/h3&gt;

&lt;p&gt;要为减少琐事的项目创建一个强大的业务案例支持，将你的目标与其他业务目标相结合。如果有一个补充性的目标，例如，安全性、可扩展性或可靠性——这对客户来说是具有吸引力的，他们会更愿意放弃当前充满琐事的系统，转向更加亮眼的新系统。这样来看，减少琐事也可以提高用户服务的质量，这也是另一个角度来看待琐事的认识。&lt;/p&gt;

&lt;p&gt;从简单的琐事开始并持续改善，不要试图设计没有琐事的系统。面对一个充满琐事的系统，首先自动化一些高优先级的项目，然后通过评估这个项目所花费的时间来改进你的解决方案，总结获得的经验和教训。在项目开始之前，选择一个明确的指标，如MTTR（平均修复时间）来评估你的项目的进展和效果。&lt;/p&gt;
&lt;h3 id=&quot;提高系统的一致性&quot;&gt;提高系统的一致性&lt;/h3&gt;
&lt;p&gt;从规模上看，多样化的生产环境是难以管理的。特殊的生产环境容易出错，管理能力会降低，事故的处理能力也会降低。你可以使用“宠物与牛”方法（https://www.engineyard.com/blog/pets-vs-cattle，译者注）来添加系统冗余并在你的生产环境中实施增加一致性的策略。是否选择“牛”取决于组织的需求和规模。将网络链路、交换机、机器、机架，甚至整个集群评估为可互换单元也是合理的。将设备转换为“牛”的理念可能会带来较高的初始成本，但会减少中长期的维护成本，增强灾难恢复能力和提高资源利用能力。为多个设备配置相同的接口意味着它们具有相同的配置，是可互换的，维护成本也就降低了。各种设备的界面一致（转移流量，恢复流量，执行关机等）使系统更加灵活和更加可扩展。Google鼓励各团队将不断发展的内部技术和工具进行统一，并有相应的鼓励机制。无论团队用什么样的方法，但他们不得不承认一些不受支持的工具或遗留的系统是产生琐事的根源。&lt;/p&gt;
&lt;h3 id=&quot;评估自动化带来的风险&quot;&gt;评估自动化带来的风险&lt;/h3&gt;

&lt;p&gt;自动化可以节省人力成本，但是也会出现未知的错误，严重时会造成停机。一般情况下，防御性软件可以控制这类事情的发生。当管理级别的行为被自动化之后，防御性软件会显得至关重要。在执行前应对每项行为的安全性进行评估。在实施自动化时，我们建议采用以下做法：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;防御性地处理用户输入，即使这个输入来自于上游的系统 ——换句话说，要对上下游的输入进行仔细的校验。&lt;/li&gt;
  &lt;li&gt;构建告警机制，使得工程师可以接收到相关告警以进行处理。安全措施可能与命令超时一样简单，也可能是对当前系统指标或当前中断次数的更复杂检查。因此，监控，报警和仪表系统应由机器和操作人员共同使用。&lt;/li&gt;
  &lt;li&gt;请注意，即使是简单的读取操作也可能会导致设备负载过高和触发服务中断。随着自动化的扩展，这些安全检查的工作量是可控的。&lt;/li&gt;
  &lt;li&gt;最大限度地减少因自动化安全检查不完整导致服务中断的影响。如果操作员遇到不安全的情况，自动化操作应该默认为人工操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;琐事自动化之后要做什么&quot;&gt;琐事自动化之后要做什么&lt;/h3&gt;

&lt;p&gt;一旦你可以将一个工作自动化后，这个自动化的工作就值得更深层次的被发掘。进一步的将自动化的任务按照人工处理的流程优化下去。但请注意，自动化不应该让工程师认为任务不会出错。在完成上述优化后，你还可以尝试将自动化的工作分解为可单独实现的组件，并用于创建可组合的软件库，其他自动化项目可在以后重复使用。正如下文中的“数据中心维修案例”研究所示，自动化提供重新评估和简化人工工作流程的机会。&lt;/p&gt;
&lt;h3 id=&quot;使用开源和第三方工具&quot;&gt;使用开源和第三方工具&lt;/h3&gt;

&lt;p&gt;有时你不必做所有的工作来减少琐事。像一次性迁移这样的工作可能自己无法建立定制型的工具，但你可能并不是第一个遇到这个任务的工程师。寻找第三方或开源库以降低开发成本，或者说，至少可以帮助你过渡到部分自动化。&lt;/p&gt;
&lt;h3 id=&quot;反馈并改进&quot;&gt;反馈并改进&lt;/h3&gt;

&lt;p&gt;积极寻求反馈，这些反馈可以来自于工具、工作流程和自动化交互相关的其他人，这是非常重要的。你的用户将根据他们对底层系统的理解将你的工具在不同使用情景下进行使用。你的用户对这些工具越不熟悉，就越要积极地寻求用户的反馈。利用用户调查，用户体验（UX）和其他机制来了解你的工具被如何使用，并整合这些反馈，以便在未来实现更有效的兼容性。
人的输入只是你应该考虑反馈中的一个方面。我们还可以根据延迟，错误率，返工率和节省的人工时间等指标（跨过流程中涉及的所有组）来衡量自动化任务的有效性。能够获得在自动化工作部署之前和之后两种状态的对比是最明确的衡量方式。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;扩展：历史遗留系统&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;大多数SRE工程师在他们的工作中都会遇到过历史遗留系统。这些旧系统经常在用户体验，安全性、可靠性或可伸缩性方面有问题。他们倾向于将遗留系统看作一个神奇的黑匣子，因为系统“大部分组件是在工作中的”，但很少有人了解它们是如何工作的。贸然的调整它们是可怕的，也是昂贵的，并且保持它们的运行通常需要大量繁琐操作步骤。
远离遗留系统通常遵循以下路径：&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;避免：我们可以为不去解决这个问题找到许多理由：可能是没有资源来替换这个系统；判断业务成本和风险发现不值得替换；可能没有找到商业上更好的解决方案。避免选择的是接受风险并从SRE转向系统管理。&lt;/li&gt;
    &lt;li&gt;封装/扩充：你可以使用SRE来构建一个抽象API的外壳，自动化，配置管理，监视和测试这些遗留系统，这些系统将卸载SA的工作。遗留系统仍然很难改变，但现在你至少可以识别它并在适当时有回滚策略。这种策略仍然可以避免，但这是将风险引入到的更好的系统中。这通常是准备增量替换的权宜之计。&lt;/li&gt;
    &lt;li&gt;替换/重构：替换遗留系统可能需要大量的决心、耐心、沟通成本和文档，最好是逐步进行。一种方法是定义遗留系统公共接口。此策略可帮助你使用发布的工程手段，将用户缓慢、安全地迁移到其他安全的架构中。通常，遗留系统的“规范”实际上只是通过其历史用途来定义，因此有助于构建生产大小的历史预期输入和输出数据集，以建立新系统不会偏离预期行为的信心（或正在以预期的方式发散）。&lt;/li&gt;
    &lt;li&gt;退出/保管所有权：最终，大多数客户或功能被迁移到一个或多个系统。这个迁移需要有激励措施，没有迁移的用户让他们自行维护历史遗留系统，并承担相应责任。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;案例研究&quot;&gt;案例研究&lt;/h2&gt;

&lt;h2 id=&quot;案例研究1利用自动化减少数据中心的工作量&quot;&gt;案例研究1：利用自动化减少数据中心的工作量&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;案例研究1中所应用的减少琐事的战略：&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;SRE工程师从琐事中解脱出来&lt;/li&gt;
    &lt;li&gt;从部分自动化开始&lt;/li&gt;
    &lt;li&gt;提高系统的一致性&lt;/li&gt;
    &lt;li&gt;使用SLO减少琐事&lt;/li&gt;
    &lt;li&gt;评估自动化带来的风险&lt;/li&gt;
    &lt;li&gt;反馈并改进&lt;/li&gt;
    &lt;li&gt;提供一种自助的服务方法&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;背景&quot;&gt;背景&lt;/h3&gt;
&lt;p&gt;此案例来源于Google数据中心。与其他的数据中心类似，Google的计算机连接到交换机，交换机连接到路由器。流量通过链路流入和流出这些路由器，而链路又连接到互联网上的其他路由器。随着谷歌对互联网流量的要求越来越高，服务该流量所需的交换机数量也急剧增加。为了能够应对大流量的情况，我们的数据中心在规模和复杂性方面都有所增长。这种增长迫使数据中心改变了手动维修的旧方法。（从偶尔和有趣到频繁和沉闷的转变。）&lt;/p&gt;

&lt;p&gt;早期，谷歌在运行数据中心时，每个数据中心的网络拓扑都只有少量的网络设备，可以管理大量服务器的流量。单个网络设备故障可能会显著影响网络性能，但是一个小规模的工程师团队就可以处理设备的故障。早期，工程师调试故障设备并手动将流量切换到其他正常组件。而我们下一代的数据中心拥有更多的机器，并引入了折叠Clos拓扑结构的软件定义网络（SDN），交换机数量显著增加。图6-2展示的是一个小型数据中心Clos交换机网络的流量复杂情况。如果将这个比例放大，意味着设备数量更多，发生故障的组件也更多。虽然可以说，每个单独的故障对网络性能的影响比以前更小，但是大量的问题同时并发也会压倒工程师们。调试问题的过程同时也会引入大量新的问题，复杂的布局也让工程师感到困惑：需要检查哪些链接？需要更换哪个线卡？为什么是Stage 2开关，而不是Stage 1或Stage 3开关？关闭交换机会给用户带来哪些问题？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;center&gt;
图6-2. 一个小型Clos网络，Stage1 支持480台机器连接 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;修复故障的线卡是一个随着系统网络增长而任务量不断增长的琐事，因此我们将此作为“数据中心网络修复自动化”项目的第一阶段的目标。本案例阐述了我们如何在第一代线卡（名为Saturn）系统上开始自动化修复的过程，并以此为基础，我们讨论了如何对自动化工作进行改进以适应下一代线卡(Jupiter光纤网络)。&lt;/p&gt;

&lt;p&gt;如图6-3所示，在自动化项目开始之前，数据中心线卡修复工作需要工程师执行如下几个操作：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;确定从故障交换机切走流量是否是安全的。&lt;/li&gt;
  &lt;li&gt;切走流量至其他交换机（“drain”操作）。&lt;/li&gt;
  &lt;li&gt;执行重启或修复（例如更换线卡）。&lt;/li&gt;
  &lt;li&gt;将流量切回至该交换机（“undrain”操作）。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Drain，更换线卡，undrain的工作是不变和重复性质的，是“琐事”的典型范例。这些重复性的工作本身就会带来一些问题——例如，工程师在处理此类故障时会并行处理其他更有挑战性的工作，分心的工程师可能会意外地将未配置的交换机加入网络。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-3.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt; 
图6-3. 自动化之前的数据中心（Saturn）线卡修复工作流程：所有步骤都需要手动工作 &lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&quot;问题陈述&quot;&gt;问题陈述&lt;/h3&gt;
&lt;p&gt;数据中心修复线卡问题具有以下几个维度：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;团队规模增长的速度跟不上系统增长的速度（故障数量也在增长），使得我们无法快速解决问题以防止对系统带来负面影响。&lt;/li&gt;
  &lt;li&gt;人为错误一定会在重复执行的步骤中发生。&lt;/li&gt;
  &lt;li&gt;并非所有线卡故障的影响都是一致的。我们没办法对线卡故障划分优先级。&lt;/li&gt;
  &lt;li&gt;一些故障是暂时的，这时我们会选择直接重新启动线卡或重新安装交换机作为修复过程的第一步。并且，我们可以用编程方式捕获这些问题，如果它再次发生，则进行设备替换。&lt;/li&gt;
  &lt;li&gt;新的拓扑环境要求我们在采取行动之前手动评估隔离容量的风险。每次的人工风险评估都有可能带来人为错误，并可能带来严重影响。系统工程师和技术人员也没有好的方法来判断有多少设备和链接会受到修复过程的影响。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;我们要如何解决这个问题&quot;&gt;我们要如何解决这个问题？&lt;/h3&gt;
&lt;p&gt;为达到最好的效果，我们决定创建一个与现场技术人员配合使用的自动化框架，而不是把每个问题分配给工程师，让其进行风险评估，流量切换，维修和验证等人工操作。&lt;/p&gt;

&lt;h3 id=&quot;自动化的第一步jupiter光纤网络的修复自动化&quot;&gt;自动化的第一步：Jupiter光纤网络的修复自动化&lt;/h3&gt;
&lt;p&gt;我们的最终目标是构建一个能够代替工程师分析和处理故障的网络设备故障检测系统。我们的程序是直接切换流量并告知工程师，而不是向工程师发送“线卡”故障的告警。新系统有一些值得注意的特点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;我们最好是利用现有工具。如图6-3所示，我们的告警已经可以检测到线卡上的问题; 所以我们可以配置告警以触发自动修复。新的工作流程还应改变工单系统，以便支持自动提交的维修请求。&lt;/li&gt;
  &lt;li&gt;我们建立自动风险评估的机制，以防止在流量切换期间意外的隔离设备，并在需要时触发安全机制。此机制可以杜绝人为错误。&lt;/li&gt;
  &lt;li&gt;我们编写程序用于跟踪告警以便作出不同的处理操作：第一次告警仅重启该线卡并重装了软件；第二次出现告警则直接请求更换线卡并告知供应商。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;执行自动化操作&quot;&gt;执行自动化操作&lt;/h3&gt;
&lt;p&gt;新的自动化工作流程（如图6-4所示）进行如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;检测到有问题的线卡，并将故障特征添加到数据库中。&lt;/li&gt;
  &lt;li&gt;维修服务组件会解决问题并对交换机进行维修。该服务还会执行风险评估以确认操作不会隔离任何容量，然后： &lt;br /&gt;
 a. 从故障交换机中切出流量。&lt;br /&gt;
 b. 关闭线卡。&lt;br /&gt;
 c. 如果这是第一次告警，则重新启动线卡，将流量恢复到此交换机。此时，工作流程已完成。&lt;br /&gt;
 d．如果这是第二次失败，则工作流程进行到步骤3。&lt;/li&gt;
  &lt;li&gt;流程管理器检测到新案例并将其发送到故障维修池，供系统工程师处理。&lt;/li&gt;
  &lt;li&gt;系统工程师对故障做出响应，在UI界面中看到红色的“停止”（表示在开始修理之前需要切走流量），并分三步执行修复步骤：&lt;br /&gt;
 a. 系统工程师通过UI界面中的“准备组件”按钮启动流量切换。&lt;br /&gt;
 b. 流量切换完成后表示交换机可操作。&lt;br /&gt;
 c. 关闭交换机并维修线卡。&lt;/li&gt;
  &lt;li&gt;自动修复系统再次启动线卡。完成修复后，启动交换机，待初始化后，流程管理器会触发恢复操作，切回交换机流量并结算故障工单。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-4.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-4.具有自动化功能的Saturn线卡维修工作流程：只需按下按钮即可完成更换线卡等全部手动工作 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;新的自动化系统将团队从大量的琐事中解放出来，使他们有更多时间在其他地方开展更高效的项目：使用下一代Clos拓扑结构Jupiter。&lt;/p&gt;

&lt;h3 id=&quot;自动化项目的第二步saturn线卡修复与jupiter线卡修复&quot;&gt;自动化项目的第二步：Saturn线卡修复与Jupiter线卡修复&lt;/h3&gt;
&lt;p&gt;数据中心的容量需求几乎每12个月翻一番。因此，我们的下一代数据中心结构Jupiter比Google以前的任何数据中心的六倍还要大，所以故障的数量也会增加六倍多。Jupiter提出了自动化故障修复的挑战目标，这个目标的难度在于每层的数千个光纤链路和数百个线路卡都可能出现故障。幸运的是，随着潜在故障点的增加系统也会伴随增加更多的冗余，这有利于我们完成自动化任务。如图6-5所示，我们保留了系统的一些常规工作流程，并添加了一些重要的修改：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在自动切流量和关闭交换机之后，确定我们要更换的硬件，将硬件故障单发送给系统工程师。在这个过程中，切流量的行为是自动的，不需要系统工程师手动按下“预备按钮切流量开关”来完成。&lt;/li&gt;
  &lt;li&gt;我们添加了自动化，用于安装和推送组件更换后的配置。&lt;/li&gt;
  &lt;li&gt;我们启用自动化功能，以便在切回流量之前验证修复是否成功。&lt;/li&gt;
  &lt;li&gt;除非绝对必要，否则我们更关注的是如何恢复流量而不用人为介入。
&lt;img src=&quot;/blog/something/images/SRE/6-5.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-5.左图为Saturn线卡宕机自动化流程，右图为Jupiter线卡宕机自动化流程&lt;/center&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;项目实现&quot;&gt;项目实现&lt;/h3&gt;
&lt;p&gt;我们为Jupiter交换机上的所有的线卡故障采用了简单而统一的工作流程：操作通报，流量切换，开始修复。&lt;/p&gt;

&lt;p&gt;自动化执行如下：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;检测到交换机故障，并将故障特征写到数据库。&lt;/li&gt;
  &lt;li&gt;维修程序开始修复交换机：停止使用交换机，并将停止原因写到数据库中。&lt;br /&gt;
 a. 如果这是六个月内的第二次故障，请执行步骤4。&lt;br /&gt;
 b. 否则，请执行步骤3。&lt;/li&gt;
  &lt;li&gt;尝试（通过两种不同的方法）重启交换机。 &lt;br /&gt;
 a. 如果重启成功，用自动化服务检查健康状态，然后安装并配置交换机使其投入使用；删除修复原因，删除数据库中的故障记录。&lt;br /&gt;
 b. 如果健康检查失败，请升级给技术人员。&lt;/li&gt;
  &lt;li&gt;如果这是第二次故障告警，请将故障案例直接升级给技术人员，向其申请新的硬件设备。硬件更新后，用自动化服务检查健康状态，然后安装并配置交换机使其投入使用。删除修复原因，删除数据库中的故障记录。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这种新的工作流程管理完全重写了以前的修复系统。同样的，我们要尽可能利用现有工具：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;配置新交换机（安装和验证）的操作与验证已更换的交换机所需的操作相同。&lt;/li&gt;
  &lt;li&gt;快速部署新的硬件需要以编程的方式进行BERT和cable-audit的能力。在恢复使用之前，我们可以使用该程序在已经修复的链路上运行功能测试。这些测试需要能够识别错误链接以进一步提高修复的效果。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下一步要提升的是自动缓解并修复Jupiter交换机线卡的内存错误。如图6-6所示，在开始自动化修复之前，此工作流程在很大程度上取决于工程师来判定故障是硬件导致还是软件导致，然后再停止使用，重启交换机等工作，并适时地安排修复。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-6.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-6. 自动化之前的Jupiter内存错误修复工作流程 &lt;/center&gt;
我们的自动化过程不再尝试对内存错误进行故障排除从而达到简化修复工作流程的目的（请参阅第119页的“有时不完美的自动化就足够了”，了解为什么这样做是有意义的）。相反，我们处理内存错误的方式与处理线卡故障的方式相同。为了将自动化覆盖到内存错误引起的故障，我们只需在配置文件中添加一个特征，使其对新的故障类型起作用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-7.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-7描述了内存错误的自动化工作流程。&lt;/center&gt;&lt;/p&gt;

&lt;h3 id=&quot;经验教训&quot;&gt;经验教训&lt;/h3&gt;
&lt;p&gt;在我们致力于实现网络故障自愈的这些年里，我们学会了如何有效减少琐事。&lt;/p&gt;

&lt;h4 id=&quot;uis-不该引入开销和复杂度&quot;&gt;UIs 不该引入开销和复杂度&lt;/h4&gt;
&lt;p&gt;替换一块Saturn-based线卡需要切走整个交换机的流量。等待备件更换以及工程师支持的时候，过早地执行全部切换操作意味着失去所有线卡的工作能力。我们在UI中增加一个“准备组件”的按钮，以允许技术人员在更换线卡前执行整个交换机的切换流量操作，从而消除了交换机不必要的停机时间（请参阅“按下准备按钮” 切出流量的开关“见图6-5）&lt;/p&gt;

&lt;h4 id=&quot;ui和维修工作的流程引入了许多非预期的问题&quot;&gt;UI和维修工作的流程引入了许多非预期的问题&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;按下切走流量的按钮后，技术人员无法得到流量切换进度的反馈，只能在结果返回后才能进行下一步操作。&lt;/li&gt;
  &lt;li&gt;该按钮可能无法反馈真实的状态。造成的结果是，有时切流量开关出问题但并没有被维修，或者技术人员可能通过其他方式中断了进程但是并没有告知系统。&lt;/li&gt;
  &lt;li&gt;问题出现时，非自动化的组件反馈了一个通用的‘contact engineering’信息。经验不丰富的技术人员无法快速找到可以提供帮助的人，而联系上的工程师并不总能够立即解决问题。
为快速对用户反馈以及因功能复杂性带来的回归问题进行响应，我们设计了更完善的工作流程，来保证按钮的安全性和可用性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;不要依赖人的经验&quot;&gt;不要依赖人的经验&lt;/h4&gt;
&lt;p&gt;我们过分依赖有经验的数据中心技术人员来识别系统中的错误（例如，当程序认为可以安全地进行维修，但实际上交换机并没有完成流量切换的动作）。这些技术人员在没有自动化提示的情况下，还必须手动执行多项工作。&lt;/p&gt;

&lt;p&gt;经验是难以复制的。在一个复杂的情节中，技术人员在等待数据中心维修时，决定启动并发切换来快速进行“按下按钮并等待结果”的操作，从而导致了网络拥塞和用户可见的数据包丢失。我们的软件无法预测并阻止这种行为，因为我们并没有测试过这种自动化。&lt;/p&gt;

&lt;h4 id=&quot;设计可重复使用的组件&quot;&gt;设计可重复使用的组件&lt;/h4&gt;

&lt;p&gt;尽可能避免采用集成化设计。使用组件来构建复杂的自动化工作流，每个组件处理一个独特且定义明确的任务。我们可以轻松地重复使用或调整早期Jupiter自动化的关键组件来用于下一代的软件设计，并且很容易针对已经存在的自动化项目增加新的功能。Jupiter类结构的连续变体可以采用早期已经完成的工作。&lt;/p&gt;
&lt;h4 id=&quot;不要过分分析问题&quot;&gt;不要过分分析问题&lt;/h4&gt;
&lt;p&gt;我们过度分析了Jupiter线卡内存错误问题。我们试图进行精确的问题诊断，我们想区分软件错误（可通过重新启动修复）与硬件错误（需要更换卡），并识别影响流量的错误与未发生的错误。我们花费将近三年（2012-2015）的时间来收集超过650个离散内存错误的数据，然后才意识到这个分析是过头了，或者至少不应该阻塞我们自动化项目的开展。&lt;/p&gt;

&lt;p&gt;一旦我们决定对检测到的任何错误都采取必要的措施，就可以直接使用我们现有的自动化修复技术来实现简单的切换策略、重启以及为修复内存错误而重置交换机。如果问题再次出现，我们可以认为，故障很可能是基于硬件的，并立即要求更换组件。我们花费了整个项目四分之一的时间来收集数据，发现大多数的错误是暂时的 ——大多数交换机在重新启动和重新安装后都恢复了。我们不需要额外的数据来执行修复，因此为了实现这种自动化花费了三年是没有必要的。&lt;/p&gt;

&lt;h4 id=&quot;有时不完美的自动化就已经足够&quot;&gt;有时不完美的自动化就已经足够&lt;/h4&gt;
&lt;p&gt;解除链路之前，通过BERT很容易确认链路状况，但BERT工具不支持网络管理链路。我们将这些链路添加到现有的链路修复自动化中，并允许跳过验证。我们很愿意绕过验证，因为链路并没有承载客户流量，如果验证结果很重要，我们可以稍后添加此功能。&lt;/p&gt;

&lt;h4 id=&quot;保证维修自动化项目的持续性和可继承性&quot;&gt;保证维修自动化项目的持续性和可继承性&lt;/h4&gt;
&lt;p&gt;自动化项目可以有很长的生命周期，需要确保人员的流动不会干扰项目的连续性。 新人工程师应该接受现有系统的培训，以便他们能够修复错误。由于Jupiter线卡部件的短缺，Saturn-based系统在其目标寿命结束后很长一段时间内还是存在的，这要求我们日后在Saturn的生命周期中进行一些改进。&lt;/p&gt;

&lt;p&gt;自动化一旦被采用，在很长的一段时间内将会被依赖使用，并伴随着一些积极和消极的后果。如果可能，以灵活的方式设计你的自动化程序。不灵活的自动化会使系统变更变得难以实现。使用基于策略的自动化可以明确地将意图与通用实现引擎分离，从而使自动化更加可持续的发展。&lt;/p&gt;

&lt;h4 id=&quot;深入开展风险评估和防御措施&quot;&gt;深入开展风险评估和防御措施&lt;/h4&gt;
&lt;p&gt;为Jupiter构建新工具以评估执行切流操作前的风险，而后由于问题的复杂性，我们需要在更深层次的防御上引入二次检查。二次检查设定了受影响链路数量的上限，以及受影响设备的额外限制。一旦超过任一限定值，便会自动触发追踪bug以请求更进一步的检查。我们不断地调整这些限制，以减少误报。最初我们认为二次检查只是一项临时措施，但是在主要风险评估平稳后，该措施已被证明可用于识别由于停电和软件错误导致的维修问题（如请参阅SRE中“自动化：在规模上实现失效”）。&lt;/p&gt;

&lt;h4 id=&quot;失败预算和管理者支持&quot;&gt;失败预算和管理者支持&lt;/h4&gt;
&lt;p&gt;修复自动化有时会失败，尤其是在首次使用时。管理者的支持对于保护项目，并鼓励团队坚持不懈是至关重要的。我们建议为通过自动化技术消除琐事项目设置错误预算。你还需要向外部的其他利益方解释：尽管存在故障风险，但自动化极其重要，并可以持续提高可靠性和效率。&lt;/p&gt;

&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;
&lt;p&gt;最终，复杂场景的自动化是真正需要解决的问题。在引入自动化系统之前要反复对系统进行评审——是否可以先简化系统和工作流？&lt;/p&gt;

&lt;p&gt;要关注自动化工作流程的各个方面，而不仅仅是造成琐事的那部分。和直接参与项目的人员共同开展测试工作，并积极寻求他们的反馈和帮助。如果他们在使用过程中出现操作问题，要想办法使工作界面更清晰，或者增加额外的安全检查。确保自动化不会带来额外的琐事——例如开启了不必要的工单以引起人的注意。给其他团队创造问题将增加自动化推进的难度。&lt;/p&gt;

&lt;h2 id=&quot;案例研究2淘汰以文件为后端的home-directories&quot;&gt;案例研究2：淘汰以文件为后端的Home directories&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;案例研究2中强调了减少琐事的方法：&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;考虑淘汰旧系统&lt;/li&gt;
    &lt;li&gt;将减少琐事作为一项工程&lt;/li&gt;
    &lt;li&gt;获得管理层和同事的支持&lt;/li&gt;
    &lt;li&gt;拒绝琐事&lt;/li&gt;
    &lt;li&gt;从部分自动化开始&lt;/li&gt;
    &lt;li&gt;提供一种自助的服务方法&lt;/li&gt;
    &lt;li&gt;从细微处开始然后改进&lt;/li&gt;
    &lt;li&gt;反馈并改进&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;背景-1&quot;&gt;背景&lt;/h3&gt;
&lt;p&gt;在谷歌的早期，公司数据存储（CDS）SRE团队为所有Google员工提供home目录服务。与企业IT中常见的Active Directory漫游配置文件类似，Google员工可以跨工作站和平台使用相同的home目录。CDS团队还为共享存储空间中的跨团队协作提供“团队共享”服务。我们通过NFS / CIFS（或“文件管理器”）上的Netapp存储设备提供home目录和团队共享。这种存储系统是很昂贵的，但Google员工对此类服务的需求是必须的。&lt;/p&gt;

&lt;h3 id=&quot;问题陈述-1&quot;&gt;问题陈述&lt;/h3&gt;
&lt;p&gt;随着时间的推移，这些文件管理系统解决方案的优势被其他更好的存储解决方案所超越：我们的版本控制系统（Piper / Git-on-borg），Google Drive，Google Team Drive，Google云存储以及全球内部共享分布式ilesystem（x20）。这些替代方案的优越性体验在如下方面：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NFS / CIFS协议并不适用于在WAN上运行，这造成即使有几十毫秒的延迟，用户体验也会迅速降低。这也为远程工作人员或全球分布的团队带来了问题——因为数据只能存在于一个地方。&lt;/li&gt;
  &lt;li&gt;与替代品相比，原系统的设备运行和规模都是昂贵的。&lt;/li&gt;
  &lt;li&gt;要使NFS / CIFS协议与Google的Beyond Corp11网络安全模型兼容，需要做大量的工作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;与本章最相关的是，home目录和团队共享会频繁的使用。存储配置的许多方面都是ticket驱动的。虽然这些工作流程通常是研发人员编写的，但它们代表了相当数量的CDS团队的工作成果。我们花了很多时间创建和配置共享，修改访问权限，解决最终用户问题，以及执行启动和调整以管理容量。除了配置、更新和备份之外，CDS还需要管理专用硬件的配置，机架和布线过程。由于延迟要求，我们经常不得不部署在远程办公室而不是在Google数据中心 – 这有时需要花费团队成员相当长的时间。&lt;/p&gt;

&lt;h3 id=&quot;我们决定做什么&quot;&gt;我们决定做什么&lt;/h3&gt;
&lt;p&gt;首先，收集数据：CDS团队开发了一个名为“Moonwalk”的工具来分析员工使用此的服务的场景。我们确定收集如下通用的指标，如每日活跃用户（DAU）和月活跃用户（MAU），并询问了诸如“哪些用户实际使用他们的home目录？”和“哪些人每天使用此系统？他们最常访问的文件是什么？“Moonwalk与用户调查相结合，验证了文件管理器当前服务的业务需求可以通过低运营开销和成本的可替代方案代替。另一个引人注目的原因促使我们放弃现有的文件系统：如果我们可以将大多数文件管理器用例迁移到G Suite / GCP，那么我们可以利用我们学到的经验来改进这些产品，从而为其他大型企业迁移到GSuite/ GCP提供支持。&lt;/p&gt;

&lt;p&gt;没有一种替代方案可以满足所有当前的文件管理器用例。然而，通过将问题转化为若干小的需求来寻找可替代系统，我们发现少数备选方案可以涵盖我们所有的使用场景。替代解决方案更专业，并且每个解决方案都带来比旧的解决方案更好的用户体验。例如：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;x20&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;全局共享静态文件对于团队来说是很好的方式，比如二进制文件。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;G Suite Team Drive&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;适用于办公文档协作，与NFS相比，用户更能容忍此延迟。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;谷歌的巨像Colossus文件系统&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;比NFS更安全，更可靠地共享大型数据文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Piper/Git-on-Borg&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;可以更好地同步dotfiles（工程师的个性化工具首选项）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;一种新的“历史服务”工具&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;可以托管跨工作站命令行的历史记录&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在我们编制用例并找到替代方案时，旧文件系统的下线计划也已经开展&lt;/p&gt;

&lt;h3 id=&quot;设计与实施&quot;&gt;设计与实施&lt;/h3&gt;
&lt;p&gt;下线旧的文件管理系统是一项持续的、迭代的、需要多年的进行的工作。需要伴随多个子项目的开展：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Moira&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;home目录下线&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Tekmor&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;迁移home目录用户的历史遗留数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Migra&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;团队共享下线&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Azog&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;下线home目录/共享基础架构和相关硬件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/blog/something/images/SRE/6-8.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-8。Moira项目的四个阶段 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;本案例研究重点关注第一个项目Moira。后续项目的开展是在Moira的学习和开展的基础上开始的。如图6-8所示，Moira由四个阶段组成。&lt;/p&gt;

&lt;h3 id=&quot;关键组件&quot;&gt;关键组件&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Moonwalk&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;虽然我们有关于用户共享（例如共享大小）的基本统计数据，但我们仍需要了解用户的工作流程，以帮助用户即使在一片反对声中仍然可以做出有利于业务发展的决策。我们建立了一个名为“Moonwalk”的系统来收集和反馈这些信息。&lt;/p&gt;

&lt;p&gt;Moonwalk存储了谁正在访问哪些文件以及何时使用BigQuery的数据，这使我们能够做出统计报告以便更好地了解用户。在BigQuery的帮助下，我们汇总了25亿个文件，共计300 TB的数据的用户访问模式。该数据来自于全球60个地理站点的124个NAS设备，共计600,000个磁盘卷，共收集了60,000名POSIX用户的使用信息。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Moira Portal&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过工单处理来完成home目录下线的想法，在我们庞大的用户基数面前看起来不太现实。我们需要在整个过程中（调查用户，告知项目下线原因，归档数据或迁移到替代系统）尽可能提供低接触服务（低接触服务是指这样的服务模式：销售服务人员在向顾客提供服务时，保持较少的面对面的接触机会。相对于高接触服务而言，低接触服务需要更多的机器和固定资产。因为通常需要由它们来自动完成顾客服务，如自动售货机、自动柜员机、自动加油机等，译者注）。我们的最终要求是：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;描述项目的登录页面&lt;/li&gt;
  &lt;li&gt;不断更新对常见问题的解答&lt;/li&gt;
  &lt;li&gt;与当前用户共享关联状态和使用信息&lt;/li&gt;
  &lt;li&gt;提供请求，停用，存档，删除，扩展或重新激活共享的选项&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;至此，我们的业务逻辑变得相当复杂——因为我们必须考虑许多用户场景。例如，用户可能会从Google离职，短暂离职，或者在诉讼状态（需要保留其拥有的数据）。图6-9提供了一个示例图，说明了其复杂性。
&lt;img src=&quot;/blog/something/images/SRE/6-9.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-9。基于用户场景的业务逻辑 &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;为主门户网站提供支持的技术相对简单。基于Flask框架下用Python语言编写，它读取并写入Bigtable，并使用大量后台作业和调度程序来管理其工作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;归档和迁移自动化&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们需要大量的辅助工具来将门户网站和配置管理组合在一起，并与用户进行通信来进行用户查询。我们还需要使用沟通技巧来鉴别出适合进行数据迁移的用户。误报（错误地报告所需行动）或漏报（未通知用户您正在取走某些东西）都是不可接受的，这里发生错误将意味着失去用户可信度和带来客户服务的额外工作。&lt;/p&gt;

&lt;p&gt;我们与其他存储系统所有者合作——为新系统添加我们需要的功能。因此，随着项目的进展，不太成熟的替代品变得更适合了。我们还可以使用和扩展其他团队的工具。例如，我们使用其他团队内部开发的工具将数据从Google云端存储迁移到Google云端硬盘，作为门户网站自动存档功能的一部分。这项工作需要在整个项目期间进行长期的软件开发。我们构建并迭代了每个组件–Moonwalk报告管道，门户和自动化，以便更好地管理下线和归档共享，以响应下一阶段的要求和用户反馈。我们在第三阶段（差不多两年）才接近达到一个功能健全的系统。即便如此，我们还需要额外的工具来处理大约800名用户的“长尾”。这种低速和慢速的方法有一定的好处。因为它允许我们：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;维持一个精炼的团队（平均三名CDS团队成员）&lt;/li&gt;
  &lt;li&gt;减少对用户工作流程的干扰&lt;/li&gt;
  &lt;li&gt;减少Techstop的琐事（谷歌内部技术支持组织）&lt;/li&gt;
  &lt;li&gt;根据需要构建工具，以避免将时间浪费在工程工作中
与所有工程决策一样，存在如下权衡：项目将长期存在，因此团队在设计解决方案时必须忍受与文件管理器相关的琐事。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;该计划于2016年正式完成。在撰写本文时，我们已将home目录从65,000个减少到约50个（目前的Azog项目旨在淘汰这些最后用户并彻底下线文件管理系统的硬件。）我们的用户体验有所改善，CDS已停止运营成本高昂的硬件。&lt;/p&gt;

&lt;h3 id=&quot;经验教训-1&quot;&gt;经验教训&lt;/h3&gt;
&lt;p&gt;虽然没有任何替代方案可以替代Google员工已使用14年之久的文件管理系统，但我们并非没必要进行批量更换。通过有效地将堆栈从通用但有限的文件系统解决方案升级到多个应用程序共同组成的解决方案，我们增加了系统的灵活性，以提高可扩展性、延迟包容度和安全性。Moira团队不得不预测各种用户的行为，并考虑不同阶段的可替代方案。我们必须围绕这些替代方案来调整我们的期望：总的来说，它们可以提供更好的用户体验，但实现这一目标并非是毫无痛苦的。我们学到了以下关于有效减少琐事的策略。&lt;/p&gt;

&lt;h4 id=&quot;发现旧系统的不足并下线昂贵的业务流程&quot;&gt;发现旧系统的不足并下线昂贵的业务流程&lt;/h4&gt;
&lt;p&gt;业务需求不断变化，新的解决方案不断涌现，因此定期评估旧的业务流程是值得的。正如我们在第101页的“琐事管理策略”中所讨论的那样，拒绝琐事（决定不执行）通常是消除它的最简单方法，即使这种方法并不总是快速或简单的。通过用户分析和业务理由来调整你的业务工作内容，而不仅仅是减少琐事。文件管理系统下线的主要业务理由归结为Beyond Corp安全模型的优势。因此，虽然Moira是减少CDS团队琐事的好方法，但强调下线系统的原因如果是考虑到了新系统诸多的安全优势，这些优势将带来更具吸引力的业务需求。&lt;/p&gt;

&lt;h4 id=&quot;构建自助服务接口&quot;&gt;构建自助服务接口&lt;/h4&gt;
&lt;p&gt;我们为Moira建立了一个自定义门户（相对昂贵），但通常有更便宜的选择。Google的许多团队使用版本控制来管理和配置他们的服务，以拉取请求（称为更改列表或CL）的形式处理请求。这种方法几乎不需要服务团队的参与，但为我们提供了代码审查和持续部署的优势，便于验证、测试和部署内部服务配置更改。&lt;/p&gt;

&lt;h4 id=&quot;从人工支持的界面开始&quot;&gt;从人工支持的界面开始&lt;/h4&gt;
&lt;p&gt;在几个方面，Moira团队采用了“幕后工程师”的方法，将自动化与工程师的人工操作相结合。共享请求在路由过程中出现bug，我们的自动化在处理请求时会及时更新。系统还会通知到终端用户，提醒他们解决类似的共性问题。工单可以作为自动化系统的应急的GUI：它们保存工作日志，更新利益相关者的数据，并在自动化出错时提供简单的人工干预机制。在我们的示例中，如果用户需要获得数据迁移工作的帮助，或者如果自动化无法处理其请求，则该错误会自动路由到SRE手动处理的队列中。&lt;/p&gt;

&lt;h4 id=&quot;零接触的自动化&quot;&gt;零接触的自动化&lt;/h4&gt;
&lt;p&gt;自动化系统要求请求合规。Moira的工程师选择重新调整我们的自动化，以专门处理共享的边缘场景请求，或者删除/修改不合格的共享以符合系统的期望。这使我们能够在大多数迁移过程中实现零接触的自动化。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;有趣的事实：在谷歌，通过改变现实去适应代码而不是通过修改代码去适应现实的方式被称为“购买侏儒”。这句话源自一个关于Froogle的故事，Froogle是一个很早就开展的购物搜索引擎服务。
在Froogle的早期阶段，发生过精确匹配搜索“跑鞋”关键字导致返回了garden gnome(花园侏儒, 穿着跑鞋）的严重错误。在几次尝试修复错误失败之后，有人注意到gnome不是批量生产的商品，而是一个带有“一口价”选项的eBay商品。他们购买了这个“花园侏儒”商品后，解决了这个返回错误搜索结果的问题（译者注：相比起修改代码，以更低的成本解决了问题）。（图6-10）。
&lt;img src=&quot;/blog/something/images/SRE/6-10.jpg&quot; alt=&quot;&quot; /&gt; &lt;center&gt;
图6-10. 不会消失的花园侏儒 &lt;/center&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;推动新系统的使用&quot;&gt;推动新系统的使用&lt;/h4&gt;
&lt;p&gt;寻找方法来推动新用户采用更好的替代方案。在这种情况下，Moira需要升级系统配置以便应对新的请求和下线旧的系统。在服务设置中，新系统使用最佳实践和用户如何配置系统也很重要。Google团队经常使用codelabs或cookbook为用户提供常见用例设置和指导如何使用他们的服务。因此，大多数用户入手都不需要额外的团队的指导。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;
&lt;p&gt;与生产服务运行相关的琐事会随着系统复杂性和规模的增长而线性增长。自动化通常是消除琐事的黄金法则，并且可以与其他策略相结合。即使一些琐事没必要完全自动化，你也可以通过部分自动化或改变业务流程等策略来减少操作的负担。本章中描述的消除琐事的模式和方法可以推广到其他各种大规模生产服务中。消除琐事可以节省工作时间，以便工程师专注于服务的更重要的方面，并允许团队将手动任务保持在最低限度。随着现代服务架构的复杂性和规模不断增加，此策略尤其重要。&lt;/p&gt;

&lt;p&gt;但是要注意，消除琐事并不总是最好的解决方案。如本章所述，你应该考虑成本，设计、改造和实施自动化解决方案都需要投入成本。一旦决定减少琐事，就必确立目标，进行投资回报率（ROI）分析，风险评估和迭代开发来确定是否减少了工作量。&lt;/p&gt;

&lt;p&gt;琐事通常是从小事开始积累的，并且可以迅速成长最终消耗整个团队的人力资源。SRE团队必须坚持不懈地消除琐事——因为即使减少琐事的项目看起来令人生畏，但其好处通常也是会超过成本的。我们所描述的每个项目都需要各自团队的坚持不懈和奉献精神，他们有时会面对质疑或者需要与制度进行斗争，并且总是面临竞争这种高优先级的任务。我们希望这些案例鼓励你识别工作中的琐事，量化它，然后努力消除它。即使今天不能开展一个大项目，你也可以从一个小概念开始，这可以帮助改变你的团队处理琐事的方式。&lt;/p&gt;</content><author><name>ZX</name></author><category term="《SRE-Google运维实践》" /><summary type="html">Google SRE的大量时间用于系统优化，通过工程化的方法，与开发一起协同努力，追求卓越。哪怕是很少的性能收益也是值得的。但优化范围不仅局限于服务器资源，SRE的工作耗时也是优化的范畴。首先，SRE的工作不是琐事（关于琐事请参阅《SRE：Google运维解密》第5章内容）。本章我们将琐事定义为：与维护服务相关的，重复的、可预测的、持续的任务流。 对于产品运维团队来说，琐事不可避免。运维不可避免地需要处理部署、升级、重启、告警等工作。如果没有系统的方法，这些工作很快将耗尽整个团队精力。Google将SRE团队日常操作的耗时占比限制在50%以内（包括琐事和非劳动密集型工作。这样做的原因，请参阅《SRE：Google运维解密》书中第5章内容）。虽然这个目标可能不适合所有团队，但花费在琐事上的时间上限仍然很重要，因为识别和量化琐事是团队时间优化的第一步。 琐事的定义 琐事往往具有如下特征：这在我们的上一本书中有所阐述（《SRE：Google运维解密》译者注）。在这里，我们列举出琐事的特征，并给出了一个具体的例子加以解释： 手动性: 当web服务器上的/tmp目录磁盘占用率达到95%时，工程师Anne登录到服务器，在文件系统中查找并删除了无用的日志文件。 重复性: 写满/tmp目录的事情不太可能只发生一次，因此我们需要反复处理。 可以被自动化: 假设修复文件的工作包括如下几个步骤：“X登录，执行此命令，检查输出，执行命令，并通过命令的输出来判断是否需要重启Y”。这些指令流本质上就是伪代码！在上面的例子中，解决方案实际上已经可以部分自动化了。如果不需要人来运行脚本，可以自动化的检测故障并修复是再好不过了。更进一步，我们可以提交一个补丁使软件不再因为文档损坏的问题而中断。 非技术性: “磁盘写满”和“服务宕机”之类的告警会分散工程师的注意力，从而忽略高价值的事情，并可能掩盖其他更严重的告警。大量类似的告警造成的后果会波及到服务的健康状况。 没有持续的价值: 完成一项任务会带来一种令人满意的成就感。但长远来看，这种重复的满足感不能给工程师带来持续的价值。比如，处理告警能够确保用户查询持续进行；确保HTTP请求状态码小于400，以便可以让应用提供持续的服务，这些固然很好。然而，今天解决的问题并不能防止将来不再出现类似的问题，所以这样做的回报只是短期的。 与服务同步增长: 许多业务工作量的增长速度与基础设施规模的增长速度一样快(或许更快)。例如，你花费在修复硬件故障的时间会随着服务器集群规模的增加而增加。但请注意，相关的辅助任务(例如，软件/配置更改)不一定是这个趋势。 我们并不能将带来琐事的原因规范化和标准化，但是我们需要知道琐事的一些的特征。除上述特征外，还要考虑某项工作对团队士气的影响。人们是乐于完成一项觉得会有回报的任务？还是会处理无益的琐碎和无聊的任务？答案显而易见，琐事会慢慢地降低团队士气——时间往往花在琐事上而不是花在批判性思考或者是表达创造力上了；只有减少琐事，工程师才能更好地将时间用于思考和进行创造的领域。 案例：人工处理琐事 作者：John Looney，Facebook资深 SRE 哪些工作内容是琐事，通常是模糊的。一个“创造性”的解决方案，可能使问题得到最优解决，因此，SRE团队应奖励那些分析根因并解决问题的人，而不是那些掩盖问题的人。 我加入Google后的第一个任务（2005年4月）是追查一批机器死机原因并修复。如果确认是硬件原因，则转交给硬件技术人员维修。这个任务并没有看似那样的简单，因为我需在截止日期前处理超过20,000台机器。 第一台机器死机原因是：Google网络驱动补丁不断打印毫无意义的日志，导致文件系统的根目录写满，类似的一千台机器都是同样的问题。 我和同事沟通了解决这个问题的方案：编写一个脚本，ssh到所有异常机器，如果根目录已满，则清空/var/log中大文件日志，并重启syslog。我的同事对此方案不认可，他说最好找到根因并修复。如果掩盖了问题，在后续一段时间内，可能会引起更多严重性问题。 理论上，每台机器每小时的成本约为1美元。我的想法是，成本是运维工作很重要的衡量指标，应该高优让机器提供服务，利用起来。但我没有考虑的是：如果只是解决了这个表象，就没有机会去追查根因。 在高级工程师指导下，我翻阅了内核源码，找到导致此问题的可疑代码，并且记录了bug，帮助内核团队完善了他们的测试用例。从成本来看，解决这个网络补丁问题，每花费一小时，Google将为此付出1,000美元。 那天晚上就发布了新的内核版本，第二天我就把它升级到所有受影响的机器，内核团队在第二周更新了他们的测试用例。这个问题的处理，我很满意，因为找到了根因并成功修复，而不是每天上班后清理日志。 琐事的度量 运维工作是辛苦的。如果你做了一些工作减少了琐事，如何知道你的努力是成功的？许多SRE团队是结合经验和直觉来回答这个问题。经验和直觉会产生好的效果，但是我们还可以将方法上升到一个理论的维度。 经验和直觉是因人而异、非客观的。根据场景的不同，琐事的定义也不同。比如，同一团队的不同成员会根据工作的投入产出比来判断一件事情是否可以定义为琐事。此外，为了减少琐事所做的工作可能会持续几个季度甚至几年的时间(本章的一些案例研究就证明了这一点)，在此期间团队的人员主要任务可能会发生改变。所以，为了保证减少琐事的工作能够长期进行，一般的，团队必须从几个确定的琐事中选择一个琐事来消灭它。我们应当将这件事上升为一个项目，并且需要建立起这个项目的长期的客观的度量机制以保证投入得到回报。 在启动项目之前，重要的是分析成本与收益，并确认通过减少琐事所节省的时间(至少)与第一次开发和维护自动化解决方案所投入的时间成正比(图6-1)。从节省的时间与投入的时间的简单比较来看，那些看起来“无利可图”的项目可能仍然值得进行，因为自动化有许多间接或无形的好处。潜在的好处包括： 随着业务规模扩大，收益越明显 提高团队士气，减少团队流失和成员的厌倦情绪 更少的中断性工作，从而提高团队工作效率 提高流程清晰度和标准化 增强团队成员的技术技能和拥有更全面的职业发展 缩短新成员的培训时间 减少人为错误导致的问题 提高安全性 缩短用户投诉的响应时间 图 6-1 预测在减少琐事工作上花费的时间，并确保其收益大于投入 琐事的度量方法 识别它。第一本SRE书的第5章提供了如何识别琐事。最能够识别琐事的人取决于团队本身。理想情况下，SRE团队既是利益相关方，也是实际操作方。 选择适当的计量单位来量化人力成本。我们可以选择“分钟”或者“小时”这么一个客观和普遍能够理解的计量单位。务必还要考虑琐事转自动化的成本。有些人力成本具有分散性和碎片化的特征，所以我们从成员工作的内容来衡量更为合适。度量单位应该要能够很好的度量如下工作：为应用增加的补丁，完成的票证，手动生产环境的变更，电子邮件交换或者是一些对硬件的操作。总的来说，只要度量单位客观，一致且易于理解，它就可以作为工作的衡量标准。 在项目的整个周期内我们需要连续跟踪并记录度量的指标。我们可以使用工具或脚本来简化度量指标的测量过程，使得收集这些测量值不会产生额外的工作。 琐事分类法 琐事，就像一座摇摇欲坠的桥梁或一座漏水的大坝，日复一日地隐藏在广阔无垠的大地之中。本节中的分类并不能够详尽无遗，但代表了一些常见的琐事类别。这些类别中有许多类似“正常”的工作，但是它们实际上就属于琐事。 商业流程 这可能是最常见的琐事来源。也许你的团队管理一些计算机资源——计算、存储、网络、负载平衡器、数据库等，以及为该资源提供支持的硬件资源。你需要处理用户登录、配置修改和计算机安全维护、软件更新以及扩缩容。你还需要最大限度地降低成本避免计算机资源的浪费。你的团队是计算机的人机界面，通常与为其需求提交票证的内部客户进行交互。你的组织甚至可能拥有多个票务系统和工作系统。 票务系统属于“隐藏”一类的琐事，因为其驱动的业务流程通常是我们需要完成的目标。用户得到了他们想要的东西，并且因为琐事往往分散在整个团队中，所以琐事并不能明显地显现出来。在以票据驱动的任何地方，都有可能悄悄地积累这琐事。即使你没有明确的自动化流程，仍然需要执行流程的改进工作，例如简化流程，使其未来更容易做到自动化，同时更加容易管理。 工作中断 中断是一类为了保证系统运行的时间敏感类任务，简单理解为被其他紧急事情打断。例如，你可能需要通过手动释放磁盘空间或重新启动泄漏内存的应用程序来解决某些资源（磁盘，内存，I/O）的严重短缺。你可能正在提交更换硬盘驱动器，“踢”出无响应的系统或手动调整容量以满足当前或预期的负载请求。通常，中断会将注意力从更重要的工作上移开。 流程监督 在许多组织中，部署工具从发布到生产需要SRE进行监督。即使有自动化，全面的代码覆盖，代码审查和多种形式的自动化测试，这个过程并不总是顺利进行。根据工具和发布节奏，发布请求、回滚、紧急补丁以及重复或手动配置更改，发布仍产生琐事。 服务迁移 服务迁移也是我们经常要处理的一类事情。你可以手动或使用有限的脚本来执行此工作，而且希望只迁移一次。迁移有多种形式，包括有数据存储、云供应商、源代码控制系统、应用程序库和工具的更改。如果你手动迁移大规模的工程，迁移很可能涉及到“琐事”。对于大规模的迁移，你可能倾向于手动执行迁移，因为这是一次性的工作。并且我们甚至会将其视为“项目”的一部分而非“琐事”，但迁移工作的很多特征与“琐事”的特征是吻合的。从技术上讲，修改一个数据库的备份工具以便与另一个数据库可以协同工作是软件开发的范畴，但这项工作本质上只是重构代码，用一个接口替换另一个接口。这项工作是重复的，并且在很大程度上，备份工具的业务价值与之前是相同的。 压缩成本和容量规划 无论是拥有硬件还是使用基础架构提供商（云），压缩成本和容量规划通常是一些劳动密集型的工作。例如： 在计算、内存或IOPS（每秒输入/输出操作）等资源的未来规划中要确保成本效益和突发情况的扩容能力。这可能转化为采购订单，AWS预留实例或云/基础设施即服务合同协商。 应对（并从中恢复）关键的高流量事件，如产品发布或者遇到假期。 排查下游和上游服务水平和容量情况。 根据专有云服务产品的计费细节优化应用程序（适用于AWS的DynamoDB或适用于GCP的Cloud Datastore）。 重构工具以便更好地利用现有资源。 处理超预算的资源，无论是基础设施提供商的上游还是与下游客户之间。 黑盒系统故障排除 分布式微服务架构现在很常见。随着系统更加分散，出现了新的故障模式。团队可能没有能力来构建复杂的分布式跟踪，高可靠监控或详细的仪表盘。即使企业确实拥有这些工具，它们也可能不适用于所有系统。故障排除甚至可能需要登录到各个系统并使用脚本工具来对日志进行实时地查询分析。 故障排除本身并不是坏事，但你应该把精力集中在新的故障模式上，而不是每周都发生的由脆弱系统架构导致的故障。随着可用度为“P”的新关键上游依赖性服务的上线，系统可用性将下降（1-P）倍。一个可用度为4个9的服务增加了9个关键的4个9的核心组件，现在就变为了是一个三个9的服务。 琐事管理战略 任何规模的生产系统，琐事管理都是至关重要的。一旦确定并量化了琐事，消除琐事的计划就要提上日程。这个工作可能需要数周才能完成，因此制定一个完善的计划是至关重要。首先，从源头上消除琐事是最佳的解决方案，但是对于源头上无法消除的琐事，则需要通过其他方式来消除。在我们深入研究两个案例之前，本节提供了此方面工作的通用性准则。正如下文的两个案例中提到的，琐事的细微差别是因团队而异。但无论如何，一些常见的准则是适用于任何规模或风格的组织。在后续案例中将以具体方式诠释每种策略。 琐事的识别与度量 采用数据驱动的方法来识别琐事，并配合客观的成本控制策略，获得此类项目最优的投入产出比。如果你的团队正在被琐事缠身，并将减少琐事作为了一个长期的项目。Google SRE团队根据多年的经验，在控制项目投入产出比方面是一个不错的借鉴。有关技术和指导，请参见第96页的“量化琐事”一节。 让SRE从琐事中解脱出来 减少琐事的最佳策略是从源头杜绝琐事。在进行系统设计和为生产环境制定流程之前，工程师要优化产品和系统来减少甚至消除琐事。 真正了解生产环境痛点和知道导致系统出现琐事原因的那部分人正是SRE，因为只有他们和生产环境紧密联系。SRE应该在与产品开发团队合作的过程中，将自己的运维经验与产品开发团队共享从而开发出人机交互友好型的软件，从源头减少琐事，并且使产品具有更好的扩展性和弹性。 拒绝琐事 一个被琐事缠身的团队应该尽早的做出“消除琐事”决策。第一种策略是对琐事说“不”！对于每个琐事，量化它并以此为原则决定是否要做，但是根据Google的经验，这一种策略可能会适得其反。另一种策略是故意拖延这些琐事，直到我们可以通过批处理或并行处理来解决它。将琐事集中在一起一并处理它们，这种方式可以减少工作中的中断，并帮助你们识别琐事的特征，并将它们作为下一个消除目标。 使用SLO减少琐事 如第2章所述，服务系统应具有文档化的SLO。明确定义SLO才能使工程师做出明智的决策。例如，如果某项工作即使做也不会减少服务的错误预算，你就可以考虑忽略某项工作。随着服务的增长，专注于整体服务的可用性而不是单个设备的SLO，这样做是非常有利的，也是可持续的。有关编写有效SLO的指导，请参阅第2章。 从部分自动化开始 如果你的业务特别复杂，请将“部分自动化”方法视为实现“完全自动化”的临时步骤。在这种方法中，你的服务通常可以通过定义的API接收结构化数据。工程师也可以进行一些操作从而得到想要的结果。虽然这样做需要一些手动的操作，但是这种“幕后工程师”方法是逐步实现全自动化的前提。使用“客户端输入”来统一收集数据；通过确定的请求格式，你可以更容易的以编程的方式对请求进行处理。这种方法让客户也能够明白你需要的信息和指标，并在你完全理解系统服务之前避免使用大型的解决方案而产生的未知问题。 提供一种自助的服务方法 一旦你们提供了交互型界面的服务产品，请进一步的为用户提供自助式的服务方法。你可以提供Web表单、二进制、脚本、API，甚至只是告诉用户如何向服务的配置文件发出拉取请求的文档。例如，软件开发工程师要求SRE工程师为其开发工作配置新虚拟机，我们为他们提供一个简单的Web表单或脚本来触发配置，而不是让他们提交相关票证来进行这件事。如果发生了特殊的情况，我们也允许使用“票证”的方式替代自助的服务，这是可接受的。部分自动化是一个良好的开端，但服务SRE工程师应该始终要致力于尽可能让服务自动化起来。 获得管理层和同事的支持 在短期内，减少琐事的项目需要投入人力成本，反之会减少处理其他日常任务的人员数量。但长远来看，如果项目达到了减少琐事的目标，团队将更加健康，并有更多的时间进行更重要的工程改进。对于团队中的每个人来说，“减少琐事”作为一个共同的价值目标是很重要的。管理层的支持对于减少工程师的干扰是至关重要。制定琐事评估的客观指标来说明项目的推进情况可以让管理层更加支持项目的进行。 减少琐事作为提高服务稳定性一部分 要为减少琐事的项目创建一个强大的业务案例支持，将你的目标与其他业务目标相结合。如果有一个补充性的目标，例如，安全性、可扩展性或可靠性——这对客户来说是具有吸引力的，他们会更愿意放弃当前充满琐事的系统，转向更加亮眼的新系统。这样来看，减少琐事也可以提高用户服务的质量，这也是另一个角度来看待琐事的认识。 从简单的琐事开始并持续改善，不要试图设计没有琐事的系统。面对一个充满琐事的系统，首先自动化一些高优先级的项目，然后通过评估这个项目所花费的时间来改进你的解决方案，总结获得的经验和教训。在项目开始之前，选择一个明确的指标，如MTTR（平均修复时间）来评估你的项目的进展和效果。 提高系统的一致性 从规模上看，多样化的生产环境是难以管理的。特殊的生产环境容易出错，管理能力会降低，事故的处理能力也会降低。你可以使用“宠物与牛”方法（https://www.engineyard.com/blog/pets-vs-cattle，译者注）来添加系统冗余并在你的生产环境中实施增加一致性的策略。是否选择“牛”取决于组织的需求和规模。将网络链路、交换机、机器、机架，甚至整个集群评估为可互换单元也是合理的。将设备转换为“牛”的理念可能会带来较高的初始成本，但会减少中长期的维护成本，增强灾难恢复能力和提高资源利用能力。为多个设备配置相同的接口意味着它们具有相同的配置，是可互换的，维护成本也就降低了。各种设备的界面一致（转移流量，恢复流量，执行关机等）使系统更加灵活和更加可扩展。Google鼓励各团队将不断发展的内部技术和工具进行统一，并有相应的鼓励机制。无论团队用什么样的方法，但他们不得不承认一些不受支持的工具或遗留的系统是产生琐事的根源。 评估自动化带来的风险 自动化可以节省人力成本，但是也会出现未知的错误，严重时会造成停机。一般情况下，防御性软件可以控制这类事情的发生。当管理级别的行为被自动化之后，防御性软件会显得至关重要。在执行前应对每项行为的安全性进行评估。在实施自动化时，我们建议采用以下做法： 防御性地处理用户输入，即使这个输入来自于上游的系统 ——换句话说，要对上下游的输入进行仔细的校验。 构建告警机制，使得工程师可以接收到相关告警以进行处理。安全措施可能与命令超时一样简单，也可能是对当前系统指标或当前中断次数的更复杂检查。因此，监控，报警和仪表系统应由机器和操作人员共同使用。 请注意，即使是简单的读取操作也可能会导致设备负载过高和触发服务中断。随着自动化的扩展，这些安全检查的工作量是可控的。 最大限度地减少因自动化安全检查不完整导致服务中断的影响。如果操作员遇到不安全的情况，自动化操作应该默认为人工操作。 琐事自动化之后要做什么 一旦你可以将一个工作自动化后，这个自动化的工作就值得更深层次的被发掘。进一步的将自动化的任务按照人工处理的流程优化下去。但请注意，自动化不应该让工程师认为任务不会出错。在完成上述优化后，你还可以尝试将自动化的工作分解为可单独实现的组件，并用于创建可组合的软件库，其他自动化项目可在以后重复使用。正如下文中的“数据中心维修案例”研究所示，自动化提供重新评估和简化人工工作流程的机会。 使用开源和第三方工具 有时你不必做所有的工作来减少琐事。像一次性迁移这样的工作可能自己无法建立定制型的工具，但你可能并不是第一个遇到这个任务的工程师。寻找第三方或开源库以降低开发成本，或者说，至少可以帮助你过渡到部分自动化。 反馈并改进 积极寻求反馈，这些反馈可以来自于工具、工作流程和自动化交互相关的其他人，这是非常重要的。你的用户将根据他们对底层系统的理解将你的工具在不同使用情景下进行使用。你的用户对这些工具越不熟悉，就越要积极地寻求用户的反馈。利用用户调查，用户体验（UX）和其他机制来了解你的工具被如何使用，并整合这些反馈，以便在未来实现更有效的兼容性。 人的输入只是你应该考虑反馈中的一个方面。我们还可以根据延迟，错误率，返工率和节省的人工时间等指标（跨过流程中涉及的所有组）来衡量自动化任务的有效性。能够获得在自动化工作部署之前和之后两种状态的对比是最明确的衡量方式。 扩展：历史遗留系统 大多数SRE工程师在他们的工作中都会遇到过历史遗留系统。这些旧系统经常在用户体验，安全性、可靠性或可伸缩性方面有问题。他们倾向于将遗留系统看作一个神奇的黑匣子，因为系统“大部分组件是在工作中的”，但很少有人了解它们是如何工作的。贸然的调整它们是可怕的，也是昂贵的，并且保持它们的运行通常需要大量繁琐操作步骤。 远离遗留系统通常遵循以下路径： 避免：我们可以为不去解决这个问题找到许多理由：可能是没有资源来替换这个系统；判断业务成本和风险发现不值得替换；可能没有找到商业上更好的解决方案。避免选择的是接受风险并从SRE转向系统管理。 封装/扩充：你可以使用SRE来构建一个抽象API的外壳，自动化，配置管理，监视和测试这些遗留系统，这些系统将卸载SA的工作。遗留系统仍然很难改变，但现在你至少可以识别它并在适当时有回滚策略。这种策略仍然可以避免，但这是将风险引入到的更好的系统中。这通常是准备增量替换的权宜之计。 替换/重构：替换遗留系统可能需要大量的决心、耐心、沟通成本和文档，最好是逐步进行。一种方法是定义遗留系统公共接口。此策略可帮助你使用发布的工程手段，将用户缓慢、安全地迁移到其他安全的架构中。通常，遗留系统的“规范”实际上只是通过其历史用途来定义，因此有助于构建生产大小的历史预期输入和输出数据集，以建立新系统不会偏离预期行为的信心（或正在以预期的方式发散）。 退出/保管所有权：最终，大多数客户或功能被迁移到一个或多个系统。这个迁移需要有激励措施，没有迁移的用户让他们自行维护历史遗留系统，并承担相应责任。 案例研究 案例研究1：利用自动化减少数据中心的工作量 案例研究1中所应用的减少琐事的战略： SRE工程师从琐事中解脱出来 从部分自动化开始 提高系统的一致性 使用SLO减少琐事 评估自动化带来的风险 反馈并改进 提供一种自助的服务方法 背景 此案例来源于Google数据中心。与其他的数据中心类似，Google的计算机连接到交换机，交换机连接到路由器。流量通过链路流入和流出这些路由器，而链路又连接到互联网上的其他路由器。随着谷歌对互联网流量的要求越来越高，服务该流量所需的交换机数量也急剧增加。为了能够应对大流量的情况，我们的数据中心在规模和复杂性方面都有所增长。这种增长迫使数据中心改变了手动维修的旧方法。（从偶尔和有趣到频繁和沉闷的转变。） 早期，谷歌在运行数据中心时，每个数据中心的网络拓扑都只有少量的网络设备，可以管理大量服务器的流量。单个网络设备故障可能会显著影响网络性能，但是一个小规模的工程师团队就可以处理设备的故障。早期，工程师调试故障设备并手动将流量切换到其他正常组件。而我们下一代的数据中心拥有更多的机器，并引入了折叠Clos拓扑结构的软件定义网络（SDN），交换机数量显著增加。图6-2展示的是一个小型数据中心Clos交换机网络的流量复杂情况。如果将这个比例放大，意味着设备数量更多，发生故障的组件也更多。虽然可以说，每个单独的故障对网络性能的影响比以前更小，但是大量的问题同时并发也会压倒工程师们。调试问题的过程同时也会引入大量新的问题，复杂的布局也让工程师感到困惑：需要检查哪些链接？需要更换哪个线卡？为什么是Stage 2开关，而不是Stage 1或Stage 3开关？关闭交换机会给用户带来哪些问题？ 图6-2. 一个小型Clos网络，Stage1 支持480台机器连接 修复故障的线卡是一个随着系统网络增长而任务量不断增长的琐事，因此我们将此作为“数据中心网络修复自动化”项目的第一阶段的目标。本案例阐述了我们如何在第一代线卡（名为Saturn）系统上开始自动化修复的过程，并以此为基础，我们讨论了如何对自动化工作进行改进以适应下一代线卡(Jupiter光纤网络)。 如图6-3所示，在自动化项目开始之前，数据中心线卡修复工作需要工程师执行如下几个操作： 确定从故障交换机切走流量是否是安全的。 切走流量至其他交换机（“drain”操作）。 执行重启或修复（例如更换线卡）。 将流量切回至该交换机（“undrain”操作）。 Drain，更换线卡，undrain的工作是不变和重复性质的，是“琐事”的典型范例。这些重复性的工作本身就会带来一些问题——例如，工程师在处理此类故障时会并行处理其他更有挑战性的工作，分心的工程师可能会意外地将未配置的交换机加入网络。 图6-3. 自动化之前的数据中心（Saturn）线卡修复工作流程：所有步骤都需要手动工作 问题陈述 数据中心修复线卡问题具有以下几个维度： 团队规模增长的速度跟不上系统增长的速度（故障数量也在增长），使得我们无法快速解决问题以防止对系统带来负面影响。 人为错误一定会在重复执行的步骤中发生。 并非所有线卡故障的影响都是一致的。我们没办法对线卡故障划分优先级。 一些故障是暂时的，这时我们会选择直接重新启动线卡或重新安装交换机作为修复过程的第一步。并且，我们可以用编程方式捕获这些问题，如果它再次发生，则进行设备替换。 新的拓扑环境要求我们在采取行动之前手动评估隔离容量的风险。每次的人工风险评估都有可能带来人为错误，并可能带来严重影响。系统工程师和技术人员也没有好的方法来判断有多少设备和链接会受到修复过程的影响。 我们要如何解决这个问题？ 为达到最好的效果，我们决定创建一个与现场技术人员配合使用的自动化框架，而不是把每个问题分配给工程师，让其进行风险评估，流量切换，维修和验证等人工操作。 自动化的第一步：Jupiter光纤网络的修复自动化 我们的最终目标是构建一个能够代替工程师分析和处理故障的网络设备故障检测系统。我们的程序是直接切换流量并告知工程师，而不是向工程师发送“线卡”故障的告警。新系统有一些值得注意的特点： 我们最好是利用现有工具。如图6-3所示，我们的告警已经可以检测到线卡上的问题; 所以我们可以配置告警以触发自动修复。新的工作流程还应改变工单系统，以便支持自动提交的维修请求。 我们建立自动风险评估的机制，以防止在流量切换期间意外的隔离设备，并在需要时触发安全机制。此机制可以杜绝人为错误。 我们编写程序用于跟踪告警以便作出不同的处理操作：第一次告警仅重启该线卡并重装了软件；第二次出现告警则直接请求更换线卡并告知供应商。 执行自动化操作 新的自动化工作流程（如图6-4所示）进行如下： 检测到有问题的线卡，并将故障特征添加到数据库中。 维修服务组件会解决问题并对交换机进行维修。该服务还会执行风险评估以确认操作不会隔离任何容量，然后： a. 从故障交换机中切出流量。 b. 关闭线卡。 c. 如果这是第一次告警，则重新启动线卡，将流量恢复到此交换机。此时，工作流程已完成。 d．如果这是第二次失败，则工作流程进行到步骤3。 流程管理器检测到新案例并将其发送到故障维修池，供系统工程师处理。 系统工程师对故障做出响应，在UI界面中看到红色的“停止”（表示在开始修理之前需要切走流量），并分三步执行修复步骤： a. 系统工程师通过UI界面中的“准备组件”按钮启动流量切换。 b. 流量切换完成后表示交换机可操作。 c. 关闭交换机并维修线卡。 自动修复系统再次启动线卡。完成修复后，启动交换机，待初始化后，流程管理器会触发恢复操作，切回交换机流量并结算故障工单。 图6-4.具有自动化功能的Saturn线卡维修工作流程：只需按下按钮即可完成更换线卡等全部手动工作 新的自动化系统将团队从大量的琐事中解放出来，使他们有更多时间在其他地方开展更高效的项目：使用下一代Clos拓扑结构Jupiter。 自动化项目的第二步：Saturn线卡修复与Jupiter线卡修复 数据中心的容量需求几乎每12个月翻一番。因此，我们的下一代数据中心结构Jupiter比Google以前的任何数据中心的六倍还要大，所以故障的数量也会增加六倍多。Jupiter提出了自动化故障修复的挑战目标，这个目标的难度在于每层的数千个光纤链路和数百个线路卡都可能出现故障。幸运的是，随着潜在故障点的增加系统也会伴随增加更多的冗余，这有利于我们完成自动化任务。如图6-5所示，我们保留了系统的一些常规工作流程，并添加了一些重要的修改： 在自动切流量和关闭交换机之后，确定我们要更换的硬件，将硬件故障单发送给系统工程师。在这个过程中，切流量的行为是自动的，不需要系统工程师手动按下“预备按钮切流量开关”来完成。 我们添加了自动化，用于安装和推送组件更换后的配置。 我们启用自动化功能，以便在切回流量之前验证修复是否成功。 除非绝对必要，否则我们更关注的是如何恢复流量而不用人为介入。 图6-5.左图为Saturn线卡宕机自动化流程，右图为Jupiter线卡宕机自动化流程 项目实现 我们为Jupiter交换机上的所有的线卡故障采用了简单而统一的工作流程：操作通报，流量切换，开始修复。 自动化执行如下： 检测到交换机故障，并将故障特征写到数据库。 维修程序开始修复交换机：停止使用交换机，并将停止原因写到数据库中。 a. 如果这是六个月内的第二次故障，请执行步骤4。 b. 否则，请执行步骤3。 尝试（通过两种不同的方法）重启交换机。 a. 如果重启成功，用自动化服务检查健康状态，然后安装并配置交换机使其投入使用；删除修复原因，删除数据库中的故障记录。 b. 如果健康检查失败，请升级给技术人员。 如果这是第二次故障告警，请将故障案例直接升级给技术人员，向其申请新的硬件设备。硬件更新后，用自动化服务检查健康状态，然后安装并配置交换机使其投入使用。删除修复原因，删除数据库中的故障记录。 这种新的工作流程管理完全重写了以前的修复系统。同样的，我们要尽可能利用现有工具： 配置新交换机（安装和验证）的操作与验证已更换的交换机所需的操作相同。 快速部署新的硬件需要以编程的方式进行BERT和cable-audit的能力。在恢复使用之前，我们可以使用该程序在已经修复的链路上运行功能测试。这些测试需要能够识别错误链接以进一步提高修复的效果。 下一步要提升的是自动缓解并修复Jupiter交换机线卡的内存错误。如图6-6所示，在开始自动化修复之前，此工作流程在很大程度上取决于工程师来判定故障是硬件导致还是软件导致，然后再停止使用，重启交换机等工作，并适时地安排修复。 图6-6. 自动化之前的Jupiter内存错误修复工作流程 我们的自动化过程不再尝试对内存错误进行故障排除从而达到简化修复工作流程的目的（请参阅第119页的“有时不完美的自动化就足够了”，了解为什么这样做是有意义的）。相反，我们处理内存错误的方式与处理线卡故障的方式相同。为了将自动化覆盖到内存错误引起的故障，我们只需在配置文件中添加一个特征，使其对新的故障类型起作用。 图6-7描述了内存错误的自动化工作流程。 经验教训 在我们致力于实现网络故障自愈的这些年里，我们学会了如何有效减少琐事。 UIs 不该引入开销和复杂度 替换一块Saturn-based线卡需要切走整个交换机的流量。等待备件更换以及工程师支持的时候，过早地执行全部切换操作意味着失去所有线卡的工作能力。我们在UI中增加一个“准备组件”的按钮，以允许技术人员在更换线卡前执行整个交换机的切换流量操作，从而消除了交换机不必要的停机时间（请参阅“按下准备按钮” 切出流量的开关“见图6-5） UI和维修工作的流程引入了许多非预期的问题 按下切走流量的按钮后，技术人员无法得到流量切换进度的反馈，只能在结果返回后才能进行下一步操作。 该按钮可能无法反馈真实的状态。造成的结果是，有时切流量开关出问题但并没有被维修，或者技术人员可能通过其他方式中断了进程但是并没有告知系统。 问题出现时，非自动化的组件反馈了一个通用的‘contact engineering’信息。经验不丰富的技术人员无法快速找到可以提供帮助的人，而联系上的工程师并不总能够立即解决问题。 为快速对用户反馈以及因功能复杂性带来的回归问题进行响应，我们设计了更完善的工作流程，来保证按钮的安全性和可用性。 不要依赖人的经验 我们过分依赖有经验的数据中心技术人员来识别系统中的错误（例如，当程序认为可以安全地进行维修，但实际上交换机并没有完成流量切换的动作）。这些技术人员在没有自动化提示的情况下，还必须手动执行多项工作。 经验是难以复制的。在一个复杂的情节中，技术人员在等待数据中心维修时，决定启动并发切换来快速进行“按下按钮并等待结果”的操作，从而导致了网络拥塞和用户可见的数据包丢失。我们的软件无法预测并阻止这种行为，因为我们并没有测试过这种自动化。 设计可重复使用的组件 尽可能避免采用集成化设计。使用组件来构建复杂的自动化工作流，每个组件处理一个独特且定义明确的任务。我们可以轻松地重复使用或调整早期Jupiter自动化的关键组件来用于下一代的软件设计，并且很容易针对已经存在的自动化项目增加新的功能。Jupiter类结构的连续变体可以采用早期已经完成的工作。 不要过分分析问题 我们过度分析了Jupiter线卡内存错误问题。我们试图进行精确的问题诊断，我们想区分软件错误（可通过重新启动修复）与硬件错误（需要更换卡），并识别影响流量的错误与未发生的错误。我们花费将近三年（2012-2015）的时间来收集超过650个离散内存错误的数据，然后才意识到这个分析是过头了，或者至少不应该阻塞我们自动化项目的开展。 一旦我们决定对检测到的任何错误都采取必要的措施，就可以直接使用我们现有的自动化修复技术来实现简单的切换策略、重启以及为修复内存错误而重置交换机。如果问题再次出现，我们可以认为，故障很可能是基于硬件的，并立即要求更换组件。我们花费了整个项目四分之一的时间来收集数据，发现大多数的错误是暂时的 ——大多数交换机在重新启动和重新安装后都恢复了。我们不需要额外的数据来执行修复，因此为了实现这种自动化花费了三年是没有必要的。 有时不完美的自动化就已经足够 解除链路之前，通过BERT很容易确认链路状况，但BERT工具不支持网络管理链路。我们将这些链路添加到现有的链路修复自动化中，并允许跳过验证。我们很愿意绕过验证，因为链路并没有承载客户流量，如果验证结果很重要，我们可以稍后添加此功能。 保证维修自动化项目的持续性和可继承性 自动化项目可以有很长的生命周期，需要确保人员的流动不会干扰项目的连续性。 新人工程师应该接受现有系统的培训，以便他们能够修复错误。由于Jupiter线卡部件的短缺，Saturn-based系统在其目标寿命结束后很长一段时间内还是存在的，这要求我们日后在Saturn的生命周期中进行一些改进。 自动化一旦被采用，在很长的一段时间内将会被依赖使用，并伴随着一些积极和消极的后果。如果可能，以灵活的方式设计你的自动化程序。不灵活的自动化会使系统变更变得难以实现。使用基于策略的自动化可以明确地将意图与通用实现引擎分离，从而使自动化更加可持续的发展。 深入开展风险评估和防御措施 为Jupiter构建新工具以评估执行切流操作前的风险，而后由于问题的复杂性，我们需要在更深层次的防御上引入二次检查。二次检查设定了受影响链路数量的上限，以及受影响设备的额外限制。一旦超过任一限定值，便会自动触发追踪bug以请求更进一步的检查。我们不断地调整这些限制，以减少误报。最初我们认为二次检查只是一项临时措施，但是在主要风险评估平稳后，该措施已被证明可用于识别由于停电和软件错误导致的维修问题（如请参阅SRE中“自动化：在规模上实现失效”）。 失败预算和管理者支持 修复自动化有时会失败，尤其是在首次使用时。管理者的支持对于保护项目，并鼓励团队坚持不懈是至关重要的。我们建议为通过自动化技术消除琐事项目设置错误预算。你还需要向外部的其他利益方解释：尽管存在故障风险，但自动化极其重要，并可以持续提高可靠性和效率。 总结 最终，复杂场景的自动化是真正需要解决的问题。在引入自动化系统之前要反复对系统进行评审——是否可以先简化系统和工作流？ 要关注自动化工作流程的各个方面，而不仅仅是造成琐事的那部分。和直接参与项目的人员共同开展测试工作，并积极寻求他们的反馈和帮助。如果他们在使用过程中出现操作问题，要想办法使工作界面更清晰，或者增加额外的安全检查。确保自动化不会带来额外的琐事——例如开启了不必要的工单以引起人的注意。给其他团队创造问题将增加自动化推进的难度。 案例研究2：淘汰以文件为后端的Home directories 案例研究2中强调了减少琐事的方法： 考虑淘汰旧系统 将减少琐事作为一项工程 获得管理层和同事的支持 拒绝琐事 从部分自动化开始 提供一种自助的服务方法 从细微处开始然后改进 反馈并改进 背景 在谷歌的早期，公司数据存储（CDS）SRE团队为所有Google员工提供home目录服务。与企业IT中常见的Active Directory漫游配置文件类似，Google员工可以跨工作站和平台使用相同的home目录。CDS团队还为共享存储空间中的跨团队协作提供“团队共享”服务。我们通过NFS / CIFS（或“文件管理器”）上的Netapp存储设备提供home目录和团队共享。这种存储系统是很昂贵的，但Google员工对此类服务的需求是必须的。 问题陈述 随着时间的推移，这些文件管理系统解决方案的优势被其他更好的存储解决方案所超越：我们的版本控制系统（Piper / Git-on-borg），Google Drive，Google Team Drive，Google云存储以及全球内部共享分布式ilesystem（x20）。这些替代方案的优越性体验在如下方面： NFS / CIFS协议并不适用于在WAN上运行，这造成即使有几十毫秒的延迟，用户体验也会迅速降低。这也为远程工作人员或全球分布的团队带来了问题——因为数据只能存在于一个地方。 与替代品相比，原系统的设备运行和规模都是昂贵的。 要使NFS / CIFS协议与Google的Beyond Corp11网络安全模型兼容，需要做大量的工作。 与本章最相关的是，home目录和团队共享会频繁的使用。存储配置的许多方面都是ticket驱动的。虽然这些工作流程通常是研发人员编写的，但它们代表了相当数量的CDS团队的工作成果。我们花了很多时间创建和配置共享，修改访问权限，解决最终用户问题，以及执行启动和调整以管理容量。除了配置、更新和备份之外，CDS还需要管理专用硬件的配置，机架和布线过程。由于延迟要求，我们经常不得不部署在远程办公室而不是在Google数据中心 – 这有时需要花费团队成员相当长的时间。 我们决定做什么 首先，收集数据：CDS团队开发了一个名为“Moonwalk”的工具来分析员工使用此的服务的场景。我们确定收集如下通用的指标，如每日活跃用户（DAU）和月活跃用户（MAU），并询问了诸如“哪些用户实际使用他们的home目录？”和“哪些人每天使用此系统？他们最常访问的文件是什么？“Moonwalk与用户调查相结合，验证了文件管理器当前服务的业务需求可以通过低运营开销和成本的可替代方案代替。另一个引人注目的原因促使我们放弃现有的文件系统：如果我们可以将大多数文件管理器用例迁移到G Suite / GCP，那么我们可以利用我们学到的经验来改进这些产品，从而为其他大型企业迁移到GSuite/ GCP提供支持。 没有一种替代方案可以满足所有当前的文件管理器用例。然而，通过将问题转化为若干小的需求来寻找可替代系统，我们发现少数备选方案可以涵盖我们所有的使用场景。替代解决方案更专业，并且每个解决方案都带来比旧的解决方案更好的用户体验。例如： x20 全局共享静态文件对于团队来说是很好的方式，比如二进制文件。 G Suite Team Drive 适用于办公文档协作，与NFS相比，用户更能容忍此延迟。 谷歌的巨像Colossus文件系统 比NFS更安全，更可靠地共享大型数据文件 Piper/Git-on-Borg 可以更好地同步dotfiles（工程师的个性化工具首选项） 一种新的“历史服务”工具 可以托管跨工作站命令行的历史记录 在我们编制用例并找到替代方案时，旧文件系统的下线计划也已经开展 设计与实施 下线旧的文件管理系统是一项持续的、迭代的、需要多年的进行的工作。需要伴随多个子项目的开展： Moira home目录下线 Tekmor 迁移home目录用户的历史遗留数据 Migra 团队共享下线 Azog 下线home目录/共享基础架构和相关硬件 图6-8。Moira项目的四个阶段 本案例研究重点关注第一个项目Moira。后续项目的开展是在Moira的学习和开展的基础上开始的。如图6-8所示，Moira由四个阶段组成。 关键组件 Moonwalk 虽然我们有关于用户共享（例如共享大小）的基本统计数据，但我们仍需要了解用户的工作流程，以帮助用户即使在一片反对声中仍然可以做出有利于业务发展的决策。我们建立了一个名为“Moonwalk”的系统来收集和反馈这些信息。 Moonwalk存储了谁正在访问哪些文件以及何时使用BigQuery的数据，这使我们能够做出统计报告以便更好地了解用户。在BigQuery的帮助下，我们汇总了25亿个文件，共计300 TB的数据的用户访问模式。该数据来自于全球60个地理站点的124个NAS设备，共计600,000个磁盘卷，共收集了60,000名POSIX用户的使用信息。 Moira Portal 通过工单处理来完成home目录下线的想法，在我们庞大的用户基数面前看起来不太现实。我们需要在整个过程中（调查用户，告知项目下线原因，归档数据或迁移到替代系统）尽可能提供低接触服务（低接触服务是指这样的服务模式：销售服务人员在向顾客提供服务时，保持较少的面对面的接触机会。相对于高接触服务而言，低接触服务需要更多的机器和固定资产。因为通常需要由它们来自动完成顾客服务，如自动售货机、自动柜员机、自动加油机等，译者注）。我们的最终要求是： 描述项目的登录页面 不断更新对常见问题的解答 与当前用户共享关联状态和使用信息 提供请求，停用，存档，删除，扩展或重新激活共享的选项 至此，我们的业务逻辑变得相当复杂——因为我们必须考虑许多用户场景。例如，用户可能会从Google离职，短暂离职，或者在诉讼状态（需要保留其拥有的数据）。图6-9提供了一个示例图，说明了其复杂性。 图6-9。基于用户场景的业务逻辑 为主门户网站提供支持的技术相对简单。基于Flask框架下用Python语言编写，它读取并写入Bigtable，并使用大量后台作业和调度程序来管理其工作。 归档和迁移自动化 我们需要大量的辅助工具来将门户网站和配置管理组合在一起，并与用户进行通信来进行用户查询。我们还需要使用沟通技巧来鉴别出适合进行数据迁移的用户。误报（错误地报告所需行动）或漏报（未通知用户您正在取走某些东西）都是不可接受的，这里发生错误将意味着失去用户可信度和带来客户服务的额外工作。 我们与其他存储系统所有者合作——为新系统添加我们需要的功能。因此，随着项目的进展，不太成熟的替代品变得更适合了。我们还可以使用和扩展其他团队的工具。例如，我们使用其他团队内部开发的工具将数据从Google云端存储迁移到Google云端硬盘，作为门户网站自动存档功能的一部分。这项工作需要在整个项目期间进行长期的软件开发。我们构建并迭代了每个组件–Moonwalk报告管道，门户和自动化，以便更好地管理下线和归档共享，以响应下一阶段的要求和用户反馈。我们在第三阶段（差不多两年）才接近达到一个功能健全的系统。即便如此，我们还需要额外的工具来处理大约800名用户的“长尾”。这种低速和慢速的方法有一定的好处。因为它允许我们： 维持一个精炼的团队（平均三名CDS团队成员） 减少对用户工作流程的干扰 减少Techstop的琐事（谷歌内部技术支持组织） 根据需要构建工具，以避免将时间浪费在工程工作中 与所有工程决策一样，存在如下权衡：项目将长期存在，因此团队在设计解决方案时必须忍受与文件管理器相关的琐事。 该计划于2016年正式完成。在撰写本文时，我们已将home目录从65,000个减少到约50个（目前的Azog项目旨在淘汰这些最后用户并彻底下线文件管理系统的硬件。）我们的用户体验有所改善，CDS已停止运营成本高昂的硬件。 经验教训 虽然没有任何替代方案可以替代Google员工已使用14年之久的文件管理系统，但我们并非没必要进行批量更换。通过有效地将堆栈从通用但有限的文件系统解决方案升级到多个应用程序共同组成的解决方案，我们增加了系统的灵活性，以提高可扩展性、延迟包容度和安全性。Moira团队不得不预测各种用户的行为，并考虑不同阶段的可替代方案。我们必须围绕这些替代方案来调整我们的期望：总的来说，它们可以提供更好的用户体验，但实现这一目标并非是毫无痛苦的。我们学到了以下关于有效减少琐事的策略。 发现旧系统的不足并下线昂贵的业务流程 业务需求不断变化，新的解决方案不断涌现，因此定期评估旧的业务流程是值得的。正如我们在第101页的“琐事管理策略”中所讨论的那样，拒绝琐事（决定不执行）通常是消除它的最简单方法，即使这种方法并不总是快速或简单的。通过用户分析和业务理由来调整你的业务工作内容，而不仅仅是减少琐事。文件管理系统下线的主要业务理由归结为Beyond Corp安全模型的优势。因此，虽然Moira是减少CDS团队琐事的好方法，但强调下线系统的原因如果是考虑到了新系统诸多的安全优势，这些优势将带来更具吸引力的业务需求。 构建自助服务接口 我们为Moira建立了一个自定义门户（相对昂贵），但通常有更便宜的选择。Google的许多团队使用版本控制来管理和配置他们的服务，以拉取请求（称为更改列表或CL）的形式处理请求。这种方法几乎不需要服务团队的参与，但为我们提供了代码审查和持续部署的优势，便于验证、测试和部署内部服务配置更改。 从人工支持的界面开始 在几个方面，Moira团队采用了“幕后工程师”的方法，将自动化与工程师的人工操作相结合。共享请求在路由过程中出现bug，我们的自动化在处理请求时会及时更新。系统还会通知到终端用户，提醒他们解决类似的共性问题。工单可以作为自动化系统的应急的GUI：它们保存工作日志，更新利益相关者的数据，并在自动化出错时提供简单的人工干预机制。在我们的示例中，如果用户需要获得数据迁移工作的帮助，或者如果自动化无法处理其请求，则该错误会自动路由到SRE手动处理的队列中。 零接触的自动化 自动化系统要求请求合规。Moira的工程师选择重新调整我们的自动化，以专门处理共享的边缘场景请求，或者删除/修改不合格的共享以符合系统的期望。这使我们能够在大多数迁移过程中实现零接触的自动化。 有趣的事实：在谷歌，通过改变现实去适应代码而不是通过修改代码去适应现实的方式被称为“购买侏儒”。这句话源自一个关于Froogle的故事，Froogle是一个很早就开展的购物搜索引擎服务。 在Froogle的早期阶段，发生过精确匹配搜索“跑鞋”关键字导致返回了garden gnome(花园侏儒, 穿着跑鞋）的严重错误。在几次尝试修复错误失败之后，有人注意到gnome不是批量生产的商品，而是一个带有“一口价”选项的eBay商品。他们购买了这个“花园侏儒”商品后，解决了这个返回错误搜索结果的问题（译者注：相比起修改代码，以更低的成本解决了问题）。（图6-10）。 图6-10. 不会消失的花园侏儒 推动新系统的使用 寻找方法来推动新用户采用更好的替代方案。在这种情况下，Moira需要升级系统配置以便应对新的请求和下线旧的系统。在服务设置中，新系统使用最佳实践和用户如何配置系统也很重要。Google团队经常使用codelabs或cookbook为用户提供常见用例设置和指导如何使用他们的服务。因此，大多数用户入手都不需要额外的团队的指导。 结论 与生产服务运行相关的琐事会随着系统复杂性和规模的增长而线性增长。自动化通常是消除琐事的黄金法则，并且可以与其他策略相结合。即使一些琐事没必要完全自动化，你也可以通过部分自动化或改变业务流程等策略来减少操作的负担。本章中描述的消除琐事的模式和方法可以推广到其他各种大规模生产服务中。消除琐事可以节省工作时间，以便工程师专注于服务的更重要的方面，并允许团队将手动任务保持在最低限度。随着现代服务架构的复杂性和规模不断增加，此策略尤其重要。 但是要注意，消除琐事并不总是最好的解决方案。如本章所述，你应该考虑成本，设计、改造和实施自动化解决方案都需要投入成本。一旦决定减少琐事，就必确立目标，进行投资回报率（ROI）分析，风险评估和迭代开发来确定是否减少了工作量。 琐事通常是从小事开始积累的，并且可以迅速成长最终消耗整个团队的人力资源。SRE团队必须坚持不懈地消除琐事——因为即使减少琐事的项目看起来令人生畏，但其好处通常也是会超过成本的。我们所描述的每个项目都需要各自团队的坚持不懈和奉献精神，他们有时会面对质疑或者需要与制度进行斗争，并且总是面临竞争这种高优先级的任务。我们希望这些案例鼓励你识别工作中的琐事，量化它，然后努力消除它。即使今天不能开展一个大项目，你也可以从一个小概念开始，这可以帮助改变你的团队处理琐事的方式。</summary></entry></feed>