<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[文章标题]]></title>
      <url>/sre/2020/05/11/%E6%96%87%E6%A1%A3%E6%A8%A1%E7%89%88/</url>
      <content type="text"><![CDATA[  前言内容前言    code]]></content>
      <categories>
        
          <category> SRE </category>
        
      </categories>
      <tags>
        
          <tag> 《SRE-Google运维实践》 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[第三章 SLO工程案例学习]]></title>
      <url>/sre/2020/04/13/SLO%E6%A1%88%E4%BE%8B%E5%AD%A6%E4%B9%A0/</url>
      <content type="text"><![CDATA[  名次解释：      SLI：服务质量指标、该服务的某项服务质量的一个具体量化指标。例如：延迟、可用性    SLO：服务质量目标、服务的某个SLI的目标值/范围。例如：搜索请求的平均延迟 &lt; 100ms。    SLA：服务质量协议、服务与用户之间的一个明确的协议，描述达到/未达到SLO之后的后果。    错误预算： 1 - 可靠性目标  尽管SRE的许多原则都是在Google内部形成的，但它的原则早已存在于Google之外。许多Google SRE的标准已被业内多个组织实践应用。SLO是SRE模型的基础。自从我们组建了客户可靠性工程（CRE）团队——这是一组帮助Google Cloud Platform（GCP）客户构建更可靠的服务的经验丰富的SRE——几乎与每个客户交互都以SLO开始以SLO结束。我们在这里介绍了两个不同行业的公司事迹，概述了他们在与Google CRE团队合作时采纳SLO和基于错误预算的方法的过程。有关SLO和错误预算的讨论，请参阅本书的第2章和第一本书的第3章。Evernote的SLO故事Evernote是一款跨平台的APP，可帮助个人和团队创建、整合和共享信息。在全球拥有超过2.2亿用户，我们在平台内存储了超过120亿条信息——包括文本笔记、文件和附件/图像。在后台，Evernote服务由750个以上的MySQL实例支持。我们向Evernote引入了SLO的概念，并将其作为更广泛的技术改造的一部分，旨在提高工程速度，同时保持服务质量。我们的目标包括：将工程重点从数据中心中冗余的繁重工作转移到客户实际关心的产品工程工作上。为此，我们停止运行物理数据中心并转移到公有云。调整运维和软件工程师的工作模式，旨在保持整体服务质量的同时提高变更速度。改进我们对SLA的看法，以确保我们更加关注故障对庞大的客户群所造成的的影响。这些目标对许多行业的组织而言可能都很熟悉。虽然没有一种方法可以全面实现这些类型的变更，但希望我们分享的经验可以为面临类似挑战的人提供有价值的参考意见。为什么Evernote采用SRE模型？过渡开始阶段的Evernote的特点是传统的运维和开发分离：运维团队维护生产环境的稳定性，而开发团队的任务是为客户开发新的产品功能。这些目标通常是冲突的：开发团队 感觉被繁琐的流程所束缚，而运维团队又会因新代码在生产环境中引入新的问题变得不满。 当我们在这两个目标之间不断动摇时，运维和开发团队之间蔓延了一种不满和紧张的关系。我们希望达到一个双方都满意的节点，更好地平衡所涉及团队的不同需求。在五年多的时间里，我们尝试了各种方式解决这种传统二分法中的差距。在尝试了“你编码，你运行”的开发模式，以及“你编码，我们为你运行”的运维模式之后，我们转向了以SLO为中心的SRE方法。那么是什么促使Evernote向这个方向发展呢？在Evernote，我们将运维和开发的核心目标视为工程师专业化的独立发展方向。一个方向关注的是近乎7*24小时地持续为客户提供服务。另一个关注的是服务的扩展和发展，以满足客户未来的需求。近年来，这两个方向已经越来越接近，例如SRE和DevOps强调将软件开发应用于运维。（数据中心自动化和公有云的发展进一步推动了这种融合，这两者都为我们提供了一个可以完全由软件控制的数据中心。）另一方面，全栈所有权和持续部署也越来越多地应用于软件开发。SRE模型完全接受并包容了运维和开发间的差异，同时鼓励团队朝着共同的目标努力。它并不试图将运维工程师转变为应用程序开发人员，反之亦然。相反，它给出了一个共同的参考框架。根据我们的经验，由于使用错误预算/SLO方法的两个团队在交流时很少带着主观感情，所以在面对同样的情况时通常会做出类似的决定。SLO简介：正在进行的旅程旅程的第一步是从物理数据中心迁移到Google云平台。当Evernote服务在GCP上稳定运行后，我们就引入了SLO。我们的目标有两个：确保所有团队都在Evernote SLO的新框架内工作。将Evernote的SLO纳入我们与Google 云团队的合作中，他们现在负责我们的底层基础架构。由于在整体模型中加入了新的合作伙伴，因此我们需要确保迁移到GCP不会影响我们对用户的承诺。在使用SLO约9个月后，Evernote已经开始实践使用其SLO的第3版了！在深入了解SLO的技术细节之前，要先从客户的角度开始提问： 你可以提供哪些承诺？与大多数服务类似，Evernote具有许多功能和选项，用户可以通过各种创造性方式使用这些功能和选项。我们希望在一开始就关注最重要和最常见的客户需求：Evernote服务的可用性，以便用户能够访问和同步多个客户端的内容。我们的SLO之旅从这个目标开始。通过关注服务正常运行时间， 我们完成了接入SLO的第一步。使用这种方法，我们可以清楚地表达我们衡量的内容以及衡量方法。我们的第一份SLO文件包含以下内容：SLO的定义这是一个（服务、系统）可用时间的计算方法：为某些服务或者是方法，在月级别的统计周期内设定了99.95%的可用性。这个数据是我们基于内部客户支持团队、产品团队，尤其重要的是用户共同讨论得来的。我们特意选择将SLO和日历月而不是与滚动的时期进行关联，就是为了使我们在进行服务复查时保持专注有序。衡量什么，以及如何衡量它衡量度量    我们指定了一个服务终点，我们可以调用它来测试服务是否按预期运行。在我们的例子中，我们在服务中内置了一个状态页面，它可以运行我们的大部分堆栈并返回200状态代码（如果一切正常）。如何度量    我们想要一个定期调用状态页面的探测器。我们希望探测器完全位于我们的环境之外并独立于我们的环境，因此我们可以测试所有组件，包括负载均衡。我们的目标是确保我们可以统计到GCP服务以及evernote应用任何、所有异常。但是，我们不希望随机网络问题触发误报。我们选择使用专门建立和运行此类探测器的第三方公司。我们选择了Pingdom，但市场上还有很多其他产品。我们按如下方式进行衡量：   如何从监控数据计算SLO最后，我们仔细记录了我们如何根据从Pingdom收到的原始数据计算SLO。例如，我们指定了如何考虑维护窗口：我们无法假设我们所有的数亿用户都知道我们发布的维护窗口。因此，不知情的用户会将这些窗口视为通用和无法解释的停机时间，因此我们的SLO计算将维护视为停机时间。一旦我们定义了SLO，我们就必须使它发挥最大的价值。 我们希望SLO能够推动软件和运维方面的变革，让我们的客户更快乐并让他们满意。 怎么做到最好？探测频率：我们每分钟轮询一次前端节点。探测器的位置：此设置是可配置的; 我们目前在北美和欧洲使用多个探测器“down”的定义：如果一个探测器检测结果为失败，那么这个节点会被标记为疑似宕机，然后第二个基于不同地理位置独立部署的探测机会进行第二次确认。如果第二次检查同样也失败了，出于计算SLO的目的这个节点会被标记为宕机。只要探测请求持续显示错误，那么这个节点会被持续标记为宕机。我们用SLO中有关错误预算的思维为方法来分配下一步工作的需要的资源。举例来说，如果我们没有达成上个月的SLO，这会促使我们高优（对系统、服务）进行目标明确的加固、改进和修复。我们制定最简原则：evernote团队以及google团队共同进行月级别 的SLO目标复查。在这个会议上，我们复核SLO的表现并对所有服务中断行为进行深入研究。基于针对上个月的上述分析而不是根因分析，我们制定了一些改进措施。在整个过程中，我们的指导原则是“过犹不及”。即使在SLO还没有达到完美的时候，它也足以在此期间指导我们进行改进。一个“完美”的SLO应该可以衡量每一个与我们服务有关的潜在用户交互设计并且解释所有的边界行为。虽然字面上看起来这是个好主意，但是如果要实现起来却要花费数月的时间去改进服务（如果真的可以这到完美）。相反，我们选择了一个初始SLO，涵盖了大多数（但不是全部）用户交互，这是服务质量的良好代理。自从我们开始执行SLO以来，根据从服务复盘以及响应客户有感的宕机事件中得到的启示，我们对SLO做了两次修改。因为我们一开始就没有追求完美SLO，为了适应业务的发展我们乐于做出改变。除了evernote团队与google进行月级别SLO复盘之外，我们也设定了一个6个月的SLO复盘周期，这个周期可以使SLO的维护达到一个平衡：既不会频繁更新，也不会使之过时。在不断修订SLO的过程中，我们也意识到了，期望的衡量标准和可以达到的衡量标准之间的平衡是很重要的。自引入SLO以来，我们的运维和开发团队之间的关系有了微妙但显著的改善。现在团队对成功有了共同的衡量标准，那就是：取消对服务质量的人为解释使两个团队达成了共同的观点和标准。在此我们试着举一个例子，2017年当我们不得不在短期内推动多个版本的发布任务时，SLO为我们提供了共同基础。当我们发现一个复杂的bug时，产品开发团队要求我们将常规的周级别发布任务分配到多个独立的发布窗口，每个发布窗口都会对客户产生潜在的影响。通过对问题进行有针对性的SLO计算以及消除方案中的人为主观因素，我们可以更好的量化客户感受并且通过把发布窗口由5个降为2个从而达到了减少了客户痛点的目的。打破客户与云服务商之间的隔阂介于客户和云服务商之间的隔阂看起来是在所难免的。虽然google已经为运行evernote的GCP平台设定了SLO和SLA（服务等级协议），但是evernote有自己的SLO和SLA。期望两个技术团队会将彼此的SLA告知对方看起来是不现实的。evernote不希望存在这样的隔阂。当然我们也可以基于自己的SLO和底层的GCP平台的SLA建立起隔离域，相反从一开始我们就希望google可以理解性能表现对我们来说是多重要以及为什么这么重要。我们期望google和我们在目标上达成一致，让两家公司把evernote在可靠性方向的成败当作共同的职责。为了实现这一目标，我们需要一种方法可以：达成一致的目标确保我们的合作伙伴（在此指google）真正清楚我们最关心哪些指标共担成败大多数服务商都为自己的云服务发布了SLO/SLA。虽然服务运行在此框架下很重要，但这并不能全面的反映我们的服务在云服务商的环境中运行的状况。例如，一个给定的云服务商可能在全球运行了数十万台虚拟机，他们为这些虚机的正常运行和可靠性负责。GCP承诺计算引擎（也就是虚机）可以达到99.95%的可靠性。即使当GCP SLO指标显示为绿色的时候（即可靠性高于99.95%），evernote的监控视图的表现可能完全不同：因为我们的虚机在GCP全球总量虚机中仅占有很小的比例，会使导致我们（服务所在）区域成为孤岛（或由于其他原因导致成为孤岛）的故障最终在全球级别的汇总中被忽略。为了修正这样的情况，我们将我们的SLO和未达成SLO的实时性能与goolge进行共享。因此，Google CRE团队和Evernote团队基于同样的性能仪表盘展开合作。这看起来似乎是一个很简单的观点，但最终被证明是一种相当有效的、可以形成真正以客户为中心的工作方法。因此，google会向我们提供更明确的环境运行情况通知，而不是那种泛泛的“x服务当前运行缓慢”的通知。举例来说，除了那种泛泛的“今天GCP负载均衡环境运行缓慢”之外，我们还会被告知这个问题已经对evernote的SLO造成了5%的影响。这种关系也有助于google内部团队了解他们的行为和决策是如何影响用户的。这种双向关系也为我们提供了一个非常有效的框架来应对重大事件。大多数情况下，P1-P5级别的工单和常规的支持渠道配合使用，产生了很好的效果，使我们能够提供稳定的服务，并与谷歌保持良好的合作关系。但众所周知，当你整个在线服务面临着拓展业务增长的压力的时候，P1级别的工单是不能满足要求的。这时，我们与CRE团队共享的SLO和（合作）关系得以实现。我们达成共识，如果SLO影响足够高，双方都会将该问题视为P1级别进行特殊处理。这就意味着evernote和google的cre团队经常要快速组织起一个可以共享的沟通渠道。Google CRE团队监控（管理）我们共同定义和商定的SLO，使我们在优先级和恰当响应方面保持同步。当前状态协调目标确保我们的合作伙伴（在本例中为Google）真正了解对我们重要的内容分享成功和失败在积极使用SLO大约九个月之后，Evernote已经在使用SLO实践的第三版了。下一个版本的SLO会以我们当前简单正常运行时间的SLO为基础进行改进。我们将关注单个API调用和客户端的指标/性能视图，以便更好地表示用户QoS。通过提供标准定义的QoS测量方法，SLO使Evernote更关注我们的服务是如何运行的。我们内部或者和谷歌进行以数据为驱动的对话，了解服务中断的影响，这能够推动服务改进，最终建立更强大的支持团队，使客户更满意。Home Depot的SLO故事Home Depot（THD）是全球最大的家居装饰零售商：我们在北美拥有2,200多家商店，每家商店都拥有超过35,000种产品（网站上有超过150万种产品）。 我们的基础架构托管各种软件应用程序，支持了近400,000名员工每年处理超过15亿的客户交易。这些商店由全球供应链和每年访问量超过20亿次电子商务网站紧密组成。最近为了提高我们软件开发的速度和质量，THD转向敏捷软件开发并改变了我们设计和管理软件的方式。我们从支持大型软件包开发的团队转变为小型独立的微服务架构开发团队。因此，我们的系统现在由一系列不断变更的微服务组成，这些微服务也是通过堆栈整合而成。我们向微服务转变的过程中，全栈所有权获得了新的“自由和责任文化”的补充。这种方法使开发人员可以自由地在需要时推送代码，同时也使他们为他们对服务的操作负责。对于这种共同所有权工作模式，运维和开发团队需要达成一种共识，即促进责任制和减少复杂性：SLO。相互依赖的服务需要知道如下信息：如果每项服务都能为这些问题提供明确的和一致的答案，那么团队就可以清楚地了解服务的依赖关系，从而达到更好地沟通，增强团队之间的信任和责任感。SLO文化项目在我们的服务模式开始转变之前，Home Depot没有SLO文化。监控工具和仪表盘特别多，但都分布在各处，并且不会随着时间的推移记录数据。我们并不总能查出服务中断的根因。我们通常从遇到的服务问题开始排查，直到我们发现问题为止，这浪费了无数个小时。如果服务需要计划停机时间，其依赖服务就会受不了。如果一个团队需要构建一个99.95%的服务，他们不确定有严格依赖的服务能否达到99.99%的标准。这些未知导致我们的软件开发团队和运维团队之间的疑惑和失望。我们需要通过建立SLO的共同文化来解决这些问题。因此，需要一个影响人员、流程和技术的总体战略。 我们的努力跨越了四个方面：内部名词规定：在THD（Home Depot）公司内部定义SLOs。 来说明如何以一致的方式来进行度量。福音主义在整个公司传播这个词。通过给销售提供培训资料，在公司进行路演、内部博客、宣传资料比如T恤和贴纸等方式，传播为什么SLO很重要。争取一些早期采用者来实施SLO并向其他人展示其价值。建立一个感兴趣的首字母缩略词（VALET;稍后讨论）以帮助传播这个想法。创建培训计划（FiRE学院：可靠性工程基础），对开发人员进行培训使其了解SLO和其他可靠性概念。自动化为了降低指标收集的难度，用一个指标收集平台去自动收集生产环境中的服务的服务等级指标。这些SLI以后可以更容易地转换为SLO。激励为所有开发经理制定年度目标，为其服务设置和衡量SLO。每个人达成共识很重要。我们还希望保持这个框架尽可能简单，以帮助这个想法更快地传播。为了开始，我们仔细研究了我们在各种服务中监控的指标，并发现了一些模式。每项服务都会监控某种形式的流量、延迟、错误和利用率指标，这些指标与Google SRE的四个黄金指标密切相关。此外，许多服务都可以从错误中明显监控正常运行时间或可用性。很遗憾，整体来看，并不是所有类型的采集项都统一添加了监控、统一了命名、或者有足够的监控数据。我们的服务都没有SLO。我们的生产系统与面向客户的SLO最接近的指标是（用户）支持数据。通过跟踪商店内咨询台接收到的支持电话数量，是我们评价部署在我们商店的应用可靠性的主要（大多数时候是唯一）方法。我们的第一套SLO我们不能对一个可度量系统的每个方面都创建SLOs，因此我们必须确定系统的哪些指标或SLIS应该具有SLOs。API调用的可用性和延迟我们决定对微服务之间的API调用设置可用性和延迟SLOs。例如，Cart微服务调用Inventory微服务。针对那些API调用，Inventory微服务发布了SLOs，Cart微服务（以及需要Inventory的其他微服务）可以获取这些SLOs并以此决定Inventory微服务是否能满足可靠性要求 基础设施利用/基础设施利用率。基础设施利用率THD团队通过不同的方式来衡量基础设施利用率，而最典型的衡量标准是分钟级别的实时基础设施利用率。我们基于某些原因并不会设置这种利用率SLOs。首先，微服务并非十分关注这个指标-只要服务可以承载流量，服务器正常运行、响应速度很快、不抛错误，且并不会耗尽容量，那么你的用户就不会真正关心利用率。此外，计划迁移服务到云端意味着资源利用率不是重点，这时我们要关注的是成本规划，而不是容量规划。（我们仍然需要监控利用率并执行容量规划，但不需要将其包括在我们的SLO框架内。）流量由于THD没有进行容量规划的传统，因此我们需要一种机制，该机制能让开发和运维团队就其服务可以承载的流量进行沟通。流量通常被定义为对服务的请求，但我们需要确定是否应该跟踪平均每秒请求数，每秒峰值请求数或报告时间段内的请求数。最终我们决定跟踪这三项，并给每项服务选择最合适的指标。我们讨论是否为流量设置SLO的原因在于这个指标是由用户行为决定的，而非我们可控的内部因素决定。我们要讨论是否为流量设置SLO，因为流量的衡量跟用户行为密切相关，我们可控的内部因素无法发挥决定作用。 最终我们认为，作为零售商，我们需要为应对黑色星期五这样的活动流量峰值增加服务的规模，并根据预期的峰值流量设置SLO。延迟我们给每个服务定义了延迟SLO并确定其最佳的衡量方式。这里我们只要求服务应该通过黑盒监控来补充我们常见的白盒性能监控，以捕获由网络或诸如缓存以及微服务外部代理失效等层面的问题。并且，我们认为，采用百分位数比算术平均值更合适。服务最少需要达到90％的目标，而面向用户的服务则最好达到95%或99%的目标。错误错误解释起来有点复杂。由于我们主要处理Web服务，因此我们必须将错误内容以及返回结果标准化。如果Web服务发生错误，我们自然会对HTTP响应代码进行标准化：. 在服务的返回内容中，不应该用2xx来标记错误; 相反，一个错误应该抛出4xx或5xx。 . 由服务端问题（如内存不足）引起的错误应该抛出5xx错误。 . 客户端错误（如发送错误格式的请求）应该抛出4xx错误.一番考虑后，我们决定跟踪4xx和5xx错误，但仅使用5xx错误来设置SLOs。与定义其他相关SLO的方法类似，我们采用通用形式来定义错误SLO，以便不同环境中的不同应用都可以使用该SLO。例如，除HTTP错误外，定义一个批处理服务的错误，可能是该服务无法处理记录的个数。工单正如前面提到的，工单最初是我们评估大多数生产软件的主要方式。由于历史原因，在我们其他的SLOs中，我们决定继续跟踪工单。你可以将该指标视为类似于“软件操作级别”的指标。VALET我们将新的SLOs概括为一个更简易的缩略词：VALET。容量（流量）服务可以处理多少业务量？可用性需要的时候服务是否正在运行？延迟使用时服务是否快速响应？错误使用时服务是否会抛出错误？工单服务是否需要人为干预才能完成请求？推广SLOs凭借这样一个易于记忆的缩略词，我们开始在企业内部推广SLOs：. 为何SLOs如此重要. SLOs是怎样与我们的“自由和责任”文化相契合的. 应该衡量什么. 如何处理结果因为开发人员现在要负责维护他们自己的软件，因此他们需要建立SLOs以体现他们开发和维护软件可靠性的能力，针对面向用户的服务，他们需要同服务使用者和产品经理进行交流。然而，他们中多数人并不熟悉诸如SLAs和SLOs这样的概念，因此他们需要接受VALET框架方面的培训。由于我们需要获得强有力的支持来推广SLOs，因此一开始我们可以面向高级领导者进行SLOs的推广讲解。然后逐个向开发团队讲述SLOs的价值观。我们鼓励团队从他们自定义的度量跟踪机制（通常是人为制定）转向VALET框架。为了保持这种推广态势，我们每周发送一份VALET格式的SLO报告给高层领导，这份报告结合了可靠性理念和从内部事件中吸取的经验。这也有助于构建业务指标，例如在VALET框架下，创建的采购订单（流量）或支付订单失败（错误）。我们还以多种方式扩展了我们的推广渠道：. 我们建立了一个内部WordPress网站来托管有关VALET和可靠性的博客，并将其链接到相关资源。. 我们组织内部技术讲座（包括Google SRE演讲嘉宾），讨论了通用可靠性概念以及如何使用VALET进行度量。. 我们开展了一系列VALET培训研讨会（之后将演变为FiRE学院），并向所有想参加的人开放，这些研讨会持续了好几个月。. 我们甚至制作了VALET笔记本电脑贴纸和文化衫，用来支持全面的内部推广活动。很快，公司里的每个人都知道了VALET这一概念，并且我们的SLOs新文化开始在公司占据主流。对开发负责人来讲，实施SLO甚至已正式成为其年度绩效评估指标。虽然大约有50项服务正在按周级别获取并报告其SLOs，但我们会将这些指标存储在电子表格中。虽然VALET的思想已经非常流行，但为了让其更广泛地被接纳，我们仍然需要自动化技术来进行数据的收集。自动化VALET数据收集虽然我们的SLO文化现在有了强大的立足点，但自动化VALET数据收集将加速SLO的应用。TPS报告我们构建了一个框架来自动捕获部署到新GCP环境的任何服务的VALET数据。我们将此框架称为TPS报告，这是我们用于数量和性能测试的术语（每秒交易次数），当然，也是为了满足多个管理者想要查看这些数据的想法。 我们在GCP的BigQuery数据库平台之上构建了TPS Reports框架。我们的Web服务前端生成的所有日志都被输入BigQuery以供TPS Reports处理。当然也包括来自各种监控系统的指标，例如Stackdriver的可用性指标。TPS报告将这些数据转换为任何人都可以查询的每小时VALET指标。新创建的服务自动注册到TPS报告中，因此可以立即查询。由于数据全部存储在BigQuery中，因此我们可以跨时间帧有效地报告VALET指标。我们使用此数据构建了各种自动报告和警报。 最有趣的集成是一个聊天机器人，让我们直接在商业聊天平台上报告服务的VALET。例如，任何服务都可以显示过去一小时的VALET，前一周的VALET，未达成SLO的服务以及聊天频道内的各种其他值得引起关注的数据。VALET服务我们的下一步是创建一个VALET应用程序来存储和报告SLO数据。因为SLO最适合用作趋势工具，所以该服务以每日、每周和每月粒度跟踪SLO。请注意，我们的SLO是一种趋势工具，我们可以将其用于错误预估，但不直接连接到我们的监控系统。相反，我们有各种不同的监控平台，每个平台都有自己的警报。这些监控系统每天汇总其SLO并发布到VALET服务以进行趋势分析。此设置的缺点是监控系统中设置的警报阈值未与SLO集成。 但是，我们可以根据需要灵活地更换监控系统。预计需要将VALET与未在GCP中运行的其他应用程序集成，我们创建了一个VALET集成层，该层提供API来收集聚合的VALET数据以生成服务日报。TPS Reports是第一个与VALET服务集成的系统，我们最终集成了各种本地应用程序平台（占在VALET中注册的服务的一半以上）。VALTE 仪表盘VALET仪表板（如图3-1所示）是我们用于可视化和报告此数据的UI，并且相对简单。 它允许用户：图3-1  VALET仪表盘注册新服务。 这通常意味着将服务分配给一个或多个URL，这些URL可能已经收集了VALET数据。为五个VALET类别中的任何一个设置SLO目标。在每个VALET类别下添加新的指标类型。 例如，一个服务采集99%的请求所用的延迟，而另一个服务采集90%的请求所用（或两者）的延迟。或者，后端处理系统可以跟踪每日总量（一天内创建的采购订单），而客户服务的前端可以跟踪每秒交易的峰值。VALET仪表盘允许用户一次报告许多服务的SLO，并以多种方式对数据进行切片和切块。例如，团队可以查看过去一周未达到SLO的所有服务的统计信息。负责复盘服务性能的团队可以查看其所有服务及其所依赖的服务的延迟。VALET仪表盘将数据存储在一个简单的Cloud SQL数据库中，开发人员使用流行的商业报告工具来构建报告。这些报告成为开发人员新的最佳实践的基础：定期对其服务进行SLO审核（通常是每周或每月）。基于这些，开发人员可以创建操作项以使服务回归SLO，或者可能按照需要调整不符合实际的SLO。SLOs的扩散一旦SLOs融入到组织的集体思想中，并且具备了有效的自动化技术和报表，那么新的SLOs就可以快速实施。在年初跟踪了约50项服务的SLOs之后，到今年年底我们正在跟踪800项服务的SLOs，每月约有50项新服务在VALET注册。由于VALET允许我们在THD中推广SLO的应用，因此自动化开发这项工作是非常有意义的。但是，不具备这种自动化开发能力的公司也不用担心采用SLO会带来的麻烦。虽然自动化为THD提供了额外的收益，但一开始就编写SLO也收益颇多。将VALET应用于批处理应用程序当我们围绕SLO开发强大的报表时，我们发现了VALET的一些其他用途。 经过一些调整，批处理应用程序可以适用此框架，如下所示：数量已处理的记录数量可用性在一定时间内完成工作的频率（百分比） 等待时间作业运行所需的时间 错误程序运行失败的记录 工单操作员必须手动修复数据和重新处理作业的次数在测试中使用VALET由于在发展SRE文化的同时，我们发现在临时环境中，VALET可以支持我们的破坏性测试（混沌工程）自动化。有了TPS Reports框架，我们就可以自动进行破坏性测试并记录对service’s VALET data造成的影响（希望没有影响）。未来展望通过800个（并且不断增长）服务来收集VALET数据，我们可以拥有大量有用的运营数据。我们对未来有几个期望。既然我们正在有效地收集SLO，我们希望使用这些数据来采取行动。我们的下一步是类似于Google的错误预算文化，当服务不在SLO时，团队停止推送新功能（除了提高可靠性相关的）。为了满足业务增长的需求，需要平衡SLO报告的生成频率（周级别或月级别）和SLO标准的更新频率。和许多采用错误预算的公司一样，我们正在权衡滚动窗口与固定窗口的优缺点。我们希望进一步优化VALET以跟踪详细的节点和服务的使用者。目前，即使特定服务具有多个节点，我们也只在整个服务中跟踪VALET。因此，很难区分不同的操作（例如，对目录的写入与对目录的读取；虽然我们对这些操作添加了单独的监控和报警，但不跟踪SLO）。同样，我们也很乐意为服务的不同消费者提供对应的VALET结果。虽然我们目前在Web服务层跟踪延迟SLO，但我们还希望跟踪最终用户的延迟SLO。此度量将捕获网络延迟和CDN缓存等因素如何影响页面开始呈现和完成呈现所需的时间。我们还想将VALET数据扩展到应用程序部署。具体来说，在将更改推广到下一个服务器、域或区域之前，我们希望自动化验证VALET是否在容差范围内。我们已经开始收集有关服务依赖性的信息，并且制作了一个可视化图表原型，该图表显示了我们在调用树中未触及到VALET指标的位置。 新兴的网格服务平台将简化这种分析。最后，我们坚信服务的SLO应该由服务的业务所有者（通常称为产品经理）根据其业务的重要性来设置。至少，我们希望业务所有者设置服务正常运行时间的最低要求，并将SLO用作产品管理和开发之间的共享目标。虽然技术人员发现VALET很直观，但对于产品经理来说，这个概念并不那么直观。我们正在努力使用与它们相关的术语来简化VALET的概念：我们既简化了正常运行时间的选择数量又提供了示例指标。我们还强调从一个级别转移到另一个级别所需的大量投入。以下是我们可能提供的简化VALET指标的示例：. 99.5％：商店员工使用次数很少的应用程序或新服务。    . 99.9％：适用于THD的大多数非销售系统 . 99.95％：销售系统（或支持销售系统的服务） . 99.99％：共享的基础设施服务以业务术语来衡量指标并在产品和开发之间共享可见目标（SLO！），这种行为将大量减少公司常见的对可靠性的错误预期。概要向大公司介绍一个新流程，都需要一个好的策略、高管的支持、强大的传播、简单的采用模式以及最重要的耐心，更不用说是一个新文化了。像SLO这样的重大变革可能需要数年才能在公司中牢固地建立起来。我们想强调的是，Home Depot是一家传统企业;如果我们能够成功地引入这么大的变化，那么你也可以。你也不必一次完成这个任务。虽然我们逐步实施SLO，但制定全面的传播策略和明确的激励结构促进了快速转型：我们在不到一年的时间内获得了从0到800的SLO服务支持。结论SLO和错误预算为解决许多不同问题提供了强大的理论支持。这些来自Evernote和Home Depot的案例研究提供了非常真实的例子，说明如何实施SLO文化可以使产品开发和运维更紧密地结合在一起。这样做可以促进沟通并更好地为制定决策提供信息。它最终将为你的客户带来更好的体验 - 无论这些客户是内部、外部、人类还是其他服务。这两个案例研究强调实现SLO文化是一个持续的过程，而不是一次性修复或解决方案。虽然它们共享哲学基础，但THD和Evernote的度量风格、SLIs、SLOs和实现细节明显不同。这两个案例都补充了谷歌对SLOs的看法，说明了SLO实现不一定是Google所特有的。正如这些公司为自己独特的环境量身定制SLO一样，其他公司和组织也可以这样做。]]></content>
      <categories>
        
          <category> SRE </category>
        
      </categories>
      <tags>
        
          <tag> 《SRE-Google运维实践》 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[第二章 实施SLO]]></title>
      <url>/sre/2020/04/12/%E5%AE%9E%E6%96%BDSLO/</url>
      <content type="text"><![CDATA[  名次解释：      SLI：服务质量指标、该服务的某项服务质量的一个具体量化指标。例如：延迟、可用性    SLO：服务质量目标、服务的某个SLI的目标值/范围。例如：搜索请求的平均延迟 &lt; 100ms。    SLA：服务质量协议、服务与用户之间的一个明确的协议，描述达到/未达到SLO之后的后果。    错误预算： 1 - 可靠性目标  SLO为服务可靠性设定了一个目标级别。它是可靠性决策的关键因素，所以是SRE实践的核心。无论从哪个角度来看，这都将是本书中最重要的一章。我们只有具备了一定的理论，设置初始的SLO并细化它们，这个过程才会变得简单。在第一本书第四章中介绍了有关SLO和SLI的相关理论，并对如何使用它们给出了一些建议。了解了SLO和错误预算这个概念之后，本章提供了一种方法让你开始SLO之旅，以及一些如何进一步迭代的建议。然后，我们将介绍如何使用SLO做出有效的业务决策，并探索一些更高级的使用场景。最后，我们介绍了一些不同场景下开展SLO的案例，以及在特定情况下开展更复杂SLO的指导方案。SRE需要SLO的原因即使在大型研发团队中，工程师也是稀缺资源，工程师的时间应投入到重要服务的核心问题上。工程师应该花时间在功能研发上以便赢得新的客户，还是花时间在提高服务可靠性和可伸缩性上以便让客户满意，这是很难找到平衡点的。谷歌认为一个深思熟虑的SLO是做出决策的关键，这些决策包括了可靠性相关工作，和确定工作优先级排序等内容。SRE的核心职责不仅仅是将“所有的事情”自动化并随时待命处理故障，他们的日常工作都将按照SLO来开展。确保SLO在短期内是合理的，并且可根据情况适时地调整。甚至可以说，如果没有SLO，就没有SRE。SLO更像一种工具，可以帮助工程师确定哪个工作优先级更高。 例如，考虑如下两个工作的优先级：将服务自动回滚和切换到备份站点。 通过计算这两个工作的“错误预算”值，我们可以确定哪个工作对用户更有利。有关详细信息，请参阅第37页上的“使用SLO和错误预算进行决策”部分，以及《Site Reliability Engineering》中的“拥抱风险”一章。入门作为建立基本SLO指标的起点，让我们来假设你的服务是某种形式的代码，它已经被编译和发布，并且运行在用户可以web访问的网络基础设施上。你的系统可能处于如下某个阶段：  起步阶段——尚未部署任何内容  在生产环境中，当出现问题时，系统的监控会通知您，但是没有正式的目标和没有错误预算的概念，也没有一个明确的正常运行时间  有SLO指标，但对其重要性理解不到位，或者不知道如何利用它来进行持续改进。  为了采用基于错误预算的站点可靠性工程方法，您需要达到以下状态：  服务的利益相关方认可此SLO  服务正常状态下可以达到SLO的要求  管理者认可此错误预算并在实际决策中发挥作用  有一个完善的SLO过程制定SLO的第一步是讨论SLO是什么，以及它应该包含哪些内容。SLO为服务客户设定了目标可靠性级别。 超过此阈值，几乎所有用户都应对你的服务感到满意（假设他们对服务的效用感到满意）。低于此阈值，用户可能会开始抱怨或停止使用该服务。最终，用户的快乐才是最重要的 - 快乐的用户使用服务，为您的组织创造收入，减少对您的客户支持团队的抱怨，并向他们的朋友推荐该服务。我们以可靠的服务让客户满意。顾客满意度是一个相当模糊的概念；我们无法精确衡量它。通常我们对它的了解很少，那么该如何开始呢?我们的经验表明，100％的可靠性是错误的目标：  服务即使使用冗余组件、自动健康检查和快速故障转移，也存在一个或多个组件同时失败的场景，服务的可靠性将低于100%。如果制定的SLO是100%，这件事将不可能实现。（运小白说：泰坦尼克号，有“”永不沉没“”的美誉，底仓有16个水密舱， 任何4个水密舱进水的情况下都不会沉没）  即使服务实现了100%的可靠性，但客户也不会体验到100％的可靠性。服务和客户之间的链路长且复杂，链路中的任何一个组件故障都会造成失败败。这也意味着当您的可靠性从99％提高到99.9％到99.99％时，每增加一个9都会增加额外的成本，但客户几乎感受不到。 （运小白说：搜索引擎搜索“运营商故障”即可感受到）  如果服务的可靠性是100%的，并希望保持这种可靠性，那么你永远无法更新或改进服务。最大的故障原因就是变化：推出新功能、应用安全补丁、部署新硬件以及扩大规模以满足客户需求都将影响100％的目标。 最终，服务将停滞不前，你的客户将转移到其他地方。（运小白说：对于节日期间没有促销活动的业务来讲，除去偶尔的硬件故障外，那是难得的平静期）  SLO为100％意味着你只有被动应对。除了对&lt;100％可用性做出反应之外，你实际上无法做任何事情，这是一定会发生的。 100％的可靠性不是工程师要追求的——它应该是运营团队的目标。（运小白说：运维和救火类似，如果消防人员的目标定位不能有任何小火苗，那消防人员估计就会疲于奔命了）一旦SLO目标低于100%，它就需要由组织中的某个人有权在迭代速率和可靠性之间进行权衡。在小型组织中，这可能是CTO；在更大的组织中，通常是产品所有者（或产品经理）。衡量标准:使用SLI一旦你认同100％是错误目标，多少才是正确的？ 在这里，服务质量指标发挥作用：我们引入SLI的概念，SLI是指服务的质量指标。虽然计算SLI有多种方法，我们建议SLI为：好的事件数量除以总事件数量。例如：  成功的HTTP请求数/总HTTP请求数（成功率）  在请求延迟小于100 ms 的成功请求数/总请求数  搜索结果数/搜索结果总数，包括那些正常降级的搜索结果  使用10分钟以上库存数据的产品搜索的“库存检查计数” 请求的数目/库存检查请求的总数  “良好用户分钟数” /“用户分钟数”这种形式的SLI有一些特别有用的属性。 SLI的范围从0％到100％，其中0％表示无效，100％表示无损。 我们发现这个比例是很直观的，这种风格很容易引入错误预算的概念：SLO是一个目标百分比，错误预算是100％减去SLO。 例如，如果您有99.9％的SLO成功率，且在四周内收到的300万个服务的请求，那么在此期间的错误预算为3,000（0.1％）。 如果单个中断导致1,500个错误，则该错误将占错误预算的50％。此外，使所有的SLI遵循一致的风格以便更好地利用工具：你可以编写报警逻辑、SLO分析工具、错误预算计算和报告，以期望得到相同的输入: 分子、分母和阈值。简化是一个额外的好处。在尝试首次制定SLI时，将SLI进一步划分为SLI规范和SLI实现是必要的：SLI规范：你认为对用户重要的服务结果的评估，与其测量方式无关。例如：加载小于100毫秒的主页请求比SLI实现：SLI规范及其测量方法。例如：  加载小于100毫秒的主页请求比，由服务器日志的延迟列进行测量，这种度量方式将遗漏未能到达后端的请求。 （运小白说：遗留有很多个地方都会发生，从用户侧到运营商，从运营商到IDC，从接入层到后端）  加载小于100毫秒的主页请求比，由在虚拟机中运行的浏览器中执行JavaScript的探测器测量。 当请求无法到达我们的网络时，此度量将捕获错误，但可能会遗漏仅影响用户子集的问题。（运小白说：例如部分地域/运营商的网络异常，仅通过该方案就无法发现）  加载小于100毫秒的主页请求比，通过在主页本身上使用JavaScript进行测量，并将其报告给专门的遥测记录服务。这个度量将更准确地捕获用户体验，尽管我们现在需要修改代码来捕获这些信息，并构建基础设施来进行记录 —— 但这是一个具有自身可靠性要求的规范。 （运小白说：俗称埋点，BAT均有使用，对访问速度优化，可用性监控，地址库优化等均能起到较好的效果）如你所见，单个SLI规范可能具有多个SLI实现，每个SLI实现在质量（它们如何准确地捕获用户体验），覆盖范围（它们如何捕获所有用户体验）和成本方面都具有自己的优缺点。您对SLI和SLO的第一次尝试不需要完全正确; 最重要的目标是获得适当的位置并加以测量，并建立反馈机制以便改进。 （我们将在本章的“持续改进SLO目标”中深入探讨这个主题。）在我们的第一本书中，我们建议不要根据当前的性能选择SLO，因为这可能会导致你执行不必要的严格SLO。虽然这个建议是正确的，但是如果你没有任何其他信息，并且有一个好的迭代过程(我们将在后面介绍)，那么你当前的性能是一个很好的起点。但是，当你优化SLO时，不要让当前性能被限制：客户也会期望服务在其SLO上执行，因此如果服务在不到10毫秒的时间内请求成功率为99.999％，任何基于该基线的退化可能让他们不开心。 （运小白说：当前的实际情况是一个参考，而非说目标制定必须以当前值为基础并高于当前值。而且，很多时候，单一目标是能上不能下的，你让用户习惯了这个响应速度，他就无法接受比这个基础值低的服务，这既是门槛也是压力。另外，对单一目标的不断追求，还需要考虑投入产出比。因此这个时候，可以参考下面的建议，从多个维度制定一组SLO。）要创建第一组SLO，您需要确定对您的服务至关重要的几个关键SLI规范。 可用性和延迟SLO非常常见; 新鲜度，耐用性，正确性，质量和覆盖范围SLO也有它们的位置（我们稍后会详细讨论它们）。（运小白说：大牛们对搜索服务的SLO总结为全，新，快，稳，准）如果你不知道从哪类SLI开始学习，那么从简单的开始:  选择一个要定义SLO的应用程序。如果产品包含许多应用程序，则可以在此之后添加这些应用程序。  在这种情况下，明确“用户”是谁。  考虑用户与系统交互的常见方式 - 常见任务和关键活动。  绘制系统的高级架构图; 显示关键组件、请求流、数据流和关键依赖项。将这些组件分组到下一节中列出的类别中(可能存在一些重叠和歧义; 运用你的直觉，不要让完美成为善意的敌人。） 你应该仔细考虑你选择什么作为你的SLI，但你也不应该过分复杂化。 特别是如果您刚刚开始SLI之旅，请选择相关但易于衡量的系统方面 ，以便可以随时进行迭代和优化。组件类型开始设置SLI的最简单方法是将系统抽象为几种常见的组件类型。然后，您可以使用我们为每个组件提供的SLI建议列表，来选择与您服务最相关的SLI:请求驱动用户创建某种类型的事件并期望响应。 例如: 这可以是HTTP服务，其中用户与浏览器或移动应用程序的API交互。管道一种系统，它将记录作为输入，对其进行转变，并将输出放在其他位置。 这可能是一个在单个实例上实时运行的简单过程，也可能是需要花费数小时的多阶段批处理过程。 例子包括：  一种定期从关系数据库中读取数据并将其写入分布式哈希表以优化服务的系统  一种将视频从一种格式转换为另一种格式的视频处理服务  一种从多个源读取日志文件以生成报告的系统  一种从远程服务器提取指标并生成时间序列和报警的监控系统    存储    接收数据(例如字节、记录、文件、视频)并使其在以后可以被检索的系统。  ##案例一个简化过的手机游戏架构，如图2-1所示。在用户手机上运行应用程序与云中运行的HTTP API交互。API将状态更改并写入永久存储系统。一个管道定期运行这些数据，来生成今天、本周和所有时间的高分排行。这些数据被写入一个单独的排行榜数据存储，可以通过移动应用程序和网站获得结果。用户可以通过API和网站将游戏中使用的自定义头像上传到用户数据表。鉴于此设置，我们可以开始考虑用户如何与系统交互，以及采用哪种SLI衡量用户体验。这些SLI中有些可能存在重叠：请求服务具有正确性SLI，管道具有可用性SLI，并且持久性SLI可能被视为正确性SLI的变体。 我们建议选择少数（五个或更少）SLI类型，这些类型代表了客户最关键的功能。。为了捕获典型的用户体验和长尾，我们还建议使用多个等级的SLO。例如，如果90％的用户请求在100毫秒内返回，但剩下的10％用户需要10秒，这部分用户将不满意。 延迟SLO可以通过设置多个阈值来捕获此用户群：90％的请求快于100毫秒，99％的请求快于400毫秒。 这个原则适用于所有的SLI – 这些SLI的参数用来衡量用户的满意程度。表2-1为不同类型的服务提供了一些常用的SLI。                          表2-1 不同类型组件常用的SLI            服务类型      SLI类型      描述                  请求驱动      可用性      成功响应的请求比例。              请求驱动      延迟      比某些阈值快的请求比例。              请求驱动      质量      如果服务在过载或后端不可用时正常降级，则需要测量在未降级状态下提供服务的响应比例。 例如，如果用户数据存储不可用，但使用通用图像游戏仍可玩。              管道      新鲜度      最近更新的数据比例超过某个时间阈值。 理想情况下，此指标会计算用户访问数据的次数，以便最准确地反映用户体验。              管道      正确性      正确输出值的比例。              管道      覆盖范围      对于批处理，处理超过某个目标数据量的作业比例。对于流处理，在某个时间窗口内成功处理的传入记录比例。              存储      耐用性      可以成功读取的记录所占的比例。特别注意持久性SLI:用户想要的数据可能只是存储数据的一小部分。例如，如果你在过去10年里有10亿个记录，但是用户只需要今天的记录(但这些记录不可用)，那么即使他们的数据几乎都是可读的，他们也会不高兴。      从SLI理论到SLI实践第一次SLI实践可以考虑选择一个资源需求相对较少的项目。比如有如下几个项目可供选择：web日志服务器的SLI项目，这个项目我们不需要准备什么；对监控系统进行SLI项目实践，但是这需要几周的时间准备，而JavaScript的项目则会需要几个月的时间。在这种情况下请使用web服务器的日志来作为第一个SLI实践的工程。衡量SLI需要足够指标：可用性指标（成功率和失败率）; 慢请求延迟（请求的响应时间）。 这些指标可能需要你重新对Web服务器进行配置才能获得。 如果是基于云服务的Web，这些指标可以在监控仪表盘中看到。在这个案例中我可选择的SLI指标有很多，每个指标都有自己的优缺点。 以下部分详细介绍三种典型的SLI指标的应用。API和HTTP服务的可用性指标和慢请求延迟指标请求是否成功可以基于HTTP的返回码。5XX就代表请求失败，会降低服务的SLO，其他响应码代表成功。 可用性的SLI是指请求成功率；慢请求延迟的SLI是指在给定请求响应时间阈值下的请求成功率。SLI应该是具体明确并可测量的。总结“衡量标准:使用SLI”中的提供的潜在候选列表，SLI可以使用以下的一个或多个数据来源:  应用服务器日志  负载均衡监控  黑盒监控  客户端插件我们的例子采用负载均衡监控，因为这些指标已经是可用的，并且数据信息比采用“应用服务器日志”更真实反应用户体验的SLI。（运小白说：采用负载均衡的数据，是因为接入层记录的耗时，是一个用户请求在整套系统所有模块处理耗时和所有网络传输耗时的总和，因此更加真实的反应用户的体验，且和客户端插件相比，实施成本相对较低。但该部分耗时，并不包含从用户端到IDC这段的耗时）管道数据延迟率，范围覆盖率和准确率使用管道系统管理一个排行榜时，它会记录一个包含有数据更新的时间戳标签。下面是我们进行SLI实践的例子：  在排行榜上周期性的运行一个进程用于查询有多少次记录被更新和总共有多少个记录。这两个指标同样重要。  在用户请求数据时默认为请求增加一个时间标签，排行榜服务收到客户端请求后会检查这个标签并将请求计数器+1， 如果数据超出了预定义的阈值，则将配置另一个用于记录超时的计数器数字+1。（运小白说：在一个数据流中各个环节或者关键环节中添加时间戳后，可以在监控，性能优化，故障定位等多种场景中使用）以上两个步骤的数据要求都在客户端实现 。这个指标与用户体验密切相关。为了计算我们覆盖率的SLI，我们的管道输出了它应该处理的记录数和它成功处理的记录数。此度量标准可能会遗漏由于管道配置错误而未记录的数据。我们采用如下方法来评估准确率：  将已知的数据作为输入写入系统，计算输出与期望值匹配率。  基于默认管道的输入输出结果作为基准，计算此管道在相同输入下的输出与前者匹配的程度（这个方法可能并不适合所有的管道系统）。我们的案例是为一个需要人工管理的数据库建立指标为准确率的SLI。所有的数据的输入都是合法的，通过管道系统输出结果， 计算输出的准确率。为了使指标能够很好的反应用户体验，我们需要确保人工输入的数据和用户真实数据是一致的。（运小白说：这段看似简单，却是经验的分享。举一个具体的例子来让大家更深刻的理解，以计算服务为例，将已知的数据作为输入写入系统，你可以每分钟输入一个不变的数，然后让系统去计算一定时间的和值，从而度量计算服务的SLI。进阶的话，就是每分钟输入的是变化的数，从而计算和值，均值，最大值，最小值，那变化的数用什么呢，一般就是当前的分钟数）SLI的计算                    图2-2展示的是用白盒监控系从示例程序的各个组件中收集指标的过程。给出一个示例来说明我们是如何使用监控系统中的指标来计算启动器的SLO指标的，以便读者可以更好的理解。虽然我们在案例中只使用了可用性和慢查询延迟指标，但是同样的思路也适用于其他潜在的SLO指标的计算。系统使用度量标准的完整列表，请参阅附录a。我们的案例都使用普罗米修斯监控系统进行数据采集（Prometheus notation）。####负载均衡指标后端的请求返回码为500的数量（“api” or “web”）：http_requests_total{host=”api”, status=”500”}总延迟，作为累积直方图；每个柱状图（bucket）表示计算花费少于或等于该时间的请求数量:http_request_duration_seconds{host="api", le="0.1"}http_request_duration_seconds{host="api", le="0.2"}http_request_duration_seconds{host="api", le="0.4"}一般来说，计算得到慢请求数要比用直方图估计得到的慢请求数更加精准。但是，由于有些信息是拿不到的，所以最终使用监测系统提供的直方图。另一种方法是根据负载均衡配置中的各种慢请求阈值(例如，阈值为100毫秒和500毫秒)来计算慢请求数量。这种方式需要对监控的配置进行修改，所以会更复杂但是会更准确。http_request_duration_seconds{host="api", le="0.1"}http_request_duration_seconds{host="api", le="0.5"}计算SLI使用前面的指标，我们可以计算前七天的当前SLI，如表2-2所示。                                            表2-2   过去七天的SLI计算            指标      计算方式                  可靠性      sum(rate(http_requests_total{host=”api”, status!~”5..”}[7d]))  /  sum(rate(http_requests_total{host=”api”}[7d])              延迟      histogram_quantile(0.9, rate(http_request_duration_seconds_bucket[7d]))  histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[7d]))      利用SLI计算初始 SLO我们可以将这些SLI划分为可管理的数字（例如，两个重要的可用性数据，或最多50 ms的延迟）来获得我们的起始SLO。例如，超过四周，API指标显示：  总请求数：3663253  总成功请求数:3,557,865 (97.123%)  90%的延迟: &lt; 432 ms      99%的延迟: &lt; 891 ms我们为其他SLI重复此过程，并为API创建一个SLO，如表2-3所示。                              表2-3  API的建 SLO                    SLO类型      目标                  可靠性      97%              延迟      90%的请求 &lt; 450ms              延迟      99%的请求 &lt; 900ms      附录A提供了SLO文档的完整示例。 本文档包含SLI实现，为简洁起见，我们在此省略。根据所提出的SLI，我们可以计算这四周的错误预算，如表2-4所示。                            表2-4    过去四周错误预算            SLO      允许失败数                  97%的可用性      09,897              90％的请求快于450ms      366,325              99％的请求快于900ms      36,632      选择合适的时间窗口可以在不同的时间间隔上定义SLO，并且可以使用滚动窗口或周期对齐窗口（例如，一个月）。选择时间窗口时需要考虑几个因素。滚动窗口与用户体验更紧密地联系在一起：如果你在一个月的最后一天发生大量中断，则你的用户在下个月的第一天不会突然忘记它。 我们建议将这段时间定义为整数周，以便它始终包含相同数量的周末。 例如，如果你使用30天的窗口，则某些时段可能包括四个周末，而其他时段包括五个周末。 如果周末流量与工作日流量明显不同，你的SLI可能会因为无趣的原因而有所不同。周期窗口与业务规划和项目工作更紧密地结合在一起。例如，你可以每个季度评估SLO，以确定下一个季度项目人员的重点工作。周期窗口还引入了一些不确定性因素:在季度中期，你不可能知道在本季度余下的时间里会收到多少请求。因此，在季度中期做出的决定必须推测系统将在本季度剩余时间内花费多少错误预算。更短的时间窗口可以让你更快地做出决策：如果你错过了前一周的SLO，那么小的更正 - 例如优先考虑相关错误 - 可以帮助避免未来几周的SLO违规。对于更具战略性的决策，更长的时间周期更好：例如，如果你只能选择三个大型项目中的一个，那么你最好转移到高可用性分布式数据库，自动执行部署和回滚过程，或者在另一个部署重复堆栈区。你需要超过一周的数据来评估大型多季度项目; 所需的数据量与建议修复它的工程量大致相当。我们发现为期四周的滚动窗口是一个很好的通用间隔。我们每周任务优先级总结和每季度的项目规划总结报告刚好补充这一时间段。如果数据源允许，则可以使用这个建议的SLO来计算在此期间的实际SLO性能：如果根据实际测量设置初始SLO，则按此设计。但我们也可以收集关于分布的有趣信息。在过去的四个星期内，有哪天我们的服务没有达到标准? 这些天与实际事件有关联吗? 在那些日子里，有没有(或者应该)采取一些行动来应对这些事件?如果没有日志、度量或任何其他历史性能来源，则需要配置数据源。例如，作为HTTP服务的基本解决方案，你可以设置远程监视服务，对HTTP服务定期的执行某种健康检查（ping或HTTP GET），并报告请求成功的数量。许多在线服务可以轻松实现这一解决方案。获得所有利益相关者同意为了使提议的SLO有用和有效，你需要让所有利益相关者同意它：  产品经理必须同意这个阈值 ——低于这个值的性能无法令人接受，需要花费时间来修复。  产品开发人员需要同意，如果错误预算已用尽，他们将采取一些措施来降低用户的风险，直到服务重新回到错误预算中（如第31页的“建立错误预算策略”中所述）。  负责维护这个SLO生产环境的团队已经同意并一致认为，不需要付出巨大的努力、过度的辛劳——这对团队和服务都是不利的。 一旦所有这些观点都达成一致，最难的部分就完成了。你已经开始了SLO之旅，剩下的步骤需要从这个起点迭代。你需要设置监控和报警来监控SLO，（请参阅第5章），以便工程师发现问题。建立错误预算策略获得SLO后，你可以使用SLO来获取错误预算。 要使用此错误预算，你需要一个策略，描述当你的服务超出预算时要执行的操作。获得所有关键的利益相关者(产品经理、开发团队和SRE)批准的错误预算策略，是对SLO是否适合目的良好测试:  如果SRE认为需要付出更多的工作才能保证SLO，那么他们可以提出放宽SLO的要求。  如果开发团队和产品经理认为，他们必须投入更多的资源来修复系统，这将导致特性发布速度低于可接受的水平，那么他们也可以争取放宽目标值。记住，降低SLO也会降低SRE响应的情况数量；产品经理需要对此做权衡。  如果产品经理认为，在错误预算策略提示任何人解决某个问题之前，SLO将给大量用户带来糟糕的体验，那么SLO可能不够紧凑。  如果所有三方都不同意执行错误预算策略，那么你需要迭代SLI和SLO，直到所有利益相关者都满意为止。决定如何前进，以及做出决定需要什么：更多的数据？更多的资源？还是对SLI或者SLO进行修改？当我们讨论强制执行错误预算时，我们的意思是一旦你耗尽了错误预算(或者接近于耗尽它)，你应该采取措施来恢复系统的稳定性要制定错误预算执行决策，你需要从书面策略开始。 此策略应涵盖服务在给定时间段内消耗全部的错误预算时必须采取的具体操作，并指定谁将采取这些操作。 共同所有者和行动包括：  开发团队将过去四周内与可靠性问题相关的bug放在了首位。  开发团队专注于可靠性问题，直到系统处于SLO范围内。 功能迭代可以推迟。  为了降低更多停机导致的风险，冻结生产系统的变更，直到有足够的错误预算来恢复变更。有时候，服务会消耗掉整个错误预算，但并不是所有利益相关者都同意制定错误预算策略是合适的。如果发生这种情况，你需要返回到错误预算策略审批阶段。记录SLO和错误预算策略一个SLO应记录在一个突出的位置，其他团队和利益相关者可以审查它。该文件应包括以下信息：  SLO的作者，审核人（检查技术准确性）和审批人（谁做出了关于它是否是正确的SLO的商业决策）。  批准的日期，以及下次审核的日期。  为读者了解相关背景的简要说明。  SLO的细节：目标和SLI实现。  关于如何计算和使用错误预算的详细信息。  这些数字背后的基本原理，以及它们是否来自实验或观察数据。 即使SLO完全是临时的，也应记录这一事实，以便未来阅读文档的工程师不会根据临时数据做出错误的决定。 你查看SLO文档的频率取决于SLO文化的成熟度。 在开始时，你应该经常审查SLO （例如每个月）。 一旦SLO变得更加符合实际，你可以降低评审频率为每季度或更低。制定错误预算策略，还应包括以下信息：  策略的制定人、审核人和审批人  批准的日期，以及下次审核的日期  编写说明文档  应对错误预算耗尽时采取的行动  如果在某种情况下商定的策略发生分歧，则将问题升级  让有经验的工程师对错误预算进行审核有关SLO文档和错误预算策略的示例，请参阅附录A。仪表盘和报表除了已发布的SLO和错误预算，及说明文档之外，还应有一个报表和仪表盘进行展示。（运小白说：通过仪表盘/大屏以及背后的存储系统，记录，展示和分析长期的SLO，从而才能帮助团队更好的发展，减少犯同样的错误）图2-3中的报表显示了几种服务的总体合规性：它们是否满足前一年的所有季度的SLO（括号中的数字表示满足的目标数量，以及目标总数），以及他们的SLI相对于上一季度和去年同一季度是否呈上升趋势或下降趋势。使用仪表盘来显示SLI的趋势也是很有用的。 这些仪表盘显示你是否以高于平常的速率消费预算，或者是否存在需要特别关注趋势。图2-4中的仪表盘显示了一个季度的错误预算，在该季度的中间部分，我们看到单个事件在两天内消耗了大约15%的错误预算。错误预算对于量化这些事件很有用——例如，“这次宕机消耗了我季度错误预算的30%”，或者“这是本季度排名前三的事件，根据它们消耗了多少错误预算来排序”。##持续改进SLO目标每项服务都可以从持续改进中受益。 这是ITIL的核心服务目标之一。 例如： 在你提高SLO目标之前，你需要拥有用户对服务满意度的信息来源。 有很多选择：  人工发现的服务不可用次数、公共论坛上的帖子、故障单和投诉电话  社交媒体上的用户反馈  用插件定期对用户的满意度进行采样  调查问卷进行用户调查和采样最佳的方法取决于你的服务，我们建议从成本较低的渠道开始。让你的产品经理把可靠性纳入到他们与客户关于定价和功能的讨论中，这会是一个很好的开始。提高SLO质量记录人工检测到的服务不可用次数， 其他相关网站支持票数也可以计算在内，检查这些数据是否与错误预算的变化趋势相关。 同时检查这短时间内SLI和SLO的变化趋势。如果你擅长统计分析，Spearman相关系数是量化这种关系的有效方法。图2-5显示了每天增加的支持票的数量与当天错误预算的关系。虽然不是所有的支持票都与可靠性问题相关，但支持票与错误预算之间是存在相关性。我们看到两个异常值：一天只有5张票，损失了10%的错误预算；一天有40张票，没有损失任何错误预算。这两个异常值都需要进一步追查。如果支持票未在SLI或SLO中体现，或者用户关注的问题并没有在指标中体现，说明指标覆盖率存在问题。 这种情况完全正常，符合预期。 你的SLI和SLO应随着时间的推移而发生变化，因为服务的实际情况会发生变化， 不要畏惧对它们进行改进！如果你的SLO或SLI覆盖率不高，你可以采取多种方案：修正SLO如果你的SLI显示现在出问题了，但是你的SLO没有异常，你可能需要更加严格的SLO。  如果该时间段内发生的问题比较严重，通过SLI计算SLO应该出现异常的时间段。调整SLO之后，把这个新的SLO应用到SLI的历史数据上，看看这个调整会发现什么问题。如果发现严格的SLO会不断地对不重要的事件作出响应，那么这个修正毫无意义。  同样地，对于SLO误报的情况，考虑放宽SLO。如果在任一方向上修正SLO会导致误报或漏报，那么你还需要修正SLI实现。修正SLI实现有两种方法可以修正SLI实现: 要么将采集点更靠近用户以提高度量精准度；要么提高覆盖率，从而获得更高的用户交互比例。例如:  在负载均衡或客户端上测量成功率/延迟，而不是在服务器上测量。  更多的功能检测任务，或者在所有客户端通过JavaScript探测，而不仅仅是使用简单的HTTP GET请求来衡量可用性。制定一个高标准的SLO有时你需要更严格的SLO才能让用户满意，但改进产品以满足SLO需要一些时间。 如果你实施更严格的SLO，你将永久地脱离SLO并受到错误预算政策的约束。 在这种情况下，你可以将高标准的SLO作为期望值，与当前SLO一起进行追踪和对比。 通过这种方式，你可以掌握与高标准SLO之间的差距，却不会一直处于高压状态。迭代有许多不同的迭代方法，评审会议可以帮你找到许多潜在改进点。选择成本最低的方法，特别是在最初的几次迭代中，错误常常出现在贪图更快，更便宜方面;  这样做可以减少指标的不确定性，并帮助你确定是否需要更昂贵的指标。 根据需要进行多次迭代。使用SLO和错误预算进行决策一旦你制定了SLO，你就可以使用它们进行决策。当你没有达到SLO，也就是说已经用尽了错误预算，这时面对的首要问题是要做什么？如前所述，错误预算策略会告诉你应该做什么。 常见策略包括降级，直到服务再次处于SLO中；或者花费时间去处理问题以提高服务的可靠性。在特殊情况下，团队可以对外宣布处于紧急状态，取消所有外部需求，直到服务满足退出条件 （退出条件通常指服务处于SLO中，并且短时间内SLO不会出现问题）。我们可以使用改进监控，改进测试，消除系统的依赖关系，或重新调整架构以排除已知的故障类型等方法。你可以根据消耗错误预算的比例来确定事件的规模，并使用这些数据来识别最关键的故障，这些故障需要更深入的追查。例如，假设新版本API的发布导致100％  NullPointerExceptions，系统直到四小时后才可以恢复。检查服务的原始日志表明该问题导致了14,066个错误。 使用制定的97%的SLO和109,897个错误预算的标准来计算，这个故障使用了13%的错误预算。或者，数据库出现问题，而从备份数据恢复需要20小时。基于历史流量估算中断导致了72,000个错误，占错误预算的65%。想象一下，假设五年内只有一次服务器故障导致数据库异常，但通常每年会有两到三个版本被回退。 可以估计发布新版本导致的错误预算是数据库故障的两倍。 这些数字表明，投入资源来解决版本问题比调查服务器故障更有益处。如果服务运行完美且几乎不需要任何监督，并且做到了服务的故障管理和高级别的监督，那么你可以减少对此服务的SLO实施。因此，你可以将精力集中在其他需要更多SRE支持的系统上。表2-5  提供了基于三个关键维度决策矩阵:  SLO指标  维护服务所需要的工作量      客户对服务的满意程度                                             表2-5   SLO决策矩阵      SLO进阶一旦你拥有成熟的SLO和错误预算的氛围，下一步要做的就是继续改进和完善如何度量服务的可靠性。模拟用户SLO最终应该以改善用户体验为中心。 因此，你应该以用户为中心制定SLO。你可以仿照用户典型流程来评估用户的体验 （典型流程是指一系列任务的集合，包括了用户体验核心部分，也是服务的重要方面）。 例如，对于在线购物体验，用户典型流程包括：（运小白说：大牛总结的电商典型流程是首页-搜索-商详-购物车-下单-支付）  搜索产品  添加购物车  完成购买这些肯定不能很好地映射到现有的SLI；每个任务都需要多个复杂的步骤，这些步骤可能在任何时候都会失败，并且从日志中推断这些操作的成功（或失败）非常困难。 （例如，如何确定用户在第三步失败了？可能他们只是被分散了注意力）。然而，你需要定位到故障发生的原因是什么，因为这是服务可靠性的一部分。一旦确定了用户最关心的问题，就可以通过监测典型流程来解决上述问题。 你可以通过将不同的日志事件连接在一起，使用高级JavaScript探测，使用客户端检测或使用其他一些过程来度量它们。 一旦你可以定位一个问题，它就变成了另一个SLI。你可以和现有的SLI和SLO一起追踪。 用户关键流程可以在不影响精度的情况下提高你的召回。###分级的重要性并不是所有的请求都是平等的。虽然来自app的HTTP请求——检查帐户通知(其中通知是由每日的管道生成的)对用户很重要，但广告客户与账单相关的请求更重要。我们需要一种方法来对请求进行分类，可以使用“bucketing”来完成此操作 - 也就是说，为SLI添加更多标签，然后对不通的标签制定不同的SLO指标。 表2-6显示了一个示例。                              表2-6  分级分配SLO你还可以按照响应对请求进行分类，如表2-7所示。                              表2-7    按照响应进行分类如果你可以为每个用户制定SLO，那么你就可以在任何时间内得到处于SLO合理范围内的用户数量。请注意，这个数字是有大量噪音——请求数量非常少的客户要么拥有100%的可用性（因为他们足够幸运地没有遇到故障）要么拥有非常低的可用性（因为他们经历的一个失败会是相当大的百分比，因为请求基数少 )。单个客户可能会因为一些低级的原因而无法满足他们的SLO。但是总的来说，跟踪这个指标是非常有用的。（运小白说：关于分级，我记忆中最深刻的是以前一个同事的总结，30%的资源，支持了10%的流量，提供了5%的收入，因此这类资源必须要优化）依赖关系建模大型系统有许多组件。单个系统可能有表示层、应用层、业务逻辑层和数据持久层。每一层都可能包含许多服务或微服务。虽然你最关心的是系统的SLO实现，但SLO也可以作为提高系统各组件间可靠度的有用方法。例如，系统某个组件被过度依赖，那么它的可靠性保障应尽可能放到最高级。相应组件的团队也应有SLO。如果某些组件可靠性存在客观限制，则SLO应该可以将该限制表现出来。如果这个组件不能满足系统的可靠性要求，要么改进它，要么使用其他组件代替他或进行主动防御（比如：添加缓存，预存储和计算，优雅降级等）。你也可以尝试用数学方法解决这些问题。例如：如果在单个区域内有一个可用性为99.90%的服务，但你需要99.95%的可用性，则在两个区域中部署该服务就可以解决这个需求。两个服务同时发生故障的概率非常低，此时服务的可用性为99.9999%。然而，这种情况的假设前提是两区域服务完全独立，但这几乎不可能。应用程序的两个实例将具有共同的依赖和故障模式，无论如何精心设计和管理，都可能会导致两个服务同时异常。除非将这些依赖项和故障模式都被枚举出来，否则任何此类计算都具有欺骗性。当故障是由其他团队负责的组件引起的，以下两种思路可以用于解决SLO的问题：  你的团队还应该继续开展发布新版本，不要把过多时间用于可靠性相关工作，因为不是你系统引起的问题  你应该制定故障隔离策略，以便最大程度地降低未来可能由此组件引起的服务故障的可能性，无论此组件故障的原因是什么第二种方法将使你的用户更快乐。你可以灵活地运用这个原则。根据停机和依赖关系的性质，冻结更改可能不实际。确定最适合你服务及其依赖项的决定，并将此决定记录在你的错误预算策略中。有关如何在实践中工作的示例，请参阅附录B中的错误预算策略示例。（运小白说：这个问题非常典型，大家都可能会碰到，我使用你的服务，你应该保障你服务承诺的可用性，而不应该是我来解决这个问题；本文给出了另外的一种思路，添加缓存，预存储和计算，优雅降级，多活，多机房部署等来保障自身服务的稳定性，因此下次遇到这类问题，希望大家可以尝试一下这些方式。另外，高内聚低耦合固然重要，但是也不要太过极端，觉得别人的东西都不靠谱，只有全部自己做才放心，很多时候，把一些依赖的服务放到手里独立部署，你觉得稳定性提升很多，其实，那只是集群拆分压力减小后的一种表象，平时很少出问题，因此你也会放松警惕，另外，加之业务众多，每个业务投入的精力也较为有限，一旦出现问题，通常难以应对）放宽SLO，进行试验如果你想对服务进行可靠性的相关试验以便了解指标（例如，增加页面加载时间的延迟）的变化对用户体验的影响程度(例如，完成购买的用户百分比)。我们建议，只有当你确信自己有足够的错误预算时，才进行这类操作和分析。由于延迟、可用性、客户、业务领域和竞争(或缺乏竞争)这些因素之间有许多微妙的相互作用。故意影响顾客体验的决策需要深思熟虑。虽然这种方法的影响听起来十分可怕，因为没有人想失去用户。但通过这种方法，你可以将得到的结论用于服务的改进，从而在未来让服务拥有更好的性能，从而获得更多的用户。这种方法还可以让你在统计学上获得关键业务度量(例如，销售额)和可靠性指标(例如，延迟)之间的关系。如果是这样，你就获得了非常有价值的数据，这些数据可以帮助你做成做出重大决策。这个尝试不应该是一次性的。随着你的服务的发展，你的客户的期望也会随之改变，确保它们之间的关系是实时有效的。这种尝试获得的关系也可能存在风险，因为你可能会误解你得到的数据。例如，如果你人为地将页面加载时间增加了50毫秒的延迟，但是并没有发现相应的损失，那么你可能会得出这样的结论: 延迟的SLO太严格了。然而，你的用户可能是不满意，只是缺乏可替代的产品，用户别无选择。一旦出现竞争对手，你的用户就会流失。一定要保数据的正确性，并采取适当的预防措施。结论本书的每个案例都有SLO理论的影子。既然你已经阅读了这一章，我们希望能够意识到，即使是形式化的SLO(它清楚地向用户传达了你的承诺)也提供了一个清晰的框架来分析你系统表现。在服务未能满足期望时，你还可以用此SLO确定可采取的补救措施。总结：  SLO是衡量服务可靠性的工具  错误预算是一种工具，它可以帮助你平衡可靠性和其他日常工作的精力，同时也是判定工作优先级的好方法  你应该立即使用SLO和错误预算有关SLO文档和错误预算策略的示例，请参阅附录 A和B。]]></content>
      <categories>
        
          <category> SRE </category>
        
      </categories>
      <tags>
        
          <tag> 《SRE-Google运维实践》 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[第一章 SRE与DevOps之间的联系]]></title>
      <url>/sre/2020/04/11/SRE%E4%B8%8EDevops%E4%B9%8B%E9%97%B4%E7%9A%84%E8%81%94%E7%B3%BB/</url>
      <content type="text"><![CDATA[  《The Site Reliability Workbook — Practical Ways to Implement SRE》 中文版运维是一门很难的学科。 不但没有解决如何很好地运行系统，即便那些已经在使用的最佳实践也是高度依赖环境且未被广泛采纳的。 并且最重要的，没有解决如何良好地管理运维团队这一问题。人们普遍认为，对这些问题的详细分析源于二战期间致力于改善盟军军事进程和产出的作战研究，但事实上，长期以来我们一直都在思考如何更好地实践。尽管有这么多的努力和想法，可靠的生产运维仍然是难以保障的,特别是在信息技术和软件可操作性领域， 例如: 企业通常将运维视为成本中心， 这使得对结果进行有意义的改进变得困难甚至不可能。 这种短视的方法还没有被广泛理解， 但对它的不满却已经引发了IT领域对如何组织工作方面的一场革命。这场革命源于试图解决一系列普遍问题， 并诞生了两个不同的解决方案: DevOps 和 SRE(Site Reli‐ability Engineering)。 尽管单从描述上看，他们是企业完全不同的两个方面，需要单独讨论，但事实上，它们的相似之处，要远比我们想象的多。但首先，我们需要来了解一下每种原则的背景。DevOps产生的背景DevOps是一套松散的实践，指南和文化，旨在打破IT开发，运维，网络和安全方面的孤立。 是由John Willis，Damon Edwards和Jez Humble提出，使用CA(L)MS表示：文化（Culture），自动化（Automation），精益（Lean， 如精益管理;也包含持续交付），测量（Measurement,）和分享（Sharing） – 这是一个记住DevOps哲学要点很有用的缩写。 而分享和合作是这一运动的重中之重。 在DevOps方法中，您可以改进某些内容（通常通过自动化实现）、测量结果，并与同事分享这些结果，以便整个组织得到改进。 所有CALMS原则都是由支持文化促进的。DevOps，Agile以及各种其他业务和软件工程技术，都是关于如何在现代世界中进行最佳实践的示例。DevOps哲学中的任何元素彼此都不能轻易分离，这是由基本设计来决定的。 但是，有一些关键的想法可以相对分开地讨论。不再孤岛关键一：不再孤岛 。有以下两种观点：  曾经流行将运维和开发团队独立分开，但现在却越来越过时。  在许多情况下，极端的知识孤岛化、纯粹的对内部优化的激励以及缺乏协作对商业是十分不利的。事故是正常的关键二：事故不仅仅是由个人的孤立行动造成的，更是当问题不可避免地发生时，缺乏防范措施的结果。例如，一个糟糕的接口在不经意间助长了压力下的错误行为；如果出现（未知的）错误，系统缺陷将不可避免地导致失败；监控失效使得人们不可能知道是否出了问题，更不用说出了什么问题。一些传统观念更强的企业，拥有开除犯错者并惩罚他们的文化。但这样做有其自身的恶果： 他会诱使人们混淆问题、掩盖真相和指责他人， 而所有这些最终没有任何价值，专注于恢复服务比阻止事故发生要更有价值。变更应该是循序渐进的第三个关键点是， 小而频繁的变更是最佳的。 一个比较激进的示例，是变更委员会每月开会讨论彻底修改大型机配置的计划，然而这种做法并不鲜见，所有变更必须由经验丰富的人员进行有效的规划，结果或多或少与最佳实践相悖。变更是有风险的，没错，但是正确的做法是将变更尽可能拆分成更小的组件或单元。然后，根据产品、设计和基础架构的变更，构建稳定的低风险变更管道。（Then you build a steady pipeline of low-risk change out of regular output from product, design, and infrastructure changes）这种策略，增加对小变更的自动化测试和异常变更的可靠回滚，就形成了管理变更的方法：持续集成(CI)和持续交付或部署(CD)。工具和文化是相互关联的工具是DevOps的一个重要组成部分，特别是强调正确管理变更的今天，变更管理依赖于高度定制化的工具。 但总的来说，DevOps的支持者十分强调组织文化 - 而不是工具 - 这是成功采用新工作方式的关键。 一个好的文化可以解决破碎的工具，但相反的情况很少适用。 俗话说， 文化能把战略当早餐吃(意味着文化的影响力远胜过策略)。 与运维一样，变更本身也很难。度量是至关重要的最后，度量在整个业务环境中尤为重要，例如，打破孤立和故障处理。 在上述的每种场景中，你可以通过客观的度量来确定正在发生事情的真实性，验证改变是否符合预期，并为不同职能部门达成一致的对话创建客观基础。 （这适用于商业和其他环境，例如On-call。）##SRE产生的背景SRE是由Google的工程副总裁Ben Treynor Sloss提出的术语（和相关的工作角色）。正如我们在上一节中所看到的，DevOps是运维和产品开发之间在整个生命周期互相协作的一系列广泛原则。SRE是一个工作角色，也是我们发现的一组实践（稍后介绍），以及一些激励这些实践的信念。如果您将DevOps看作一种哲学和工作方法，则可以认为SRE实现了DevOps描述的一些哲学，并且比“DevOps工程师”更接近于这个工作或角色的具体定义。因此，在某种程度上，SRE类实现了DevOps接口。与DevOps运动不同，DevOps运动起源于多家公司的领导者和从业者之间的合作，而SRE在整个行业广泛普及之前，则是由Google的SRE继承周围公司的大部分文化。 考虑到这一轨迹，整个SRE学科目前并没有像DevOps那样在文化上突然增长。 （当然，这并不能说明文化变革对在任意组织中进行SRE是否是必要的）SRE由以下具体原则定义。运维是一个软件问题SRE的基本原则是：做好运维是一个软件问题。 因此，SRE应使用软件工程来解决这一问题。 这涉及广泛的领域，涵盖了从流程和业务变更到同样复杂但更传统的软件问题的所有内容，例如重写堆栈以消除业务逻辑中的单点故障。通过SLOs进行管理SRE不会尝试提供100％的可用性。 正如我们的第一本书 《Site Reliability Engineering》中所讨论的，这是错误的目标， 原因有很多。 相反，产品团队和SRE团队为服务及其用户群选择适当的可用性目标，并管理服务达到该目标，选定这样的目标需要业务部门的强大协作。 SLOs也具有文化内涵：作为利益相关者之间的协作决策，SLO违规行为将团队无可厚非地带回到原点。###减少琐事对于SRE来说，任何手动的， 重复性的的运维任务都是令人憎恶的。 （这并不意味着我们没有任何此类任务：我们有很多这样的操作。我们只是不喜欢它们。）我们相信，如果机器可以执行所需的操作，那么通常应该让机器来执行。 这是一种区别(也是一种价值)，在其他组织中并不常见。在那里，琐事就是工作，而这就是你付钱让一个人去做的事情。而对于在谷歌环境下的SRE来说，琐事不是工作——它不可能是。任何在操作任务上花费的时间都意味着无法再投入到项目工作上——项目工作才能使我们的服务可靠和可扩展。然而，通过“the wisdom of production”，执行运维任务确实为决策提供了重要的参考。这项工作通过提供来自给定系统的实时反馈信息来保持稳定。（This work keeps us grounded by providing real-time feedback from a given system.）琐事的来源需要明确，这样可以最小化或消除它们。然而，如果你发现自己处于操作不足的状态，那么你可能需要更频繁地推动新特性和更改，以便工程师依旧熟悉你所支持的服务的工作方式。                                         The Wisdom of Production A note on “the wisdom of production”: by this phrase, we mean the wisdom you get from something running in production—the messy details of how it actually behaves, and how software should actually be designed, rather than a whiteboarded view of a service isolated from the facts on the ground. All of the pages you get, the tickets the team gets, and so on, are a direct connection with reality that should inform better system design and behavior.把今年的工作自动化在这个领域真正要做是，确定哪些工作基于什么样的条件，以什么样的方式要完成自动化。（The real work in this area is determining what to automate, under what conditions, and how to automate it.）在Google，经验丰富的SRE严格限制团队成员花费在琐事上的时间，与之相反的是他们会在产生持续价值的工程类工作中花费50%的时间。许多人认为这个限制是一个上限。 事实上，将它视为一种保证更为有用，一种明确的声明和启用机制，采用基于工程的方法来解决问题，而不是一遍又一遍地辛劳的解决问题。当我们考虑自动化和琐事时，基线和其如何发挥作用并不直观。（There is an unintuitive and interesting interaction between this benchmark and how it plays out when we think about automation and toil.） 随着时间的推移，一个SRE团队最终会将服务的大部分操作自动化，只留下无法自动化的（Murphy-Beyer效应）。 在其他条件相同的情况下，除非采取其他行动，否则SRE团队所做的事情就会受到影响。 在google你更倾向于通过不断新增服务来达到填满50%的工程设计时间的限制，或者你在自动化方向做的非常成功，以至于你可以去做一些完全不同的事情。通过降低故障成本来快速行动日益提高的可靠性只是SRE带来的众多收益中的一种，事实的确如此，它实际上提高了开发的产出。为什么呢？对于常见故障，减少故障平均修复时间（ Mean Time To Repair）会提高产品开发人员的速度，因为工程师不必在这些故障问题之后耗费时间和精力进行处理。这源于一个众所周知的事实，在一个产品的生命周期里，问题发现的越晚，修复它所付出的代价越高。SREs专门负责改善异常问题的过晚发现，为公司整体带来收益。与开发分享权限“应用程序开发”和“生产”（有时被称为Dev和Ops）之间的严格界限会适得其反。 如果项目事务处的职责划分和作为成本中心的分类，导致权力不平衡、尊重或薪酬方面的差异，则尤其如此。SREs往往倾向于关注生产而不是业务逻辑问题，但随着问题被他们用软件工程工具所解决，他们与产品开发团队分享技术栈。 通常，SREs在他们正在关注的服务的可用性，延迟，性能，效率，变更管理，监控，应急响应和容量规划方面具有特殊的专业知识。 那些特定的（通常定义明确的）能力是SRE对产品和产品的开发团队所做的贡献。理想情况下，产品开发和SRE团队应该对技术栈有一个整体的看法 - 前端，后端，库，存储，内核和物理机器 - 没有团队应该令人嫉妒的拥有着单个组件。事实证明，如果你“模糊线条”并使用SREs共苦JavaScript，或者产品开发人员对内核进行限定，你可以做得更多：知识如何进行更改，权限更加广泛，而激励小心翼翼地保护任何特定的功能这一想法都应摒弃。在《Site Reliability Engineering》这本书里,，我们没有明确表明Google中的产品开发团队默认拥有自己的服务。 SRE既不可用也不保证大部分服务，尽管如此，SRE原则仍然可以告知整个Google如何管理服务。 SRE团队与产品开发团队合作时的所有权模式最终也是一个共享模型。使用相同的工具，无论功能或职位工具是一个非常重要的行为决定因素，  在Google的环境中，SRE如果没有统一的代码库、软件和系统的各种工具、高度优化和专有的生产堆栈等是非常天真的。我们与DevOps分享这个绝对的假设：团队服务应该使用相同的工具，无论他们在组织中的角色如何。 没有好的方法来管理一个服务，当该服务具有一个用于SRE的工具，一个用于产品开发人员的工具，在不同情况下表现不同（并且可能具有灾难性）。 当拥有的分歧越多，公司从改进每个工具努力中的获益就越少。比较和对比从上面聊到的原则中，我们可以立即看到他们之间有很多共性：  DevOps和SRE都接受一种理念，即为了改进，变更是必要的（都接受对于提高而言，变更是必要的）。否则，就没有多少可操作的空间。  协作是DevOps工作的前沿和中心。 有效的分享所有权模式和合作伙伴关系是SRE发挥作用所必需的。 与DevOps一样， SRE也具有跨组织分享的强大价值，这样更容易打破团队之间的孤立。  变更的最佳实践是: 持续小而频繁的变更，大多数操作理想情况下应该是：自动化测试和部署。变更和可靠性之间的关键交互使得这对于SRE尤为重要。  正确的工具至关重要，工具在一定程度上决定了你的行为范围。然而，我们决不能过于关注是否使用某些特定工具实现某种操作；归根结底，面向系统管理的API是更为重要的哲学，它将比任何特定的实现都持久。  度量绝对是DevOps和SRE如何工作的关键。对于SRE, SLOs (服务质量目标) 决定着是否改善和优化服务。当然，如果没有度量( 以及在产品、基础设施/SRE和业务之间的跨团队合作)，就不可能有SLOs。对于DevOps，度量行为通常用于理解流程的输出是什么，反馈周期的持续时间是什么，等等。DevOps和SRE都是面向数据的东西， 无论是从专业角度还是从哲学角度。  管理生产服务的残酷现实意味着故障时有发生，您必须说明原因。SRE和DevOps都进行了无可厚非的故障复盘，以抵消争议。  最终，实施DevOps或SRE是一种整体行为; 两者都希望通过高度特定的方式共同合作，使整个团队（或单位或组织）更好。 对于DevOps和SRE，更好的速度就是产出。如你所见，DevOps和SRE之间有许多的共同点。然而，也存在着显著的差异。DevOps在某种意义上是一个更广泛的哲学和文化。因为它影响的变化范围比SRE更广，所以DevOps对上下文更敏感。 DevOps对于如何在一个具体层面上执行操作没有更详细的说明。例如，它不是关于服务的精确管理的规定。相反，它选择专注于在更广泛的组织中打破壁垒。这就很有价值。另一方面，SRE的职责定义相对狭窄，其职权范围通常是面向服务（面向最终用户）而非整体业务。 因此，它为如何有效运行系统的问题带来了自以为是的知识框架（包括错误预算等概念）。 虽然作为一个职业，SRE非常清楚激励措施及其影响，但它反过来却对孤立化和信息壁垒等主题保持沉默。 它将支持CI和CD，不一定是因为业务需要，而是因为所涉及的操作实践得到了改进。 或者，换句话说，SRE相信和DevOps一样的东西，但原因略有不同。组织环境与成功采纳的培养DevOps和SRE在其运行方式上存在非常大的概念重叠。 正如您所料，他们也有一组类似的条件必须在组织内成立，以便他们  a)首先可以实现，并且  b)从该实现中获得最大的好处。 正如托尔斯泰几乎从未说过的：有效的操作方法都是相似的，而失败的方法都有各自的失败之处。 激励机制可以部分解释这一点。如果组织的文化重视DevOps方法的好处并且愿意承担这些成本 - 通常表现为招聘困难，维持团队和责任，流动性所需的能量，以及用于补偿必要技能所增加的财务资源，这更为罕见 。然后该组织还必须确保激励措施是正确的，以实现这种方法的全部好处。具体而言，以下内容应在DevOps和SRE的环境中都应成立。教条，刚性的激励措施限制了你的成功许多公司无意中定义了破坏集体绩效的激励措施。为了避免这种错误，不要将激励机制局限于与发布相关或可靠性相关的结果。正如任何一位经济学家都能告诉你的那样，如果有一个数字衡量标准，人们总会找到一种方法，让它产生不好的效果，有时甚至是以一种完全出于善意的方式。相反，你应该允许其他人自由地找到正确的选择。正如前面所讨论的，DevOps或SRE通常可以作为产品团队的催化剂，允许其他软件组织以连续可靠的方式向客户提供功能。这种动态机制还解决了传统和分散的系统/软件组方法的一个持久性问题：设计和生产之间缺乏循环反馈。具有早期SRE参与的系统（理想情况下，在设计时）通常在部署后的生产中工作得更好，而不用管是谁负责管理服务。 （没有什么比丢失用户数据更能阻碍功能开发的进展。）最好自己解决这个问题; 不要责怪别人此外，要避免把生产事故或系统故障的责任推给其他组。在许多方面，推卸责任的动力是传统工程操作模型的核心问题，因为运维和软件团队允许出现单独的激励机制。不管怎样，考虑采用以下做法来反驳组织层面的指责:不仅仅只是允许，而是积极鼓励工程师在产品需要时改变代码和配置。还应允许这些团队在其任务范围内采取激进行动，从而消除采取缓慢行动的想法。支持事后总结。这样做排除了淡化或掩盖问题的动机。这一步骤对于充分理解产品并实际优化其性能和功能至关重要，并且依赖于前面提到的生产经验。允许对“运行困难并且不可挽救的”产品不进行支持。 支持暂停这种产品，直到产品开发在支持准备阶段和产品本身得到支持之后再修复该问题，从而节省每个人的时间。 根据您的背景，“运行困难并且不可挽救的”的含义可能会有所不同 - 这里的动态应该是相互理解的责任之一。 对其他组织的推迟可能会更为温和，“我们认为使用这种技能的人有更多的时间”，或者限制在“这些人员将会因为过多的操作工作而没有机会使用他们的工程技能。“在Google，直接撤销此类产品支持的做法已成为一种制度。将可靠性工作视为一个专门的角色在谷歌，SRE和产品开发是独立的组织。每个小组都有自己的重点、优先级和管理，而不需要对另一个小组发号施令。然而，当产品成功时，产品开发团队将有效地资助新员工SRE的提升。这样，产品开发与SRE团队的成功息息相关，就像SREs与产品开发团队的成功息息相关一样。SRE也很幸运地得到了管理层的大力支持，这使得工程师团队认可了“SRE”这一角色。尽管如此，你不需要有一个组织结构图来做不同的事情，但是你需要一个不同的实践社区。不管你是使用组织结构图还是使用非正式的机制，重要的是要认识到专业化会带来挑战。DevOps和SRE的实践者可以从一个同伴社区中获得支持和职业发展，以及一个用来奖励他们独特的技能和观点的职业阶梯。值得注意的是，Google采用的组织结构以及上述一些激励措施在某种程度上依赖于规模庞大的组织。 例如，如果您的20人创业公司只有一个（相对较小的）产品，那么允许运维退出支持没有多大意义。 仍然可以采用DevOps风格的方法，但是，如果您能做的仅仅只是帮助它成长，那么改善操作性差的产品的能力就会受到损害。不过，对于如何满足这些增长需求，与技术债务累积的速度相比，人们通常有比想象中更多的选择。何时可以替代但是，当您的组织或产品增长超过一定规模时，您可以在支持哪些产品或如何确定支持优先级方面行使更大的自由度。 如果很明显，对系统X的支持将比支持系统Y更快地发生，那么隐式条件可以发挥同样的作用,选择不支持服务的行为。在谷歌，SRE与产品开发的强大合作关系已被证明至关重要：如果您的组织存在这种关系，那么撤回（或提供）支持的决定可以基于有关的客观数据来比较运营特征，从而避免非生产性的交涉。SRE与产品开发之间的富有成效的关系也有助于避免产品开发团队在产品或功能准备就绪之前必须交付的组织反模式。相反，SRE可以与开发团队合作，在维护负担转移到具有最多专业知识的人员之前改进产品。争取平等的尊重：职业与薪酬最后，确保正确的职业激励措施到位：我们希望我们的DevOps/SRE组织能够像他们的产品开发伙伴一样受到尊重。因此，每个团队的成员应按大致相同的方法进行评级，并具有相同的薪酬激励。结论在IT运维整体领域的许多方面，DevOps和SRE在实践和理念上都非常接近。DevOps和SRE都需要讨论、管理支持、并从实际工作人员那里来获得重大进展。 实施其中任何一个都是一段旅程，而不是一个快速解决方案：rename-and-shame（重命名和羞耻的）做法是空洞的，不太可能带来收益。 鉴于它是对如何执行操作的更具见解性的实现，SRE对于如何在此过程中更早地更改您的工作实践有更具体的建议，尽管需要进行特定的调整。DevOps关注的范围更广了，因此很难对其进行推理并将其转化为具体的步骤，但恰恰是因为更广泛的关注，可能会遇到较弱的初始阻力。但是，每种方法的实践者都使用许多相同的工具、相同的方法来改变管理，以及相同的基于数据的决策思维方式。最终，我们都面临着同样的问题:生产，让它变得更好——不管我们被称为什么。对于那些有兴趣进一步阅读的人，以下建议可以帮助您更广泛地了解目前正在进行的运维革命的文化，业务和技术基础：  Site Reliability Engineering  Effective DevOps  The Phoenix Project  The Practice of Cloud System Administration: DevOps and SRE Practices for Web Services, Volume 2  Accelerate: The Science of Lean Software and DevOps]]></content>
      <categories>
        
          <category> SRE </category>
        
      </categories>
      <tags>
        
          <tag> 《SRE-Google运维实践》 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[《SRE Google运维实践》介绍]]></title>
      <url>/sre/2020/04/11/SRE-Google%E8%BF%90%E7%BB%B4%E5%AE%9E%E8%B7%B5/</url>
      <content type="text"><![CDATA[  《The Site Reliability Workbook — Practical Ways to Implement SRE》 中文版Edited by:    Betsy Beyer, Niall Richard Murphy, David K. Rensin, Kent Kawahara and Stephen Thorne前言    The Site Reliability Workbook is the hands-on companion to the bestselling Site Reliability Engineeringbook and uses concrete examples to show how to put SRE principles and practices to work. This book contains practical examples from Google’s experiences and case studies from Google’s Cloud Platform customers. Evernote, The Home Depot, The New York Times, and other companies outline hard-won experiences of what worked for them and what didn’t.《The Site Reliability Workbook》操作手册 具体的用途来展示如何在工作中实践SRE。 本书包含了Google自身经历或是从Google’s Cloud Platform的客户案例，Evernote、The Home Depot、The New York Times。]]></content>
      <categories>
        
          <category> SRE </category>
        
      </categories>
      <tags>
        
          <tag> 《SRE-Google运维实践》 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[第一篇blog]]></title>
      <url>/something/2020/03/11/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E6%96%87/</url>
      <content type="text"><![CDATA[  如何搭建该网站！.准备工作  github pages  jekyll具体过程      github pages 创建  首先新建仓库， 需要以username.github.io作为仓库名。        jekyll  选择模版，可以在Jekyll Themes选择自己喜欢的模版。我选择的是 Next模版，之所以选择该模版，是因为可以按照Next 使用文档一步一步进行操作，对于小白来说比较友好。  遇到问题jekyll安装， ps：mac os  ruby版本过低  解决方案，安装rvm更新ruby，Rvm是一个命令行工具，可以管理多个版本的Ruby。Mac使用RVM更新Ruby  权限问题  使用gem遇到 write permissions for the /usr/bin directory；      sudo gem install -n /usr/local/bin jekyll      I]]></content>
      <categories>
        
          <category> something </category>
        
      </categories>
      <tags>
        
          <tag> something </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
