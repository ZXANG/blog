---
title: kafka相关运维案例
description: 介绍一些kafka大集群运维遇到的相关case
categories: 大数据
tags: ELK 
---

>
> 版本：kafka_2.11-0.10.1.1  
> 数据：1P+  
> 规模：20+节点  
>
 

# __consumer_offsets topic占用空间过大

## 现象：
   一日kafka集群单节点磁盘报警，上服务器排查发现三个实例的三块盘均已使用85%，其余磁盘均使用40%左右，再次排查发现这三块盘中均是__consumer_offsets_43 这个partition占用将尽3T。
   
>  线上集群未做raid，单实例上有12块盘，__consumer_offsets 共有3副本，因此3台机器均受到影响

## __consumer_offsets 有什么用？
0.9.0版本之后，kafka默认将consumer的offset信息记录在该系统topic里面。

    格式：[consumer group, topic name, partition] :: [offsetmetadata [offset value,  metadata],    committime value,         expiratintime value]   
    示例：[test_consumer,  test_topic, 90]        :: [OffsetMetadata [68248895952,   NO_METADATA], CommitTime 1590481622713, ExpirationTime 1590568022713]   
         [消费者组,        topic名称,   分区]      :: [ offset元数据    [offset ,      nometadata ],  提交时间,                  过期时间 ]   

    通过该命令可以查看最近100条__consumer_offsets数据
    
    ./kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server x.x.x.x:9092 --formatter "kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter"   --max-message 100 
  
## 定位突增的数据？

通过消费具体的partition来定位具体是哪个消费者组在提交offset。

### 0.9.0.0之后版本(含)
    /kafka-simple-consumer-shell.sh  --partition 20 --formatter "kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter" --broker-list x.x.x.x:9092 --topic __consumer_offsets --max-message 3000


## 为什么会出现该现象？
   * 线上kafka集群默认配置了数据清楚策略，因此其余topic均按照保留时间进行删除，但 __consumer_offsets 是用来保存 kafka 其余topic消费offset信息的topic，如果log.cleaner.enable设置为false，是不进行数据清除的。  
   * 研发提交consumer offset方式不对
   
# 总结
__connsumer_offsets部分分区数据量异常的问题是由于以下两方面原因共同造成:

* __connsumer_offsets默认清理策略设置不当，导致过期历史数据无法正常清理。
* 部分应用消费方式不当，导致产生大量commit信息。
